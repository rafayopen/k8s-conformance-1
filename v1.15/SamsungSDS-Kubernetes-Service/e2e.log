I0709 02:39:38.346327      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-630487638
I0709 02:39:38.346424      15 e2e.go:241] Starting e2e run "7a179ce6-6390-48b2-9e41-0b860f64d153" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1562639977 - Will randomize all specs
Will run 215 of 4411 specs

Jul  9 02:39:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:39:38.480: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  9 02:39:38.490: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul  9 02:39:38.508: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  9 02:39:38.508: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jul  9 02:39:38.508: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  9 02:39:38.512: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul  9 02:39:38.512: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jul  9 02:39:38.512: INFO: e2e test version: v1.15.0
Jul  9 02:39:38.513: INFO: kube-apiserver version: v1.15.0
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:39:38.513: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
Jul  9 02:39:38.550: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-8ee82e82-fe8f-429b-9f52-3a9e5e47f7f2
STEP: Creating a pod to test consume configMaps
Jul  9 02:39:38.570: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc" in namespace "projected-5516" to be "success or failure"
Jul  9 02:39:38.604: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc": Phase="Pending", Reason="", readiness=false. Elapsed: 34.7289ms
Jul  9 02:39:40.608: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0385365s
Jul  9 02:39:42.611: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0415851s
Jul  9 02:39:44.617: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0474246s
Jul  9 02:39:46.622: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0521614s
STEP: Saw pod success
Jul  9 02:39:46.622: INFO: Pod "pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc" satisfied condition "success or failure"
Jul  9 02:39:46.625: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 02:39:46.677: INFO: Waiting for pod pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc to disappear
Jul  9 02:39:46.680: INFO: Pod pod-projected-configmaps-3a849753-485a-4741-bd28-6d4bb2c860bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:39:46.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5516" for this suite.
Jul  9 02:39:52.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:39:52.876: INFO: namespace projected-5516 deletion completed in 6.1929915s

• [SLOW TEST:14.363 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:39:52.877: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d in namespace container-probe-5723
Jul  9 02:40:02.924: INFO: Started pod liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d in namespace container-probe-5723
STEP: checking the pod's current state and verifying that restartCount is present
Jul  9 02:40:02.926: INFO: Initial restart count of pod liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is 0
Jul  9 02:40:14.955: INFO: Restart count of pod container-probe-5723/liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is now 1 (12.0286781s elapsed)
Jul  9 02:40:35.013: INFO: Restart count of pod container-probe-5723/liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is now 2 (32.0869258s elapsed)
Jul  9 02:40:53.069: INFO: Restart count of pod container-probe-5723/liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is now 3 (50.1430389s elapsed)
Jul  9 02:41:13.135: INFO: Restart count of pod container-probe-5723/liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is now 4 (1m10.2087866s elapsed)
Jul  9 02:42:15.343: INFO: Restart count of pod container-probe-5723/liveness-0638c0f6-1970-43c0-b8b2-593a75a9085d is now 5 (2m12.4167962s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:42:15.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5723" for this suite.
Jul  9 02:42:21.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:42:21.558: INFO: namespace container-probe-5723 deletion completed in 6.1935905s

• [SLOW TEST:148.681 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:42:21.558: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul  9 02:42:21.593: INFO: Waiting up to 5m0s for pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac" in namespace "var-expansion-2715" to be "success or failure"
Jul  9 02:42:21.598: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4218ms
Jul  9 02:42:23.600: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0066699s
Jul  9 02:42:25.613: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0202467s
Jul  9 02:42:27.617: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0235101s
Jul  9 02:42:29.628: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0348039s
STEP: Saw pod success
Jul  9 02:42:29.628: INFO: Pod "var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac" satisfied condition "success or failure"
Jul  9 02:42:29.630: INFO: Trying to get logs from node node2 pod var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac container dapi-container: <nil>
STEP: delete the pod
Jul  9 02:42:29.692: INFO: Waiting for pod var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac to disappear
Jul  9 02:42:29.695: INFO: Pod var-expansion-61177be9-6822-4321-a987-3fbbbf5827ac no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:42:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2715" for this suite.
Jul  9 02:42:35.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:42:35.845: INFO: namespace var-expansion-2715 deletion completed in 6.1469341s

• [SLOW TEST:14.287 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:42:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul  9 02:42:35.874: INFO: Waiting up to 5m0s for pod "var-expansion-89d62088-c63e-41aa-9359-214f03acfe34" in namespace "var-expansion-3726" to be "success or failure"
Jul  9 02:42:35.876: INFO: Pod "var-expansion-89d62088-c63e-41aa-9359-214f03acfe34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7474ms
Jul  9 02:42:37.888: INFO: Pod "var-expansion-89d62088-c63e-41aa-9359-214f03acfe34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0143521s
STEP: Saw pod success
Jul  9 02:42:37.889: INFO: Pod "var-expansion-89d62088-c63e-41aa-9359-214f03acfe34" satisfied condition "success or failure"
Jul  9 02:42:37.890: INFO: Trying to get logs from node node2 pod var-expansion-89d62088-c63e-41aa-9359-214f03acfe34 container dapi-container: <nil>
STEP: delete the pod
Jul  9 02:42:37.906: INFO: Waiting for pod var-expansion-89d62088-c63e-41aa-9359-214f03acfe34 to disappear
Jul  9 02:42:37.908: INFO: Pod var-expansion-89d62088-c63e-41aa-9359-214f03acfe34 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:42:37.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3726" for this suite.
Jul  9 02:42:43.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:42:44.072: INFO: namespace var-expansion-3726 deletion completed in 6.1620167s

• [SLOW TEST:8.227 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:42:44.073: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 02:42:44.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-96'
Jul  9 02:42:44.534: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  9 02:42:44.534: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul  9 02:42:44.543: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul  9 02:42:44.549: INFO: scanned /root for discovery docs: <nil>
Jul  9 02:42:44.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-96'
Jul  9 02:43:00.377: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  9 02:43:00.377: INFO: stdout: "Created e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325\nScaling up e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul  9 02:43:00.377: INFO: stdout: "Created e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325\nScaling up e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul  9 02:43:00.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-96'
Jul  9 02:43:00.443: INFO: stderr: ""
Jul  9 02:43:00.443: INFO: stdout: "e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325-x9gvs "
Jul  9 02:43:00.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325-x9gvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-96'
Jul  9 02:43:00.522: INFO: stderr: ""
Jul  9 02:43:00.522: INFO: stdout: "true"
Jul  9 02:43:00.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325-x9gvs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-96'
Jul  9 02:43:00.584: INFO: stderr: ""
Jul  9 02:43:00.584: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul  9 02:43:00.584: INFO: e2e-test-nginx-rc-93e7f613c385820038797cb88cb12325-x9gvs is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul  9 02:43:00.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete rc e2e-test-nginx-rc --namespace=kubectl-96'
Jul  9 02:43:00.655: INFO: stderr: ""
Jul  9 02:43:00.655: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:43:00.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-96" for this suite.
Jul  9 02:43:22.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:43:22.826: INFO: namespace kubectl-96 deletion completed in 22.1645545s

• [SLOW TEST:38.754 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:43:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  9 02:43:22.886: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7388,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  9 02:43:22.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7389,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  9 02:43:22.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7390,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  9 02:43:32.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7407,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  9 02:43:32.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7408,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul  9 02:43:32.917: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7336,SelfLink:/api/v1/namespaces/watch-7336/configmaps/e2e-watch-test-label-changed,UID:4b5506de-308e-4931-8abb-dddfdb9fe734,ResourceVersion:7409,Generation:0,CreationTimestamp:2019-07-09 02:43:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:43:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7336" for this suite.
Jul  9 02:43:38.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:43:39.101: INFO: namespace watch-7336 deletion completed in 6.1801252s

• [SLOW TEST:16.274 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:43:39.101: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 02:43:39.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6728'
Jul  9 02:43:39.225: INFO: stderr: ""
Jul  9 02:43:39.225: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul  9 02:43:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete pods e2e-test-nginx-pod --namespace=kubectl-6728'
Jul  9 02:43:42.633: INFO: stderr: ""
Jul  9 02:43:42.633: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:43:42.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6728" for this suite.
Jul  9 02:43:48.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:43:48.817: INFO: namespace kubectl-6728 deletion completed in 6.1744488s

• [SLOW TEST:9.716 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:43:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  9 02:43:48.849: INFO: Waiting up to 5m0s for pod "pod-08ea68bf-761a-4265-b179-961e9f0a5bb7" in namespace "emptydir-6294" to be "success or failure"
Jul  9 02:43:48.853: INFO: Pod "pod-08ea68bf-761a-4265-b179-961e9f0a5bb7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3228ms
Jul  9 02:43:50.860: INFO: Pod "pod-08ea68bf-761a-4265-b179-961e9f0a5bb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0107053s
STEP: Saw pod success
Jul  9 02:43:50.860: INFO: Pod "pod-08ea68bf-761a-4265-b179-961e9f0a5bb7" satisfied condition "success or failure"
Jul  9 02:43:50.862: INFO: Trying to get logs from node node2 pod pod-08ea68bf-761a-4265-b179-961e9f0a5bb7 container test-container: <nil>
STEP: delete the pod
Jul  9 02:43:50.882: INFO: Waiting for pod pod-08ea68bf-761a-4265-b179-961e9f0a5bb7 to disappear
Jul  9 02:43:50.885: INFO: Pod pod-08ea68bf-761a-4265-b179-961e9f0a5bb7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:43:50.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6294" for this suite.
Jul  9 02:43:56.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:43:57.091: INFO: namespace emptydir-6294 deletion completed in 6.2008021s

• [SLOW TEST:8.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:43:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:43:57.133: INFO: Creating deployment "test-recreate-deployment"
Jul  9 02:43:57.140: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  9 02:43:57.151: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul  9 02:43:59.160: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  9 02:43:59.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 02:44:01.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 02:44:03.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237036, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 02:44:05.167: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  9 02:44:05.179: INFO: Updating deployment test-recreate-deployment
Jul  9 02:44:05.179: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  9 02:44:05.244: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3741,SelfLink:/apis/apps/v1/namespaces/deployment-3741/deployments/test-recreate-deployment,UID:1f7d8e30-0726-4e10-8dc6-74f2557fd00b,ResourceVersion:7564,Generation:2,CreationTimestamp:2019-07-09 02:43:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-09 02:44:04 +0000 UTC 2019-07-09 02:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-09 02:44:04 +0000 UTC 2019-07-09 02:43:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul  9 02:44:05.247: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-3741,SelfLink:/apis/apps/v1/namespaces/deployment-3741/replicasets/test-recreate-deployment-5c8c9cc69d,UID:b99a72dc-7344-4a8c-b7a2-2cee66889e81,ResourceVersion:7562,Generation:1,CreationTimestamp:2019-07-09 02:44:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1f7d8e30-0726-4e10-8dc6-74f2557fd00b 0xc000dc7cc7 0xc000dc7cc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 02:44:05.247: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  9 02:44:05.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-3741,SelfLink:/apis/apps/v1/namespaces/deployment-3741/replicasets/test-recreate-deployment-6df85df6b9,UID:4849455f-2164-4cb5-90fa-d349a2c7824c,ResourceVersion:7553,Generation:2,CreationTimestamp:2019-07-09 02:43:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1f7d8e30-0726-4e10-8dc6-74f2557fd00b 0xc000dc7de7 0xc000dc7de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 02:44:05.249: INFO: Pod "test-recreate-deployment-5c8c9cc69d-7pbp9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-7pbp9,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-3741,SelfLink:/api/v1/namespaces/deployment-3741/pods/test-recreate-deployment-5c8c9cc69d-7pbp9,UID:281e0a37-52ce-49b1-983d-7ecc8b436906,ResourceVersion:7558,Generation:0,CreationTimestamp:2019-07-09 02:44:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d b99a72dc-7344-4a8c-b7a2-2cee66889e81 0xc0018ebd27 0xc0018ebd28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zvtkx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zvtkx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zvtkx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018ebde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018ebe50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 02:44:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:44:05.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3741" for this suite.
Jul  9 02:44:11.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:44:11.350: INFO: namespace deployment-3741 deletion completed in 6.098403s

• [SLOW TEST:14.258 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:44:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-djnf
STEP: Creating a pod to test atomic-volume-subpath
Jul  9 02:44:11.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-djnf" in namespace "subpath-8288" to be "success or failure"
Jul  9 02:44:11.436: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.9495ms
Jul  9 02:44:13.441: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 2.0097696s
Jul  9 02:44:15.448: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 4.0167161s
Jul  9 02:44:17.454: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 6.0228106s
Jul  9 02:44:19.468: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 8.0364849s
Jul  9 02:44:21.476: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 10.0448536s
Jul  9 02:44:23.483: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 12.0512375s
Jul  9 02:44:25.485: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 14.0533142s
Jul  9 02:44:27.491: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 16.0600602s
Jul  9 02:44:29.502: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 18.0705284s
Jul  9 02:44:31.509: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Running", Reason="", readiness=true. Elapsed: 20.0772392s
Jul  9 02:44:33.516: INFO: Pod "pod-subpath-test-configmap-djnf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0850131s
STEP: Saw pod success
Jul  9 02:44:33.516: INFO: Pod "pod-subpath-test-configmap-djnf" satisfied condition "success or failure"
Jul  9 02:44:33.519: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-djnf container test-container-subpath-configmap-djnf: <nil>
STEP: delete the pod
Jul  9 02:44:33.543: INFO: Waiting for pod pod-subpath-test-configmap-djnf to disappear
Jul  9 02:44:33.545: INFO: Pod pod-subpath-test-configmap-djnf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-djnf
Jul  9 02:44:33.545: INFO: Deleting pod "pod-subpath-test-configmap-djnf" in namespace "subpath-8288"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:44:33.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8288" for this suite.
Jul  9 02:44:39.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:44:39.665: INFO: namespace subpath-8288 deletion completed in 6.1079398s

• [SLOW TEST:28.315 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:44:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  9 02:44:39.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7198,SelfLink:/api/v1/namespaces/watch-7198/configmaps/e2e-watch-test-resource-version,UID:ffd5a707-3813-47b9-bc23-4839caea8333,ResourceVersion:7688,Generation:0,CreationTimestamp:2019-07-09 02:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  9 02:44:39.714: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7198,SelfLink:/api/v1/namespaces/watch-7198/configmaps/e2e-watch-test-resource-version,UID:ffd5a707-3813-47b9-bc23-4839caea8333,ResourceVersion:7689,Generation:0,CreationTimestamp:2019-07-09 02:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:44:39.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7198" for this suite.
Jul  9 02:44:45.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:44:45.849: INFO: namespace watch-7198 deletion completed in 6.1322106s

• [SLOW TEST:6.184 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:44:45.849: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  9 02:44:45.908: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:45.911: INFO: Number of nodes with available pods: 0
Jul  9 02:44:45.911: INFO: Node node1 is running more than one daemon pod
Jul  9 02:44:46.923: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:46.925: INFO: Number of nodes with available pods: 0
Jul  9 02:44:46.925: INFO: Node node1 is running more than one daemon pod
Jul  9 02:44:47.926: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:47.932: INFO: Number of nodes with available pods: 2
Jul  9 02:44:47.932: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  9 02:44:47.961: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:47.963: INFO: Number of nodes with available pods: 1
Jul  9 02:44:47.963: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:48.971: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:48.976: INFO: Number of nodes with available pods: 1
Jul  9 02:44:48.976: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:49.975: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:49.981: INFO: Number of nodes with available pods: 1
Jul  9 02:44:49.981: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:50.968: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:50.970: INFO: Number of nodes with available pods: 1
Jul  9 02:44:50.970: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:51.970: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:51.972: INFO: Number of nodes with available pods: 1
Jul  9 02:44:51.972: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:52.974: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:52.979: INFO: Number of nodes with available pods: 1
Jul  9 02:44:52.979: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:53.966: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:53.968: INFO: Number of nodes with available pods: 1
Jul  9 02:44:53.968: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:54.966: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:54.968: INFO: Number of nodes with available pods: 1
Jul  9 02:44:54.968: INFO: Node node2 is running more than one daemon pod
Jul  9 02:44:55.973: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 02:44:55.978: INFO: Number of nodes with available pods: 2
Jul  9 02:44:55.978: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1564, will wait for the garbage collector to delete the pods
Jul  9 02:44:56.042: INFO: Deleting DaemonSet.extensions daemon-set took: 5.9338ms
Jul  9 02:44:56.342: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.1208ms
Jul  9 02:45:04.148: INFO: Number of nodes with available pods: 0
Jul  9 02:45:04.148: INFO: Number of running nodes: 0, number of available pods: 0
Jul  9 02:45:04.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1564/daemonsets","resourceVersion":"7789"},"items":null}

Jul  9 02:45:04.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1564/pods","resourceVersion":"7789"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:45:04.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1564" for this suite.
Jul  9 02:45:10.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:45:10.339: INFO: namespace daemonsets-1564 deletion completed in 6.1586542s

• [SLOW TEST:24.490 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:45:10.339: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-e976b5ab-6e74-4978-a848-fc5f9da57a8f
STEP: Creating secret with name s-test-opt-upd-079ca8fe-497a-4353-b1bd-bd004575bcbf
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e976b5ab-6e74-4978-a848-fc5f9da57a8f
STEP: Updating secret s-test-opt-upd-079ca8fe-497a-4353-b1bd-bd004575bcbf
STEP: Creating secret with name s-test-opt-create-589a577a-f80d-4e4e-82b6-2837810df224
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:46:41.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3506" for this suite.
Jul  9 02:47:03.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:03.247: INFO: namespace projected-3506 deletion completed in 22.1923558s

• [SLOW TEST:112.908 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul  9 02:47:03.820: INFO: created pod pod-service-account-defaultsa
Jul  9 02:47:03.820: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  9 02:47:03.826: INFO: created pod pod-service-account-mountsa
Jul  9 02:47:03.826: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  9 02:47:03.843: INFO: created pod pod-service-account-nomountsa
Jul  9 02:47:03.843: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  9 02:47:03.847: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  9 02:47:03.848: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  9 02:47:03.856: INFO: created pod pod-service-account-mountsa-mountspec
Jul  9 02:47:03.856: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  9 02:47:03.864: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  9 02:47:03.864: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  9 02:47:03.871: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  9 02:47:03.871: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  9 02:47:03.882: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  9 02:47:03.882: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  9 02:47:03.886: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  9 02:47:03.886: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:03.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7471" for this suite.
Jul  9 02:47:09.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:10.111: INFO: namespace svcaccounts-7471 deletion completed in 6.2085185s

• [SLOW TEST:6.864 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:10.111: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  9 02:47:12.690: INFO: Successfully updated pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353"
Jul  9 02:47:12.690: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353" in namespace "pods-9927" to be "terminated due to deadline exceeded"
Jul  9 02:47:12.693: INFO: Pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353": Phase="Running", Reason="", readiness=true. Elapsed: 2.9426ms
Jul  9 02:47:14.702: INFO: Pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353": Phase="Running", Reason="", readiness=true. Elapsed: 2.011728s
Jul  9 02:47:16.708: INFO: Pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.0178175s
Jul  9 02:47:16.708: INFO: Pod "pod-update-activedeadlineseconds-62aec5f9-9810-4bb2-934b-f2bbac039353" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:16.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9927" for this suite.
Jul  9 02:47:22.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:22.963: INFO: namespace pods-9927 deletion completed in 6.244408s

• [SLOW TEST:12.852 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:22.963: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  9 02:47:23.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8830,SelfLink:/api/v1/namespaces/watch-8830/configmaps/e2e-watch-test-watch-closed,UID:20458924-6bb4-4a54-8cb1-4a34b68e95b6,ResourceVersion:8194,Generation:0,CreationTimestamp:2019-07-09 02:47:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  9 02:47:23.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8830,SelfLink:/api/v1/namespaces/watch-8830/configmaps/e2e-watch-test-watch-closed,UID:20458924-6bb4-4a54-8cb1-4a34b68e95b6,ResourceVersion:8195,Generation:0,CreationTimestamp:2019-07-09 02:47:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  9 02:47:23.024: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8830,SelfLink:/api/v1/namespaces/watch-8830/configmaps/e2e-watch-test-watch-closed,UID:20458924-6bb4-4a54-8cb1-4a34b68e95b6,ResourceVersion:8196,Generation:0,CreationTimestamp:2019-07-09 02:47:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  9 02:47:23.024: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8830,SelfLink:/api/v1/namespaces/watch-8830/configmaps/e2e-watch-test-watch-closed,UID:20458924-6bb4-4a54-8cb1-4a34b68e95b6,ResourceVersion:8197,Generation:0,CreationTimestamp:2019-07-09 02:47:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:23.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8830" for this suite.
Jul  9 02:47:29.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:29.228: INFO: namespace watch-8830 deletion completed in 6.2018901s

• [SLOW TEST:6.265 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:29.228: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul  9 02:47:29.308: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8267" to be "success or failure"
Jul  9 02:47:29.312: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1396ms
Jul  9 02:47:31.320: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0121969s
Jul  9 02:47:33.327: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0184384s
STEP: Saw pod success
Jul  9 02:47:33.327: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul  9 02:47:33.333: INFO: Trying to get logs from node node2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul  9 02:47:33.375: INFO: Waiting for pod pod-host-path-test to disappear
Jul  9 02:47:33.378: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:33.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8267" for this suite.
Jul  9 02:47:39.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:39.556: INFO: namespace hostpath-8267 deletion completed in 6.1748334s

• [SLOW TEST:10.328 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:39.556: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  9 02:47:39.611: INFO: Waiting up to 5m0s for pod "downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f" in namespace "downward-api-1711" to be "success or failure"
Jul  9 02:47:39.615: INFO: Pod "downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042ms
Jul  9 02:47:41.619: INFO: Pod "downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0083172s
Jul  9 02:47:43.628: INFO: Pod "downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0164356s
STEP: Saw pod success
Jul  9 02:47:43.628: INFO: Pod "downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f" satisfied condition "success or failure"
Jul  9 02:47:43.630: INFO: Trying to get logs from node node2 pod downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f container dapi-container: <nil>
STEP: delete the pod
Jul  9 02:47:43.655: INFO: Waiting for pod downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f to disappear
Jul  9 02:47:43.658: INFO: Pod downward-api-ee02b2fe-b4fe-42b9-adbf-9bf98945f66f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:43.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1711" for this suite.
Jul  9 02:47:49.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:47:49.882: INFO: namespace downward-api-1711 deletion completed in 6.2191448s

• [SLOW TEST:10.327 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:47:49.882: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:47:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:47:52.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8859" for this suite.
Jul  9 02:48:30.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:48:30.267: INFO: namespace pods-8859 deletion completed in 38.1635931s

• [SLOW TEST:40.384 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:48:30.267: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul  9 02:48:30.304: INFO: Waiting up to 5m0s for pod "var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b" in namespace "var-expansion-5363" to be "success or failure"
Jul  9 02:48:30.310: INFO: Pod "var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5476ms
Jul  9 02:48:32.316: INFO: Pod "var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0114908s
STEP: Saw pod success
Jul  9 02:48:32.316: INFO: Pod "var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b" satisfied condition "success or failure"
Jul  9 02:48:32.318: INFO: Trying to get logs from node node2 pod var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b container dapi-container: <nil>
STEP: delete the pod
Jul  9 02:48:32.337: INFO: Waiting for pod var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b to disappear
Jul  9 02:48:32.339: INFO: Pod var-expansion-0f78f4d6-0348-400f-9ed0-c4919bc72f7b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:48:32.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5363" for this suite.
Jul  9 02:48:38.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:48:38.458: INFO: namespace var-expansion-5363 deletion completed in 6.1158653s

• [SLOW TEST:8.191 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:48:38.458: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  9 02:48:38.505: INFO: PodSpec: initContainers in spec.initContainers
Jul  9 02:49:21.648: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0a6fe496-5740-44b7-961d-56ba9453f66a", GenerateName:"", Namespace:"init-container-4549", SelfLink:"/api/v1/namespaces/init-container-4549/pods/pod-init-0a6fe496-5740-44b7-961d-56ba9453f66a", UID:"4f659a24-c9c1-4842-9745-677aaf93dcfd", ResourceVersion:"8517", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63698237318, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"505945600"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-cmgfs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000f64c00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cmgfs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cmgfs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cmgfs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001017ca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0021b9b00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001017d30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001017d50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001017d58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001017d5c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237318, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237318, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237318, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698237318, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.28.128.13", PodIP:"172.30.64.3", StartTime:(*v1.Time)(0xc001f08540), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00146ce70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00146cee0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ab5af4605bf46daa6cc7e22d3d26d86de6ea2711d4cf6b133cfd1b16f6c17c17"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f085a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f08580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:49:21.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4549" for this suite.
Jul  9 02:49:41.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:49:41.869: INFO: namespace init-container-4549 deletion completed in 20.214575s

• [SLOW TEST:63.411 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:49:41.869: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9269eae9-7e59-475d-be21-a5dc8a219df1
STEP: Creating a pod to test consume configMaps
Jul  9 02:49:41.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e" in namespace "projected-4078" to be "success or failure"
Jul  9 02:49:41.970: INFO: Pod "pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.588ms
Jul  9 02:49:43.977: INFO: Pod "pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0133589s
STEP: Saw pod success
Jul  9 02:49:43.977: INFO: Pod "pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e" satisfied condition "success or failure"
Jul  9 02:49:43.981: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 02:49:43.996: INFO: Waiting for pod pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e to disappear
Jul  9 02:49:44.001: INFO: Pod pod-projected-configmaps-2a5faa3a-0bbc-4979-b692-0e9a4d4c939e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:49:44.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4078" for this suite.
Jul  9 02:49:50.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:49:50.193: INFO: namespace projected-4078 deletion completed in 6.1884086s

• [SLOW TEST:8.324 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:49:50.193: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul  9 02:49:50.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-5656'
Jul  9 02:49:50.389: INFO: stderr: ""
Jul  9 02:49:50.389: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 02:49:50.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5656'
Jul  9 02:49:50.467: INFO: stderr: ""
Jul  9 02:49:50.468: INFO: stdout: "update-demo-nautilus-gh9d6 update-demo-nautilus-pnrlq "
Jul  9 02:49:50.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-gh9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:49:50.533: INFO: stderr: ""
Jul  9 02:49:50.533: INFO: stdout: ""
Jul  9 02:49:50.533: INFO: update-demo-nautilus-gh9d6 is created but not running
Jul  9 02:49:55.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5656'
Jul  9 02:49:55.596: INFO: stderr: ""
Jul  9 02:49:55.596: INFO: stdout: "update-demo-nautilus-gh9d6 update-demo-nautilus-pnrlq "
Jul  9 02:49:55.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-gh9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:49:55.658: INFO: stderr: ""
Jul  9 02:49:55.658: INFO: stdout: ""
Jul  9 02:49:55.658: INFO: update-demo-nautilus-gh9d6 is created but not running
Jul  9 02:50:00.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5656'
Jul  9 02:50:00.718: INFO: stderr: ""
Jul  9 02:50:00.718: INFO: stdout: "update-demo-nautilus-gh9d6 update-demo-nautilus-pnrlq "
Jul  9 02:50:00.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-gh9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:50:00.777: INFO: stderr: ""
Jul  9 02:50:00.777: INFO: stdout: ""
Jul  9 02:50:00.777: INFO: update-demo-nautilus-gh9d6 is created but not running
Jul  9 02:50:05.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5656'
Jul  9 02:50:05.837: INFO: stderr: ""
Jul  9 02:50:05.837: INFO: stdout: "update-demo-nautilus-gh9d6 update-demo-nautilus-pnrlq "
Jul  9 02:50:05.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-gh9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:50:05.902: INFO: stderr: ""
Jul  9 02:50:05.902: INFO: stdout: "true"
Jul  9 02:50:05.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-gh9d6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:50:05.964: INFO: stderr: ""
Jul  9 02:50:05.964: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 02:50:05.964: INFO: validating pod update-demo-nautilus-gh9d6
Jul  9 02:50:05.980: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 02:50:05.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 02:50:05.980: INFO: update-demo-nautilus-gh9d6 is verified up and running
Jul  9 02:50:05.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-pnrlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:50:06.055: INFO: stderr: ""
Jul  9 02:50:06.055: INFO: stdout: "true"
Jul  9 02:50:06.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-pnrlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5656'
Jul  9 02:50:06.111: INFO: stderr: ""
Jul  9 02:50:06.111: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 02:50:06.111: INFO: validating pod update-demo-nautilus-pnrlq
Jul  9 02:50:06.116: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 02:50:06.116: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 02:50:06.116: INFO: update-demo-nautilus-pnrlq is verified up and running
STEP: using delete to clean up resources
Jul  9 02:50:06.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-5656'
Jul  9 02:50:06.174: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 02:50:06.174: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  9 02:50:06.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5656'
Jul  9 02:50:06.237: INFO: stderr: "No resources found.\n"
Jul  9 02:50:06.238: INFO: stdout: ""
Jul  9 02:50:06.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=update-demo --namespace=kubectl-5656 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 02:50:06.300: INFO: stderr: ""
Jul  9 02:50:06.300: INFO: stdout: "update-demo-nautilus-gh9d6\nupdate-demo-nautilus-pnrlq\n"
Jul  9 02:50:06.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5656'
Jul  9 02:50:06.876: INFO: stderr: "No resources found.\n"
Jul  9 02:50:06.876: INFO: stdout: ""
Jul  9 02:50:06.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=update-demo --namespace=kubectl-5656 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 02:50:06.931: INFO: stderr: ""
Jul  9 02:50:06.931: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:50:06.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5656" for this suite.
Jul  9 02:50:12.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:50:13.110: INFO: namespace kubectl-5656 deletion completed in 6.1745296s

• [SLOW TEST:22.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:50:13.110: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 02:50:13.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db" in namespace "projected-2831" to be "success or failure"
Jul  9 02:50:13.164: INFO: Pod "downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0424ms
Jul  9 02:50:15.170: INFO: Pod "downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010143s
Jul  9 02:50:17.178: INFO: Pod "downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0179059s
STEP: Saw pod success
Jul  9 02:50:17.178: INFO: Pod "downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db" satisfied condition "success or failure"
Jul  9 02:50:17.185: INFO: Trying to get logs from node node2 pod downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db container client-container: <nil>
STEP: delete the pod
Jul  9 02:50:17.221: INFO: Waiting for pod downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db to disappear
Jul  9 02:50:17.225: INFO: Pod downwardapi-volume-98f46dc3-d74d-4b98-9f75-76f3137ff7db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:50:17.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2831" for this suite.
Jul  9 02:50:23.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:50:23.444: INFO: namespace projected-2831 deletion completed in 6.2154354s

• [SLOW TEST:10.334 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:50:23.444: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:50:49.506: INFO: Container started at 2019-07-09 02:50:28 +0000 UTC, pod became ready at 2019-07-09 02:50:48 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:50:49.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1638" for this suite.
Jul  9 02:51:11.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:51:11.614: INFO: namespace container-probe-1638 deletion completed in 22.1054994s

• [SLOW TEST:48.170 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:51:11.614: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-620a9734-aa75-4661-b55c-7e8f1d72e8f7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-620a9734-aa75-4661-b55c-7e8f1d72e8f7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:51:15.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3106" for this suite.
Jul  9 02:51:37.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:51:37.904: INFO: namespace projected-3106 deletion completed in 22.1805621s

• [SLOW TEST:26.290 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:51:37.904: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  9 02:51:37.944: INFO: Waiting up to 5m0s for pod "pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b" in namespace "emptydir-7986" to be "success or failure"
Jul  9 02:51:37.947: INFO: Pod "pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.7686ms
Jul  9 02:51:39.950: INFO: Pod "pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0065699s
Jul  9 02:51:41.959: INFO: Pod "pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0150298s
STEP: Saw pod success
Jul  9 02:51:41.959: INFO: Pod "pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b" satisfied condition "success or failure"
Jul  9 02:51:41.961: INFO: Trying to get logs from node node2 pod pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b container test-container: <nil>
STEP: delete the pod
Jul  9 02:51:41.977: INFO: Waiting for pod pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b to disappear
Jul  9 02:51:41.979: INFO: Pod pod-04b6382e-5a41-40e7-8ce6-f604f43b6b2b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:51:41.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7986" for this suite.
Jul  9 02:51:47.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:51:48.164: INFO: namespace emptydir-7986 deletion completed in 6.1818511s

• [SLOW TEST:10.260 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:51:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul  9 02:51:48.242: INFO: observed the pod list
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul  9 02:51:57.293: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:51:57.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4578" for this suite.
Jul  9 02:52:03.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:52:03.536: INFO: namespace pods-4578 deletion completed in 6.2332619s

• [SLOW TEST:15.371 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:52:03.536: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  9 02:52:15.602: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:15.602: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:15.743: INFO: Exec stderr: ""
Jul  9 02:52:15.743: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:15.743: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:15.886: INFO: Exec stderr: ""
Jul  9 02:52:15.886: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:15.886: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.031: INFO: Exec stderr: ""
Jul  9 02:52:16.031: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.031: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.164: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  9 02:52:16.164: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.164: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.280: INFO: Exec stderr: ""
Jul  9 02:52:16.280: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.280: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.399: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  9 02:52:16.399: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.550: INFO: Exec stderr: ""
Jul  9 02:52:16.550: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.550: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.688: INFO: Exec stderr: ""
Jul  9 02:52:16.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.688: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.813: INFO: Exec stderr: ""
Jul  9 02:52:16.813: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7098 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:52:16.813: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:52:16.946: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:52:16.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7098" for this suite.
Jul  9 02:53:06.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:53:07.129: INFO: namespace e2e-kubelet-etc-hosts-7098 deletion completed in 50.1803027s

• [SLOW TEST:63.594 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:53:07.130: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:53:07.187: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul  9 02:53:09.241: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:53:10.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7501" for this suite.
Jul  9 02:53:16.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:53:16.460: INFO: namespace replication-controller-7501 deletion completed in 6.1978224s

• [SLOW TEST:9.331 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:53:16.460: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 02:53:16.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5107'
Jul  9 02:53:16.836: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  9 02:53:16.836: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul  9 02:53:16.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete jobs e2e-test-nginx-job --namespace=kubectl-5107'
Jul  9 02:53:16.912: INFO: stderr: ""
Jul  9 02:53:16.912: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:53:16.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5107" for this suite.
Jul  9 02:53:22.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:53:23.064: INFO: namespace kubectl-5107 deletion completed in 6.1486874s

• [SLOW TEST:6.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:53:23.064: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4466
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  9 02:53:23.133: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  9 02:53:47.223: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.64.4:8080/dial?request=hostName&protocol=http&host=172.30.64.3&port=8080&tries=1'] Namespace:pod-network-test-4466 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:53:47.223: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:53:47.366: INFO: Waiting for endpoints: map[]
Jul  9 02:53:47.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.64.4:8080/dial?request=hostName&protocol=http&host=172.30.192.4&port=8080&tries=1'] Namespace:pod-network-test-4466 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 02:53:47.368: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 02:53:47.522: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:53:47.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4466" for this suite.
Jul  9 02:54:09.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:54:09.682: INFO: namespace pod-network-test-4466 deletion completed in 22.1579267s

• [SLOW TEST:46.618 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:54:09.682: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:54:09.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 version'
Jul  9 02:54:09.815: INFO: stderr: ""
Jul  9 02:54:09.815: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:54:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8224" for this suite.
Jul  9 02:54:15.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:54:15.976: INFO: namespace kubectl-8224 deletion completed in 6.1578608s

• [SLOW TEST:6.294 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:54:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  9 02:54:19.045: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:54:19.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1809" for this suite.
Jul  9 02:54:41.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:54:41.240: INFO: namespace replicaset-1809 deletion completed in 22.1537726s

• [SLOW TEST:25.264 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:54:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  9 02:54:47.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  9 02:54:47.335: INFO: Pod pod-with-prestop-http-hook still exists
Jul  9 02:54:49.335: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  9 02:54:49.340: INFO: Pod pod-with-prestop-http-hook still exists
Jul  9 02:54:51.335: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  9 02:54:51.339: INFO: Pod pod-with-prestop-http-hook still exists
Jul  9 02:54:53.339: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  9 02:54:53.341: INFO: Pod pod-with-prestop-http-hook still exists
Jul  9 02:54:55.335: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  9 02:54:55.339: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:54:55.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1703" for this suite.
Jul  9 02:55:17.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:55:17.503: INFO: namespace container-lifecycle-hook-1703 deletion completed in 22.1444807s

• [SLOW TEST:36.263 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:55:17.503: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9471
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9471
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9471
Jul  9 02:55:17.560: INFO: Found 0 stateful pods, waiting for 1
Jul  9 02:55:27.565: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  9 02:55:27.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 02:55:27.813: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 02:55:27.813: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 02:55:27.813: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 02:55:27.816: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  9 02:55:37.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 02:55:37.826: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 02:55:37.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
Jul  9 02:55:38.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9927246s
Jul  9 02:55:39.860: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9828717s
Jul  9 02:55:40.865: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9805003s
Jul  9 02:55:41.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9760999s
Jul  9 02:55:42.873: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9727804s
Jul  9 02:55:43.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9679359s
Jul  9 02:55:44.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.9651492s
Jul  9 02:55:45.882: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9623396s
Jul  9 02:55:46.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.0617ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9471
Jul  9 02:55:47.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 02:55:48.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 02:55:48.130: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 02:55:48.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 02:55:48.132: INFO: Found 1 stateful pods, waiting for 3
Jul  9 02:55:58.140: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 02:55:58.140: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 02:55:58.140: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  9 02:55:58.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 02:55:58.356: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 02:55:58.356: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 02:55:58.356: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 02:55:58.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 02:55:58.571: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 02:55:58.571: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 02:55:58.571: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 02:55:58.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 02:55:58.811: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 02:55:58.811: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 02:55:58.811: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 02:55:58.811: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 02:55:58.815: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul  9 02:56:08.823: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 02:56:08.823: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 02:56:08.823: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 02:56:08.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Jul  9 02:56:09.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9766228s
Jul  9 02:56:10.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9715339s
Jul  9 02:56:11.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9683328s
Jul  9 02:56:12.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9622818s
Jul  9 02:56:13.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95765s
Jul  9 02:56:14.905: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9517047s
Jul  9 02:56:15.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9469796s
Jul  9 02:56:16.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9429083s
Jul  9 02:56:17.916: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.0252ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9471
Jul  9 02:56:18.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 02:56:19.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 02:56:19.152: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 02:56:19.152: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 02:56:19.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 02:56:19.361: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 02:56:19.361: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 02:56:19.361: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 02:56:19.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9471 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 02:56:19.590: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 02:56:19.590: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 02:56:19.590: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 02:56:19.590: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  9 02:56:29.608: INFO: Deleting all statefulset in ns statefulset-9471
Jul  9 02:56:29.610: INFO: Scaling statefulset ss to 0
Jul  9 02:56:29.625: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 02:56:29.627: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:56:29.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9471" for this suite.
Jul  9 02:56:35.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:56:35.810: INFO: namespace statefulset-9471 deletion completed in 6.1561415s

• [SLOW TEST:78.307 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:56:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3668, will wait for the garbage collector to delete the pods
Jul  9 02:56:39.906: INFO: Deleting Job.batch foo took: 3.5126ms
Jul  9 02:56:40.006: INFO: Terminating Job.batch foo pods took: 100.2412ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:57:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3668" for this suite.
Jul  9 02:57:18.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:57:18.364: INFO: namespace job-3668 deletion completed in 6.1468201s

• [SLOW TEST:42.553 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:57:18.364: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-sqj9
STEP: Creating a pod to test atomic-volume-subpath
Jul  9 02:57:18.422: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sqj9" in namespace "subpath-8385" to be "success or failure"
Jul  9 02:57:18.428: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.2063ms
Jul  9 02:57:20.431: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.0091108s
Jul  9 02:57:22.435: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.013344s
Jul  9 02:57:24.444: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 6.0216147s
Jul  9 02:57:26.446: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 8.0236967s
Jul  9 02:57:28.451: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 10.029532s
Jul  9 02:57:30.456: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 12.034479s
Jul  9 02:57:32.461: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 14.0393354s
Jul  9 02:57:34.466: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 16.043585s
Jul  9 02:57:36.468: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.0456226s
Jul  9 02:57:38.470: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Running", Reason="", readiness=true. Elapsed: 20.0485706s
Jul  9 02:57:40.478: INFO: Pod "pod-subpath-test-downwardapi-sqj9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0556372s
STEP: Saw pod success
Jul  9 02:57:40.478: INFO: Pod "pod-subpath-test-downwardapi-sqj9" satisfied condition "success or failure"
Jul  9 02:57:40.480: INFO: Trying to get logs from node node2 pod pod-subpath-test-downwardapi-sqj9 container test-container-subpath-downwardapi-sqj9: <nil>
STEP: delete the pod
Jul  9 02:57:40.506: INFO: Waiting for pod pod-subpath-test-downwardapi-sqj9 to disappear
Jul  9 02:57:40.509: INFO: Pod pod-subpath-test-downwardapi-sqj9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-sqj9
Jul  9 02:57:40.509: INFO: Deleting pod "pod-subpath-test-downwardapi-sqj9" in namespace "subpath-8385"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:57:40.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8385" for this suite.
Jul  9 02:57:46.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:57:46.661: INFO: namespace subpath-8385 deletion completed in 6.1469658s

• [SLOW TEST:28.297 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:57:46.661: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:57:46.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-3526'
Jul  9 02:57:46.824: INFO: stderr: ""
Jul  9 02:57:46.824: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul  9 02:57:46.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-3526'
Jul  9 02:57:46.968: INFO: stderr: ""
Jul  9 02:57:46.968: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  9 02:57:47.971: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 02:57:47.971: INFO: Found 0 / 1
Jul  9 02:57:48.973: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 02:57:48.973: INFO: Found 1 / 1
Jul  9 02:57:48.973: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  9 02:57:48.975: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 02:57:48.975: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  9 02:57:48.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 describe pod redis-master-dl5ql --namespace=kubectl-3526'
Jul  9 02:57:49.053: INFO: stderr: ""
Jul  9 02:57:49.053: INFO: stdout: "Name:           redis-master-dl5ql\nNamespace:      kubectl-3526\nPriority:       0\nNode:           node2/172.28.128.13\nStart Time:     Tue, 09 Jul 2019 02:57:46 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.30.64.3\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://2f426a781d1650ab5b68038de1eec4b2143ef58ab304aaf94f5acf89fa1e72f9\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 Jul 2019 02:57:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nmv5w (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nmv5w:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nmv5w\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 60s\n                 node.kubernetes.io/unreachable:NoExecute for 60s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3526/redis-master-dl5ql to node2\n  Normal  Pulled     2s    kubelet, node2     Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, node2     Created container redis-master\n  Normal  Started    1s    kubelet, node2     Started container redis-master\n"
Jul  9 02:57:49.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 describe rc redis-master --namespace=kubectl-3526'
Jul  9 02:57:49.132: INFO: stderr: ""
Jul  9 02:57:49.132: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3526\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-dl5ql\n"
Jul  9 02:57:49.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 describe service redis-master --namespace=kubectl-3526'
Jul  9 02:57:49.206: INFO: stderr: ""
Jul  9 02:57:49.206: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3526\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.24.186.171\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.64.3:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  9 02:57:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 describe node master'
Jul  9 02:57:49.321: INFO: stderr: ""
Jul  9 02:57:49.321: INFO: stdout: "Name:               master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 Jul 2019 01:52:03 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 Jul 2019 01:52:26 +0000   Tue, 09 Jul 2019 01:52:26 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Tue, 09 Jul 2019 02:57:26 +0000   Tue, 09 Jul 2019 01:51:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 Jul 2019 02:57:26 +0000   Tue, 09 Jul 2019 01:51:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 Jul 2019 02:57:26 +0000   Tue, 09 Jul 2019 01:51:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 Jul 2019 02:57:26 +0000   Tue, 09 Jul 2019 02:08:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.28.128.11\n  Hostname:    master\nCapacity:\n cpu:                2\n ephemeral-storage:  41152736Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1797360Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37926361435\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1694960Ki\n pods:               110\nSystem Info:\n Machine ID:                 63fc159b073c42bd81b9ac3c209a8ecd\n System UUID:                FA2A646A-F50B-C548-86DB-8CADE6659268\n Boot ID:                    a8dba6cb-6dba-416e-93db-b2a1e982250a\n Kernel Version:             3.10.0-957.21.3.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.7\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     172.30.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-5gssg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                etcd-master                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                kube-apiserver-master                                      250m (12%)    0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                kube-controller-manager-master                             200m (10%)    0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                kube-proxy-5f9rd                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                kube-scheduler-master                                      100m (5%)     0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                weave-net-2n8mt                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         65m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                Message\n  ----    ------                   ----               ----                -------\n  Normal  Starting                 65m                kubelet, master     Starting kubelet.\n  Normal  NodeHasSufficientMemory  65m (x8 over 65m)  kubelet, master     Node master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    65m (x8 over 65m)  kubelet, master     Node master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     65m (x7 over 65m)  kubelet, master     Node master status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  65m                kubelet, master     Updated Node Allocatable limit across pods\n  Normal  Starting                 65m                kube-proxy, master  Starting kube-proxy.\n  Normal  KubeletConfigChanged     65m                kubelet, master     Kubelet restarting to use /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13, UID: 27d94e18-a1ec-11e9-a38d-00155daa6402, ResourceVersion: 178, KubeletConfigKey: kubelet\n  Normal  NodeHasSufficientMemory  64m                kubelet, master     Node master status is now: NodeHasSufficientMemory\n  Normal  Starting                 64m                kubelet, master     Starting kubelet.\n  Normal  NodeHasNoDiskPressure    64m                kubelet, master     Node master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     64m                kubelet, master     Node master status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             64m                kubelet, master     Node master status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  64m                kubelet, master     Updated Node Allocatable limit across pods\n  Normal  NodeReady                64m                kubelet, master     Node master status is now: NodeReady\n  Normal  Starting                 62m                kube-proxy, master  Starting kube-proxy.\n  Normal  Starting                 55m                kube-proxy, master  Starting kube-proxy.\n  Normal  NodeReady                51m                kubelet, master     Node master status is now: NodeReady\n  Normal  NodeHasSufficientMemory  51m                kubelet, master     Node master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m                kubelet, master     Node master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m                kubelet, master     Node master status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             51m                kubelet, master     Node master status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  51m                kubelet, master     Updated Node Allocatable limit across pods\n  Normal  Starting                 51m                kubelet, master     Starting kubelet.\n  Normal  Starting                 51m                kube-proxy, master  Starting kube-proxy.\n  Normal  Starting                 49m                kube-proxy, master  Starting kube-proxy.\n  Normal  Starting                 48m                kubelet, master     Starting kubelet.\n  Normal  NodeHasSufficientMemory  48m                kubelet, master     Node master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    48m                kubelet, master     Node master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     48m                kubelet, master     Node master status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             48m                kubelet, master     Node master status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  48m                kubelet, master     Updated Node Allocatable limit across pods\n  Normal  NodeReady                48m                kubelet, master     Node master status is now: NodeReady\n  Normal  Starting                 48m                kube-proxy, master  Starting kube-proxy.\n"
Jul  9 02:57:49.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 describe namespace kubectl-3526'
Jul  9 02:57:49.394: INFO: stderr: ""
Jul  9 02:57:49.394: INFO: stdout: "Name:         kubectl-3526\nLabels:       e2e-framework=kubectl\n              e2e-run=7a179ce6-6390-48b2-9e41-0b860f64d153\n              kubernetes.io/namespace=kubectl-3526\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:57:49.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3526" for this suite.
Jul  9 02:58:11.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:58:11.582: INFO: namespace kubectl-3526 deletion completed in 22.1789429s

• [SLOW TEST:24.921 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:58:11.582: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  9 02:58:11.631: INFO: Waiting up to 5m0s for pod "pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f" in namespace "emptydir-343" to be "success or failure"
Jul  9 02:58:11.639: INFO: Pod "pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6362ms
Jul  9 02:58:13.642: INFO: Pod "pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0107497s
STEP: Saw pod success
Jul  9 02:58:13.642: INFO: Pod "pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f" satisfied condition "success or failure"
Jul  9 02:58:13.644: INFO: Trying to get logs from node node2 pod pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f container test-container: <nil>
STEP: delete the pod
Jul  9 02:58:13.658: INFO: Waiting for pod pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f to disappear
Jul  9 02:58:13.660: INFO: Pod pod-48fd3366-d527-4fc4-b064-f8b3e7edc02f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:58:13.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-343" for this suite.
Jul  9 02:58:19.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:58:19.827: INFO: namespace emptydir-343 deletion completed in 6.1646129s

• [SLOW TEST:8.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:58:19.827: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 02:58:19.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88" in namespace "downward-api-4277" to be "success or failure"
Jul  9 02:58:19.878: INFO: Pod "downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0878ms
Jul  9 02:58:21.882: INFO: Pod "downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0064548s
STEP: Saw pod success
Jul  9 02:58:21.882: INFO: Pod "downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88" satisfied condition "success or failure"
Jul  9 02:58:21.885: INFO: Trying to get logs from node node2 pod downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88 container client-container: <nil>
STEP: delete the pod
Jul  9 02:58:21.904: INFO: Waiting for pod downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88 to disappear
Jul  9 02:58:21.907: INFO: Pod downwardapi-volume-f8131699-3558-4f4b-9a64-775266220b88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:58:21.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4277" for this suite.
Jul  9 02:58:27.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:58:28.077: INFO: namespace downward-api-4277 deletion completed in 6.1666143s

• [SLOW TEST:8.250 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:58:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-806fcdad-484c-4799-a25a-59e4f6548511
STEP: Creating a pod to test consume secrets
Jul  9 02:58:28.114: INFO: Waiting up to 5m0s for pod "pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d" in namespace "secrets-3713" to be "success or failure"
Jul  9 02:58:28.120: INFO: Pod "pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5008ms
Jul  9 02:58:30.126: INFO: Pod "pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0115442s
STEP: Saw pod success
Jul  9 02:58:30.126: INFO: Pod "pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d" satisfied condition "success or failure"
Jul  9 02:58:30.128: INFO: Trying to get logs from node node2 pod pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 02:58:30.153: INFO: Waiting for pod pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d to disappear
Jul  9 02:58:30.160: INFO: Pod pod-secrets-a9f788a9-114c-4e2f-bb4e-b63d66a9fc5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:58:30.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3713" for this suite.
Jul  9 02:58:36.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:58:36.349: INFO: namespace secrets-3713 deletion completed in 6.184093s

• [SLOW TEST:8.272 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:58:36.349: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b385308e-f555-4ee4-b7bd-ecc6f0ed49d9
STEP: Creating configMap with name cm-test-opt-upd-14ef2a76-2d17-4826-b717-6081c8b07181
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b385308e-f555-4ee4-b7bd-ecc6f0ed49d9
STEP: Updating configmap cm-test-opt-upd-14ef2a76-2d17-4826-b717-6081c8b07181
STEP: Creating configMap with name cm-test-opt-create-5eb80611-65de-406b-bf6f-9601469ffe4f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:58:40.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2545" for this suite.
Jul  9 02:59:02.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:59:02.672: INFO: namespace projected-2545 deletion completed in 22.1885087s

• [SLOW TEST:26.323 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:59:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul  9 02:59:08.761: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 103
	[quantile=0.9] = 83089
	[quantile=0.99] = 139708
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 203421
	[quantile=0.9] = 298090
	[quantile=0.99] = 303165
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 5
	[quantile=0.99] = 24
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 8
	[quantile=0.9] = 17
	[quantile=0.99] = 41
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 16
	[quantile=0.99] = 71
For namespace_queue_latency_sum:
	[] = 1270
For namespace_queue_latency_count:
	[] = 96
For namespace_retries:
	[] = 99
For namespace_work_duration:
	[quantile=0.5] = 111035
	[quantile=0.9] = 156918
	[quantile=0.99] = 225095
For namespace_work_duration_sum:
	[] = 12118979
For namespace_work_duration_count:
	[] = 96
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:59:08.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-653" for this suite.
Jul  9 02:59:14.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 02:59:14.928: INFO: namespace gc-653 deletion completed in 6.156751s

• [SLOW TEST:12.256 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 02:59:14.929: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 02:59:14.997: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 02:59:17.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1296" for this suite.
Jul  9 03:00:07.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:00:07.220: INFO: namespace pods-1296 deletion completed in 50.1747479s

• [SLOW TEST:52.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:00:07.220: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:00:12.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9469" for this suite.
Jul  9 03:00:18.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:00:19.019: INFO: namespace watch-9469 deletion completed in 6.1948463s

• [SLOW TEST:11.798 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:00:19.019: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:00:19.066: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  9 03:00:19.072: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:19.076: INFO: Number of nodes with available pods: 0
Jul  9 03:00:19.076: INFO: Node node1 is running more than one daemon pod
Jul  9 03:00:20.083: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:20.085: INFO: Number of nodes with available pods: 0
Jul  9 03:00:20.085: INFO: Node node1 is running more than one daemon pod
Jul  9 03:00:21.087: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:21.088: INFO: Number of nodes with available pods: 1
Jul  9 03:00:21.089: INFO: Node node1 is running more than one daemon pod
Jul  9 03:00:22.088: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:22.090: INFO: Number of nodes with available pods: 2
Jul  9 03:00:22.090: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  9 03:00:22.119: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:22.119: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:22.123: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:23.126: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:23.126: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:23.131: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:24.128: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:24.128: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:24.130: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:25.129: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:25.129: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:25.129: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:25.134: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:26.131: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:26.131: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:26.131: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:26.133: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:27.125: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:27.125: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:27.125: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:27.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:28.131: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:28.131: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:28.131: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:28.133: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:29.127: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:29.127: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:29.127: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:29.132: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:30.131: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:30.131: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:30.131: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:30.133: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:31.125: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:31.125: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:31.125: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:31.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:32.132: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:32.132: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:32.132: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:32.134: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:33.128: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:33.128: INFO: Wrong image for pod: daemon-set-bmddq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:33.128: INFO: Pod daemon-set-bmddq is not available
Jul  9 03:00:33.133: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:34.132: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:34.132: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:34.134: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:35.156: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:35.156: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:35.159: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:36.129: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:36.129: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:36.131: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:37.127: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:37.127: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:37.131: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:38.127: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:38.127: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:38.130: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:39.127: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:39.127: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:39.130: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:40.133: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:40.133: INFO: Pod daemon-set-wnk5h is not available
Jul  9 03:00:40.138: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:41.131: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:41.138: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:42.127: INFO: Wrong image for pod: daemon-set-95l8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  9 03:00:42.127: INFO: Pod daemon-set-95l8c is not available
Jul  9 03:00:42.131: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:43.126: INFO: Pod daemon-set-jrl7f is not available
Jul  9 03:00:43.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  9 03:00:43.130: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:43.132: INFO: Number of nodes with available pods: 1
Jul  9 03:00:43.132: INFO: Node node2 is running more than one daemon pod
Jul  9 03:00:44.140: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:44.144: INFO: Number of nodes with available pods: 1
Jul  9 03:00:44.144: INFO: Node node2 is running more than one daemon pod
Jul  9 03:00:45.139: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:00:45.141: INFO: Number of nodes with available pods: 2
Jul  9 03:00:45.141: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9768, will wait for the garbage collector to delete the pods
Jul  9 03:00:45.218: INFO: Deleting DaemonSet.extensions daemon-set took: 3.2917ms
Jul  9 03:00:45.518: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.1763ms
Jul  9 03:00:54.121: INFO: Number of nodes with available pods: 0
Jul  9 03:00:54.121: INFO: Number of running nodes: 0, number of available pods: 0
Jul  9 03:00:54.124: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9768/daemonsets","resourceVersion":"11004"},"items":null}

Jul  9 03:00:54.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9768/pods","resourceVersion":"11004"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:00:54.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9768" for this suite.
Jul  9 03:01:00.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:01:00.374: INFO: namespace daemonsets-9768 deletion completed in 6.2291935s

• [SLOW TEST:41.356 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:01:00.374: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  9 03:01:02.945: INFO: Successfully updated pod "pod-update-bd3806e1-882c-4bf6-b730-d7a2c57ebb7e"
STEP: verifying the updated pod is in kubernetes
Jul  9 03:01:02.956: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:01:02.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1638" for this suite.
Jul  9 03:01:24.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:01:25.177: INFO: namespace pods-1638 deletion completed in 22.2142829s

• [SLOW TEST:24.802 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:01:25.177: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f26b8a27-1862-4499-958a-fff40ff2b3dc
STEP: Creating a pod to test consume configMaps
Jul  9 03:01:25.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c" in namespace "configmap-4554" to be "success or failure"
Jul  9 03:01:25.217: INFO: Pod "pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6778ms
Jul  9 03:01:27.224: INFO: Pod "pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0100756s
Jul  9 03:01:29.230: INFO: Pod "pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0163772s
STEP: Saw pod success
Jul  9 03:01:29.230: INFO: Pod "pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c" satisfied condition "success or failure"
Jul  9 03:01:29.232: INFO: Trying to get logs from node node2 pod pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:01:29.251: INFO: Waiting for pod pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c to disappear
Jul  9 03:01:29.253: INFO: Pod pod-configmaps-334fe08f-7736-4a3d-945b-447d830e9b3c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:01:29.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4554" for this suite.
Jul  9 03:01:35.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:01:35.441: INFO: namespace configmap-4554 deletion completed in 6.1853038s

• [SLOW TEST:10.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:01:35.453: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  9 03:01:35.481: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  9 03:01:35.488: INFO: Waiting for terminating namespaces to be deleted...
Jul  9 03:01:35.493: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jul  9 03:01:35.503: INFO: weave-net-wl5x5 from kube-system started at 2019-07-09 01:52:47 +0000 UTC (2 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container weave ready: true, restart count 3
Jul  9 03:01:35.503: INFO: 	Container weave-npc ready: true, restart count 2
Jul  9 03:01:35.503: INFO: coredns-7d66d4b8cd-bpcx9 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container coredns ready: true, restart count 2
Jul  9 03:01:35.503: INFO: sonobuoy-e2e-job-daa7340bb2c14627 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container e2e ready: true, restart count 0
Jul  9 03:01:35.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:01:35.503: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-gjrs2 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:01:35.503: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  9 03:01:35.503: INFO: kube-proxy-dsfg4 from kube-system started at 2019-07-09 02:08:06 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 03:01:35.503: INFO: sdspaas-ns-controller-5bf85d9c4b-mrzhn from kube-system started at 2019-07-09 02:11:14 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.503: INFO: 	Container sdspaas-ns-controller ready: true, restart count 0
Jul  9 03:01:35.503: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jul  9 03:01:35.518: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-zgdws from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:01:35.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:01:35.518: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  9 03:01:35.518: INFO: coredns-7d66d4b8cd-6wkt8 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.518: INFO: 	Container coredns ready: true, restart count 2
Jul  9 03:01:35.518: INFO: kube-proxy-4bhgt from kube-system started at 2019-07-09 02:07:56 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.518: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 03:01:35.518: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-09 02:39:33 +0000 UTC (1 container statuses recorded)
Jul  9 03:01:35.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  9 03:01:35.518: INFO: weave-net-zfhsl from kube-system started at 2019-07-09 01:53:41 +0000 UTC (2 container statuses recorded)
Jul  9 03:01:35.518: INFO: 	Container weave ready: true, restart count 3
Jul  9 03:01:35.518: INFO: 	Container weave-npc ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
Jul  9 03:01:35.559: INFO: Pod sonobuoy requesting resource cpu=0m on Node node2
Jul  9 03:01:35.559: INFO: Pod sonobuoy-e2e-job-daa7340bb2c14627 requesting resource cpu=0m on Node node1
Jul  9 03:01:35.559: INFO: Pod sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-gjrs2 requesting resource cpu=0m on Node node1
Jul  9 03:01:35.559: INFO: Pod sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-zgdws requesting resource cpu=0m on Node node2
Jul  9 03:01:35.559: INFO: Pod coredns-7d66d4b8cd-6wkt8 requesting resource cpu=100m on Node node2
Jul  9 03:01:35.559: INFO: Pod coredns-7d66d4b8cd-bpcx9 requesting resource cpu=100m on Node node1
Jul  9 03:01:35.559: INFO: Pod kube-proxy-4bhgt requesting resource cpu=0m on Node node2
Jul  9 03:01:35.559: INFO: Pod kube-proxy-dsfg4 requesting resource cpu=0m on Node node1
Jul  9 03:01:35.559: INFO: Pod sdspaas-ns-controller-5bf85d9c4b-mrzhn requesting resource cpu=0m on Node node1
Jul  9 03:01:35.559: INFO: Pod weave-net-wl5x5 requesting resource cpu=20m on Node node1
Jul  9 03:01:35.559: INFO: Pod weave-net-zfhsl requesting resource cpu=20m on Node node2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-117d2305-810f-4231-863b-7ee611281d3a.15af9dfa95b97060], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2562/filler-pod-117d2305-810f-4231-863b-7ee611281d3a to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-117d2305-810f-4231-863b-7ee611281d3a.15af9dfae458f290], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-117d2305-810f-4231-863b-7ee611281d3a.15af9dfae6ccee3c], Reason = [Created], Message = [Created container filler-pod-117d2305-810f-4231-863b-7ee611281d3a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-117d2305-810f-4231-863b-7ee611281d3a.15af9dfaefa3eccc], Reason = [Started], Message = [Started container filler-pod-117d2305-810f-4231-863b-7ee611281d3a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93250953-9086-491c-b51c-853bf74fbe75.15af9dfa955924d0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2562/filler-pod-93250953-9086-491c-b51c-853bf74fbe75 to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93250953-9086-491c-b51c-853bf74fbe75.15af9dfae618204c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93250953-9086-491c-b51c-853bf74fbe75.15af9dfcd201c944], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93250953-9086-491c-b51c-853bf74fbe75.15af9dfcd4917b14], Reason = [Created], Message = [Created container filler-pod-93250953-9086-491c-b51c-853bf74fbe75]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93250953-9086-491c-b51c-853bf74fbe75.15af9dfcdddd8744], Reason = [Started], Message = [Started container filler-pod-93250953-9086-491c-b51c-853bf74fbe75]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15af9dfd619a579c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:01:48.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2562" for this suite.
Jul  9 03:01:54.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:01:54.778: INFO: namespace sched-pred-2562 deletion completed in 6.1539184s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:19.325 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:01:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3f6aff62-357c-4d78-abdc-c3806722007e
STEP: Creating a pod to test consume secrets
Jul  9 03:01:54.861: INFO: Waiting up to 5m0s for pod "pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558" in namespace "secrets-7729" to be "success or failure"
Jul  9 03:01:54.870: INFO: Pod "pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558": Phase="Pending", Reason="", readiness=false. Elapsed: 9.5018ms
Jul  9 03:01:56.877: INFO: Pod "pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0162577s
STEP: Saw pod success
Jul  9 03:01:56.877: INFO: Pod "pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558" satisfied condition "success or failure"
Jul  9 03:01:56.879: INFO: Trying to get logs from node node2 pod pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:01:56.892: INFO: Waiting for pod pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558 to disappear
Jul  9 03:01:56.896: INFO: Pod pod-secrets-366d5037-d948-4c95-9e1b-966a4c9bb558 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:01:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7729" for this suite.
Jul  9 03:02:02.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:02:03.093: INFO: namespace secrets-7729 deletion completed in 6.1956601s

• [SLOW TEST:8.315 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:02:03.094: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:02:03.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c" in namespace "downward-api-9719" to be "success or failure"
Jul  9 03:02:03.166: INFO: Pod "downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.1433ms
Jul  9 03:02:05.171: INFO: Pod "downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0104903s
STEP: Saw pod success
Jul  9 03:02:05.171: INFO: Pod "downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c" satisfied condition "success or failure"
Jul  9 03:02:05.173: INFO: Trying to get logs from node node2 pod downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c container client-container: <nil>
STEP: delete the pod
Jul  9 03:02:05.192: INFO: Waiting for pod downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c to disappear
Jul  9 03:02:05.199: INFO: Pod downwardapi-volume-ea94c65e-0f6b-4be9-aa33-21794e7e2f4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:02:05.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9719" for this suite.
Jul  9 03:02:11.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:02:11.363: INFO: namespace downward-api-9719 deletion completed in 6.1565279s

• [SLOW TEST:8.269 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:02:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-158
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-158
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-158
Jul  9 03:02:11.445: INFO: Found 0 stateful pods, waiting for 1
Jul  9 03:02:21.457: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  9 03:02:21.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:02:21.690: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:02:21.690: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:02:21.690: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 03:02:21.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  9 03:02:31.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 03:02:31.705: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 03:02:31.727: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:31.728: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:31.728: INFO: 
Jul  9 03:02:31.728: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  9 03:02:32.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9918052s
Jul  9 03:02:33.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984543s
Jul  9 03:02:34.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9756393s
Jul  9 03:02:35.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9720995s
Jul  9 03:02:36.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9642679s
Jul  9 03:02:37.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9582232s
Jul  9 03:02:38.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9561289s
Jul  9 03:02:39.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9537303s
Jul  9 03:02:40.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.0371ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-158
Jul  9 03:02:41.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:02:42.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 03:02:42.007: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 03:02:42.007: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 03:02:42.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:02:42.212: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  9 03:02:42.212: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 03:02:42.212: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 03:02:42.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:02:42.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  9 03:02:42.460: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 03:02:42.460: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 03:02:42.462: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 03:02:42.462: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 03:02:42.462: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  9 03:02:42.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:02:42.697: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:02:42.697: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:02:42.697: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 03:02:42.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:02:42.937: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:02:42.937: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:02:42.937: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 03:02:42.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:02:43.177: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:02:43.177: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:02:43.177: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 03:02:43.177: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 03:02:43.180: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  9 03:02:53.195: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 03:02:53.195: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 03:02:53.195: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  9 03:02:53.210: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:53.210: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:53.210: INFO: ss-1  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:53.210: INFO: ss-2  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:53.210: INFO: 
Jul  9 03:02:53.210: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:54.214: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:54.214: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:54.214: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:54.214: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:54.214: INFO: 
Jul  9 03:02:54.214: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:55.219: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:55.219: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:55.219: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:55.219: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:55.219: INFO: 
Jul  9 03:02:55.219: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:56.221: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:56.221: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:56.221: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:56.221: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:56.221: INFO: 
Jul  9 03:02:56.221: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:57.223: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:57.223: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:57.223: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:57.223: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:57.223: INFO: 
Jul  9 03:02:57.223: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:58.227: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:58.227: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:58.227: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:58.227: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:58.227: INFO: 
Jul  9 03:02:58.227: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:02:59.231: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:02:59.231: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:02:59.231: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:59.231: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:02:59.231: INFO: 
Jul  9 03:02:59.231: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:03:00.247: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:03:00.247: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:03:00.247: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:00.247: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:00.247: INFO: 
Jul  9 03:03:00.247: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:03:01.257: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:03:01.258: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:03:01.258: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:01.258: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:01.258: INFO: 
Jul  9 03:03:01.258: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  9 03:03:02.260: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jul  9 03:03:02.260: INFO: ss-0  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:11 +0000 UTC  }]
Jul  9 03:03:02.260: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:02.260: INFO: ss-2  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:02:31 +0000 UTC  }]
Jul  9 03:03:02.260: INFO: 
Jul  9 03:03:02.260: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-158
Jul  9 03:03:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:03.364: INFO: rc: 1
Jul  9 03:03:03.365: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0036110e0 exit status 1 <nil> <nil> true [0xc000bd1500 0xc000bd1518 0xc000bd1530] [0xc000bd1500 0xc000bd1518 0xc000bd1530] [0xc000bd1510 0xc000bd1528] [0x9d17b0 0x9d17b0] 0xc001b2ea80 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul  9 03:03:13.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:13.439: INFO: rc: 1
Jul  9 03:03:13.439: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00374f260 exit status 1 <nil> <nil> true [0xc003ddd338 0xc003ddd350 0xc003ddd368] [0xc003ddd338 0xc003ddd350 0xc003ddd368] [0xc003ddd348 0xc003ddd360] [0x9d17b0 0x9d17b0] 0xc001a2b7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:03:23.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:23.815: INFO: rc: 1
Jul  9 03:03:23.815: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00374f5f0 exit status 1 <nil> <nil> true [0xc003ddd370 0xc003ddd388 0xc003ddd3a0] [0xc003ddd370 0xc003ddd388 0xc003ddd3a0] [0xc003ddd380 0xc003ddd398] [0x9d17b0 0x9d17b0] 0xc001a2bbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:03:33.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:33.888: INFO: rc: 1
Jul  9 03:03:33.889: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00374f980 exit status 1 <nil> <nil> true [0xc003ddd3a8 0xc003ddd3c0 0xc003ddd3d8] [0xc003ddd3a8 0xc003ddd3c0 0xc003ddd3d8] [0xc003ddd3b8 0xc003ddd3d0] [0x9d17b0 0x9d17b0] 0xc001a2bf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:03:43.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:43.966: INFO: rc: 1
Jul  9 03:03:43.966: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dc990 exit status 1 <nil> <nil> true [0xc0000c41c8 0xc0000c4508 0xc0000c4660] [0xc0000c41c8 0xc0000c4508 0xc0000c4660] [0xc0000c4378 0xc0000c45b0] [0x9d17b0 0x9d17b0] 0xc00172a2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:03:53.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:03:54.028: INFO: rc: 1
Jul  9 03:03:54.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574390 exit status 1 <nil> <nil> true [0xc0003f93f0 0xc0003f9638 0xc0003f9830] [0xc0003f93f0 0xc0003f9638 0xc0003f9830] [0xc0003f95e0 0xc0003f9818] [0x9d17b0 0x9d17b0] 0xc002500480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:04.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:04.089: INFO: rc: 1
Jul  9 03:04:04.089: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574750 exit status 1 <nil> <nil> true [0xc0003f9848 0xc0003f9908 0xc0003f9930] [0xc0003f9848 0xc0003f9908 0xc0003f9930] [0xc0003f98b0 0xc0003f9918] [0x9d17b0 0x9d17b0] 0xc002500a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:14.147: INFO: rc: 1
Jul  9 03:04:14.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dcd20 exit status 1 <nil> <nil> true [0xc0000c47a0 0xc0000c4820 0xc0000c4a48] [0xc0000c47a0 0xc0000c4820 0xc0000c4a48] [0xc0000c47c0 0xc0000c4a30] [0x9d17b0 0x9d17b0] 0xc00172a660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:24.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:24.220: INFO: rc: 1
Jul  9 03:04:24.220: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dd0b0 exit status 1 <nil> <nil> true [0xc0000c4a70 0xc0000c4b38 0xc0000c4c50] [0xc0000c4a70 0xc0000c4b38 0xc0000c4c50] [0xc0000c4b28 0xc0000c4bd0] [0x9d17b0 0x9d17b0] 0xc00172a9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:34.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:34.278: INFO: rc: 1
Jul  9 03:04:34.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574ab0 exit status 1 <nil> <nil> true [0xc0003f9970 0xc0003f99f0 0xc0003f9b20] [0xc0003f9970 0xc0003f99f0 0xc0003f9b20] [0xc0003f99d8 0xc0003f9a78] [0x9d17b0 0x9d17b0] 0xc002500de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:44.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:44.343: INFO: rc: 1
Jul  9 03:04:44.343: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dd470 exit status 1 <nil> <nil> true [0xc0000c5158 0xc0000c5338 0xc0000c5400] [0xc0000c5158 0xc0000c5338 0xc0000c5400] [0xc0000c5258 0xc0000c53e0] [0x9d17b0 0x9d17b0] 0xc00172ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:04:54.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:04:54.403: INFO: rc: 1
Jul  9 03:04:54.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dd800 exit status 1 <nil> <nil> true [0xc0000c54d8 0xc0000c56a8 0xc0000c5840] [0xc0000c54d8 0xc0000c56a8 0xc0000c5840] [0xc0000c5640 0xc0000c5798] [0x9d17b0 0x9d17b0] 0xc00172b080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:04.459: INFO: rc: 1
Jul  9 03:05:04.459: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574e70 exit status 1 <nil> <nil> true [0xc0003f9b48 0xc0003f9c30 0xc0003f9d30] [0xc0003f9b48 0xc0003f9c30 0xc0003f9d30] [0xc0003f9bf8 0xc0003f9cf8] [0x9d17b0 0x9d17b0] 0xc002501200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:14.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:14.532: INFO: rc: 1
Jul  9 03:05:14.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0035751d0 exit status 1 <nil> <nil> true [0xc0003f9dc0 0xc0003f9ed8 0xc000010200] [0xc0003f9dc0 0xc0003f9ed8 0xc000010200] [0xc0003f9ea8 0xc0000100b8] [0x9d17b0 0x9d17b0] 0xc002501560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:24.607: INFO: rc: 1
Jul  9 03:05:24.607: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039ddbc0 exit status 1 <nil> <nil> true [0xc0000c58d8 0xc0000c5a28 0xc0000c5be8] [0xc0000c58d8 0xc0000c5a28 0xc0000c5be8] [0xc0000c5988 0xc0000c5b70] [0x9d17b0 0x9d17b0] 0xc00172b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:34.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:34.692: INFO: rc: 1
Jul  9 03:05:34.692: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003575590 exit status 1 <nil> <nil> true [0xc000010208 0xc001170258 0xc001170450] [0xc000010208 0xc001170258 0xc001170450] [0xc001170178 0xc001170418] [0x9d17b0 0x9d17b0] 0xc0025018c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:44.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:44.756: INFO: rc: 1
Jul  9 03:05:44.756: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574360 exit status 1 <nil> <nil> true [0xc000010200 0xc0003f9488 0xc0003f97c8] [0xc000010200 0xc0003f9488 0xc0003f97c8] [0xc0003f93f0 0xc0003f9638] [0x9d17b0 0x9d17b0] 0xc002500480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:05:54.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:05:54.823: INFO: rc: 1
Jul  9 03:05:54.823: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574780 exit status 1 <nil> <nil> true [0xc0003f9818 0xc0003f9870 0xc0003f9910] [0xc0003f9818 0xc0003f9870 0xc0003f9910] [0xc0003f9848 0xc0003f9908] [0x9d17b0 0x9d17b0] 0xc002500a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:04.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:04.893: INFO: rc: 1
Jul  9 03:06:04.893: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574b40 exit status 1 <nil> <nil> true [0xc0003f9918 0xc0003f99b0 0xc0003f9a10] [0xc0003f9918 0xc0003f99b0 0xc0003f9a10] [0xc0003f9970 0xc0003f99f0] [0x9d17b0 0x9d17b0] 0xc002500de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:14.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:14.957: INFO: rc: 1
Jul  9 03:06:14.957: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dc9f0 exit status 1 <nil> <nil> true [0xc001170060 0xc001170348 0xc0011704d8] [0xc001170060 0xc001170348 0xc0011704d8] [0xc001170258 0xc001170450] [0x9d17b0 0x9d17b0] 0xc00172a2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:24.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:25.065: INFO: rc: 1
Jul  9 03:06:25.065: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003574f30 exit status 1 <nil> <nil> true [0xc0003f9a78 0xc0003f9bc0 0xc0003f9c88] [0xc0003f9a78 0xc0003f9bc0 0xc0003f9c88] [0xc0003f9b48 0xc0003f9c30] [0x9d17b0 0x9d17b0] 0xc002501200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:35.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:35.155: INFO: rc: 1
Jul  9 03:06:35.155: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dcdb0 exit status 1 <nil> <nil> true [0xc001170500 0xc001170780 0xc001170838] [0xc001170500 0xc001170780 0xc001170838] [0xc001170658 0xc001170818] [0x9d17b0 0x9d17b0] 0xc00172a660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:45.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:45.229: INFO: rc: 1
Jul  9 03:06:45.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0035752f0 exit status 1 <nil> <nil> true [0xc0003f9cf8 0xc0003f9de8 0xc0003f9f90] [0xc0003f9cf8 0xc0003f9de8 0xc0003f9f90] [0xc0003f9dc0 0xc0003f9ed8] [0x9d17b0 0x9d17b0] 0xc002501560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:06:55.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:06:55.313: INFO: rc: 1
Jul  9 03:06:55.313: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0035756e0 exit status 1 <nil> <nil> true [0xc0000c4180 0xc0000c4378 0xc0000c45b0] [0xc0000c4180 0xc0000c4378 0xc0000c45b0] [0xc0000c4288 0xc0000c4570] [0x9d17b0 0x9d17b0] 0xc0025018c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:05.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:05.372: INFO: rc: 1
Jul  9 03:07:05.372: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dd170 exit status 1 <nil> <nil> true [0xc001170908 0xc001170c10 0xc001170d60] [0xc001170908 0xc001170c10 0xc001170d60] [0xc001170b88 0xc001170c68] [0x9d17b0 0x9d17b0] 0xc00172a9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:15.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:15.447: INFO: rc: 1
Jul  9 03:07:15.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003575a40 exit status 1 <nil> <nil> true [0xc0000c4660 0xc0000c47c0 0xc0000c4a30] [0xc0000c4660 0xc0000c47c0 0xc0000c4a30] [0xc0000c47b8 0xc0000c49d8] [0x9d17b0 0x9d17b0] 0xc002501c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:25.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:25.510: INFO: rc: 1
Jul  9 03:07:25.510: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003575e00 exit status 1 <nil> <nil> true [0xc0000c4a48 0xc0000c4b28 0xc0000c4bd0] [0xc0000c4a48 0xc0000c4b28 0xc0000c4bd0] [0xc0000c4aa8 0xc0000c4b90] [0x9d17b0 0x9d17b0] 0xc002501f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:35.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:35.584: INFO: rc: 1
Jul  9 03:07:35.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dd530 exit status 1 <nil> <nil> true [0xc001170e60 0xc001171088 0xc001171388] [0xc001170e60 0xc001171088 0xc001171388] [0xc001171058 0xc001171250] [0x9d17b0 0x9d17b0] 0xc00172ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:45.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:45.646: INFO: rc: 1
Jul  9 03:07:45.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dc990 exit status 1 <nil> <nil> true [0xc0003f9488 0xc0003f97c8 0xc0003f9848] [0xc0003f9488 0xc0003f97c8 0xc0003f9848] [0xc0003f9638 0xc0003f9830] [0x9d17b0 0x9d17b0] 0xc002500480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:07:55.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:07:55.706: INFO: rc: 1
Jul  9 03:07:55.706: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0039dcd50 exit status 1 <nil> <nil> true [0xc0003f9870 0xc0003f9910 0xc0003f9970] [0xc0003f9870 0xc0003f9910 0xc0003f9970] [0xc0003f9908 0xc0003f9930] [0x9d17b0 0x9d17b0] 0xc002500a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul  9 03:08:05.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-158 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:08:05.781: INFO: rc: 1
Jul  9 03:08:05.781: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jul  9 03:08:05.781: INFO: Scaling statefulset ss to 0
Jul  9 03:08:05.790: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  9 03:08:05.791: INFO: Deleting all statefulset in ns statefulset-158
Jul  9 03:08:05.793: INFO: Scaling statefulset ss to 0
Jul  9 03:08:05.801: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 03:08:05.803: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:08:05.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-158" for this suite.
Jul  9 03:08:11.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:08:12.003: INFO: namespace statefulset-158 deletion completed in 6.1828605s

• [SLOW TEST:360.641 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:08:12.004: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6dq5p in namespace proxy-456
I0709 03:08:12.094534      15 runners.go:180] Created replication controller with name: proxy-service-6dq5p, namespace: proxy-456, replica count: 1
I0709 03:08:13.144965      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:14.145205      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:15.145563      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:16.145761      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:17.145963      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:18.146309      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:19.146487      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:20.146736      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:08:21.146933      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0709 03:08:22.147107      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0709 03:08:23.154073      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0709 03:08:24.156633      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0709 03:08:25.156855      15 runners.go:180] proxy-service-6dq5p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  9 03:08:25.158: INFO: setup took 13.0814622s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  9 03:08:25.168: INFO: (0) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 9.7726ms)
Jul  9 03:08:25.168: INFO: (0) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 9.7203ms)
Jul  9 03:08:25.171: INFO: (0) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 12.603ms)
Jul  9 03:08:25.172: INFO: (0) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 14.0864ms)
Jul  9 03:08:25.173: INFO: (0) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 14.2763ms)
Jul  9 03:08:25.173: INFO: (0) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 14.375ms)
Jul  9 03:08:25.173: INFO: (0) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 14.6651ms)
Jul  9 03:08:25.176: INFO: (0) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 17.0906ms)
Jul  9 03:08:25.176: INFO: (0) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 17.8711ms)
Jul  9 03:08:25.177: INFO: (0) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 17.9195ms)
Jul  9 03:08:25.177: INFO: (0) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 18.1059ms)
Jul  9 03:08:25.181: INFO: (0) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 22.6894ms)
Jul  9 03:08:25.181: INFO: (0) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 22.6937ms)
Jul  9 03:08:25.181: INFO: (0) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 22.7673ms)
Jul  9 03:08:25.183: INFO: (0) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 24.1638ms)
Jul  9 03:08:25.184: INFO: (0) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 25.6994ms)
Jul  9 03:08:25.189: INFO: (1) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 4.8781ms)
Jul  9 03:08:25.190: INFO: (1) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 5.8666ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.3799ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 6.2962ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.5541ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 6.1111ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.1332ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 6.4427ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.8254ms)
Jul  9 03:08:25.191: INFO: (1) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.9155ms)
Jul  9 03:08:25.194: INFO: (1) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 9.2356ms)
Jul  9 03:08:25.194: INFO: (1) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 9.2939ms)
Jul  9 03:08:25.196: INFO: (1) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 10.9041ms)
Jul  9 03:08:25.196: INFO: (1) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 10.9557ms)
Jul  9 03:08:25.196: INFO: (1) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 11.5383ms)
Jul  9 03:08:25.198: INFO: (1) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 13.1461ms)
Jul  9 03:08:25.202: INFO: (2) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 4.0831ms)
Jul  9 03:08:25.203: INFO: (2) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 5.1967ms)
Jul  9 03:08:25.203: INFO: (2) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 5.2604ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 7.8516ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 8.0396ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 8.0564ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 8.214ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 8.2078ms)
Jul  9 03:08:25.206: INFO: (2) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.2975ms)
Jul  9 03:08:25.210: INFO: (2) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 11.5652ms)
Jul  9 03:08:25.210: INFO: (2) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 11.9902ms)
Jul  9 03:08:25.210: INFO: (2) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 11.4883ms)
Jul  9 03:08:25.210: INFO: (2) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 11.6459ms)
Jul  9 03:08:25.210: INFO: (2) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 11.4939ms)
Jul  9 03:08:25.212: INFO: (2) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 13.3878ms)
Jul  9 03:08:25.212: INFO: (2) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 13.4054ms)
Jul  9 03:08:25.216: INFO: (3) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 4.4861ms)
Jul  9 03:08:25.217: INFO: (3) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 4.3027ms)
Jul  9 03:08:25.218: INFO: (3) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.1253ms)
Jul  9 03:08:25.218: INFO: (3) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 5.8883ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 8.4109ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.052ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 8.0347ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 8.1806ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 8.3551ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 8.1284ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 8.284ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 8.6915ms)
Jul  9 03:08:25.220: INFO: (3) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 8.7932ms)
Jul  9 03:08:25.221: INFO: (3) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 9.2197ms)
Jul  9 03:08:25.221: INFO: (3) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.9798ms)
Jul  9 03:08:25.221: INFO: (3) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 9.041ms)
Jul  9 03:08:25.226: INFO: (4) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 4.637ms)
Jul  9 03:08:25.226: INFO: (4) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 4.7698ms)
Jul  9 03:08:25.237: INFO: (4) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 16.3115ms)
Jul  9 03:08:25.237: INFO: (4) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 15.8041ms)
Jul  9 03:08:25.237: INFO: (4) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 15.7544ms)
Jul  9 03:08:25.237: INFO: (4) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 16.1019ms)
Jul  9 03:08:25.237: INFO: (4) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 15.9363ms)
Jul  9 03:08:25.238: INFO: (4) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 16.4053ms)
Jul  9 03:08:25.238: INFO: (4) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 16.7214ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 20.0915ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 19.825ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 20.4439ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 19.7633ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 20.0477ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 19.8778ms)
Jul  9 03:08:25.242: INFO: (4) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 20.2773ms)
Jul  9 03:08:25.248: INFO: (5) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 5.5034ms)
Jul  9 03:08:25.248: INFO: (5) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 5.9346ms)
Jul  9 03:08:25.249: INFO: (5) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 7.3695ms)
Jul  9 03:08:25.250: INFO: (5) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 7.1388ms)
Jul  9 03:08:25.250: INFO: (5) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 7.7098ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 8.688ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 8.0963ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 8.6035ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 8.2089ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 8.754ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 8.7204ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 9.1281ms)
Jul  9 03:08:25.251: INFO: (5) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 8.7995ms)
Jul  9 03:08:25.255: INFO: (5) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 12.3955ms)
Jul  9 03:08:25.255: INFO: (5) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 12.4792ms)
Jul  9 03:08:25.255: INFO: (5) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 12.332ms)
Jul  9 03:08:25.261: INFO: (6) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 5.7669ms)
Jul  9 03:08:25.262: INFO: (6) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.2157ms)
Jul  9 03:08:25.262: INFO: (6) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 6.878ms)
Jul  9 03:08:25.262: INFO: (6) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 6.7674ms)
Jul  9 03:08:25.262: INFO: (6) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 6.4889ms)
Jul  9 03:08:25.263: INFO: (6) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 7.7502ms)
Jul  9 03:08:25.263: INFO: (6) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 7.86ms)
Jul  9 03:08:25.263: INFO: (6) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 7.7827ms)
Jul  9 03:08:25.263: INFO: (6) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 7.8108ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 8.5638ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 8.5574ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 8.3517ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 8.3449ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 8.3347ms)
Jul  9 03:08:25.264: INFO: (6) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.4748ms)
Jul  9 03:08:25.266: INFO: (6) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 11.3281ms)
Jul  9 03:08:25.273: INFO: (7) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.1774ms)
Jul  9 03:08:25.273: INFO: (7) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.0952ms)
Jul  9 03:08:25.273: INFO: (7) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 5.6675ms)
Jul  9 03:08:25.273: INFO: (7) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 5.9237ms)
Jul  9 03:08:25.273: INFO: (7) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 5.7595ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 10.506ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 10.6004ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 10.7802ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 10.5276ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 10.7133ms)
Jul  9 03:08:25.278: INFO: (7) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.7618ms)
Jul  9 03:08:25.279: INFO: (7) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 12.7157ms)
Jul  9 03:08:25.280: INFO: (7) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 13.5119ms)
Jul  9 03:08:25.280: INFO: (7) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 13.4972ms)
Jul  9 03:08:25.280: INFO: (7) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 13.317ms)
Jul  9 03:08:25.280: INFO: (7) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 13.4724ms)
Jul  9 03:08:25.286: INFO: (8) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.1419ms)
Jul  9 03:08:25.290: INFO: (8) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 9.3827ms)
Jul  9 03:08:25.292: INFO: (8) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 11.0264ms)
Jul  9 03:08:25.292: INFO: (8) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 10.6804ms)
Jul  9 03:08:25.292: INFO: (8) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 11.1811ms)
Jul  9 03:08:25.292: INFO: (8) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 11.4123ms)
Jul  9 03:08:25.292: INFO: (8) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 11.2545ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 11.8055ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 11.9731ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 12.0543ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 11.9126ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 11.9547ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 12.279ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 12.6606ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 12.394ms)
Jul  9 03:08:25.293: INFO: (8) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 12.7066ms)
Jul  9 03:08:25.299: INFO: (9) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 5.5615ms)
Jul  9 03:08:25.299: INFO: (9) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 5.8105ms)
Jul  9 03:08:25.300: INFO: (9) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 5.6326ms)
Jul  9 03:08:25.300: INFO: (9) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 5.8286ms)
Jul  9 03:08:25.300: INFO: (9) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 6.4328ms)
Jul  9 03:08:25.300: INFO: (9) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.6148ms)
Jul  9 03:08:25.301: INFO: (9) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.9502ms)
Jul  9 03:08:25.301: INFO: (9) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 6.8314ms)
Jul  9 03:08:25.301: INFO: (9) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 7.1577ms)
Jul  9 03:08:25.301: INFO: (9) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.8242ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 10.2753ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 9.8513ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 10.1727ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 10.5903ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 10.2064ms)
Jul  9 03:08:25.304: INFO: (9) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 10.2931ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 6.7009ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 7.2815ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 6.7663ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 7.1791ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.6848ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 7.2936ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 7.5716ms)
Jul  9 03:08:25.312: INFO: (10) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 7.5328ms)
Jul  9 03:08:25.313: INFO: (10) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 7.4463ms)
Jul  9 03:08:25.313: INFO: (10) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 7.8707ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 9.7742ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 10.0287ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 9.9788ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 10.1192ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 10.5908ms)
Jul  9 03:08:25.315: INFO: (10) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 10.2983ms)
Jul  9 03:08:25.320: INFO: (11) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 4.9325ms)
Jul  9 03:08:25.321: INFO: (11) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 5.8771ms)
Jul  9 03:08:25.322: INFO: (11) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 6.3438ms)
Jul  9 03:08:25.322: INFO: (11) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.4715ms)
Jul  9 03:08:25.323: INFO: (11) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 7.2498ms)
Jul  9 03:08:25.323: INFO: (11) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 7.3182ms)
Jul  9 03:08:25.323: INFO: (11) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 7.7101ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 11.6593ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 11.6776ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 12.2877ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 12.6806ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 12.2198ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 12.5978ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 12.1954ms)
Jul  9 03:08:25.328: INFO: (11) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 12.9565ms)
Jul  9 03:08:25.329: INFO: (11) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 13.0907ms)
Jul  9 03:08:25.335: INFO: (12) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.0373ms)
Jul  9 03:08:25.336: INFO: (12) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.2241ms)
Jul  9 03:08:25.336: INFO: (12) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.2825ms)
Jul  9 03:08:25.336: INFO: (12) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 6.406ms)
Jul  9 03:08:25.336: INFO: (12) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.5023ms)
Jul  9 03:08:25.336: INFO: (12) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 6.5562ms)
Jul  9 03:08:25.337: INFO: (12) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.0127ms)
Jul  9 03:08:25.337: INFO: (12) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 7.612ms)
Jul  9 03:08:25.337: INFO: (12) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 8.1457ms)
Jul  9 03:08:25.337: INFO: (12) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 8.2472ms)
Jul  9 03:08:25.338: INFO: (12) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 8.5064ms)
Jul  9 03:08:25.344: INFO: (12) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 14.4407ms)
Jul  9 03:08:25.344: INFO: (12) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 14.0621ms)
Jul  9 03:08:25.344: INFO: (12) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 14.2151ms)
Jul  9 03:08:25.344: INFO: (12) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 14.0369ms)
Jul  9 03:08:25.344: INFO: (12) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 14.6549ms)
Jul  9 03:08:25.352: INFO: (13) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 7.4085ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.636ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 10.4254ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 11.055ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.6621ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 10.9505ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 10.6508ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 10.7127ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 10.6613ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 11.0796ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 10.9765ms)
Jul  9 03:08:25.355: INFO: (13) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 11.076ms)
Jul  9 03:08:25.356: INFO: (13) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 12.1361ms)
Jul  9 03:08:25.356: INFO: (13) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 12.0908ms)
Jul  9 03:08:25.356: INFO: (13) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 11.9968ms)
Jul  9 03:08:25.356: INFO: (13) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 12.3184ms)
Jul  9 03:08:25.360: INFO: (14) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 3.6073ms)
Jul  9 03:08:25.360: INFO: (14) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 3.908ms)
Jul  9 03:08:25.360: INFO: (14) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 4.0113ms)
Jul  9 03:08:25.360: INFO: (14) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 3.9425ms)
Jul  9 03:08:25.366: INFO: (14) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 9.8996ms)
Jul  9 03:08:25.368: INFO: (14) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 11.5341ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 12.659ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 12.6319ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 13.2811ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 13.3898ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 13.0569ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 13.0163ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 13.1446ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 13.4912ms)
Jul  9 03:08:25.370: INFO: (14) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 13.7918ms)
Jul  9 03:08:25.371: INFO: (14) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 14.7303ms)
Jul  9 03:08:25.377: INFO: (15) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 5.6308ms)
Jul  9 03:08:25.378: INFO: (15) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 6.3369ms)
Jul  9 03:08:25.382: INFO: (15) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 10.8867ms)
Jul  9 03:08:25.382: INFO: (15) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 11.1007ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.6981ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.7904ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 10.5568ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 10.5896ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 11.1548ms)
Jul  9 03:08:25.383: INFO: (15) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 11.1467ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 14.9759ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 15.3788ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 15.199ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 15.2913ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 15.2641ms)
Jul  9 03:08:25.387: INFO: (15) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 15.4353ms)
Jul  9 03:08:25.392: INFO: (16) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 4.8581ms)
Jul  9 03:08:25.392: INFO: (16) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 4.8655ms)
Jul  9 03:08:25.394: INFO: (16) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.3498ms)
Jul  9 03:08:25.395: INFO: (16) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 7.4176ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 8.822ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 8.7922ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 8.9988ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 8.9994ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 9.0794ms)
Jul  9 03:08:25.396: INFO: (16) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 8.9698ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 12.156ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 12.5471ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 12.4769ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 12.8281ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 12.9872ms)
Jul  9 03:08:25.400: INFO: (16) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 12.8718ms)
Jul  9 03:08:25.408: INFO: (17) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 6.9187ms)
Jul  9 03:08:25.411: INFO: (17) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 10.1127ms)
Jul  9 03:08:25.411: INFO: (17) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 10.6833ms)
Jul  9 03:08:25.411: INFO: (17) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 9.9576ms)
Jul  9 03:08:25.411: INFO: (17) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 10.5275ms)
Jul  9 03:08:25.414: INFO: (17) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 12.877ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 13.5805ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 14.0613ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 14.0165ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 13.9199ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 13.401ms)
Jul  9 03:08:25.415: INFO: (17) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 14.8001ms)
Jul  9 03:08:25.416: INFO: (17) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 14.3502ms)
Jul  9 03:08:25.416: INFO: (17) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 14.8408ms)
Jul  9 03:08:25.416: INFO: (17) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 15.3112ms)
Jul  9 03:08:25.416: INFO: (17) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 14.7167ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 6.6759ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 6.7502ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 6.9482ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.5434ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 6.9024ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 6.8373ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 7.0711ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 6.7158ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 7.0341ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 6.8935ms)
Jul  9 03:08:25.423: INFO: (18) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 6.7618ms)
Jul  9 03:08:25.425: INFO: (18) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 8.8642ms)
Jul  9 03:08:25.425: INFO: (18) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 8.556ms)
Jul  9 03:08:25.425: INFO: (18) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 8.6135ms)
Jul  9 03:08:25.425: INFO: (18) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 9.1612ms)
Jul  9 03:08:25.425: INFO: (18) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 8.5717ms)
Jul  9 03:08:25.430: INFO: (19) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:460/proxy/: tls baz (200; 4.7881ms)
Jul  9 03:08:25.434: INFO: (19) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 7.9212ms)
Jul  9 03:08:25.435: INFO: (19) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname1/proxy/: foo (200; 9.3876ms)
Jul  9 03:08:25.435: INFO: (19) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname1/proxy/: foo (200; 9.8707ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:462/proxy/: tls qux (200; 9.6007ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:162/proxy/: bar (200; 9.6272ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/https:proxy-service-6dq5p-kdpzw:443/proxy/tlsrewriteme... (200; 10.0636ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw/proxy/rewriteme">test</a> (200; 9.9329ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">test</... (200; 9.8649ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/services/proxy-service-6dq5p:portname2/proxy/: bar (200; 10.742ms)
Jul  9 03:08:25.436: INFO: (19) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/: <a href="/api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:1080/proxy/rewriteme">t... (200; 9.7836ms)
Jul  9 03:08:25.437: INFO: (19) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname2/proxy/: tls qux (200; 11.5312ms)
Jul  9 03:08:25.437: INFO: (19) /api/v1/namespaces/proxy-456/services/https:proxy-service-6dq5p:tlsportname1/proxy/: tls baz (200; 11.0132ms)
Jul  9 03:08:25.437: INFO: (19) /api/v1/namespaces/proxy-456/pods/proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 11.7282ms)
Jul  9 03:08:25.437: INFO: (19) /api/v1/namespaces/proxy-456/pods/http:proxy-service-6dq5p-kdpzw:160/proxy/: foo (200; 11.5748ms)
Jul  9 03:08:25.437: INFO: (19) /api/v1/namespaces/proxy-456/services/http:proxy-service-6dq5p:portname2/proxy/: bar (200; 11.977ms)
STEP: deleting ReplicationController proxy-service-6dq5p in namespace proxy-456, will wait for the garbage collector to delete the pods
Jul  9 03:08:25.510: INFO: Deleting ReplicationController proxy-service-6dq5p took: 13.8101ms
Jul  9 03:08:25.810: INFO: Terminating ReplicationController proxy-service-6dq5p pods took: 300.2712ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:08:34.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-456" for this suite.
Jul  9 03:08:40.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:08:40.265: INFO: namespace proxy-456 deletion completed in 6.1504799s

• [SLOW TEST:28.261 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:08:40.265: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul  9 03:08:40.299: INFO: Waiting up to 5m0s for pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7" in namespace "containers-6811" to be "success or failure"
Jul  9 03:08:40.307: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4739ms
Jul  9 03:08:42.313: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0134355s
Jul  9 03:08:44.322: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0228395s
Jul  9 03:08:46.328: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0287123s
Jul  9 03:08:48.332: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0322096s
Jul  9 03:08:50.337: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0375224s
Jul  9 03:08:52.340: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.0405635s
STEP: Saw pod success
Jul  9 03:08:52.340: INFO: Pod "client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7" satisfied condition "success or failure"
Jul  9 03:08:52.342: INFO: Trying to get logs from node node2 pod client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7 container test-container: <nil>
STEP: delete the pod
Jul  9 03:08:52.358: INFO: Waiting for pod client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7 to disappear
Jul  9 03:08:52.360: INFO: Pod client-containers-d47d6a2b-c595-415a-9b75-f203aa573eb7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:08:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6811" for this suite.
Jul  9 03:08:58.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:08:58.481: INFO: namespace containers-6811 deletion completed in 6.1183331s

• [SLOW TEST:18.216 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:08:58.482: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul  9 03:08:59.581: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 121
	[quantile=0.99] = 121
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 211633
	[quantile=0.9] = 218901
	[quantile=0.99] = 218901
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 5
	[quantile=0.99] = 19
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 8
	[quantile=0.9] = 16
	[quantile=0.99] = 36
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 8
	[quantile=0.9] = 16
	[quantile=0.99] = 41
For namespace_queue_latency_sum:
	[] = 1524
For namespace_queue_latency_count:
	[] = 122
For namespace_retries:
	[] = 126
For namespace_work_duration:
	[quantile=0.5] = 86653
	[quantile=0.9] = 159386
	[quantile=0.99] = 215229
For namespace_work_duration_sum:
	[] = 15056506
For namespace_work_duration_count:
	[] = 122
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:08:59.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4529" for this suite.
Jul  9 03:09:05.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:05.736: INFO: namespace gc-4529 deletion completed in 6.1496112s

• [SLOW TEST:7.255 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:05.737: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  9 03:09:05.817: INFO: Waiting up to 5m0s for pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5" in namespace "emptydir-8769" to be "success or failure"
Jul  9 03:09:05.819: INFO: Pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3875ms
Jul  9 03:09:07.828: INFO: Pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0104688s
Jul  9 03:09:09.836: INFO: Pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0186217s
Jul  9 03:09:11.843: INFO: Pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0262399s
STEP: Saw pod success
Jul  9 03:09:11.843: INFO: Pod "pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5" satisfied condition "success or failure"
Jul  9 03:09:11.846: INFO: Trying to get logs from node node2 pod pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5 container test-container: <nil>
STEP: delete the pod
Jul  9 03:09:11.865: INFO: Waiting for pod pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5 to disappear
Jul  9 03:09:11.869: INFO: Pod pod-4f225ec3-32c9-4a2d-8165-3d6eb3dcdac5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:11.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8769" for this suite.
Jul  9 03:09:17.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:18.019: INFO: namespace emptydir-8769 deletion completed in 6.147284s

• [SLOW TEST:12.282 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:18.019: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-8d3e1b72-b9d4-4f08-aa8d-04cf2ae70e79
STEP: Creating a pod to test consume secrets
Jul  9 03:09:18.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518" in namespace "projected-7793" to be "success or failure"
Jul  9 03:09:18.071: INFO: Pod "pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6186ms
Jul  9 03:09:20.073: INFO: Pod "pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0066405s
STEP: Saw pod success
Jul  9 03:09:20.073: INFO: Pod "pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518" satisfied condition "success or failure"
Jul  9 03:09:20.075: INFO: Trying to get logs from node node2 pod pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:09:20.092: INFO: Waiting for pod pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518 to disappear
Jul  9 03:09:20.094: INFO: Pod pod-projected-secrets-500cc0be-db98-4f31-9844-721475d06518 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:20.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7793" for this suite.
Jul  9 03:09:26.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:26.284: INFO: namespace projected-7793 deletion completed in 6.1872767s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:26.284: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  9 03:09:26.328: INFO: Waiting up to 5m0s for pod "pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e" in namespace "emptydir-7797" to be "success or failure"
Jul  9 03:09:26.330: INFO: Pod "pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8625ms
Jul  9 03:09:28.337: INFO: Pod "pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0089768s
STEP: Saw pod success
Jul  9 03:09:28.337: INFO: Pod "pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e" satisfied condition "success or failure"
Jul  9 03:09:28.340: INFO: Trying to get logs from node node2 pod pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e container test-container: <nil>
STEP: delete the pod
Jul  9 03:09:28.369: INFO: Waiting for pod pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e to disappear
Jul  9 03:09:28.384: INFO: Pod pod-74e2cde7-adfb-489e-a151-4e0b71f2b05e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:28.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7797" for this suite.
Jul  9 03:09:34.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:34.564: INFO: namespace emptydir-7797 deletion completed in 6.1726885s

• [SLOW TEST:8.280 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:34.564: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-2956/secret-test-52fe6cad-3c96-4504-a982-1c493afb9703
STEP: Creating a pod to test consume secrets
Jul  9 03:09:34.601: INFO: Waiting up to 5m0s for pod "pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088" in namespace "secrets-2956" to be "success or failure"
Jul  9 03:09:34.606: INFO: Pod "pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088": Phase="Pending", Reason="", readiness=false. Elapsed: 4.8666ms
Jul  9 03:09:36.613: INFO: Pod "pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0124636s
STEP: Saw pod success
Jul  9 03:09:36.613: INFO: Pod "pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088" satisfied condition "success or failure"
Jul  9 03:09:36.618: INFO: Trying to get logs from node node2 pod pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088 container env-test: <nil>
STEP: delete the pod
Jul  9 03:09:36.643: INFO: Waiting for pod pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088 to disappear
Jul  9 03:09:36.646: INFO: Pod pod-configmaps-6695fe02-557a-421b-8c73-8165a4b86088 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:36.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2956" for this suite.
Jul  9 03:09:42.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:42.863: INFO: namespace secrets-2956 deletion completed in 6.2139956s

• [SLOW TEST:8.299 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:42.863: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul  9 03:09:42.892: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-630487638 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:42.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6606" for this suite.
Jul  9 03:09:48.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:49.130: INFO: namespace kubectl-6606 deletion completed in 6.1856059s

• [SLOW TEST:6.267 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:49.131: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:09:49.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755" in namespace "projected-809" to be "success or failure"
Jul  9 03:09:49.221: INFO: Pod "downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755": Phase="Pending", Reason="", readiness=false. Elapsed: 11.7127ms
Jul  9 03:09:51.223: INFO: Pod "downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0139018s
Jul  9 03:09:53.227: INFO: Pod "downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0174372s
STEP: Saw pod success
Jul  9 03:09:53.227: INFO: Pod "downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755" satisfied condition "success or failure"
Jul  9 03:09:53.230: INFO: Trying to get logs from node node2 pod downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755 container client-container: <nil>
STEP: delete the pod
Jul  9 03:09:53.260: INFO: Waiting for pod downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755 to disappear
Jul  9 03:09:53.266: INFO: Pod downwardapi-volume-8dc50239-071c-4524-b95c-ad5c0566f755 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:09:53.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-809" for this suite.
Jul  9 03:09:59.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:09:59.447: INFO: namespace projected-809 deletion completed in 6.1758018s

• [SLOW TEST:10.316 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:09:59.447: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-f92e9b95-4439-407f-808f-661385deb035
STEP: Creating secret with name secret-projected-all-test-volume-6547849e-447c-4326-996a-d4ce3c183fec
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  9 03:09:59.493: INFO: Waiting up to 5m0s for pod "projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba" in namespace "projected-866" to be "success or failure"
Jul  9 03:09:59.496: INFO: Pod "projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2013ms
Jul  9 03:10:01.500: INFO: Pod "projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0066683s
STEP: Saw pod success
Jul  9 03:10:01.500: INFO: Pod "projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba" satisfied condition "success or failure"
Jul  9 03:10:01.502: INFO: Trying to get logs from node node2 pod projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  9 03:10:01.563: INFO: Waiting for pod projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba to disappear
Jul  9 03:10:01.565: INFO: Pod projected-volume-4277f284-e6f6-4357-afe0-8080bc1b21ba no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:10:01.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-866" for this suite.
Jul  9 03:10:07.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:10:07.681: INFO: namespace projected-866 deletion completed in 6.1127154s

• [SLOW TEST:8.234 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:10:07.682: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:10:07.722: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul  9 03:10:12.726: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  9 03:10:12.726: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  9 03:10:14.731: INFO: Creating deployment "test-rollover-deployment"
Jul  9 03:10:14.748: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  9 03:10:16.756: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  9 03:10:16.768: INFO: Ensure that both replica sets have 1 created replica
Jul  9 03:10:16.772: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  9 03:10:16.777: INFO: Updating deployment test-rollover-deployment
Jul  9 03:10:16.777: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  9 03:10:18.790: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  9 03:10:18.796: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  9 03:10:18.809: INFO: all replica sets need to contain the pod-template-hash label
Jul  9 03:10:18.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238618, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:10:20.819: INFO: all replica sets need to contain the pod-template-hash label
Jul  9 03:10:20.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238618, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:10:22.820: INFO: all replica sets need to contain the pod-template-hash label
Jul  9 03:10:22.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238618, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:10:24.816: INFO: all replica sets need to contain the pod-template-hash label
Jul  9 03:10:24.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238618, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:10:26.817: INFO: all replica sets need to contain the pod-template-hash label
Jul  9 03:10:26.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238618, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238614, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:10:28.820: INFO: 
Jul  9 03:10:28.821: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  9 03:10:28.828: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9480,SelfLink:/apis/apps/v1/namespaces/deployment-9480/deployments/test-rollover-deployment,UID:aa905b6e-4c07-4cf5-b909-451b04d99c3c,ResourceVersion:12593,Generation:2,CreationTimestamp:2019-07-09 03:10:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-09 03:10:14 +0000 UTC 2019-07-09 03:10:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-09 03:10:28 +0000 UTC 2019-07-09 03:10:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  9 03:10:28.831: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9480,SelfLink:/apis/apps/v1/namespaces/deployment-9480/replicasets/test-rollover-deployment-854595fc44,UID:bd48c6ae-be5f-4e5b-988c-0a49740f665e,ResourceVersion:12582,Generation:2,CreationTimestamp:2019-07-09 03:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aa905b6e-4c07-4cf5-b909-451b04d99c3c 0xc001c07557 0xc001c07558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  9 03:10:28.831: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  9 03:10:28.831: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9480,SelfLink:/apis/apps/v1/namespaces/deployment-9480/replicasets/test-rollover-controller,UID:699086cd-9118-4ca0-a525-e9dae9011f23,ResourceVersion:12591,Generation:2,CreationTimestamp:2019-07-09 03:10:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aa905b6e-4c07-4cf5-b909-451b04d99c3c 0xc001c073c7 0xc001c073c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 03:10:28.831: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9480,SelfLink:/apis/apps/v1/namespaces/deployment-9480/replicasets/test-rollover-deployment-9b8b997cf,UID:3d92d777-252c-4717-ae71-0f5909429ad5,ResourceVersion:12549,Generation:2,CreationTimestamp:2019-07-09 03:10:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aa905b6e-4c07-4cf5-b909-451b04d99c3c 0xc001c07690 0xc001c07691}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 03:10:28.834: INFO: Pod "test-rollover-deployment-854595fc44-rflrw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-rflrw,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9480,SelfLink:/api/v1/namespaces/deployment-9480/pods/test-rollover-deployment-854595fc44-rflrw,UID:8a6379ad-8aa4-45e8-85d0-bb7d473d17c1,ResourceVersion:12563,Generation:0,CreationTimestamp:2019-07-09 03:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 bd48c6ae-be5f-4e5b-988c-0a49740f665e 0xc00211fbc7 0xc00211fbc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lf7bz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lf7bz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lf7bz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211fc60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211fdb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:10:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:10:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:10:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:10:16 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.4,StartTime:2019-07-09 03:10:16 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-09 03:10:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d997784a503f82f76e94fc7c281aea1c0577f6205bd5875bc452cf47ff01ce4e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:10:28.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9480" for this suite.
Jul  9 03:10:34.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:10:35.046: INFO: namespace deployment-9480 deletion completed in 6.1986581s

• [SLOW TEST:27.364 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:10:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:10:35.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9933" for this suite.
Jul  9 03:10:57.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:10:57.307: INFO: namespace pods-9933 deletion completed in 22.2050702s

• [SLOW TEST:22.261 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:10:57.308: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-fc080584-5549-49c0-b96a-5a87ed2be326
STEP: Creating a pod to test consume secrets
Jul  9 03:10:57.345: INFO: Waiting up to 5m0s for pod "pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112" in namespace "secrets-5864" to be "success or failure"
Jul  9 03:10:57.357: INFO: Pod "pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112": Phase="Pending", Reason="", readiness=false. Elapsed: 12.6125ms
Jul  9 03:10:59.361: INFO: Pod "pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0165286s
STEP: Saw pod success
Jul  9 03:10:59.361: INFO: Pod "pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112" satisfied condition "success or failure"
Jul  9 03:10:59.364: INFO: Trying to get logs from node node2 pod pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:10:59.379: INFO: Waiting for pod pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112 to disappear
Jul  9 03:10:59.384: INFO: Pod pod-secrets-bb5cb3ec-4322-43d7-8e96-98e7060d3112 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:10:59.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5864" for this suite.
Jul  9 03:11:05.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:11:05.601: INFO: namespace secrets-5864 deletion completed in 6.2146743s

• [SLOW TEST:8.294 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:11:05.602: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-abf6465a-5882-489f-91fc-be5c1efc4831
STEP: Creating a pod to test consume secrets
Jul  9 03:11:05.636: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b" in namespace "projected-8551" to be "success or failure"
Jul  9 03:11:05.644: INFO: Pod "pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.5761ms
Jul  9 03:11:07.649: INFO: Pod "pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0126644s
STEP: Saw pod success
Jul  9 03:11:07.649: INFO: Pod "pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b" satisfied condition "success or failure"
Jul  9 03:11:07.656: INFO: Trying to get logs from node node2 pod pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:11:07.676: INFO: Waiting for pod pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b to disappear
Jul  9 03:11:07.679: INFO: Pod pod-projected-secrets-2d043ff3-c8d4-4bf0-8997-1ea028e14a6b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:11:07.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8551" for this suite.
Jul  9 03:11:13.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:11:13.894: INFO: namespace projected-8551 deletion completed in 6.212465s

• [SLOW TEST:8.292 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:11:13.894: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  9 03:11:13.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12772,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  9 03:11:13.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12772,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  9 03:11:23.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12787,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  9 03:11:23.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12787,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  9 03:11:33.954: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12804,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  9 03:11:33.954: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12804,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  9 03:11:43.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12819,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  9 03:11:43.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-a,UID:948f0c6f-d4d0-4935-a992-31cc7fbf3690,ResourceVersion:12819,Generation:0,CreationTimestamp:2019-07-09 03:11:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  9 03:11:53.964: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-b,UID:d8a9301b-b5ce-4195-a06b-2835a858b3d5,ResourceVersion:12834,Generation:0,CreationTimestamp:2019-07-09 03:11:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  9 03:11:53.964: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-b,UID:d8a9301b-b5ce-4195-a06b-2835a858b3d5,ResourceVersion:12834,Generation:0,CreationTimestamp:2019-07-09 03:11:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  9 03:12:03.972: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-b,UID:d8a9301b-b5ce-4195-a06b-2835a858b3d5,ResourceVersion:12850,Generation:0,CreationTimestamp:2019-07-09 03:11:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  9 03:12:03.972: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8549,SelfLink:/api/v1/namespaces/watch-8549/configmaps/e2e-watch-test-configmap-b,UID:d8a9301b-b5ce-4195-a06b-2835a858b3d5,ResourceVersion:12850,Generation:0,CreationTimestamp:2019-07-09 03:11:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:12:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8549" for this suite.
Jul  9 03:12:19.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:12:20.103: INFO: namespace watch-8549 deletion completed in 6.1284455s

• [SLOW TEST:66.209 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:12:20.103: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  9 03:12:20.139: INFO: Waiting up to 5m0s for pod "pod-027acd84-728d-4824-9eb7-1f09cd197832" in namespace "emptydir-7772" to be "success or failure"
Jul  9 03:12:20.142: INFO: Pod "pod-027acd84-728d-4824-9eb7-1f09cd197832": Phase="Pending", Reason="", readiness=false. Elapsed: 3.0032ms
Jul  9 03:12:22.145: INFO: Pod "pod-027acd84-728d-4824-9eb7-1f09cd197832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0064909s
STEP: Saw pod success
Jul  9 03:12:22.145: INFO: Pod "pod-027acd84-728d-4824-9eb7-1f09cd197832" satisfied condition "success or failure"
Jul  9 03:12:22.148: INFO: Trying to get logs from node node2 pod pod-027acd84-728d-4824-9eb7-1f09cd197832 container test-container: <nil>
STEP: delete the pod
Jul  9 03:12:22.163: INFO: Waiting for pod pod-027acd84-728d-4824-9eb7-1f09cd197832 to disappear
Jul  9 03:12:22.166: INFO: Pod pod-027acd84-728d-4824-9eb7-1f09cd197832 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:12:22.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7772" for this suite.
Jul  9 03:12:28.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:12:28.310: INFO: namespace emptydir-7772 deletion completed in 6.1413642s

• [SLOW TEST:8.207 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:12:28.310: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-ac1fc603-5f74-4113-aae9-daf9976871c6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ac1fc603-5f74-4113-aae9-daf9976871c6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:12:32.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4739" for this suite.
Jul  9 03:12:54.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:12:54.551: INFO: namespace configmap-4739 deletion completed in 22.1242213s

• [SLOW TEST:26.241 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:12:54.551: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:12:54.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0" in namespace "projected-5869" to be "success or failure"
Jul  9 03:12:54.589: INFO: Pod "downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0459ms
Jul  9 03:12:56.592: INFO: Pod "downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0063647s
STEP: Saw pod success
Jul  9 03:12:56.592: INFO: Pod "downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0" satisfied condition "success or failure"
Jul  9 03:12:56.594: INFO: Trying to get logs from node node2 pod downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0 container client-container: <nil>
STEP: delete the pod
Jul  9 03:12:56.614: INFO: Waiting for pod downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0 to disappear
Jul  9 03:12:56.615: INFO: Pod downwardapi-volume-880847f5-3d1b-4a5d-b914-4e5776ab2ea0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:12:56.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5869" for this suite.
Jul  9 03:13:02.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:13:02.751: INFO: namespace projected-5869 deletion completed in 6.1332075s

• [SLOW TEST:8.200 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:13:02.751: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul  9 03:13:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul  9 03:13:03.271: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul  9 03:13:05.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:07.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:09.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:11.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:13.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:15.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:17.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:19.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:21.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:23.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:25.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:27.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:29.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:31.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:33.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:35.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:37.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:39.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:41.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:43.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:45.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:47.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:49.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:51.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:53.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:55.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:57.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:13:59.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:01.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:03.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:05.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:07.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:09.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:11.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:13.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:15.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:17.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:19.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:21.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:23.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:25.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:27.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:29.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:31.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:33.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:35.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:37.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:39.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:41.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238783, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698238782, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  9 03:14:46.964: INFO: Waited 3.6249706s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:14:47.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3758" for this suite.
Jul  9 03:14:53.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:14:53.656: INFO: namespace aggregator-3758 deletion completed in 6.2640955s

• [SLOW TEST:110.905 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:14:53.656: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4754.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4754.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 03:16:09.710: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.713: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.716: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.733: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.735: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.738: INFO: Unable to read jessie_udp@PodARecord from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.741: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620: the server could not find the requested resource (get pods dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620)
Jul  9 03:16:09.741: INFO: Lookups using dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul  9 03:16:14.792: INFO: DNS probes using dns-4754/dns-test-4c3a8ded-7431-44a3-b04b-f71c0066e620 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:16:14.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4754" for this suite.
Jul  9 03:16:20.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:16:20.991: INFO: namespace dns-4754 deletion completed in 6.1769804s

• [SLOW TEST:87.335 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:16:20.991: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul  9 03:16:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-9882'
Jul  9 03:16:21.477: INFO: stderr: ""
Jul  9 03:16:21.477: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  9 03:16:22.490: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 03:16:22.490: INFO: Found 0 / 1
Jul  9 03:16:23.482: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 03:16:23.482: INFO: Found 1 / 1
Jul  9 03:16:23.482: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  9 03:16:23.488: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 03:16:23.488: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  9 03:16:23.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 patch pod redis-master-slp4j --namespace=kubectl-9882 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  9 03:16:23.546: INFO: stderr: ""
Jul  9 03:16:23.546: INFO: stdout: "pod/redis-master-slp4j patched\n"
STEP: checking annotations
Jul  9 03:16:23.550: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 03:16:23.550: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:16:23.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9882" for this suite.
Jul  9 03:16:45.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:16:45.775: INFO: namespace kubectl-9882 deletion completed in 22.2219451s

• [SLOW TEST:24.783 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:16:45.775: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:16:45.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62" in namespace "downward-api-225" to be "success or failure"
Jul  9 03:16:45.858: INFO: Pod "downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3397ms
Jul  9 03:16:47.860: INFO: Pod "downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0043092s
STEP: Saw pod success
Jul  9 03:16:47.860: INFO: Pod "downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62" satisfied condition "success or failure"
Jul  9 03:16:47.861: INFO: Trying to get logs from node node2 pod downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62 container client-container: <nil>
STEP: delete the pod
Jul  9 03:16:47.883: INFO: Waiting for pod downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62 to disappear
Jul  9 03:16:47.888: INFO: Pod downwardapi-volume-a8932238-dabc-4431-a4c7-79eae3fc6c62 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:16:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-225" for this suite.
Jul  9 03:16:53.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:16:54.048: INFO: namespace downward-api-225 deletion completed in 6.1518004s

• [SLOW TEST:8.273 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:16:54.048: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f8d21684-e046-4c3f-ace9-5b0a8c51750a
STEP: Creating a pod to test consume configMaps
Jul  9 03:16:54.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40" in namespace "configmap-6269" to be "success or failure"
Jul  9 03:16:54.098: INFO: Pod "pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7019ms
Jul  9 03:16:56.104: INFO: Pod "pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0126448s
STEP: Saw pod success
Jul  9 03:16:56.104: INFO: Pod "pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40" satisfied condition "success or failure"
Jul  9 03:16:56.106: INFO: Trying to get logs from node node2 pod pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:16:56.122: INFO: Waiting for pod pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40 to disappear
Jul  9 03:16:56.126: INFO: Pod pod-configmaps-218f1de1-4fec-4b2f-a7d2-a0aa1d4f0e40 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:16:56.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6269" for this suite.
Jul  9 03:17:02.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:17:02.295: INFO: namespace configmap-6269 deletion completed in 6.1658132s

• [SLOW TEST:8.247 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:17:02.295: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  9 03:17:02.561: INFO: Pod name wrapped-volume-race-ef0d0d99-4273-4b00-9432-b5840798d164: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ef0d0d99-4273-4b00-9432-b5840798d164 in namespace emptydir-wrapper-5064, will wait for the garbage collector to delete the pods
Jul  9 03:17:40.698: INFO: Deleting ReplicationController wrapped-volume-race-ef0d0d99-4273-4b00-9432-b5840798d164 took: 4.6663ms
Jul  9 03:17:40.998: INFO: Terminating ReplicationController wrapped-volume-race-ef0d0d99-4273-4b00-9432-b5840798d164 pods took: 300.2861ms
STEP: Creating RC which spawns configmap-volume pods
Jul  9 03:18:16.816: INFO: Pod name wrapped-volume-race-1b6e9d3f-2630-42ea-b289-cb689965dda9: Found 0 pods out of 5
Jul  9 03:18:21.827: INFO: Pod name wrapped-volume-race-1b6e9d3f-2630-42ea-b289-cb689965dda9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1b6e9d3f-2630-42ea-b289-cb689965dda9 in namespace emptydir-wrapper-5064, will wait for the garbage collector to delete the pods
Jul  9 03:18:35.913: INFO: Deleting ReplicationController wrapped-volume-race-1b6e9d3f-2630-42ea-b289-cb689965dda9 took: 3.673ms
Jul  9 03:18:36.213: INFO: Terminating ReplicationController wrapped-volume-race-1b6e9d3f-2630-42ea-b289-cb689965dda9 pods took: 300.2463ms
STEP: Creating RC which spawns configmap-volume pods
Jul  9 03:19:14.125: INFO: Pod name wrapped-volume-race-1521f9f7-7613-4553-8d73-88998a4cc90a: Found 0 pods out of 5
Jul  9 03:19:19.142: INFO: Pod name wrapped-volume-race-1521f9f7-7613-4553-8d73-88998a4cc90a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1521f9f7-7613-4553-8d73-88998a4cc90a in namespace emptydir-wrapper-5064, will wait for the garbage collector to delete the pods
Jul  9 03:19:31.242: INFO: Deleting ReplicationController wrapped-volume-race-1521f9f7-7613-4553-8d73-88998a4cc90a took: 8.7497ms
Jul  9 03:19:31.542: INFO: Terminating ReplicationController wrapped-volume-race-1521f9f7-7613-4553-8d73-88998a4cc90a pods took: 300.2692ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:20:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5064" for this suite.
Jul  9 03:20:21.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:20:21.569: INFO: namespace emptydir-wrapper-5064 deletion completed in 6.1551571s

• [SLOW TEST:199.275 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:20:21.570: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-cf2c1918-008c-4353-abfb-af0dfed4d2e6
STEP: Creating a pod to test consume secrets
Jul  9 03:20:21.725: INFO: Waiting up to 5m0s for pod "pod-secrets-00eee027-f131-46b4-a9c9-457000795776" in namespace "secrets-6482" to be "success or failure"
Jul  9 03:20:21.730: INFO: Pod "pod-secrets-00eee027-f131-46b4-a9c9-457000795776": Phase="Pending", Reason="", readiness=false. Elapsed: 4.9322ms
Jul  9 03:20:23.735: INFO: Pod "pod-secrets-00eee027-f131-46b4-a9c9-457000795776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0100796s
Jul  9 03:20:25.738: INFO: Pod "pod-secrets-00eee027-f131-46b4-a9c9-457000795776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0131923s
STEP: Saw pod success
Jul  9 03:20:25.738: INFO: Pod "pod-secrets-00eee027-f131-46b4-a9c9-457000795776" satisfied condition "success or failure"
Jul  9 03:20:25.740: INFO: Trying to get logs from node node2 pod pod-secrets-00eee027-f131-46b4-a9c9-457000795776 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:20:25.762: INFO: Waiting for pod pod-secrets-00eee027-f131-46b4-a9c9-457000795776 to disappear
Jul  9 03:20:25.764: INFO: Pod pod-secrets-00eee027-f131-46b4-a9c9-457000795776 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:20:25.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6482" for this suite.
Jul  9 03:20:31.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:20:31.953: INFO: namespace secrets-6482 deletion completed in 6.1825766s
STEP: Destroying namespace "secret-namespace-8857" for this suite.
Jul  9 03:20:37.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:20:38.141: INFO: namespace secret-namespace-8857 deletion completed in 6.1875056s

• [SLOW TEST:16.571 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:20:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul  9 03:20:38.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-585'
Jul  9 03:20:38.424: INFO: stderr: ""
Jul  9 03:20:38.424: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 03:20:38.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:38.564: INFO: stderr: ""
Jul  9 03:20:38.564: INFO: stdout: "update-demo-nautilus-8q5jp update-demo-nautilus-fpchf "
Jul  9 03:20:38.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:38.686: INFO: stderr: ""
Jul  9 03:20:38.686: INFO: stdout: ""
Jul  9 03:20:38.686: INFO: update-demo-nautilus-8q5jp is created but not running
Jul  9 03:20:43.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:43.748: INFO: stderr: ""
Jul  9 03:20:43.748: INFO: stdout: "update-demo-nautilus-8q5jp update-demo-nautilus-fpchf "
Jul  9 03:20:43.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:43.812: INFO: stderr: ""
Jul  9 03:20:43.812: INFO: stdout: "true"
Jul  9 03:20:43.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:43.884: INFO: stderr: ""
Jul  9 03:20:43.884: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:43.884: INFO: validating pod update-demo-nautilus-8q5jp
Jul  9 03:20:43.898: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:43.898: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:43.898: INFO: update-demo-nautilus-8q5jp is verified up and running
Jul  9 03:20:43.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-fpchf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:43.967: INFO: stderr: ""
Jul  9 03:20:43.967: INFO: stdout: "true"
Jul  9 03:20:43.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-fpchf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:44.050: INFO: stderr: ""
Jul  9 03:20:44.050: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:44.050: INFO: validating pod update-demo-nautilus-fpchf
Jul  9 03:20:44.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:44.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:44.059: INFO: update-demo-nautilus-fpchf is verified up and running
STEP: scaling down the replication controller
Jul  9 03:20:44.060: INFO: scanned /root for discovery docs: <nil>
Jul  9 03:20:44.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-585'
Jul  9 03:20:45.167: INFO: stderr: ""
Jul  9 03:20:45.167: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 03:20:45.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:45.289: INFO: stderr: ""
Jul  9 03:20:45.289: INFO: stdout: "update-demo-nautilus-8q5jp update-demo-nautilus-fpchf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  9 03:20:50.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:50.356: INFO: stderr: ""
Jul  9 03:20:50.356: INFO: stdout: "update-demo-nautilus-8q5jp "
Jul  9 03:20:50.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:50.422: INFO: stderr: ""
Jul  9 03:20:50.422: INFO: stdout: "true"
Jul  9 03:20:50.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:50.499: INFO: stderr: ""
Jul  9 03:20:50.499: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:50.499: INFO: validating pod update-demo-nautilus-8q5jp
Jul  9 03:20:50.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:50.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:50.503: INFO: update-demo-nautilus-8q5jp is verified up and running
STEP: scaling up the replication controller
Jul  9 03:20:50.504: INFO: scanned /root for discovery docs: <nil>
Jul  9 03:20:50.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-585'
Jul  9 03:20:51.613: INFO: stderr: ""
Jul  9 03:20:51.613: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 03:20:51.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:51.683: INFO: stderr: ""
Jul  9 03:20:51.683: INFO: stdout: "update-demo-nautilus-8q5jp update-demo-nautilus-bwts9 "
Jul  9 03:20:51.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:51.759: INFO: stderr: ""
Jul  9 03:20:51.759: INFO: stdout: "true"
Jul  9 03:20:51.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:51.837: INFO: stderr: ""
Jul  9 03:20:51.837: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:51.837: INFO: validating pod update-demo-nautilus-8q5jp
Jul  9 03:20:51.842: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:51.842: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:51.842: INFO: update-demo-nautilus-8q5jp is verified up and running
Jul  9 03:20:51.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-bwts9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:51.926: INFO: stderr: ""
Jul  9 03:20:51.926: INFO: stdout: ""
Jul  9 03:20:51.926: INFO: update-demo-nautilus-bwts9 is created but not running
Jul  9 03:20:56.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-585'
Jul  9 03:20:57.021: INFO: stderr: ""
Jul  9 03:20:57.021: INFO: stdout: "update-demo-nautilus-8q5jp update-demo-nautilus-bwts9 "
Jul  9 03:20:57.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:57.081: INFO: stderr: ""
Jul  9 03:20:57.081: INFO: stdout: "true"
Jul  9 03:20:57.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-8q5jp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:57.143: INFO: stderr: ""
Jul  9 03:20:57.143: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:57.143: INFO: validating pod update-demo-nautilus-8q5jp
Jul  9 03:20:57.147: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:57.147: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:57.147: INFO: update-demo-nautilus-8q5jp is verified up and running
Jul  9 03:20:57.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-bwts9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:57.219: INFO: stderr: ""
Jul  9 03:20:57.219: INFO: stdout: "true"
Jul  9 03:20:57.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-bwts9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-585'
Jul  9 03:20:57.283: INFO: stderr: ""
Jul  9 03:20:57.283: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:20:57.283: INFO: validating pod update-demo-nautilus-bwts9
Jul  9 03:20:57.290: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:20:57.290: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:20:57.290: INFO: update-demo-nautilus-bwts9 is verified up and running
STEP: using delete to clean up resources
Jul  9 03:20:57.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-585'
Jul  9 03:20:57.359: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 03:20:57.359: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  9 03:20:57.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-585'
Jul  9 03:20:57.435: INFO: stderr: "No resources found.\n"
Jul  9 03:20:57.435: INFO: stdout: ""
Jul  9 03:20:57.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=update-demo --namespace=kubectl-585 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 03:20:57.506: INFO: stderr: ""
Jul  9 03:20:57.506: INFO: stdout: "update-demo-nautilus-8q5jp\nupdate-demo-nautilus-bwts9\n"
Jul  9 03:20:58.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-585'
Jul  9 03:20:58.094: INFO: stderr: "No resources found.\n"
Jul  9 03:20:58.094: INFO: stdout: ""
Jul  9 03:20:58.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=update-demo --namespace=kubectl-585 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 03:20:58.168: INFO: stderr: ""
Jul  9 03:20:58.168: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:20:58.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-585" for this suite.
Jul  9 03:21:20.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:21:20.371: INFO: namespace kubectl-585 deletion completed in 22.2006421s

• [SLOW TEST:42.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:21:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:21:20.482: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"dd5a3394-bcbf-4a18-8f9c-134156c862b5", Controller:(*bool)(0xc003946156), BlockOwnerDeletion:(*bool)(0xc003946157)}}
Jul  9 03:21:20.507: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8c608449-9a37-4261-a9b2-1feba26a9f07", Controller:(*bool)(0xc0038b1226), BlockOwnerDeletion:(*bool)(0xc0038b1227)}}
Jul  9 03:21:20.522: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"06dadffc-1b60-4459-a835-047d53a202e7", Controller:(*bool)(0xc00394636e), BlockOwnerDeletion:(*bool)(0xc00394636f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:21:25.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5985" for this suite.
Jul  9 03:21:31.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:21:31.684: INFO: namespace gc-5985 deletion completed in 6.1526582s

• [SLOW TEST:11.313 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:21:31.684: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6744.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6744.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6744.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6744.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6744.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6744.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 03:22:45.812: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.818: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.848: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-6744.svc.cluster.local from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.852: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.860: INFO: Unable to read jessie_udp@PodARecord from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.868: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6: the server could not find the requested resource (get pods dns-test-3626a448-4dc4-4165-896f-53302d19daf6)
Jul  9 03:22:45.868: INFO: Lookups using dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-6744.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul  9 03:22:50.966: INFO: DNS probes using dns-6744/dns-test-3626a448-4dc4-4165-896f-53302d19daf6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:22:50.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6744" for this suite.
Jul  9 03:22:56.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:22:57.106: INFO: namespace dns-6744 deletion completed in 6.1189165s

• [SLOW TEST:85.421 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:22:57.106: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul  9 03:22:57.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 cluster-info'
Jul  9 03:22:57.239: INFO: stderr: ""
Jul  9 03:22:57.239: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:22:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4957" for this suite.
Jul  9 03:23:03.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:03.387: INFO: namespace kubectl-4957 deletion completed in 6.1418763s

• [SLOW TEST:6.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  9 03:23:03.413: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  9 03:23:03.418: INFO: Waiting for terminating namespaces to be deleted...
Jul  9 03:23:03.419: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jul  9 03:23:03.442: INFO: kube-proxy-dsfg4 from kube-system started at 2019-07-09 02:08:06 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 03:23:03.442: INFO: sdspaas-ns-controller-5bf85d9c4b-mrzhn from kube-system started at 2019-07-09 02:11:14 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container sdspaas-ns-controller ready: true, restart count 0
Jul  9 03:23:03.442: INFO: weave-net-wl5x5 from kube-system started at 2019-07-09 01:52:47 +0000 UTC (2 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container weave ready: true, restart count 3
Jul  9 03:23:03.442: INFO: 	Container weave-npc ready: true, restart count 2
Jul  9 03:23:03.442: INFO: coredns-7d66d4b8cd-bpcx9 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container coredns ready: true, restart count 2
Jul  9 03:23:03.442: INFO: sonobuoy-e2e-job-daa7340bb2c14627 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container e2e ready: true, restart count 0
Jul  9 03:23:03.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:23:03.442: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-gjrs2 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:23:03.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:23:03.442: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  9 03:23:03.442: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jul  9 03:23:03.450: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-zgdws from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 03:23:03.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 03:23:03.450: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  9 03:23:03.450: INFO: kube-proxy-4bhgt from kube-system started at 2019-07-09 02:07:56 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.450: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 03:23:03.450: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-09 02:39:33 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.450: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  9 03:23:03.450: INFO: weave-net-zfhsl from kube-system started at 2019-07-09 01:53:41 +0000 UTC (2 container statuses recorded)
Jul  9 03:23:03.450: INFO: 	Container weave ready: true, restart count 3
Jul  9 03:23:03.450: INFO: 	Container weave-npc ready: true, restart count 2
Jul  9 03:23:03.450: INFO: coredns-7d66d4b8cd-6wkt8 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 03:23:03.450: INFO: 	Container coredns ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15af9f2671fefb68], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:04.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1543" for this suite.
Jul  9 03:23:10.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:10.696: INFO: namespace sched-pred-1543 deletion completed in 6.2249116s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.309 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  9 03:23:10.741: INFO: Waiting up to 5m0s for pod "pod-5d902079-843a-4902-b394-ab52d2f2aefb" in namespace "emptydir-4885" to be "success or failure"
Jul  9 03:23:10.745: INFO: Pod "pod-5d902079-843a-4902-b394-ab52d2f2aefb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8639ms
Jul  9 03:23:12.750: INFO: Pod "pod-5d902079-843a-4902-b394-ab52d2f2aefb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0086316s
STEP: Saw pod success
Jul  9 03:23:12.750: INFO: Pod "pod-5d902079-843a-4902-b394-ab52d2f2aefb" satisfied condition "success or failure"
Jul  9 03:23:12.752: INFO: Trying to get logs from node node2 pod pod-5d902079-843a-4902-b394-ab52d2f2aefb container test-container: <nil>
STEP: delete the pod
Jul  9 03:23:12.770: INFO: Waiting for pod pod-5d902079-843a-4902-b394-ab52d2f2aefb to disappear
Jul  9 03:23:12.779: INFO: Pod pod-5d902079-843a-4902-b394-ab52d2f2aefb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4885" for this suite.
Jul  9 03:23:18.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:18.943: INFO: namespace emptydir-4885 deletion completed in 6.1613184s

• [SLOW TEST:8.247 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:18.943: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  9 03:23:18.985: INFO: Waiting up to 5m0s for pod "pod-b973b4e2-2daf-497c-b564-cde639647db0" in namespace "emptydir-8079" to be "success or failure"
Jul  9 03:23:18.998: INFO: Pod "pod-b973b4e2-2daf-497c-b564-cde639647db0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.7197ms
Jul  9 03:23:21.006: INFO: Pod "pod-b973b4e2-2daf-497c-b564-cde639647db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0213596s
STEP: Saw pod success
Jul  9 03:23:21.006: INFO: Pod "pod-b973b4e2-2daf-497c-b564-cde639647db0" satisfied condition "success or failure"
Jul  9 03:23:21.008: INFO: Trying to get logs from node node2 pod pod-b973b4e2-2daf-497c-b564-cde639647db0 container test-container: <nil>
STEP: delete the pod
Jul  9 03:23:21.021: INFO: Waiting for pod pod-b973b4e2-2daf-497c-b564-cde639647db0 to disappear
Jul  9 03:23:21.023: INFO: Pod pod-b973b4e2-2daf-497c-b564-cde639647db0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8079" for this suite.
Jul  9 03:23:27.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:27.280: INFO: namespace emptydir-8079 deletion completed in 6.2508473s

• [SLOW TEST:8.337 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:27.280: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-87bc2a0d-12e0-4e0c-8484-8b3d457207aa
STEP: Creating a pod to test consume configMaps
Jul  9 03:23:27.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3" in namespace "configmap-6800" to be "success or failure"
Jul  9 03:23:27.319: INFO: Pod "pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5054ms
Jul  9 03:23:29.336: INFO: Pod "pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0228921s
Jul  9 03:23:31.346: INFO: Pod "pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0322763s
STEP: Saw pod success
Jul  9 03:23:31.346: INFO: Pod "pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3" satisfied condition "success or failure"
Jul  9 03:23:31.349: INFO: Trying to get logs from node node2 pod pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:23:31.377: INFO: Waiting for pod pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3 to disappear
Jul  9 03:23:31.388: INFO: Pod pod-configmaps-07b8f6c1-10f3-43ba-8df6-a584696852a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6800" for this suite.
Jul  9 03:23:37.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:37.554: INFO: namespace configmap-6800 deletion completed in 6.1614031s

• [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-30bac42d-4b03-4f59-9dc4-5da4629b49c1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:37.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1288" for this suite.
Jul  9 03:23:43.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:43.751: INFO: namespace configmap-1288 deletion completed in 6.1469245s

• [SLOW TEST:6.196 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:43.751: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:23:43.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41" in namespace "downward-api-1282" to be "success or failure"
Jul  9 03:23:43.798: INFO: Pod "downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0645ms
Jul  9 03:23:45.800: INFO: Pod "downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0084669s
Jul  9 03:23:47.805: INFO: Pod "downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0133218s
STEP: Saw pod success
Jul  9 03:23:47.805: INFO: Pod "downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41" satisfied condition "success or failure"
Jul  9 03:23:47.811: INFO: Trying to get logs from node node2 pod downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41 container client-container: <nil>
STEP: delete the pod
Jul  9 03:23:47.827: INFO: Waiting for pod downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41 to disappear
Jul  9 03:23:47.830: INFO: Pod downwardapi-volume-5e6f3ceb-2ec6-461e-a649-1a2e94ed8f41 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:47.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1282" for this suite.
Jul  9 03:23:53.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:23:53.997: INFO: namespace downward-api-1282 deletion completed in 6.1610604s

• [SLOW TEST:10.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:23:53.997: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul  9 03:23:54.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-1961'
Jul  9 03:23:54.141: INFO: stderr: ""
Jul  9 03:23:54.141: INFO: stdout: "pod/pause created\n"
Jul  9 03:23:54.141: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  9 03:23:54.141: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1961" to be "running and ready"
Jul  9 03:23:54.146: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059ms
Jul  9 03:23:56.149: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.0078812s
Jul  9 03:23:56.149: INFO: Pod "pause" satisfied condition "running and ready"
Jul  9 03:23:56.149: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  9 03:23:56.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 label pods pause testing-label=testing-label-value --namespace=kubectl-1961'
Jul  9 03:23:56.207: INFO: stderr: ""
Jul  9 03:23:56.207: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  9 03:23:56.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pod pause -L testing-label --namespace=kubectl-1961'
Jul  9 03:23:56.273: INFO: stderr: ""
Jul  9 03:23:56.273: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  9 03:23:56.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 label pods pause testing-label- --namespace=kubectl-1961'
Jul  9 03:23:56.340: INFO: stderr: ""
Jul  9 03:23:56.340: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  9 03:23:56.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pod pause -L testing-label --namespace=kubectl-1961'
Jul  9 03:23:56.398: INFO: stderr: ""
Jul  9 03:23:56.398: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul  9 03:23:56.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-1961'
Jul  9 03:23:56.468: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 03:23:56.468: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  9 03:23:56.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=pause --no-headers --namespace=kubectl-1961'
Jul  9 03:23:56.546: INFO: stderr: "No resources found.\n"
Jul  9 03:23:56.546: INFO: stdout: ""
Jul  9 03:23:56.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=pause --namespace=kubectl-1961 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 03:23:56.621: INFO: stderr: ""
Jul  9 03:23:56.621: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:23:56.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1961" for this suite.
Jul  9 03:24:02.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:24:02.793: INFO: namespace kubectl-1961 deletion completed in 6.1695668s

• [SLOW TEST:8.796 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:24:02.793: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-a5841c43-86ce-4f9c-ab3b-ac51f41bda87
STEP: Creating a pod to test consume configMaps
Jul  9 03:24:02.834: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c" in namespace "projected-7246" to be "success or failure"
Jul  9 03:24:02.841: INFO: Pod "pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615ms
Jul  9 03:24:04.846: INFO: Pod "pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0114781s
STEP: Saw pod success
Jul  9 03:24:04.846: INFO: Pod "pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c" satisfied condition "success or failure"
Jul  9 03:24:04.848: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:24:04.866: INFO: Waiting for pod pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c to disappear
Jul  9 03:24:04.871: INFO: Pod pod-projected-configmaps-43213971-50e6-4a72-acdc-c4eeb536f78c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:24:04.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7246" for this suite.
Jul  9 03:24:10.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:24:11.052: INFO: namespace projected-7246 deletion completed in 6.1781265s

• [SLOW TEST:8.259 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:24:11.052: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  9 03:24:11.080: INFO: Waiting up to 5m0s for pod "pod-8d8797d4-6110-4077-ae87-ac388215bfdd" in namespace "emptydir-9204" to be "success or failure"
Jul  9 03:24:11.085: INFO: Pod "pod-8d8797d4-6110-4077-ae87-ac388215bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5719ms
Jul  9 03:24:13.091: INFO: Pod "pod-8d8797d4-6110-4077-ae87-ac388215bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0110738s
Jul  9 03:24:15.094: INFO: Pod "pod-8d8797d4-6110-4077-ae87-ac388215bfdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0139155s
STEP: Saw pod success
Jul  9 03:24:15.094: INFO: Pod "pod-8d8797d4-6110-4077-ae87-ac388215bfdd" satisfied condition "success or failure"
Jul  9 03:24:15.100: INFO: Trying to get logs from node node2 pod pod-8d8797d4-6110-4077-ae87-ac388215bfdd container test-container: <nil>
STEP: delete the pod
Jul  9 03:24:15.127: INFO: Waiting for pod pod-8d8797d4-6110-4077-ae87-ac388215bfdd to disappear
Jul  9 03:24:15.133: INFO: Pod pod-8d8797d4-6110-4077-ae87-ac388215bfdd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:24:15.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9204" for this suite.
Jul  9 03:24:21.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:24:21.313: INFO: namespace emptydir-9204 deletion completed in 6.1746611s

• [SLOW TEST:10.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:24:21.313: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  9 03:24:23.907: INFO: Successfully updated pod "annotationupdatefa6eb877-656d-45a0-8ebf-7c67ea3e57e7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:24:25.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-81" for this suite.
Jul  9 03:24:55.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:24:56.100: INFO: namespace downward-api-81 deletion completed in 30.1623637s

• [SLOW TEST:34.786 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:24:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 03:24:58.158: INFO: File wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-8f6c72cb-8dda-4f31-97c5-06eabf5da344 contains '' instead of 'foo.example.com.'
Jul  9 03:24:58.160: INFO: File jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-8f6c72cb-8dda-4f31-97c5-06eabf5da344 contains '' instead of 'foo.example.com.'
Jul  9 03:24:58.160: INFO: Lookups using dns-3974/dns-test-8f6c72cb-8dda-4f31-97c5-06eabf5da344 failed for: [wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local]

Jul  9 03:25:03.170: INFO: DNS probes using dns-test-8f6c72cb-8dda-4f31-97c5-06eabf5da344 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 03:25:07.241: INFO: File wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-144bcdaf-0e49-479d-a6a2-7397e31333e2 contains '' instead of 'bar.example.com.'
Jul  9 03:25:07.246: INFO: File jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-144bcdaf-0e49-479d-a6a2-7397e31333e2 contains '' instead of 'bar.example.com.'
Jul  9 03:25:07.246: INFO: Lookups using dns-3974/dns-test-144bcdaf-0e49-479d-a6a2-7397e31333e2 failed for: [wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local]

Jul  9 03:25:12.255: INFO: DNS probes using dns-test-144bcdaf-0e49-479d-a6a2-7397e31333e2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3974.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 03:25:14.324: INFO: File wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-fc2bc791-0cb4-458e-a70b-fb4ec28d2d87 contains '' instead of '172.24.235.12'
Jul  9 03:25:14.328: INFO: File jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local from pod  dns-3974/dns-test-fc2bc791-0cb4-458e-a70b-fb4ec28d2d87 contains '' instead of '172.24.235.12'
Jul  9 03:25:14.328: INFO: Lookups using dns-3974/dns-test-fc2bc791-0cb4-458e-a70b-fb4ec28d2d87 failed for: [wheezy_udp@dns-test-service-3.dns-3974.svc.cluster.local jessie_udp@dns-test-service-3.dns-3974.svc.cluster.local]

Jul  9 03:25:19.342: INFO: DNS probes using dns-test-fc2bc791-0cb4-458e-a70b-fb4ec28d2d87 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:25:19.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3974" for this suite.
Jul  9 03:25:25.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:25:25.547: INFO: namespace dns-3974 deletion completed in 6.1611484s

• [SLOW TEST:29.447 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:25:25.547: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul  9 03:25:25.583: INFO: Waiting up to 5m0s for pod "client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c" in namespace "containers-8587" to be "success or failure"
Jul  9 03:25:25.586: INFO: Pod "client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9758ms
Jul  9 03:25:27.591: INFO: Pod "client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0072826s
STEP: Saw pod success
Jul  9 03:25:27.591: INFO: Pod "client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c" satisfied condition "success or failure"
Jul  9 03:25:27.593: INFO: Trying to get logs from node node2 pod client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c container test-container: <nil>
STEP: delete the pod
Jul  9 03:25:27.609: INFO: Waiting for pod client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c to disappear
Jul  9 03:25:27.618: INFO: Pod client-containers-66bd275b-5a9c-4446-92f6-d07c02fa7d6c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:25:27.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8587" for this suite.
Jul  9 03:25:33.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:25:33.799: INFO: namespace containers-8587 deletion completed in 6.1787782s

• [SLOW TEST:8.252 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:25:33.799: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f674b28e-0f26-481f-b249-e08b4133115c
STEP: Creating a pod to test consume secrets
Jul  9 03:25:33.889: INFO: Waiting up to 5m0s for pod "pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088" in namespace "secrets-4468" to be "success or failure"
Jul  9 03:25:33.892: INFO: Pod "pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7921ms
Jul  9 03:25:35.896: INFO: Pod "pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0065869s
Jul  9 03:25:37.902: INFO: Pod "pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0129775s
STEP: Saw pod success
Jul  9 03:25:37.902: INFO: Pod "pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088" satisfied condition "success or failure"
Jul  9 03:25:37.904: INFO: Trying to get logs from node node2 pod pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:25:37.931: INFO: Waiting for pod pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088 to disappear
Jul  9 03:25:37.941: INFO: Pod pod-secrets-134d1f68-6075-4e6f-9cc4-d2d19adeb088 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:25:37.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4468" for this suite.
Jul  9 03:25:43.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:25:44.107: INFO: namespace secrets-4468 deletion completed in 6.1557069s

• [SLOW TEST:10.308 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:25:44.107: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  9 03:25:44.139: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:25:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9045" for this suite.
Jul  9 03:25:54.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:25:54.240: INFO: namespace init-container-9045 deletion completed in 6.1279509s

• [SLOW TEST:10.133 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:25:54.240: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d05006ca-a497-423e-8b20-c711ea8d2ae1 in namespace container-probe-7654
Jul  9 03:25:56.283: INFO: Started pod busybox-d05006ca-a497-423e-8b20-c711ea8d2ae1 in namespace container-probe-7654
STEP: checking the pod's current state and verifying that restartCount is present
Jul  9 03:25:56.286: INFO: Initial restart count of pod busybox-d05006ca-a497-423e-8b20-c711ea8d2ae1 is 0
Jul  9 03:26:46.451: INFO: Restart count of pod container-probe-7654/busybox-d05006ca-a497-423e-8b20-c711ea8d2ae1 is now 1 (50.1654891s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:26:46.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7654" for this suite.
Jul  9 03:26:52.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:26:52.669: INFO: namespace container-probe-7654 deletion completed in 6.2067731s

• [SLOW TEST:58.429 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:26:52.669: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 03:26:52.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6863'
Jul  9 03:26:53.090: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  9 03:26:53.090: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul  9 03:26:53.102: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-lzf5g]
Jul  9 03:26:53.102: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-lzf5g" in namespace "kubectl-6863" to be "running and ready"
Jul  9 03:26:53.105: INFO: Pod "e2e-test-nginx-rc-lzf5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7218ms
Jul  9 03:26:55.107: INFO: Pod "e2e-test-nginx-rc-lzf5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004789s
Jul  9 03:26:57.111: INFO: Pod "e2e-test-nginx-rc-lzf5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.0087103s
Jul  9 03:26:57.111: INFO: Pod "e2e-test-nginx-rc-lzf5g" satisfied condition "running and ready"
Jul  9 03:26:57.111: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-lzf5g]
Jul  9 03:26:57.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 logs rc/e2e-test-nginx-rc --namespace=kubectl-6863'
Jul  9 03:26:57.202: INFO: stderr: ""
Jul  9 03:26:57.202: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul  9 03:26:57.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete rc e2e-test-nginx-rc --namespace=kubectl-6863'
Jul  9 03:26:57.270: INFO: stderr: ""
Jul  9 03:26:57.270: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:26:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6863" for this suite.
Jul  9 03:27:19.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:27:19.439: INFO: namespace kubectl-6863 deletion completed in 22.165579s

• [SLOW TEST:26.770 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:27:19.439: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul  9 03:27:19.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 api-versions'
Jul  9 03:27:19.543: INFO: stderr: ""
Jul  9 03:27:19.543: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:27:19.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5643" for this suite.
Jul  9 03:27:25.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:27:25.695: INFO: namespace kubectl-5643 deletion completed in 6.1488922s

• [SLOW TEST:6.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:27:25.696: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e1ebc442-e46b-487a-acb9-e123ee2d4b50
STEP: Creating configMap with name cm-test-opt-upd-97056db3-b142-4c61-be88-bea12575ed53
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e1ebc442-e46b-487a-acb9-e123ee2d4b50
STEP: Updating configmap cm-test-opt-upd-97056db3-b142-4c61-be88-bea12575ed53
STEP: Creating configMap with name cm-test-opt-create-9ddf7899-1ff0-4a45-80f4-3cc6562a2ff7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:28:52.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6373" for this suite.
Jul  9 03:29:14.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:29:14.484: INFO: namespace configmap-6373 deletion completed in 22.1635412s

• [SLOW TEST:108.788 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:29:14.484: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:29:16.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4418" for this suite.
Jul  9 03:30:06.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:30:06.752: INFO: namespace kubelet-test-4418 deletion completed in 50.1523995s

• [SLOW TEST:52.268 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:30:06.752: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  9 03:30:09.323: INFO: Successfully updated pod "labelsupdate8abef34b-3ba6-4b61-a450-27cd903946d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:30:11.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-138" for this suite.
Jul  9 03:30:33.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:30:33.522: INFO: namespace downward-api-138 deletion completed in 22.1692923s

• [SLOW TEST:26.770 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:30:33.522: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:30:33.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3" in namespace "projected-3631" to be "success or failure"
Jul  9 03:30:33.568: INFO: Pod "downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.0513ms
Jul  9 03:30:35.581: INFO: Pod "downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0175148s
STEP: Saw pod success
Jul  9 03:30:35.581: INFO: Pod "downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3" satisfied condition "success or failure"
Jul  9 03:30:35.584: INFO: Trying to get logs from node node2 pod downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3 container client-container: <nil>
STEP: delete the pod
Jul  9 03:30:35.602: INFO: Waiting for pod downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3 to disappear
Jul  9 03:30:35.605: INFO: Pod downwardapi-volume-9d19e7ab-0b29-4c07-9b38-e8e84f285cf3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:30:35.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3631" for this suite.
Jul  9 03:30:41.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:30:41.757: INFO: namespace projected-3631 deletion completed in 6.1465214s

• [SLOW TEST:8.235 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:30:41.757: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul  9 03:31:21.848: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 64
	[quantile=0.99] = 64
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 7
	[quantile=0.9] = 205318
	[quantile=0.99] = 205318
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 4
	[quantile=0.99] = 4
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 287711
	[quantile=0.9] = 287711
	[quantile=0.99] = 287711
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 4
	[quantile=0.99] = 31
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 8
	[quantile=0.9] = 14
	[quantile=0.99] = 40
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 27
	[quantile=0.99] = 28
For namespace_queue_latency_sum:
	[] = 2232
For namespace_queue_latency_count:
	[] = 193
For namespace_retries:
	[] = 199
For namespace_work_duration:
	[quantile=0.5] = 96952
	[quantile=0.9] = 140800
	[quantile=0.99] = 225500
For namespace_work_duration_sum:
	[] = 23604249
For namespace_work_duration_count:
	[] = 193
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:31:21.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-676" for this suite.
Jul  9 03:31:27.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:31:28.007: INFO: namespace gc-676 deletion completed in 6.1551154s

• [SLOW TEST:46.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:31:28.007: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  9 03:31:28.053: INFO: Waiting up to 5m0s for pod "pod-a53931b0-e892-445f-9eaa-3ce86f465325" in namespace "emptydir-304" to be "success or failure"
Jul  9 03:31:28.061: INFO: Pod "pod-a53931b0-e892-445f-9eaa-3ce86f465325": Phase="Pending", Reason="", readiness=false. Elapsed: 8.4226ms
Jul  9 03:31:30.065: INFO: Pod "pod-a53931b0-e892-445f-9eaa-3ce86f465325": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0116064s
Jul  9 03:31:32.068: INFO: Pod "pod-a53931b0-e892-445f-9eaa-3ce86f465325": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0155206s
STEP: Saw pod success
Jul  9 03:31:32.069: INFO: Pod "pod-a53931b0-e892-445f-9eaa-3ce86f465325" satisfied condition "success or failure"
Jul  9 03:31:32.070: INFO: Trying to get logs from node node2 pod pod-a53931b0-e892-445f-9eaa-3ce86f465325 container test-container: <nil>
STEP: delete the pod
Jul  9 03:31:32.084: INFO: Waiting for pod pod-a53931b0-e892-445f-9eaa-3ce86f465325 to disappear
Jul  9 03:31:32.087: INFO: Pod pod-a53931b0-e892-445f-9eaa-3ce86f465325 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:31:32.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-304" for this suite.
Jul  9 03:31:38.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:31:38.276: INFO: namespace emptydir-304 deletion completed in 6.1867094s

• [SLOW TEST:10.268 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:31:38.276: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-3083
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3083 to expose endpoints map[]
Jul  9 03:31:38.345: INFO: successfully validated that service multi-endpoint-test in namespace services-3083 exposes endpoints map[] (12.0566ms elapsed)
STEP: Creating pod pod1 in namespace services-3083
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3083 to expose endpoints map[pod1:[100]]
Jul  9 03:31:41.414: INFO: successfully validated that service multi-endpoint-test in namespace services-3083 exposes endpoints map[pod1:[100]] (3.0437003s elapsed)
STEP: Creating pod pod2 in namespace services-3083
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3083 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  9 03:31:43.473: INFO: successfully validated that service multi-endpoint-test in namespace services-3083 exposes endpoints map[pod1:[100] pod2:[101]] (2.0554643s elapsed)
STEP: Deleting pod pod1 in namespace services-3083
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3083 to expose endpoints map[pod2:[101]]
Jul  9 03:31:44.506: INFO: successfully validated that service multi-endpoint-test in namespace services-3083 exposes endpoints map[pod2:[101]] (1.0278797s elapsed)
STEP: Deleting pod pod2 in namespace services-3083
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3083 to expose endpoints map[]
Jul  9 03:31:44.517: INFO: successfully validated that service multi-endpoint-test in namespace services-3083 exposes endpoints map[] (7.168ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:31:44.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3083" for this suite.
Jul  9 03:32:06.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:32:06.692: INFO: namespace services-3083 deletion completed in 22.1574814s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:28.416 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:32:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  9 03:32:06.726: INFO: Waiting up to 5m0s for pod "downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94" in namespace "downward-api-4786" to be "success or failure"
Jul  9 03:32:06.745: INFO: Pod "downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94": Phase="Pending", Reason="", readiness=false. Elapsed: 18.9855ms
Jul  9 03:32:08.750: INFO: Pod "downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0244315s
STEP: Saw pod success
Jul  9 03:32:08.750: INFO: Pod "downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94" satisfied condition "success or failure"
Jul  9 03:32:08.753: INFO: Trying to get logs from node node2 pod downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94 container dapi-container: <nil>
STEP: delete the pod
Jul  9 03:32:08.771: INFO: Waiting for pod downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94 to disappear
Jul  9 03:32:08.775: INFO: Pod downward-api-39d9d337-1b66-4bf1-b522-6fcf7a80df94 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:32:08.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4786" for this suite.
Jul  9 03:32:14.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:32:14.937: INFO: namespace downward-api-4786 deletion completed in 6.159529s

• [SLOW TEST:8.245 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:32:14.937: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-65a46900-b147-4885-ac55-7954d327b9b6
STEP: Creating secret with name s-test-opt-upd-ee321a76-29ee-4b7b-8590-677883826c75
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-65a46900-b147-4885-ac55-7954d327b9b6
STEP: Updating secret s-test-opt-upd-ee321a76-29ee-4b7b-8590-677883826c75
STEP: Creating secret with name s-test-opt-create-50bf5ce5-6392-4f64-adea-c64dfe545864
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:32:19.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4725" for this suite.
Jul  9 03:32:41.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:32:41.272: INFO: namespace secrets-4725 deletion completed in 22.1670795s

• [SLOW TEST:26.334 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:32:41.272: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:32:41.344: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84" in namespace "projected-9559" to be "success or failure"
Jul  9 03:32:41.354: INFO: Pod "downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0717ms
Jul  9 03:32:43.358: INFO: Pod "downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014181s
Jul  9 03:32:45.360: INFO: Pod "downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0160673s
STEP: Saw pod success
Jul  9 03:32:45.360: INFO: Pod "downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84" satisfied condition "success or failure"
Jul  9 03:32:45.362: INFO: Trying to get logs from node node2 pod downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84 container client-container: <nil>
STEP: delete the pod
Jul  9 03:32:45.377: INFO: Waiting for pod downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84 to disappear
Jul  9 03:32:45.381: INFO: Pod downwardapi-volume-3ffd213e-89d4-41e3-8ba8-735189196d84 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:32:45.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9559" for this suite.
Jul  9 03:32:51.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:32:51.539: INFO: namespace projected-9559 deletion completed in 6.1545775s

• [SLOW TEST:10.266 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:32:51.539: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:32:59.607: INFO: Waiting up to 5m0s for pod "client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3" in namespace "pods-4730" to be "success or failure"
Jul  9 03:32:59.611: INFO: Pod "client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1831ms
Jul  9 03:33:01.618: INFO: Pod "client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0112364s
Jul  9 03:33:03.624: INFO: Pod "client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0175127s
STEP: Saw pod success
Jul  9 03:33:03.625: INFO: Pod "client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3" satisfied condition "success or failure"
Jul  9 03:33:03.627: INFO: Trying to get logs from node node2 pod client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3 container env3cont: <nil>
STEP: delete the pod
Jul  9 03:33:03.661: INFO: Waiting for pod client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3 to disappear
Jul  9 03:33:03.665: INFO: Pod client-envvars-fcb07462-6147-4d7c-82fb-86fd0d3c95b3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:33:03.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4730" for this suite.
Jul  9 03:33:45.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:33:45.815: INFO: namespace pods-4730 deletion completed in 42.143125s

• [SLOW TEST:54.277 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:33:45.816: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-225ad179-9a8a-4448-bbe0-2ffec7ed5cf4
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:33:45.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4361" for this suite.
Jul  9 03:33:51.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:33:51.973: INFO: namespace secrets-4361 deletion completed in 6.0716528s

• [SLOW TEST:6.158 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:33:51.974: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  9 03:33:52.018: INFO: Waiting up to 5m0s for pod "pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d" in namespace "emptydir-3719" to be "success or failure"
Jul  9 03:33:52.022: INFO: Pod "pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3244ms
Jul  9 03:33:54.025: INFO: Pod "pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0063922s
Jul  9 03:33:56.028: INFO: Pod "pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0096027s
STEP: Saw pod success
Jul  9 03:33:56.028: INFO: Pod "pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d" satisfied condition "success or failure"
Jul  9 03:33:56.031: INFO: Trying to get logs from node node2 pod pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d container test-container: <nil>
STEP: delete the pod
Jul  9 03:33:56.051: INFO: Waiting for pod pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d to disappear
Jul  9 03:33:56.057: INFO: Pod pod-c2cc0da6-2f2b-47bc-b21c-a4592312db1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:33:56.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3719" for this suite.
Jul  9 03:34:02.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:34:02.182: INFO: namespace emptydir-3719 deletion completed in 6.1220501s

• [SLOW TEST:10.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:34:02.183: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  9 03:34:02.253: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:02.256: INFO: Number of nodes with available pods: 0
Jul  9 03:34:02.256: INFO: Node node1 is running more than one daemon pod
Jul  9 03:34:03.264: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:03.269: INFO: Number of nodes with available pods: 0
Jul  9 03:34:03.269: INFO: Node node1 is running more than one daemon pod
Jul  9 03:34:04.260: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:04.267: INFO: Number of nodes with available pods: 0
Jul  9 03:34:04.267: INFO: Node node1 is running more than one daemon pod
Jul  9 03:34:05.260: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:05.263: INFO: Number of nodes with available pods: 2
Jul  9 03:34:05.263: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  9 03:34:05.285: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:05.291: INFO: Number of nodes with available pods: 1
Jul  9 03:34:05.291: INFO: Node node1 is running more than one daemon pod
Jul  9 03:34:06.293: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:06.295: INFO: Number of nodes with available pods: 1
Jul  9 03:34:06.295: INFO: Node node1 is running more than one daemon pod
Jul  9 03:34:07.295: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 03:34:07.297: INFO: Number of nodes with available pods: 2
Jul  9 03:34:07.297: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1272, will wait for the garbage collector to delete the pods
Jul  9 03:34:07.357: INFO: Deleting DaemonSet.extensions daemon-set took: 4.2769ms
Jul  9 03:34:07.658: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.2001ms
Jul  9 03:34:14.159: INFO: Number of nodes with available pods: 0
Jul  9 03:34:14.160: INFO: Number of running nodes: 0, number of available pods: 0
Jul  9 03:34:14.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1272/daemonsets","resourceVersion":"17369"},"items":null}

Jul  9 03:34:14.163: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1272/pods","resourceVersion":"17369"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:34:14.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1272" for this suite.
Jul  9 03:34:20.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:34:20.306: INFO: namespace daemonsets-1272 deletion completed in 6.1338175s

• [SLOW TEST:18.123 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:34:20.306: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:34:40.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5949" for this suite.
Jul  9 03:34:46.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:34:46.663: INFO: namespace namespaces-5949 deletion completed in 6.1738524s
STEP: Destroying namespace "nsdeletetest-3410" for this suite.
Jul  9 03:34:46.667: INFO: Namespace nsdeletetest-3410 was already deleted
STEP: Destroying namespace "nsdeletetest-7334" for this suite.
Jul  9 03:34:52.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:34:52.813: INFO: namespace nsdeletetest-7334 deletion completed in 6.1460964s

• [SLOW TEST:32.507 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:34:52.813: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-ab65d016-89c0-43df-a843-b49d48531173
STEP: Creating a pod to test consume configMaps
Jul  9 03:34:52.894: INFO: Waiting up to 5m0s for pod "pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490" in namespace "configmap-7200" to be "success or failure"
Jul  9 03:34:52.898: INFO: Pod "pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9663ms
Jul  9 03:34:54.902: INFO: Pod "pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0071063s
Jul  9 03:34:56.906: INFO: Pod "pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0113343s
STEP: Saw pod success
Jul  9 03:34:56.906: INFO: Pod "pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490" satisfied condition "success or failure"
Jul  9 03:34:56.908: INFO: Trying to get logs from node node2 pod pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:34:56.931: INFO: Waiting for pod pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490 to disappear
Jul  9 03:34:56.933: INFO: Pod pod-configmaps-068d1bad-b41a-4d07-bc81-0180493d3490 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:34:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7200" for this suite.
Jul  9 03:35:02.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:35:03.090: INFO: namespace configmap-7200 deletion completed in 6.1497677s

• [SLOW TEST:10.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:35:03.090: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:35:27.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7332" for this suite.
Jul  9 03:35:33.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:35:33.476: INFO: namespace container-runtime-7332 deletion completed in 6.1238963s

• [SLOW TEST:30.386 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:35:33.477: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9952
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  9 03:35:33.513: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  9 03:35:59.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.64.4:8080/dial?request=hostName&protocol=udp&host=172.30.192.4&port=8081&tries=1'] Namespace:pod-network-test-9952 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 03:35:59.613: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 03:35:59.803: INFO: Waiting for endpoints: map[]
Jul  9 03:35:59.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.64.4:8080/dial?request=hostName&protocol=udp&host=172.30.64.3&port=8081&tries=1'] Namespace:pod-network-test-9952 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 03:35:59.806: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 03:35:59.964: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:35:59.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9952" for this suite.
Jul  9 03:36:21.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:36:22.118: INFO: namespace pod-network-test-9952 deletion completed in 22.1513515s

• [SLOW TEST:48.642 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:36:22.119: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:36:22.211: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  9 03:36:24.221: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  9 03:36:28.247: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7691,SelfLink:/apis/apps/v1/namespaces/deployment-7691/deployments/test-cleanup-deployment,UID:a1584436-d011-4623-9a35-0428375d1749,ResourceVersion:17857,Generation:1,CreationTimestamp:2019-07-09 03:36:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-09 03:36:23 +0000 UTC 2019-07-09 03:36:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-09 03:36:26 +0000 UTC 2019-07-09 03:36:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  9 03:36:28.250: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-7691,SelfLink:/apis/apps/v1/namespaces/deployment-7691/replicasets/test-cleanup-deployment-55bbcbc84c,UID:16b2ffd7-e1bc-4420-90be-0802e9326122,ResourceVersion:17846,Generation:1,CreationTimestamp:2019-07-09 03:36:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a1584436-d011-4623-9a35-0428375d1749 0xc000ab6a17 0xc000ab6a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  9 03:36:28.253: INFO: Pod "test-cleanup-deployment-55bbcbc84c-pn7js" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-pn7js,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-7691,SelfLink:/api/v1/namespaces/deployment-7691/pods/test-cleanup-deployment-55bbcbc84c-pn7js,UID:b96a9484-4796-490d-a13d-6ee8ae39ffc2,ResourceVersion:17845,Generation:0,CreationTimestamp:2019-07-09 03:36:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 16b2ffd7-e1bc-4420-90be-0802e9326122 0xc00357bfc7 0xc00357bfc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z5t7p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z5t7p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-z5t7p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011ee040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011ee060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:36:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:36:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:36:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:36:23 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.4,StartTime:2019-07-09 03:36:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-09 03:36:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://beb14ac0bb58e392ec26a947d857419e4633987de39627c9026b8cd41d0ec9ef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:36:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7691" for this suite.
Jul  9 03:36:34.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:36:34.430: INFO: namespace deployment-7691 deletion completed in 6.1747025s

• [SLOW TEST:12.312 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:36:34.430: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-st5h
STEP: Creating a pod to test atomic-volume-subpath
Jul  9 03:36:34.475: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-st5h" in namespace "subpath-6042" to be "success or failure"
Jul  9 03:36:34.478: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8619ms
Jul  9 03:36:36.482: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.0070448s
Jul  9 03:36:38.486: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 4.0115113s
Jul  9 03:36:40.489: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 6.0145863s
Jul  9 03:36:42.493: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 8.0184691s
Jul  9 03:36:44.500: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 10.0252342s
Jul  9 03:36:46.505: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 12.0302133s
Jul  9 03:36:48.508: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 14.0331169s
Jul  9 03:36:50.515: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 16.0398184s
Jul  9 03:36:52.519: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 18.0441413s
Jul  9 03:36:54.522: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Running", Reason="", readiness=true. Elapsed: 20.0473311s
Jul  9 03:36:56.524: INFO: Pod "pod-subpath-test-configmap-st5h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0490819s
STEP: Saw pod success
Jul  9 03:36:56.524: INFO: Pod "pod-subpath-test-configmap-st5h" satisfied condition "success or failure"
Jul  9 03:36:56.526: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-st5h container test-container-subpath-configmap-st5h: <nil>
STEP: delete the pod
Jul  9 03:36:56.579: INFO: Waiting for pod pod-subpath-test-configmap-st5h to disappear
Jul  9 03:36:56.582: INFO: Pod pod-subpath-test-configmap-st5h no longer exists
STEP: Deleting pod pod-subpath-test-configmap-st5h
Jul  9 03:36:56.582: INFO: Deleting pod "pod-subpath-test-configmap-st5h" in namespace "subpath-6042"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:36:56.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6042" for this suite.
Jul  9 03:37:02.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:37:02.726: INFO: namespace subpath-6042 deletion completed in 6.1377396s

• [SLOW TEST:28.296 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:37:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:37:06.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8213" for this suite.
Jul  9 03:37:12.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:37:12.985: INFO: namespace kubelet-test-8213 deletion completed in 6.1954379s

• [SLOW TEST:10.258 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:37:12.985: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-6945
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6945
STEP: Deleting pre-stop pod
Jul  9 03:37:40.062: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:37:40.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6945" for this suite.
Jul  9 03:38:18.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:38:18.267: INFO: namespace prestop-6945 deletion completed in 38.186276s

• [SLOW TEST:65.283 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:38:18.268: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:38:18.313: INFO: Creating ReplicaSet my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8
Jul  9 03:38:18.323: INFO: Pod name my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8: Found 0 pods out of 1
Jul  9 03:38:23.330: INFO: Pod name my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8: Found 1 pods out of 1
Jul  9 03:38:23.330: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8" is running
Jul  9 03:38:23.332: INFO: Pod "my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8-jcmx5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 03:38:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 03:38:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 03:38:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 03:38:18 +0000 UTC Reason: Message:}])
Jul  9 03:38:23.332: INFO: Trying to dial the pod
Jul  9 03:38:28.353: INFO: Controller my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8: Got expected result from replica 1 [my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8-jcmx5]: "my-hostname-basic-3f17597f-aa93-48a7-b8ad-8974cc7dffe8-jcmx5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:38:28.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2556" for this suite.
Jul  9 03:38:34.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:38:34.499: INFO: namespace replicaset-2556 deletion completed in 6.1410184s

• [SLOW TEST:16.231 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:38:34.499: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:38:34.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1" in namespace "downward-api-4135" to be "success or failure"
Jul  9 03:38:34.545: INFO: Pod "downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.9612ms
Jul  9 03:38:36.549: INFO: Pod "downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0135037s
Jul  9 03:38:38.555: INFO: Pod "downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0192964s
STEP: Saw pod success
Jul  9 03:38:38.555: INFO: Pod "downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1" satisfied condition "success or failure"
Jul  9 03:38:38.559: INFO: Trying to get logs from node node2 pod downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1 container client-container: <nil>
STEP: delete the pod
Jul  9 03:38:38.585: INFO: Waiting for pod downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1 to disappear
Jul  9 03:38:38.590: INFO: Pod downwardapi-volume-049f976d-5f5f-44ec-adfc-38c6edde8ba1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:38:38.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4135" for this suite.
Jul  9 03:38:44.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:38:44.728: INFO: namespace downward-api-4135 deletion completed in 6.1285074s

• [SLOW TEST:10.230 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:38:44.729: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-cb88debe-16c4-4fc3-9017-a2522e6bda67
STEP: Creating a pod to test consume configMaps
Jul  9 03:38:44.813: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8" in namespace "projected-2709" to be "success or failure"
Jul  9 03:38:44.816: INFO: Pod "pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3183ms
Jul  9 03:38:46.823: INFO: Pod "pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0094518s
STEP: Saw pod success
Jul  9 03:38:46.823: INFO: Pod "pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8" satisfied condition "success or failure"
Jul  9 03:38:46.828: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:38:46.848: INFO: Waiting for pod pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8 to disappear
Jul  9 03:38:46.853: INFO: Pod pod-projected-configmaps-3a57dfb8-9b95-43ef-8f6e-512dcdf9c1f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:38:46.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2709" for this suite.
Jul  9 03:38:52.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:38:53.014: INFO: namespace projected-2709 deletion completed in 6.1580943s

• [SLOW TEST:8.286 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:38:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:38:53.057: INFO: (0) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 9.5836ms)
Jul  9 03:38:53.064: INFO: (1) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.056ms)
Jul  9 03:38:53.068: INFO: (2) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.4565ms)
Jul  9 03:38:53.071: INFO: (3) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.0378ms)
Jul  9 03:38:53.075: INFO: (4) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.6904ms)
Jul  9 03:38:53.080: INFO: (5) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.9048ms)
Jul  9 03:38:53.083: INFO: (6) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.6153ms)
Jul  9 03:38:53.090: INFO: (7) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.4058ms)
Jul  9 03:38:53.092: INFO: (8) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 2.5754ms)
Jul  9 03:38:53.095: INFO: (9) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 2.7028ms)
Jul  9 03:38:53.099: INFO: (10) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.9714ms)
Jul  9 03:38:53.103: INFO: (11) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.3973ms)
Jul  9 03:38:53.106: INFO: (12) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.3321ms)
Jul  9 03:38:53.109: INFO: (13) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 2.9009ms)
Jul  9 03:38:53.116: INFO: (14) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.809ms)
Jul  9 03:38:53.118: INFO: (15) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 2.8267ms)
Jul  9 03:38:53.129: INFO: (16) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 10.1941ms)
Jul  9 03:38:53.139: INFO: (17) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 9.9606ms)
Jul  9 03:38:53.147: INFO: (18) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.3543ms)
Jul  9 03:38:53.150: INFO: (19) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.245ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:38:53.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4668" for this suite.
Jul  9 03:38:59.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:38:59.326: INFO: namespace proxy-4668 deletion completed in 6.1722102s

• [SLOW TEST:6.312 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:38:59.326: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c1dcdeae-700b-40b8-bf24-688de813a21f
STEP: Creating a pod to test consume secrets
Jul  9 03:38:59.377: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4" in namespace "projected-5450" to be "success or failure"
Jul  9 03:38:59.389: INFO: Pod "pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.6691ms
Jul  9 03:39:01.391: INFO: Pod "pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0142513s
STEP: Saw pod success
Jul  9 03:39:01.391: INFO: Pod "pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4" satisfied condition "success or failure"
Jul  9 03:39:01.393: INFO: Trying to get logs from node node2 pod pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:39:01.409: INFO: Waiting for pod pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4 to disappear
Jul  9 03:39:01.418: INFO: Pod pod-projected-secrets-10cdecc8-7d6d-4833-9d62-9e303eec7dc4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:39:01.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5450" for this suite.
Jul  9 03:39:07.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:39:07.596: INFO: namespace projected-5450 deletion completed in 6.1750375s

• [SLOW TEST:8.270 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:39:07.596: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  9 03:39:10.165: INFO: Successfully updated pod "labelsupdate2d5b6348-9de0-4df5-8947-d74e1fffe9b6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:39:12.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8284" for this suite.
Jul  9 03:39:34.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:39:34.317: INFO: namespace projected-8284 deletion completed in 22.12384s

• [SLOW TEST:26.720 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:39:34.317: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:39:34.365: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  9 03:39:34.373: INFO: Number of nodes with available pods: 0
Jul  9 03:39:34.373: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul  9 03:39:34.392: INFO: Number of nodes with available pods: 0
Jul  9 03:39:34.392: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:35.457: INFO: Number of nodes with available pods: 0
Jul  9 03:39:35.457: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:36.398: INFO: Number of nodes with available pods: 0
Jul  9 03:39:36.398: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:37.395: INFO: Number of nodes with available pods: 1
Jul  9 03:39:37.395: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  9 03:39:37.417: INFO: Number of nodes with available pods: 1
Jul  9 03:39:37.417: INFO: Number of running nodes: 0, number of available pods: 1
Jul  9 03:39:38.421: INFO: Number of nodes with available pods: 0
Jul  9 03:39:38.421: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  9 03:39:38.558: INFO: Number of nodes with available pods: 0
Jul  9 03:39:38.558: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:39.560: INFO: Number of nodes with available pods: 0
Jul  9 03:39:39.560: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:40.560: INFO: Number of nodes with available pods: 0
Jul  9 03:39:40.560: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:41.581: INFO: Number of nodes with available pods: 0
Jul  9 03:39:41.581: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:42.560: INFO: Number of nodes with available pods: 0
Jul  9 03:39:42.560: INFO: Node node1 is running more than one daemon pod
Jul  9 03:39:43.562: INFO: Number of nodes with available pods: 1
Jul  9 03:39:43.562: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7526, will wait for the garbage collector to delete the pods
Jul  9 03:39:43.627: INFO: Deleting DaemonSet.extensions daemon-set took: 4.2362ms
Jul  9 03:39:43.928: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.2167ms
Jul  9 03:39:46.735: INFO: Number of nodes with available pods: 0
Jul  9 03:39:46.735: INFO: Number of running nodes: 0, number of available pods: 0
Jul  9 03:39:46.737: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7526/daemonsets","resourceVersion":"18525"},"items":null}

Jul  9 03:39:46.741: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7526/pods","resourceVersion":"18525"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:39:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7526" for this suite.
Jul  9 03:39:52.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:39:52.952: INFO: namespace daemonsets-7526 deletion completed in 6.193946s

• [SLOW TEST:18.636 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:39:52.953: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul  9 03:39:55.007: INFO: Pod pod-hostip-01a5f8f9-142b-4aeb-b21f-d2839de556fe has hostIP: 172.28.128.13
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:39:55.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3927" for this suite.
Jul  9 03:40:17.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:40:17.224: INFO: namespace pods-3927 deletion completed in 22.2061756s

• [SLOW TEST:24.271 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:40:17.224: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9520
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul  9 03:40:17.305: INFO: Found 0 stateful pods, waiting for 3
Jul  9 03:40:27.308: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 03:40:27.308: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 03:40:27.308: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 03:40:27.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9520 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:40:27.822: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:40:27.822: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:40:27.822: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  9 03:40:37.854: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  9 03:40:47.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9520 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:40:48.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 03:40:48.093: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 03:40:48.093: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 03:40:58.119: INFO: Waiting for StatefulSet statefulset-9520/ss2 to complete update
Jul  9 03:40:58.119: INFO: Waiting for Pod statefulset-9520/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 03:40:58.119: INFO: Waiting for Pod statefulset-9520/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 03:41:08.130: INFO: Waiting for StatefulSet statefulset-9520/ss2 to complete update
Jul  9 03:41:08.130: INFO: Waiting for Pod statefulset-9520/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 03:41:08.130: INFO: Waiting for Pod statefulset-9520/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 03:41:18.130: INFO: Waiting for StatefulSet statefulset-9520/ss2 to complete update
Jul  9 03:41:18.130: INFO: Waiting for Pod statefulset-9520/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 03:41:28.127: INFO: Waiting for StatefulSet statefulset-9520/ss2 to complete update
STEP: Rolling back to a previous revision
Jul  9 03:41:38.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9520 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  9 03:41:38.366: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  9 03:41:38.366: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  9 03:41:38.366: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  9 03:41:48.394: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  9 03:41:58.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec --namespace=statefulset-9520 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  9 03:41:58.635: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  9 03:41:58.635: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  9 03:41:58.635: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  9 03:42:18.665: INFO: Waiting for StatefulSet statefulset-9520/ss2 to complete update
Jul  9 03:42:18.665: INFO: Waiting for Pod statefulset-9520/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  9 03:42:28.672: INFO: Deleting all statefulset in ns statefulset-9520
Jul  9 03:42:28.675: INFO: Scaling statefulset ss2 to 0
Jul  9 03:42:48.692: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 03:42:48.695: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:42:48.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9520" for this suite.
Jul  9 03:42:54.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:42:54.924: INFO: namespace statefulset-9520 deletion completed in 6.2019923s

• [SLOW TEST:157.700 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:42:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul  9 03:42:54.974: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-630487638 proxy --unix-socket=/tmp/kubectl-proxy-unix520512065/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:42:55.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3839" for this suite.
Jul  9 03:43:01.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:01.181: INFO: namespace kubectl-3839 deletion completed in 6.1309519s

• [SLOW TEST:6.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:01.181: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4854" for this suite.
Jul  9 03:43:07.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:07.413: INFO: namespace kubelet-test-4854 deletion completed in 6.1765093s

• [SLOW TEST:6.232 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-98fd555f-7327-47fe-b7ed-e53e3da3e109
STEP: Creating a pod to test consume secrets
Jul  9 03:43:07.463: INFO: Waiting up to 5m0s for pod "pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5" in namespace "secrets-562" to be "success or failure"
Jul  9 03:43:07.469: INFO: Pod "pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0046ms
Jul  9 03:43:09.471: INFO: Pod "pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0084238s
Jul  9 03:43:11.480: INFO: Pod "pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0171099s
STEP: Saw pod success
Jul  9 03:43:11.480: INFO: Pod "pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5" satisfied condition "success or failure"
Jul  9 03:43:11.482: INFO: Trying to get logs from node node2 pod pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:43:11.507: INFO: Waiting for pod pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5 to disappear
Jul  9 03:43:11.510: INFO: Pod pod-secrets-c658ae54-61dc-4902-8dbb-aae23cce92b5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:11.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-562" for this suite.
Jul  9 03:43:17.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:17.675: INFO: namespace secrets-562 deletion completed in 6.1604269s

• [SLOW TEST:10.262 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:17.675: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:43:17.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59" in namespace "projected-5078" to be "success or failure"
Jul  9 03:43:17.715: INFO: Pod "downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5034ms
Jul  9 03:43:19.722: INFO: Pod "downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0093843s
Jul  9 03:43:21.724: INFO: Pod "downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0114158s
STEP: Saw pod success
Jul  9 03:43:21.724: INFO: Pod "downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59" satisfied condition "success or failure"
Jul  9 03:43:21.730: INFO: Trying to get logs from node node2 pod downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59 container client-container: <nil>
STEP: delete the pod
Jul  9 03:43:21.752: INFO: Waiting for pod downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59 to disappear
Jul  9 03:43:21.756: INFO: Pod downwardapi-volume-0534bbf4-0e94-432e-b549-6fdf1c608d59 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:21.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5078" for this suite.
Jul  9 03:43:27.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:27.921: INFO: namespace projected-5078 deletion completed in 6.1581519s

• [SLOW TEST:10.246 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:27.922: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul  9 03:43:27.954: INFO: Waiting up to 5m0s for pod "client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76" in namespace "containers-1046" to be "success or failure"
Jul  9 03:43:27.960: INFO: Pod "client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089ms
Jul  9 03:43:29.964: INFO: Pod "client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0095099s
Jul  9 03:43:31.971: INFO: Pod "client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0170607s
STEP: Saw pod success
Jul  9 03:43:31.971: INFO: Pod "client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76" satisfied condition "success or failure"
Jul  9 03:43:31.974: INFO: Trying to get logs from node node2 pod client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76 container test-container: <nil>
STEP: delete the pod
Jul  9 03:43:32.004: INFO: Waiting for pod client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76 to disappear
Jul  9 03:43:32.009: INFO: Pod client-containers-8c1fc4bb-df63-42ca-a3b3-712d989ffd76 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:32.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1046" for this suite.
Jul  9 03:43:38.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:38.164: INFO: namespace containers-1046 deletion completed in 6.1504727s

• [SLOW TEST:10.242 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:38.164: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  9 03:43:38.221: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  9 03:43:43.224: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:44.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1321" for this suite.
Jul  9 03:43:50.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:50.444: INFO: namespace replication-controller-1321 deletion completed in 6.1942176s

• [SLOW TEST:12.280 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 03:43:50.513: INFO: (0) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 9.6503ms)
Jul  9 03:43:50.517: INFO: (1) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.1483ms)
Jul  9 03:43:50.524: INFO: (2) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.4918ms)
Jul  9 03:43:50.529: INFO: (3) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.2099ms)
Jul  9 03:43:50.533: INFO: (4) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 3.6895ms)
Jul  9 03:43:50.537: INFO: (5) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.4578ms)
Jul  9 03:43:50.542: INFO: (6) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.4023ms)
Jul  9 03:43:50.547: INFO: (7) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.8816ms)
Jul  9 03:43:50.552: INFO: (8) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.4773ms)
Jul  9 03:43:50.560: INFO: (9) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.9754ms)
Jul  9 03:43:50.565: INFO: (10) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.8414ms)
Jul  9 03:43:50.574: INFO: (11) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.7341ms)
Jul  9 03:43:50.580: INFO: (12) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.3838ms)
Jul  9 03:43:50.586: INFO: (13) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.2537ms)
Jul  9 03:43:50.591: INFO: (14) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.8178ms)
Jul  9 03:43:50.595: INFO: (15) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.1684ms)
Jul  9 03:43:50.602: INFO: (16) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.0517ms)
Jul  9 03:43:50.613: INFO: (17) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 11.0729ms)
Jul  9 03:43:50.618: INFO: (18) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.4987ms)
Jul  9 03:43:50.623: INFO: (19) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.8673ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:50.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5900" for this suite.
Jul  9 03:43:56.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:43:56.780: INFO: namespace proxy-5900 deletion completed in 6.1522305s

• [SLOW TEST:6.336 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:43:56.780: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:43:58.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3009" for this suite.
Jul  9 03:44:48.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:44:49.029: INFO: namespace kubelet-test-3009 deletion completed in 50.1858661s

• [SLOW TEST:52.248 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:44:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul  9 03:44:53.106: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-630487638 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul  9 03:45:08.191: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:45:08.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6750" for this suite.
Jul  9 03:45:14.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:45:14.337: INFO: namespace pods-6750 deletion completed in 6.1417133s

• [SLOW TEST:25.308 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:45:14.337: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:45:14.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1865" for this suite.
Jul  9 03:45:20.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:45:20.589: INFO: namespace services-1865 deletion completed in 6.2059672s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.252 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:45:20.589: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-86895648-7091-441e-95fb-3dcd4b751823
STEP: Creating a pod to test consume secrets
Jul  9 03:45:20.631: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066" in namespace "projected-8687" to be "success or failure"
Jul  9 03:45:20.634: INFO: Pod "pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2676ms
Jul  9 03:45:22.639: INFO: Pod "pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0077388s
Jul  9 03:45:24.643: INFO: Pod "pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0117032s
STEP: Saw pod success
Jul  9 03:45:24.643: INFO: Pod "pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066" satisfied condition "success or failure"
Jul  9 03:45:24.648: INFO: Trying to get logs from node node2 pod pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:45:24.667: INFO: Waiting for pod pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066 to disappear
Jul  9 03:45:24.670: INFO: Pod pod-projected-secrets-b89ebadf-5471-439e-9037-9cc11f88f066 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:45:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8687" for this suite.
Jul  9 03:45:30.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:45:30.841: INFO: namespace projected-8687 deletion completed in 6.1649843s

• [SLOW TEST:10.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:45:30.841: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-6051ab87-4236-485f-b662-4f2977982282
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:45:34.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4383" for this suite.
Jul  9 03:45:56.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:45:57.104: INFO: namespace configmap-4383 deletion completed in 22.1526065s

• [SLOW TEST:26.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:45:57.104: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  9 03:45:57.135: INFO: Waiting up to 5m0s for pod "downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd" in namespace "downward-api-2161" to be "success or failure"
Jul  9 03:45:57.141: INFO: Pod "downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489ms
Jul  9 03:45:59.147: INFO: Pod "downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0123582s
Jul  9 03:46:01.151: INFO: Pod "downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015446s
STEP: Saw pod success
Jul  9 03:46:01.151: INFO: Pod "downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd" satisfied condition "success or failure"
Jul  9 03:46:01.153: INFO: Trying to get logs from node node2 pod downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd container dapi-container: <nil>
STEP: delete the pod
Jul  9 03:46:01.184: INFO: Waiting for pod downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd to disappear
Jul  9 03:46:01.188: INFO: Pod downward-api-b6c11500-45a7-4d09-b1c5-920843c505bd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:46:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2161" for this suite.
Jul  9 03:46:07.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:46:07.382: INFO: namespace downward-api-2161 deletion completed in 6.1886244s

• [SLOW TEST:10.277 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:46:07.382: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-41c6ed22-3941-4e8d-9e0a-b85a9b69e990
STEP: Creating a pod to test consume configMaps
Jul  9 03:46:07.434: INFO: Waiting up to 5m0s for pod "pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a" in namespace "configmap-4660" to be "success or failure"
Jul  9 03:46:07.445: INFO: Pod "pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.7105ms
Jul  9 03:46:09.450: INFO: Pod "pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0166445s
STEP: Saw pod success
Jul  9 03:46:09.450: INFO: Pod "pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a" satisfied condition "success or failure"
Jul  9 03:46:09.455: INFO: Trying to get logs from node node2 pod pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 03:46:09.571: INFO: Waiting for pod pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a to disappear
Jul  9 03:46:09.575: INFO: Pod pod-configmaps-12f6d5de-a146-42b7-ac12-63d8a517ed1a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:46:09.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4660" for this suite.
Jul  9 03:46:15.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:46:15.714: INFO: namespace configmap-4660 deletion completed in 6.1338742s

• [SLOW TEST:8.333 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:46:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 03:46:15.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2778'
Jul  9 03:46:15.820: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  9 03:46:15.820: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul  9 03:46:17.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2778'
Jul  9 03:46:17.947: INFO: stderr: ""
Jul  9 03:46:17.947: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:46:17.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2778" for this suite.
Jul  9 03:46:39.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:46:40.139: INFO: namespace kubectl-2778 deletion completed in 22.1868295s

• [SLOW TEST:24.425 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:46:40.140: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1716
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  9 03:46:40.192: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  9 03:47:00.329: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.192.4:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 03:47:00.329: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 03:47:00.592: INFO: Found all expected endpoints: [netserver-0]
Jul  9 03:47:00.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.64.3:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 03:47:00.595: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 03:47:00.822: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:47:00.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1716" for this suite.
Jul  9 03:47:22.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:47:23.006: INFO: namespace pod-network-test-1716 deletion completed in 22.180002s

• [SLOW TEST:42.867 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:47:23.007: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  9 03:47:23.049: INFO: Waiting up to 5m0s for pod "pod-ac5f0e17-c0c4-4497-badc-e4465201fe06" in namespace "emptydir-7963" to be "success or failure"
Jul  9 03:47:23.052: INFO: Pod "pod-ac5f0e17-c0c4-4497-badc-e4465201fe06": Phase="Pending", Reason="", readiness=false. Elapsed: 3.0175ms
Jul  9 03:47:25.057: INFO: Pod "pod-ac5f0e17-c0c4-4497-badc-e4465201fe06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0077871s
STEP: Saw pod success
Jul  9 03:47:25.057: INFO: Pod "pod-ac5f0e17-c0c4-4497-badc-e4465201fe06" satisfied condition "success or failure"
Jul  9 03:47:25.062: INFO: Trying to get logs from node node2 pod pod-ac5f0e17-c0c4-4497-badc-e4465201fe06 container test-container: <nil>
STEP: delete the pod
Jul  9 03:47:25.084: INFO: Waiting for pod pod-ac5f0e17-c0c4-4497-badc-e4465201fe06 to disappear
Jul  9 03:47:25.089: INFO: Pod pod-ac5f0e17-c0c4-4497-badc-e4465201fe06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:47:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7963" for this suite.
Jul  9 03:47:31.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:47:31.265: INFO: namespace emptydir-7963 deletion completed in 6.1716436s

• [SLOW TEST:8.259 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:47:31.266: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul  9 03:47:31.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4314'
Jul  9 03:47:31.447: INFO: stderr: ""
Jul  9 03:47:31.447: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 03:47:31.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4314'
Jul  9 03:47:31.524: INFO: stderr: ""
Jul  9 03:47:31.524: INFO: stdout: "update-demo-nautilus-5pl8n update-demo-nautilus-dn7qh "
Jul  9 03:47:31.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-5pl8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:47:31.591: INFO: stderr: ""
Jul  9 03:47:31.591: INFO: stdout: ""
Jul  9 03:47:31.591: INFO: update-demo-nautilus-5pl8n is created but not running
Jul  9 03:47:36.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4314'
Jul  9 03:47:36.670: INFO: stderr: ""
Jul  9 03:47:36.670: INFO: stdout: "update-demo-nautilus-5pl8n update-demo-nautilus-dn7qh "
Jul  9 03:47:36.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-5pl8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:47:36.741: INFO: stderr: ""
Jul  9 03:47:36.741: INFO: stdout: "true"
Jul  9 03:47:36.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-5pl8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:47:36.810: INFO: stderr: ""
Jul  9 03:47:36.810: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:47:36.810: INFO: validating pod update-demo-nautilus-5pl8n
Jul  9 03:47:36.824: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:47:36.824: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:47:36.824: INFO: update-demo-nautilus-5pl8n is verified up and running
Jul  9 03:47:36.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-dn7qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:47:36.884: INFO: stderr: ""
Jul  9 03:47:36.884: INFO: stdout: "true"
Jul  9 03:47:36.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-nautilus-dn7qh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:47:36.943: INFO: stderr: ""
Jul  9 03:47:36.943: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  9 03:47:36.943: INFO: validating pod update-demo-nautilus-dn7qh
Jul  9 03:47:36.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  9 03:47:36.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  9 03:47:36.951: INFO: update-demo-nautilus-dn7qh is verified up and running
STEP: rolling-update to new replication controller
Jul  9 03:47:36.952: INFO: scanned /root for discovery docs: <nil>
Jul  9 03:47:36.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4314'
Jul  9 03:48:12.441: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  9 03:48:12.441: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  9 03:48:12.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4314'
Jul  9 03:48:12.513: INFO: stderr: ""
Jul  9 03:48:12.513: INFO: stdout: "update-demo-kitten-22nn8 update-demo-kitten-hv5nn update-demo-nautilus-dn7qh "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jul  9 03:48:17.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4314'
Jul  9 03:48:17.625: INFO: stderr: ""
Jul  9 03:48:17.625: INFO: stdout: "update-demo-kitten-22nn8 update-demo-kitten-hv5nn "
Jul  9 03:48:17.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-kitten-22nn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:48:17.729: INFO: stderr: ""
Jul  9 03:48:17.729: INFO: stdout: "true"
Jul  9 03:48:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-kitten-22nn8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:48:17.842: INFO: stderr: ""
Jul  9 03:48:17.842: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  9 03:48:17.842: INFO: validating pod update-demo-kitten-22nn8
Jul  9 03:48:24.860: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  9 03:48:24.860: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  9 03:48:24.860: INFO: update-demo-kitten-22nn8 is verified up and running
Jul  9 03:48:24.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-kitten-hv5nn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:48:24.930: INFO: stderr: ""
Jul  9 03:48:24.930: INFO: stdout: "true"
Jul  9 03:48:24.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods update-demo-kitten-hv5nn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4314'
Jul  9 03:48:25.006: INFO: stderr: ""
Jul  9 03:48:25.006: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  9 03:48:25.006: INFO: validating pod update-demo-kitten-hv5nn
Jul  9 03:48:25.014: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  9 03:48:25.014: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  9 03:48:25.014: INFO: update-demo-kitten-hv5nn is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:48:25.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4314" for this suite.
Jul  9 03:48:47.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:48:47.167: INFO: namespace kubectl-4314 deletion completed in 22.1495914s

• [SLOW TEST:75.901 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:48:47.167: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3537
I0709 03:48:47.204437      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3537, replica count: 1
I0709 03:48:48.254874      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0709 03:48:49.255078      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  9 03:48:49.373: INFO: Created: latency-svc-5d6wv
Jul  9 03:48:49.377: INFO: Got endpoints: latency-svc-5d6wv [21.9283ms]
Jul  9 03:48:49.395: INFO: Created: latency-svc-p4ndw
Jul  9 03:48:49.398: INFO: Got endpoints: latency-svc-p4ndw [21.062ms]
Jul  9 03:48:49.413: INFO: Created: latency-svc-z79gh
Jul  9 03:48:49.420: INFO: Got endpoints: latency-svc-z79gh [42.4104ms]
Jul  9 03:48:49.428: INFO: Created: latency-svc-rwm2q
Jul  9 03:48:49.446: INFO: Got endpoints: latency-svc-rwm2q [67.99ms]
Jul  9 03:48:49.447: INFO: Created: latency-svc-fczbb
Jul  9 03:48:49.457: INFO: Got endpoints: latency-svc-fczbb [77.1671ms]
Jul  9 03:48:49.465: INFO: Created: latency-svc-2n58h
Jul  9 03:48:49.472: INFO: Got endpoints: latency-svc-2n58h [93.6025ms]
Jul  9 03:48:49.483: INFO: Created: latency-svc-mv92m
Jul  9 03:48:49.493: INFO: Got endpoints: latency-svc-mv92m [114.7889ms]
Jul  9 03:48:49.499: INFO: Created: latency-svc-fjgv8
Jul  9 03:48:49.502: INFO: Got endpoints: latency-svc-fjgv8 [123.3543ms]
Jul  9 03:48:49.516: INFO: Created: latency-svc-n7kbz
Jul  9 03:48:49.523: INFO: Got endpoints: latency-svc-n7kbz [144.074ms]
Jul  9 03:48:49.527: INFO: Created: latency-svc-9dfsj
Jul  9 03:48:49.536: INFO: Got endpoints: latency-svc-9dfsj [156.829ms]
Jul  9 03:48:49.542: INFO: Created: latency-svc-xtwg4
Jul  9 03:48:49.552: INFO: Got endpoints: latency-svc-xtwg4 [172.5191ms]
Jul  9 03:48:49.556: INFO: Created: latency-svc-4w5qp
Jul  9 03:48:49.562: INFO: Got endpoints: latency-svc-4w5qp [183.249ms]
Jul  9 03:48:49.573: INFO: Created: latency-svc-g67zz
Jul  9 03:48:49.577: INFO: Got endpoints: latency-svc-g67zz [197.9425ms]
Jul  9 03:48:49.588: INFO: Created: latency-svc-bh57r
Jul  9 03:48:49.599: INFO: Got endpoints: latency-svc-bh57r [219.4681ms]
Jul  9 03:48:49.604: INFO: Created: latency-svc-fkqq5
Jul  9 03:48:49.613: INFO: Got endpoints: latency-svc-fkqq5 [233.2476ms]
Jul  9 03:48:49.619: INFO: Created: latency-svc-m9f2r
Jul  9 03:48:49.621: INFO: Got endpoints: latency-svc-m9f2r [242.4093ms]
Jul  9 03:48:49.636: INFO: Created: latency-svc-zr2bh
Jul  9 03:48:49.643: INFO: Got endpoints: latency-svc-zr2bh [244.6788ms]
Jul  9 03:48:49.656: INFO: Created: latency-svc-z6vw6
Jul  9 03:48:49.664: INFO: Got endpoints: latency-svc-z6vw6 [243.8042ms]
Jul  9 03:48:49.664: INFO: Created: latency-svc-vjg9w
Jul  9 03:48:49.680: INFO: Got endpoints: latency-svc-vjg9w [234.0571ms]
Jul  9 03:48:49.681: INFO: Created: latency-svc-rl74j
Jul  9 03:48:49.693: INFO: Created: latency-svc-g8fs2
Jul  9 03:48:49.694: INFO: Got endpoints: latency-svc-rl74j [236.7912ms]
Jul  9 03:48:49.708: INFO: Got endpoints: latency-svc-g8fs2 [236.373ms]
Jul  9 03:48:49.713: INFO: Created: latency-svc-rg5wg
Jul  9 03:48:49.717: INFO: Got endpoints: latency-svc-rg5wg [223.9922ms]
Jul  9 03:48:49.732: INFO: Created: latency-svc-v228s
Jul  9 03:48:49.738: INFO: Got endpoints: latency-svc-v228s [236.3665ms]
Jul  9 03:48:49.744: INFO: Created: latency-svc-l2jr9
Jul  9 03:48:49.746: INFO: Got endpoints: latency-svc-l2jr9 [223.6376ms]
Jul  9 03:48:49.753: INFO: Created: latency-svc-sxgsj
Jul  9 03:48:49.762: INFO: Got endpoints: latency-svc-sxgsj [225.9806ms]
Jul  9 03:48:49.775: INFO: Created: latency-svc-5459m
Jul  9 03:48:49.784: INFO: Got endpoints: latency-svc-5459m [232.0036ms]
Jul  9 03:48:49.787: INFO: Created: latency-svc-fl4t4
Jul  9 03:48:49.792: INFO: Got endpoints: latency-svc-fl4t4 [229.9224ms]
Jul  9 03:48:49.805: INFO: Created: latency-svc-4llmk
Jul  9 03:48:49.811: INFO: Got endpoints: latency-svc-4llmk [233.4923ms]
Jul  9 03:48:49.820: INFO: Created: latency-svc-tjph5
Jul  9 03:48:49.827: INFO: Got endpoints: latency-svc-tjph5 [228.5017ms]
Jul  9 03:48:49.833: INFO: Created: latency-svc-z9vn5
Jul  9 03:48:49.839: INFO: Got endpoints: latency-svc-z9vn5 [226.5016ms]
Jul  9 03:48:49.849: INFO: Created: latency-svc-drkfw
Jul  9 03:48:49.854: INFO: Got endpoints: latency-svc-drkfw [232.7872ms]
Jul  9 03:48:49.861: INFO: Created: latency-svc-9kfn7
Jul  9 03:48:49.867: INFO: Got endpoints: latency-svc-9kfn7 [224.071ms]
Jul  9 03:48:49.878: INFO: Created: latency-svc-4fpv6
Jul  9 03:48:49.889: INFO: Got endpoints: latency-svc-4fpv6 [225.2504ms]
Jul  9 03:48:49.893: INFO: Created: latency-svc-r8tfn
Jul  9 03:48:49.904: INFO: Got endpoints: latency-svc-r8tfn [224.4559ms]
Jul  9 03:48:49.909: INFO: Created: latency-svc-548jm
Jul  9 03:48:49.914: INFO: Got endpoints: latency-svc-548jm [220.1806ms]
Jul  9 03:48:49.925: INFO: Created: latency-svc-twftm
Jul  9 03:48:49.930: INFO: Got endpoints: latency-svc-twftm [221.703ms]
Jul  9 03:48:49.938: INFO: Created: latency-svc-bvq4b
Jul  9 03:48:49.942: INFO: Got endpoints: latency-svc-bvq4b [224.8708ms]
Jul  9 03:48:49.953: INFO: Created: latency-svc-gw7xk
Jul  9 03:48:49.961: INFO: Got endpoints: latency-svc-gw7xk [223.2514ms]
Jul  9 03:48:49.964: INFO: Created: latency-svc-kx5hl
Jul  9 03:48:49.973: INFO: Got endpoints: latency-svc-kx5hl [226.493ms]
Jul  9 03:48:49.974: INFO: Created: latency-svc-cnqn7
Jul  9 03:48:49.986: INFO: Got endpoints: latency-svc-cnqn7 [224.2038ms]
Jul  9 03:48:50.002: INFO: Created: latency-svc-hjjdp
Jul  9 03:48:50.011: INFO: Got endpoints: latency-svc-hjjdp [226.89ms]
Jul  9 03:48:50.024: INFO: Created: latency-svc-mqzcb
Jul  9 03:48:50.029: INFO: Got endpoints: latency-svc-mqzcb [236.8144ms]
Jul  9 03:48:50.040: INFO: Created: latency-svc-s9m62
Jul  9 03:48:50.046: INFO: Got endpoints: latency-svc-s9m62 [235.0437ms]
Jul  9 03:48:50.058: INFO: Created: latency-svc-pbbrb
Jul  9 03:48:50.074: INFO: Created: latency-svc-fthdr
Jul  9 03:48:50.086: INFO: Got endpoints: latency-svc-pbbrb [258.2918ms]
Jul  9 03:48:50.102: INFO: Created: latency-svc-6q4t6
Jul  9 03:48:50.110: INFO: Created: latency-svc-flzzj
Jul  9 03:48:50.117: INFO: Created: latency-svc-sjg92
Jul  9 03:48:50.128: INFO: Created: latency-svc-k95nr
Jul  9 03:48:50.133: INFO: Got endpoints: latency-svc-fthdr [293.7221ms]
Jul  9 03:48:50.139: INFO: Created: latency-svc-5zmhw
Jul  9 03:48:50.154: INFO: Created: latency-svc-shlvq
Jul  9 03:48:50.165: INFO: Created: latency-svc-bjtf2
Jul  9 03:48:50.179: INFO: Got endpoints: latency-svc-6q4t6 [325.1616ms]
Jul  9 03:48:50.184: INFO: Created: latency-svc-rj8n5
Jul  9 03:48:50.192: INFO: Created: latency-svc-gbd72
Jul  9 03:48:50.206: INFO: Created: latency-svc-h7z5x
Jul  9 03:48:50.218: INFO: Created: latency-svc-wl2p5
Jul  9 03:48:50.229: INFO: Created: latency-svc-bxt2s
Jul  9 03:48:50.232: INFO: Got endpoints: latency-svc-flzzj [365.604ms]
Jul  9 03:48:50.239: INFO: Created: latency-svc-pz8vr
Jul  9 03:48:50.251: INFO: Created: latency-svc-hglpl
Jul  9 03:48:50.262: INFO: Created: latency-svc-6njvb
Jul  9 03:48:50.271: INFO: Created: latency-svc-d2g6s
Jul  9 03:48:50.284: INFO: Got endpoints: latency-svc-sjg92 [395.1938ms]
Jul  9 03:48:50.285: INFO: Created: latency-svc-f49xg
Jul  9 03:48:50.295: INFO: Created: latency-svc-bj5mg
Jul  9 03:48:50.329: INFO: Got endpoints: latency-svc-k95nr [424.8016ms]
Jul  9 03:48:50.342: INFO: Created: latency-svc-xc48n
Jul  9 03:48:50.377: INFO: Got endpoints: latency-svc-5zmhw [462.8916ms]
Jul  9 03:48:50.395: INFO: Created: latency-svc-9pzlt
Jul  9 03:48:50.427: INFO: Got endpoints: latency-svc-shlvq [497.471ms]
Jul  9 03:48:50.440: INFO: Created: latency-svc-tjqw6
Jul  9 03:48:50.478: INFO: Got endpoints: latency-svc-bjtf2 [536.4559ms]
Jul  9 03:48:50.493: INFO: Created: latency-svc-9qt2g
Jul  9 03:48:50.526: INFO: Got endpoints: latency-svc-rj8n5 [564.6192ms]
Jul  9 03:48:50.543: INFO: Created: latency-svc-24fs5
Jul  9 03:48:50.579: INFO: Got endpoints: latency-svc-gbd72 [605.8891ms]
Jul  9 03:48:50.591: INFO: Created: latency-svc-gpb6t
Jul  9 03:48:50.629: INFO: Got endpoints: latency-svc-h7z5x [643.5629ms]
Jul  9 03:48:50.642: INFO: Created: latency-svc-564f8
Jul  9 03:48:50.676: INFO: Got endpoints: latency-svc-wl2p5 [665.3313ms]
Jul  9 03:48:50.690: INFO: Created: latency-svc-79j5s
Jul  9 03:48:50.728: INFO: Got endpoints: latency-svc-bxt2s [698.7037ms]
Jul  9 03:48:50.746: INFO: Created: latency-svc-bftcg
Jul  9 03:48:50.777: INFO: Got endpoints: latency-svc-pz8vr [731.163ms]
Jul  9 03:48:50.804: INFO: Created: latency-svc-76f8d
Jul  9 03:48:50.827: INFO: Got endpoints: latency-svc-hglpl [741.7037ms]
Jul  9 03:48:50.843: INFO: Created: latency-svc-tb9v2
Jul  9 03:48:50.877: INFO: Got endpoints: latency-svc-6njvb [743.9388ms]
Jul  9 03:48:50.888: INFO: Created: latency-svc-459h8
Jul  9 03:48:50.929: INFO: Got endpoints: latency-svc-d2g6s [749.7975ms]
Jul  9 03:48:50.943: INFO: Created: latency-svc-ksv8f
Jul  9 03:48:50.976: INFO: Got endpoints: latency-svc-f49xg [743.9629ms]
Jul  9 03:48:50.989: INFO: Created: latency-svc-nfzrk
Jul  9 03:48:51.028: INFO: Got endpoints: latency-svc-bj5mg [743.5604ms]
Jul  9 03:48:51.039: INFO: Created: latency-svc-htn68
Jul  9 03:48:51.077: INFO: Got endpoints: latency-svc-xc48n [747.8048ms]
Jul  9 03:48:51.094: INFO: Created: latency-svc-97csz
Jul  9 03:48:51.128: INFO: Got endpoints: latency-svc-9pzlt [751.2999ms]
Jul  9 03:48:51.146: INFO: Created: latency-svc-7gnk2
Jul  9 03:48:51.177: INFO: Got endpoints: latency-svc-tjqw6 [750.005ms]
Jul  9 03:48:51.191: INFO: Created: latency-svc-n65cn
Jul  9 03:48:51.228: INFO: Got endpoints: latency-svc-9qt2g [749.1117ms]
Jul  9 03:48:51.246: INFO: Created: latency-svc-2nplh
Jul  9 03:48:51.277: INFO: Got endpoints: latency-svc-24fs5 [750.9179ms]
Jul  9 03:48:51.289: INFO: Created: latency-svc-j9mnm
Jul  9 03:48:51.328: INFO: Got endpoints: latency-svc-gpb6t [748.9799ms]
Jul  9 03:48:51.346: INFO: Created: latency-svc-j8sbz
Jul  9 03:48:51.378: INFO: Got endpoints: latency-svc-564f8 [748.401ms]
Jul  9 03:48:51.391: INFO: Created: latency-svc-n8kb9
Jul  9 03:48:51.427: INFO: Got endpoints: latency-svc-79j5s [751.1222ms]
Jul  9 03:48:51.444: INFO: Created: latency-svc-7nbrw
Jul  9 03:48:51.478: INFO: Got endpoints: latency-svc-bftcg [749.8983ms]
Jul  9 03:48:51.489: INFO: Created: latency-svc-gzcqp
Jul  9 03:48:51.528: INFO: Got endpoints: latency-svc-76f8d [750.9057ms]
Jul  9 03:48:51.544: INFO: Created: latency-svc-cx6gr
Jul  9 03:48:51.582: INFO: Got endpoints: latency-svc-tb9v2 [754.4083ms]
Jul  9 03:48:51.596: INFO: Created: latency-svc-pdzhq
Jul  9 03:48:51.628: INFO: Got endpoints: latency-svc-459h8 [751.4449ms]
Jul  9 03:48:51.644: INFO: Created: latency-svc-w6krx
Jul  9 03:48:51.677: INFO: Got endpoints: latency-svc-ksv8f [748.2651ms]
Jul  9 03:48:51.697: INFO: Created: latency-svc-fdr4v
Jul  9 03:48:51.727: INFO: Got endpoints: latency-svc-nfzrk [750.4409ms]
Jul  9 03:48:51.738: INFO: Created: latency-svc-s9lp7
Jul  9 03:48:51.777: INFO: Got endpoints: latency-svc-htn68 [749.3729ms]
Jul  9 03:48:51.788: INFO: Created: latency-svc-lfwnl
Jul  9 03:48:51.829: INFO: Got endpoints: latency-svc-97csz [752.1056ms]
Jul  9 03:48:51.848: INFO: Created: latency-svc-fb4w4
Jul  9 03:48:51.877: INFO: Got endpoints: latency-svc-7gnk2 [749.0365ms]
Jul  9 03:48:51.889: INFO: Created: latency-svc-mbn78
Jul  9 03:48:51.928: INFO: Got endpoints: latency-svc-n65cn [750.756ms]
Jul  9 03:48:51.943: INFO: Created: latency-svc-z2shq
Jul  9 03:48:51.977: INFO: Got endpoints: latency-svc-2nplh [749.6291ms]
Jul  9 03:48:52.013: INFO: Created: latency-svc-5dm2h
Jul  9 03:48:52.027: INFO: Got endpoints: latency-svc-j9mnm [749.8871ms]
Jul  9 03:48:52.041: INFO: Created: latency-svc-xf8fl
Jul  9 03:48:52.078: INFO: Got endpoints: latency-svc-j8sbz [749.7099ms]
Jul  9 03:48:52.096: INFO: Created: latency-svc-zmmpn
Jul  9 03:48:52.127: INFO: Got endpoints: latency-svc-n8kb9 [748.9166ms]
Jul  9 03:48:52.142: INFO: Created: latency-svc-8pvmx
Jul  9 03:48:52.177: INFO: Got endpoints: latency-svc-7nbrw [749.8787ms]
Jul  9 03:48:52.190: INFO: Created: latency-svc-lmc66
Jul  9 03:48:52.227: INFO: Got endpoints: latency-svc-gzcqp [749.3835ms]
Jul  9 03:48:52.241: INFO: Created: latency-svc-qqpkq
Jul  9 03:48:52.276: INFO: Got endpoints: latency-svc-cx6gr [747.9211ms]
Jul  9 03:48:52.287: INFO: Created: latency-svc-d448l
Jul  9 03:48:52.331: INFO: Got endpoints: latency-svc-pdzhq [749.2075ms]
Jul  9 03:48:52.340: INFO: Created: latency-svc-445zf
Jul  9 03:48:52.376: INFO: Got endpoints: latency-svc-w6krx [747.3613ms]
Jul  9 03:48:52.387: INFO: Created: latency-svc-8fscp
Jul  9 03:48:52.427: INFO: Got endpoints: latency-svc-fdr4v [749.936ms]
Jul  9 03:48:52.445: INFO: Created: latency-svc-6brwq
Jul  9 03:48:52.476: INFO: Got endpoints: latency-svc-s9lp7 [748.6927ms]
Jul  9 03:48:52.485: INFO: Created: latency-svc-d42k9
Jul  9 03:48:52.526: INFO: Got endpoints: latency-svc-lfwnl [748.738ms]
Jul  9 03:48:52.537: INFO: Created: latency-svc-f2kdf
Jul  9 03:48:52.578: INFO: Got endpoints: latency-svc-fb4w4 [748.4859ms]
Jul  9 03:48:52.589: INFO: Created: latency-svc-7bspm
Jul  9 03:48:52.629: INFO: Got endpoints: latency-svc-mbn78 [751.4044ms]
Jul  9 03:48:52.646: INFO: Created: latency-svc-c5s6s
Jul  9 03:48:52.676: INFO: Got endpoints: latency-svc-z2shq [747.771ms]
Jul  9 03:48:52.685: INFO: Created: latency-svc-824dr
Jul  9 03:48:52.727: INFO: Got endpoints: latency-svc-5dm2h [749.3889ms]
Jul  9 03:48:52.735: INFO: Created: latency-svc-br56g
Jul  9 03:48:52.779: INFO: Got endpoints: latency-svc-xf8fl [751.5565ms]
Jul  9 03:48:52.793: INFO: Created: latency-svc-4f97d
Jul  9 03:48:52.826: INFO: Got endpoints: latency-svc-zmmpn [748.123ms]
Jul  9 03:48:52.837: INFO: Created: latency-svc-njt2p
Jul  9 03:48:52.876: INFO: Got endpoints: latency-svc-8pvmx [749.1312ms]
Jul  9 03:48:52.885: INFO: Created: latency-svc-87nrl
Jul  9 03:48:52.926: INFO: Got endpoints: latency-svc-lmc66 [748.6846ms]
Jul  9 03:48:52.943: INFO: Created: latency-svc-9b88j
Jul  9 03:48:52.977: INFO: Got endpoints: latency-svc-qqpkq [750.0936ms]
Jul  9 03:48:52.986: INFO: Created: latency-svc-x2lc9
Jul  9 03:48:53.026: INFO: Got endpoints: latency-svc-d448l [750.2773ms]
Jul  9 03:48:53.037: INFO: Created: latency-svc-fj9vn
Jul  9 03:48:53.076: INFO: Got endpoints: latency-svc-445zf [744.6674ms]
Jul  9 03:48:53.086: INFO: Created: latency-svc-8rpxp
Jul  9 03:48:53.127: INFO: Got endpoints: latency-svc-8fscp [750.5482ms]
Jul  9 03:48:53.137: INFO: Created: latency-svc-qpkwz
Jul  9 03:48:53.177: INFO: Got endpoints: latency-svc-6brwq [749.8524ms]
Jul  9 03:48:53.187: INFO: Created: latency-svc-v9wfx
Jul  9 03:48:53.227: INFO: Got endpoints: latency-svc-d42k9 [750.9248ms]
Jul  9 03:48:53.238: INFO: Created: latency-svc-hlsjd
Jul  9 03:48:53.277: INFO: Got endpoints: latency-svc-f2kdf [751.2841ms]
Jul  9 03:48:53.287: INFO: Created: latency-svc-5w5xt
Jul  9 03:48:53.326: INFO: Got endpoints: latency-svc-7bspm [748.4019ms]
Jul  9 03:48:53.335: INFO: Created: latency-svc-wlpgk
Jul  9 03:48:53.376: INFO: Got endpoints: latency-svc-c5s6s [747.0646ms]
Jul  9 03:48:53.391: INFO: Created: latency-svc-lzth2
Jul  9 03:48:53.433: INFO: Got endpoints: latency-svc-824dr [756.9768ms]
Jul  9 03:48:53.442: INFO: Created: latency-svc-brw6f
Jul  9 03:48:53.481: INFO: Got endpoints: latency-svc-br56g [754.1222ms]
Jul  9 03:48:53.490: INFO: Created: latency-svc-vkdcf
Jul  9 03:48:53.525: INFO: Got endpoints: latency-svc-4f97d [746.7193ms]
Jul  9 03:48:53.539: INFO: Created: latency-svc-kjsnf
Jul  9 03:48:53.579: INFO: Got endpoints: latency-svc-njt2p [752.9759ms]
Jul  9 03:48:53.603: INFO: Created: latency-svc-5zqt5
Jul  9 03:48:53.626: INFO: Got endpoints: latency-svc-87nrl [749.6148ms]
Jul  9 03:48:53.640: INFO: Created: latency-svc-xcs96
Jul  9 03:48:53.677: INFO: Got endpoints: latency-svc-9b88j [751.2781ms]
Jul  9 03:48:53.686: INFO: Created: latency-svc-z47rw
Jul  9 03:48:53.726: INFO: Got endpoints: latency-svc-x2lc9 [748.4933ms]
Jul  9 03:48:53.739: INFO: Created: latency-svc-9q4zz
Jul  9 03:48:53.778: INFO: Got endpoints: latency-svc-fj9vn [752.3747ms]
Jul  9 03:48:53.790: INFO: Created: latency-svc-rcbgr
Jul  9 03:48:53.826: INFO: Got endpoints: latency-svc-8rpxp [750.3361ms]
Jul  9 03:48:53.837: INFO: Created: latency-svc-d96jf
Jul  9 03:48:53.876: INFO: Got endpoints: latency-svc-qpkwz [749.0648ms]
Jul  9 03:48:53.887: INFO: Created: latency-svc-mvwg4
Jul  9 03:48:53.926: INFO: Got endpoints: latency-svc-v9wfx [748.415ms]
Jul  9 03:48:53.937: INFO: Created: latency-svc-77x2j
Jul  9 03:48:53.976: INFO: Got endpoints: latency-svc-hlsjd [748.8906ms]
Jul  9 03:48:53.986: INFO: Created: latency-svc-cz6hl
Jul  9 03:48:54.027: INFO: Got endpoints: latency-svc-5w5xt [750.2428ms]
Jul  9 03:48:54.041: INFO: Created: latency-svc-wn9s7
Jul  9 03:48:54.078: INFO: Got endpoints: latency-svc-wlpgk [751.9375ms]
Jul  9 03:48:54.089: INFO: Created: latency-svc-dskdj
Jul  9 03:48:54.127: INFO: Got endpoints: latency-svc-lzth2 [751.1164ms]
Jul  9 03:48:54.135: INFO: Created: latency-svc-7ms6n
Jul  9 03:48:54.177: INFO: Got endpoints: latency-svc-brw6f [743.8429ms]
Jul  9 03:48:54.189: INFO: Created: latency-svc-99l6f
Jul  9 03:48:54.226: INFO: Got endpoints: latency-svc-vkdcf [745.2012ms]
Jul  9 03:48:54.238: INFO: Created: latency-svc-tzkm7
Jul  9 03:48:54.281: INFO: Got endpoints: latency-svc-kjsnf [755.8881ms]
Jul  9 03:48:54.295: INFO: Created: latency-svc-sdkcq
Jul  9 03:48:54.326: INFO: Got endpoints: latency-svc-5zqt5 [746.8587ms]
Jul  9 03:48:54.337: INFO: Created: latency-svc-kggfc
Jul  9 03:48:54.377: INFO: Got endpoints: latency-svc-xcs96 [750.8688ms]
Jul  9 03:48:54.388: INFO: Created: latency-svc-wtcbd
Jul  9 03:48:54.426: INFO: Got endpoints: latency-svc-z47rw [748.5884ms]
Jul  9 03:48:54.434: INFO: Created: latency-svc-ccdz5
Jul  9 03:48:54.476: INFO: Got endpoints: latency-svc-9q4zz [750.3372ms]
Jul  9 03:48:54.487: INFO: Created: latency-svc-hz8d9
Jul  9 03:48:54.533: INFO: Got endpoints: latency-svc-rcbgr [754.2829ms]
Jul  9 03:48:54.542: INFO: Created: latency-svc-b8f47
Jul  9 03:48:54.576: INFO: Got endpoints: latency-svc-d96jf [749.7614ms]
Jul  9 03:48:54.586: INFO: Created: latency-svc-lqbq2
Jul  9 03:48:54.626: INFO: Got endpoints: latency-svc-mvwg4 [750.0297ms]
Jul  9 03:48:54.640: INFO: Created: latency-svc-lgl4c
Jul  9 03:48:54.677: INFO: Got endpoints: latency-svc-77x2j [751.5195ms]
Jul  9 03:48:54.689: INFO: Created: latency-svc-8mmf8
Jul  9 03:48:54.727: INFO: Got endpoints: latency-svc-cz6hl [751.2153ms]
Jul  9 03:48:54.735: INFO: Created: latency-svc-44sh6
Jul  9 03:48:54.776: INFO: Got endpoints: latency-svc-wn9s7 [748.7452ms]
Jul  9 03:48:54.785: INFO: Created: latency-svc-dbfkx
Jul  9 03:48:54.826: INFO: Got endpoints: latency-svc-dskdj [747.9483ms]
Jul  9 03:48:54.836: INFO: Created: latency-svc-cv268
Jul  9 03:48:54.877: INFO: Got endpoints: latency-svc-7ms6n [749.7858ms]
Jul  9 03:48:54.886: INFO: Created: latency-svc-pwmcb
Jul  9 03:48:54.927: INFO: Got endpoints: latency-svc-99l6f [750.395ms]
Jul  9 03:48:54.939: INFO: Created: latency-svc-cr2xs
Jul  9 03:48:54.977: INFO: Got endpoints: latency-svc-tzkm7 [750.667ms]
Jul  9 03:48:54.986: INFO: Created: latency-svc-hgs6w
Jul  9 03:48:55.027: INFO: Got endpoints: latency-svc-sdkcq [745.4185ms]
Jul  9 03:48:55.041: INFO: Created: latency-svc-x7dck
Jul  9 03:48:55.076: INFO: Got endpoints: latency-svc-kggfc [750.6523ms]
Jul  9 03:48:55.085: INFO: Created: latency-svc-72n7l
Jul  9 03:48:55.125: INFO: Got endpoints: latency-svc-wtcbd [748.9121ms]
Jul  9 03:48:55.134: INFO: Created: latency-svc-xqtnd
Jul  9 03:48:55.176: INFO: Got endpoints: latency-svc-ccdz5 [750.5936ms]
Jul  9 03:48:55.187: INFO: Created: latency-svc-sgvx7
Jul  9 03:48:55.227: INFO: Got endpoints: latency-svc-hz8d9 [750.6876ms]
Jul  9 03:48:55.238: INFO: Created: latency-svc-5zz66
Jul  9 03:48:55.276: INFO: Got endpoints: latency-svc-b8f47 [742.9264ms]
Jul  9 03:48:55.283: INFO: Created: latency-svc-8g5jn
Jul  9 03:48:55.329: INFO: Got endpoints: latency-svc-lqbq2 [752.8294ms]
Jul  9 03:48:55.338: INFO: Created: latency-svc-hhqr2
Jul  9 03:48:55.379: INFO: Got endpoints: latency-svc-lgl4c [753.0052ms]
Jul  9 03:48:55.389: INFO: Created: latency-svc-2k5xj
Jul  9 03:48:55.426: INFO: Got endpoints: latency-svc-8mmf8 [748.7227ms]
Jul  9 03:48:55.436: INFO: Created: latency-svc-zjv52
Jul  9 03:48:55.479: INFO: Got endpoints: latency-svc-44sh6 [752.0076ms]
Jul  9 03:48:55.488: INFO: Created: latency-svc-kjcg8
Jul  9 03:48:55.526: INFO: Got endpoints: latency-svc-dbfkx [749.7617ms]
Jul  9 03:48:55.538: INFO: Created: latency-svc-b52mh
Jul  9 03:48:55.576: INFO: Got endpoints: latency-svc-cv268 [750.1825ms]
Jul  9 03:48:55.592: INFO: Created: latency-svc-zwt7n
Jul  9 03:48:55.627: INFO: Got endpoints: latency-svc-pwmcb [749.7857ms]
Jul  9 03:48:55.636: INFO: Created: latency-svc-8rkk4
Jul  9 03:48:55.676: INFO: Got endpoints: latency-svc-cr2xs [748.9608ms]
Jul  9 03:48:55.692: INFO: Created: latency-svc-92r9g
Jul  9 03:48:55.727: INFO: Got endpoints: latency-svc-hgs6w [750.3735ms]
Jul  9 03:48:55.745: INFO: Created: latency-svc-c2h59
Jul  9 03:48:55.779: INFO: Got endpoints: latency-svc-x7dck [752.4459ms]
Jul  9 03:48:55.795: INFO: Created: latency-svc-kr799
Jul  9 03:48:55.827: INFO: Got endpoints: latency-svc-72n7l [750.34ms]
Jul  9 03:48:55.836: INFO: Created: latency-svc-7zs68
Jul  9 03:48:55.876: INFO: Got endpoints: latency-svc-xqtnd [750.2535ms]
Jul  9 03:48:55.884: INFO: Created: latency-svc-d2cmm
Jul  9 03:48:55.926: INFO: Got endpoints: latency-svc-sgvx7 [749.4101ms]
Jul  9 03:48:55.933: INFO: Created: latency-svc-7clrs
Jul  9 03:48:55.975: INFO: Got endpoints: latency-svc-5zz66 [748.6283ms]
Jul  9 03:48:55.986: INFO: Created: latency-svc-mgcxt
Jul  9 03:48:56.026: INFO: Got endpoints: latency-svc-8g5jn [750.5506ms]
Jul  9 03:48:56.035: INFO: Created: latency-svc-pmbsk
Jul  9 03:48:56.079: INFO: Got endpoints: latency-svc-hhqr2 [749.7017ms]
Jul  9 03:48:56.088: INFO: Created: latency-svc-2nlfg
Jul  9 03:48:56.126: INFO: Got endpoints: latency-svc-2k5xj [746.9772ms]
Jul  9 03:48:56.145: INFO: Created: latency-svc-r59rg
Jul  9 03:48:56.176: INFO: Got endpoints: latency-svc-zjv52 [749.6884ms]
Jul  9 03:48:56.184: INFO: Created: latency-svc-2nszr
Jul  9 03:48:56.226: INFO: Got endpoints: latency-svc-kjcg8 [747.4738ms]
Jul  9 03:48:56.236: INFO: Created: latency-svc-v6x8m
Jul  9 03:48:56.278: INFO: Got endpoints: latency-svc-b52mh [751.4777ms]
Jul  9 03:48:56.289: INFO: Created: latency-svc-2vm64
Jul  9 03:48:56.326: INFO: Got endpoints: latency-svc-zwt7n [750.2022ms]
Jul  9 03:48:56.338: INFO: Created: latency-svc-dm2p5
Jul  9 03:48:56.377: INFO: Got endpoints: latency-svc-8rkk4 [750.1436ms]
Jul  9 03:48:56.387: INFO: Created: latency-svc-8cbbj
Jul  9 03:48:56.427: INFO: Got endpoints: latency-svc-92r9g [750.5739ms]
Jul  9 03:48:56.437: INFO: Created: latency-svc-pst6f
Jul  9 03:48:56.476: INFO: Got endpoints: latency-svc-c2h59 [749.3768ms]
Jul  9 03:48:56.487: INFO: Created: latency-svc-x8pcp
Jul  9 03:48:56.527: INFO: Got endpoints: latency-svc-kr799 [747.6056ms]
Jul  9 03:48:56.539: INFO: Created: latency-svc-bvgjb
Jul  9 03:48:56.601: INFO: Got endpoints: latency-svc-7zs68 [774.4345ms]
Jul  9 03:48:56.697: INFO: Got endpoints: latency-svc-7clrs [771.4791ms]
Jul  9 03:48:56.698: INFO: Got endpoints: latency-svc-d2cmm [821.8962ms]
Jul  9 03:48:56.698: INFO: Created: latency-svc-bnnj6
Jul  9 03:48:56.721: INFO: Created: latency-svc-dzqds
Jul  9 03:48:56.728: INFO: Got endpoints: latency-svc-mgcxt [752.6934ms]
Jul  9 03:48:56.737: INFO: Created: latency-svc-bzql7
Jul  9 03:48:56.745: INFO: Created: latency-svc-gdqc5
Jul  9 03:48:56.777: INFO: Got endpoints: latency-svc-pmbsk [750.7711ms]
Jul  9 03:48:56.798: INFO: Created: latency-svc-g4nn9
Jul  9 03:48:56.828: INFO: Got endpoints: latency-svc-2nlfg [749.134ms]
Jul  9 03:48:56.843: INFO: Created: latency-svc-fm4ww
Jul  9 03:48:56.878: INFO: Got endpoints: latency-svc-r59rg [751.8484ms]
Jul  9 03:48:56.890: INFO: Created: latency-svc-7m9g6
Jul  9 03:48:56.927: INFO: Got endpoints: latency-svc-2nszr [750.7406ms]
Jul  9 03:48:56.938: INFO: Created: latency-svc-2dhz8
Jul  9 03:48:56.978: INFO: Got endpoints: latency-svc-v6x8m [751.1995ms]
Jul  9 03:48:56.994: INFO: Created: latency-svc-8fsjv
Jul  9 03:48:57.027: INFO: Got endpoints: latency-svc-2vm64 [749.2586ms]
Jul  9 03:48:57.042: INFO: Created: latency-svc-dxvmw
Jul  9 03:48:57.079: INFO: Got endpoints: latency-svc-dm2p5 [751.9768ms]
Jul  9 03:48:57.094: INFO: Created: latency-svc-qqsgz
Jul  9 03:48:57.129: INFO: Got endpoints: latency-svc-8cbbj [751.8266ms]
Jul  9 03:48:57.140: INFO: Created: latency-svc-cc8gd
Jul  9 03:48:57.177: INFO: Got endpoints: latency-svc-pst6f [749.9979ms]
Jul  9 03:48:57.192: INFO: Created: latency-svc-t58mr
Jul  9 03:48:57.227: INFO: Got endpoints: latency-svc-x8pcp [750.326ms]
Jul  9 03:48:57.277: INFO: Got endpoints: latency-svc-bvgjb [749.7285ms]
Jul  9 03:48:57.328: INFO: Got endpoints: latency-svc-bnnj6 [726.4501ms]
Jul  9 03:48:57.378: INFO: Got endpoints: latency-svc-dzqds [680.3194ms]
Jul  9 03:48:57.428: INFO: Got endpoints: latency-svc-bzql7 [730.2617ms]
Jul  9 03:48:57.481: INFO: Got endpoints: latency-svc-gdqc5 [752.8364ms]
Jul  9 03:48:57.540: INFO: Got endpoints: latency-svc-g4nn9 [763.1935ms]
Jul  9 03:48:57.578: INFO: Got endpoints: latency-svc-fm4ww [750.3456ms]
Jul  9 03:48:57.629: INFO: Got endpoints: latency-svc-7m9g6 [750.7448ms]
Jul  9 03:48:57.677: INFO: Got endpoints: latency-svc-2dhz8 [750.577ms]
Jul  9 03:48:57.728: INFO: Got endpoints: latency-svc-8fsjv [750.5063ms]
Jul  9 03:48:57.779: INFO: Got endpoints: latency-svc-dxvmw [751.6774ms]
Jul  9 03:48:57.827: INFO: Got endpoints: latency-svc-qqsgz [748.5696ms]
Jul  9 03:48:57.877: INFO: Got endpoints: latency-svc-cc8gd [748.6404ms]
Jul  9 03:48:57.927: INFO: Got endpoints: latency-svc-t58mr [750.084ms]
Jul  9 03:48:57.927: INFO: Latencies: [21.062ms 42.4104ms 67.99ms 77.1671ms 93.6025ms 114.7889ms 123.3543ms 144.074ms 156.829ms 172.5191ms 183.249ms 197.9425ms 219.4681ms 220.1806ms 221.703ms 223.2514ms 223.6376ms 223.9922ms 224.071ms 224.2038ms 224.4559ms 224.8708ms 225.2504ms 225.9806ms 226.493ms 226.5016ms 226.89ms 228.5017ms 229.9224ms 232.0036ms 232.7872ms 233.2476ms 233.4923ms 234.0571ms 235.0437ms 236.3665ms 236.373ms 236.7912ms 236.8144ms 242.4093ms 243.8042ms 244.6788ms 258.2918ms 293.7221ms 325.1616ms 365.604ms 395.1938ms 424.8016ms 462.8916ms 497.471ms 536.4559ms 564.6192ms 605.8891ms 643.5629ms 665.3313ms 680.3194ms 698.7037ms 726.4501ms 730.2617ms 731.163ms 741.7037ms 742.9264ms 743.5604ms 743.8429ms 743.9388ms 743.9629ms 744.6674ms 745.2012ms 745.4185ms 746.7193ms 746.8587ms 746.9772ms 747.0646ms 747.3613ms 747.4738ms 747.6056ms 747.771ms 747.8048ms 747.9211ms 747.9483ms 748.123ms 748.2651ms 748.401ms 748.4019ms 748.415ms 748.4859ms 748.4933ms 748.5696ms 748.5884ms 748.6283ms 748.6404ms 748.6846ms 748.6927ms 748.7227ms 748.738ms 748.7452ms 748.8906ms 748.9121ms 748.9166ms 748.9608ms 748.9799ms 749.0365ms 749.0648ms 749.1117ms 749.1312ms 749.134ms 749.2075ms 749.2586ms 749.3729ms 749.3768ms 749.3835ms 749.3889ms 749.4101ms 749.6148ms 749.6291ms 749.6884ms 749.7017ms 749.7099ms 749.7285ms 749.7614ms 749.7617ms 749.7857ms 749.7858ms 749.7975ms 749.8524ms 749.8787ms 749.8871ms 749.8983ms 749.936ms 749.9979ms 750.005ms 750.0297ms 750.084ms 750.0936ms 750.1436ms 750.1825ms 750.2022ms 750.2428ms 750.2535ms 750.2773ms 750.326ms 750.3361ms 750.3372ms 750.34ms 750.3456ms 750.3735ms 750.395ms 750.4409ms 750.5063ms 750.5482ms 750.5506ms 750.5739ms 750.577ms 750.5936ms 750.6523ms 750.667ms 750.6876ms 750.7406ms 750.7448ms 750.756ms 750.7711ms 750.8688ms 750.9057ms 750.9179ms 750.9248ms 751.1164ms 751.1222ms 751.1995ms 751.2153ms 751.2781ms 751.2841ms 751.2999ms 751.4044ms 751.4449ms 751.4777ms 751.5195ms 751.5565ms 751.6774ms 751.8266ms 751.8484ms 751.9375ms 751.9768ms 752.0076ms 752.1056ms 752.3747ms 752.4459ms 752.6934ms 752.8294ms 752.8364ms 752.9759ms 753.0052ms 754.1222ms 754.2829ms 754.4083ms 755.8881ms 756.9768ms 763.1935ms 771.4791ms 774.4345ms 821.8962ms]
Jul  9 03:48:57.927: INFO: 50 %ile: 748.9799ms
Jul  9 03:48:57.927: INFO: 90 %ile: 751.9375ms
Jul  9 03:48:57.927: INFO: 99 %ile: 774.4345ms
Jul  9 03:48:57.927: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:48:57.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3537" for this suite.
Jul  9 03:49:11.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:49:12.083: INFO: namespace svc-latency-3537 deletion completed in 14.1490127s

• [SLOW TEST:24.917 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:49:12.083: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul  9 03:49:22.210: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 160
	[quantile=0.9] = 299809
	[quantile=0.99] = 704051
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 221421
	[quantile=0.9] = 555361
	[quantile=0.99] = 755167
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 2
	[quantile=0.9] = 2
	[quantile=0.99] = 2
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 209365
	[quantile=0.9] = 219784
	[quantile=0.99] = 219784
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 6
	[quantile=0.99] = 29
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 9
	[quantile=0.9] = 18
	[quantile=0.99] = 40
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 8
	[quantile=0.9] = 27
	[quantile=0.99] = 47
For namespace_queue_latency_sum:
	[] = 3319
For namespace_queue_latency_count:
	[] = 286
For namespace_retries:
	[] = 294
For namespace_work_duration:
	[quantile=0.5] = 108432
	[quantile=0.9] = 212007
	[quantile=0.99] = 4740826
For namespace_work_duration_sum:
	[] = 40459028
For namespace_work_duration_count:
	[] = 286
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:49:22.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3379" for this suite.
Jul  9 03:49:28.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:49:28.401: INFO: namespace gc-3379 deletion completed in 6.1837243s

• [SLOW TEST:16.317 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:49:28.401: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-185ac766-a315-4eff-afac-d240ec72ba6b in namespace container-probe-2562
Jul  9 03:49:32.493: INFO: Started pod test-webserver-185ac766-a315-4eff-afac-d240ec72ba6b in namespace container-probe-2562
STEP: checking the pod's current state and verifying that restartCount is present
Jul  9 03:49:32.495: INFO: Initial restart count of pod test-webserver-185ac766-a315-4eff-afac-d240ec72ba6b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:53:33.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2562" for this suite.
Jul  9 03:53:39.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:53:39.347: INFO: namespace container-probe-2562 deletion completed in 6.1768468s

• [SLOW TEST:250.946 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:53:39.347: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:53:39.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b" in namespace "downward-api-9759" to be "success or failure"
Jul  9 03:53:39.417: INFO: Pod "downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7645ms
Jul  9 03:53:41.420: INFO: Pod "downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0057405s
Jul  9 03:53:43.423: INFO: Pod "downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0089686s
STEP: Saw pod success
Jul  9 03:53:43.423: INFO: Pod "downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b" satisfied condition "success or failure"
Jul  9 03:53:43.425: INFO: Trying to get logs from node node2 pod downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b container client-container: <nil>
STEP: delete the pod
Jul  9 03:53:43.445: INFO: Waiting for pod downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b to disappear
Jul  9 03:53:43.447: INFO: Pod downwardapi-volume-780ab741-6cf9-4ad3-abe2-14f97177952b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:53:43.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9759" for this suite.
Jul  9 03:53:49.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:53:49.616: INFO: namespace downward-api-9759 deletion completed in 6.1656729s

• [SLOW TEST:10.269 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:53:49.616: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 03:53:49.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3" in namespace "projected-5194" to be "success or failure"
Jul  9 03:53:49.675: INFO: Pod "downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.9162ms
Jul  9 03:53:51.677: INFO: Pod "downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0098614s
STEP: Saw pod success
Jul  9 03:53:51.677: INFO: Pod "downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3" satisfied condition "success or failure"
Jul  9 03:53:51.682: INFO: Trying to get logs from node node2 pod downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3 container client-container: <nil>
STEP: delete the pod
Jul  9 03:53:51.702: INFO: Waiting for pod downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3 to disappear
Jul  9 03:53:51.705: INFO: Pod downwardapi-volume-d61479f2-adca-4b50-a299-6b3440e903b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:53:51.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5194" for this suite.
Jul  9 03:53:57.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:53:57.891: INFO: namespace projected-5194 deletion completed in 6.1813007s

• [SLOW TEST:8.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:53:57.891: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-2f8cec8d-fc11-4573-8b59-39df19b4aeaa
STEP: Creating a pod to test consume secrets
Jul  9 03:53:57.953: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663" in namespace "projected-6462" to be "success or failure"
Jul  9 03:53:57.967: INFO: Pod "pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663": Phase="Pending", Reason="", readiness=false. Elapsed: 14.2117ms
Jul  9 03:53:59.970: INFO: Pod "pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0176829s
STEP: Saw pod success
Jul  9 03:53:59.970: INFO: Pod "pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663" satisfied condition "success or failure"
Jul  9 03:53:59.972: INFO: Trying to get logs from node node2 pod pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663 container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 03:53:59.997: INFO: Waiting for pod pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663 to disappear
Jul  9 03:53:59.999: INFO: Pod pod-projected-secrets-10f4abb5-3f95-4b71-ba05-d85b4bff4663 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:53:59.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6462" for this suite.
Jul  9 03:54:06.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:54:06.156: INFO: namespace projected-6462 deletion completed in 6.1544916s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:54:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul  9 03:54:10.208: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-6dc4adc2-be60-48bc-baeb-1046151998f8,GenerateName:,Namespace:events-8698,SelfLink:/api/v1/namespaces/events-8698/pods/send-events-6dc4adc2-be60-48bc-baeb-1046151998f8,UID:3bca2ccd-8a7e-4c35-be1d-30de57bfcb4f,ResourceVersion:22364,Generation:0,CreationTimestamp:2019-07-09 03:54:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 186896700,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p5xzw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p5xzw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-p5xzw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a07d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a07d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:54:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:54:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:54:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 03:54:05 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.3,StartTime:2019-07-09 03:54:06 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-09 03:54:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6d08b438bb9f34218c782d43ac293eff699812d538c9100883c484d022a841d2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul  9 03:54:12.210: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul  9 03:54:14.217: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:54:14.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8698" for this suite.
Jul  9 03:54:52.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:54:52.436: INFO: namespace events-8698 deletion completed in 38.2034956s

• [SLOW TEST:46.279 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:54:52.436: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-6cb82e32-14cb-4830-88ee-fea20ed27de3 in namespace container-probe-3157
Jul  9 03:54:54.488: INFO: Started pod liveness-6cb82e32-14cb-4830-88ee-fea20ed27de3 in namespace container-probe-3157
STEP: checking the pod's current state and verifying that restartCount is present
Jul  9 03:54:54.491: INFO: Initial restart count of pod liveness-6cb82e32-14cb-4830-88ee-fea20ed27de3 is 0
Jul  9 03:55:16.534: INFO: Restart count of pod container-probe-3157/liveness-6cb82e32-14cb-4830-88ee-fea20ed27de3 is now 1 (22.0431833s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:55:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3157" for this suite.
Jul  9 03:55:22.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:55:22.694: INFO: namespace container-probe-3157 deletion completed in 6.1444462s

• [SLOW TEST:30.258 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:55:22.694: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul  9 03:55:32.768: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 162
	[quantile=0.9] = 299809
	[quantile=0.99] = 704051
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 235982
	[quantile=0.9] = 555361
	[quantile=0.99] = 755167
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 2
	[quantile=0.9] = 2
	[quantile=0.99] = 2
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 209365
	[quantile=0.9] = 219784
	[quantile=0.99] = 219784
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 6
	[quantile=0.99] = 36
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 9
	[quantile=0.9] = 16
	[quantile=0.99] = 40
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 8
	[quantile=0.9] = 13
	[quantile=0.99] = 32
For namespace_queue_latency_sum:
	[] = 3389
For namespace_queue_latency_count:
	[] = 295
For namespace_retries:
	[] = 304
For namespace_work_duration:
	[quantile=0.5] = 125853
	[quantile=0.9] = 288883
	[quantile=0.99] = 4740826
For namespace_work_duration_sum:
	[] = 42120412
For namespace_work_duration_count:
	[] = 295
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:55:32.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-83" for this suite.
Jul  9 03:55:38.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:55:38.984: INFO: namespace gc-83 deletion completed in 6.2089686s

• [SLOW TEST:16.290 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:55:38.984: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 03:55:39.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-448'
Jul  9 03:55:39.430: INFO: stderr: ""
Jul  9 03:55:39.430: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul  9 03:55:44.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pod e2e-test-nginx-pod --namespace=kubectl-448 -o json'
Jul  9 03:55:44.551: INFO: stderr: ""
Jul  9 03:55:44.551: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-09T03:55:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-448\",\n        \"resourceVersion\": \"22623\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-448/pods/e2e-test-nginx-pod\",\n        \"uid\": \"bd14b913-a9c2-4b01-9156-3fb8044e442d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-n56bt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-n56bt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-n56bt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-09T03:55:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-09T03:55:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-09T03:55:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-09T03:55:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://8644cf765a1a8fa30567a3b18bd18cb4c1075e7e36c6f83cf6386a8334a77d96\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-09T03:55:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.28.128.13\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.64.3\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-09T03:55:39Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  9 03:55:44.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 replace -f - --namespace=kubectl-448'
Jul  9 03:55:44.687: INFO: stderr: ""
Jul  9 03:55:44.687: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul  9 03:55:44.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete pods e2e-test-nginx-pod --namespace=kubectl-448'
Jul  9 03:55:46.281: INFO: stderr: ""
Jul  9 03:55:46.281: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:55:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-448" for this suite.
Jul  9 03:55:52.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:55:52.436: INFO: namespace kubectl-448 deletion completed in 6.1516557s

• [SLOW TEST:13.452 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:55:52.436: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  9 03:55:52.473: INFO: Waiting up to 5m0s for pod "downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3" in namespace "downward-api-7822" to be "success or failure"
Jul  9 03:55:52.478: INFO: Pod "downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.8586ms
Jul  9 03:55:54.484: INFO: Pod "downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0111908s
STEP: Saw pod success
Jul  9 03:55:54.484: INFO: Pod "downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3" satisfied condition "success or failure"
Jul  9 03:55:54.487: INFO: Trying to get logs from node node2 pod downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3 container dapi-container: <nil>
STEP: delete the pod
Jul  9 03:55:54.517: INFO: Waiting for pod downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3 to disappear
Jul  9 03:55:54.524: INFO: Pod downward-api-0aba4d4d-8e53-49db-b5a9-c4640c02d0c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:55:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7822" for this suite.
Jul  9 03:56:00.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:56:00.705: INFO: namespace downward-api-7822 deletion completed in 6.1763358s

• [SLOW TEST:8.270 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:56:00.705: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:57:00.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5491" for this suite.
Jul  9 03:57:22.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:57:22.906: INFO: namespace container-probe-5491 deletion completed in 22.1563252s

• [SLOW TEST:82.201 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:57:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3893/configmap-test-1ca65745-548b-486d-a882-6e852ba15a9e
STEP: Creating a pod to test consume configMaps
Jul  9 03:57:22.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485" in namespace "configmap-3893" to be "success or failure"
Jul  9 03:57:22.955: INFO: Pod "pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485": Phase="Pending", Reason="", readiness=false. Elapsed: 5.1586ms
Jul  9 03:57:24.959: INFO: Pod "pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0096612s
STEP: Saw pod success
Jul  9 03:57:24.959: INFO: Pod "pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485" satisfied condition "success or failure"
Jul  9 03:57:24.961: INFO: Trying to get logs from node node2 pod pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485 container env-test: <nil>
STEP: delete the pod
Jul  9 03:57:24.981: INFO: Waiting for pod pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485 to disappear
Jul  9 03:57:24.983: INFO: Pod pod-configmaps-d51ec91b-d08f-40ed-b21c-48cb62694485 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 03:57:24.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3893" for this suite.
Jul  9 03:57:30.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 03:57:31.164: INFO: namespace configmap-3893 deletion completed in 6.1788683s

• [SLOW TEST:8.258 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 03:57:31.165: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-576a3547-898c-4d3f-bc2f-a26256d7dd4b in namespace container-probe-535
Jul  9 03:57:33.221: INFO: Started pod busybox-576a3547-898c-4d3f-bc2f-a26256d7dd4b in namespace container-probe-535
STEP: checking the pod's current state and verifying that restartCount is present
Jul  9 03:57:33.223: INFO: Initial restart count of pod busybox-576a3547-898c-4d3f-bc2f-a26256d7dd4b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:01:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-535" for this suite.
Jul  9 04:01:39.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:01:40.062: INFO: namespace container-probe-535 deletion completed in 6.2158362s

• [SLOW TEST:248.897 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:01:40.062: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul  9 04:01:42.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4205 pod-service-account-29132c3f-4df5-4835-91d0-56623a4d1c48 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  9 04:01:42.843: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4205 pod-service-account-29132c3f-4df5-4835-91d0-56623a4d1c48 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  9 04:01:43.069: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4205 pod-service-account-29132c3f-4df5-4835-91d0-56623a4d1c48 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:01:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4205" for this suite.
Jul  9 04:01:49.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:01:49.494: INFO: namespace svcaccounts-4205 deletion completed in 6.1728093s

• [SLOW TEST:9.432 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:01:49.494: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7292/configmap-test-8ee72d84-70e1-4191-b624-506af61f4ccd
STEP: Creating a pod to test consume configMaps
Jul  9 04:01:49.526: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d" in namespace "configmap-7292" to be "success or failure"
Jul  9 04:01:49.534: INFO: Pod "pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.0966ms
Jul  9 04:01:51.538: INFO: Pod "pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0113443s
STEP: Saw pod success
Jul  9 04:01:51.538: INFO: Pod "pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d" satisfied condition "success or failure"
Jul  9 04:01:51.540: INFO: Trying to get logs from node node2 pod pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d container env-test: <nil>
STEP: delete the pod
Jul  9 04:01:51.552: INFO: Waiting for pod pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d to disappear
Jul  9 04:01:51.555: INFO: Pod pod-configmaps-fa3e44db-9f1d-4f8d-a45c-5ca736d0ea3d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:01:51.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7292" for this suite.
Jul  9 04:01:57.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:01:57.748: INFO: namespace configmap-7292 deletion completed in 6.1897903s

• [SLOW TEST:8.254 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:01:57.748: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  9 04:01:57.788: INFO: Waiting up to 5m0s for pod "pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69" in namespace "emptydir-6192" to be "success or failure"
Jul  9 04:01:57.793: INFO: Pod "pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5317ms
Jul  9 04:01:59.800: INFO: Pod "pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0114141s
STEP: Saw pod success
Jul  9 04:01:59.800: INFO: Pod "pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69" satisfied condition "success or failure"
Jul  9 04:01:59.804: INFO: Trying to get logs from node node2 pod pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69 container test-container: <nil>
STEP: delete the pod
Jul  9 04:01:59.816: INFO: Waiting for pod pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69 to disappear
Jul  9 04:01:59.818: INFO: Pod pod-b559da35-aba8-4fc5-ad0f-9d98bd5afa69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:01:59.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6192" for this suite.
Jul  9 04:02:05.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:02:06.090: INFO: namespace emptydir-6192 deletion completed in 6.2697858s

• [SLOW TEST:8.342 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:02:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  9 04:02:09.195: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:02:09.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3271" for this suite.
Jul  9 04:02:15.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:02:15.393: INFO: namespace container-runtime-3271 deletion completed in 6.1808501s

• [SLOW TEST:9.303 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:02:15.393: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  9 04:02:19.975: INFO: Successfully updated pod "annotationupdatecc55ef8a-aa20-40f4-9ef8-1ffaaeb0431c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:02:22.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7184" for this suite.
Jul  9 04:02:44.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:02:44.154: INFO: namespace projected-7184 deletion completed in 22.1437841s

• [SLOW TEST:28.761 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:02:44.155: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:02:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3840" for this suite.
Jul  9 04:02:56.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:02:56.421: INFO: namespace namespaces-3840 deletion completed in 6.1454622s
STEP: Destroying namespace "nsdeletetest-6152" for this suite.
Jul  9 04:02:56.423: INFO: Namespace nsdeletetest-6152 was already deleted
STEP: Destroying namespace "nsdeletetest-9173" for this suite.
Jul  9 04:03:02.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:02.611: INFO: namespace nsdeletetest-9173 deletion completed in 6.1887748s

• [SLOW TEST:18.457 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:02.612: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  9 04:03:02.661: INFO: Waiting up to 5m0s for pod "downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa" in namespace "downward-api-4566" to be "success or failure"
Jul  9 04:03:02.663: INFO: Pod "downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6905ms
Jul  9 04:03:04.670: INFO: Pod "downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009417s
Jul  9 04:03:06.672: INFO: Pod "downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0116266s
STEP: Saw pod success
Jul  9 04:03:06.672: INFO: Pod "downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa" satisfied condition "success or failure"
Jul  9 04:03:06.678: INFO: Trying to get logs from node node2 pod downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa container dapi-container: <nil>
STEP: delete the pod
Jul  9 04:03:06.701: INFO: Waiting for pod downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa to disappear
Jul  9 04:03:06.707: INFO: Pod downward-api-2096d505-6175-4d30-a0ad-ae21c12f1efa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:06.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4566" for this suite.
Jul  9 04:03:12.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:12.894: INFO: namespace downward-api-4566 deletion completed in 6.1845722s

• [SLOW TEST:10.282 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 04:03:12.922: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:14.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3185" for this suite.
Jul  9 04:03:20.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:20.191: INFO: namespace custom-resource-definition-3185 deletion completed in 6.1807393s

• [SLOW TEST:7.297 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:20.191: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul  9 04:03:20.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4127'
Jul  9 04:03:20.403: INFO: stderr: ""
Jul  9 04:03:20.403: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul  9 04:03:21.410: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:03:21.410: INFO: Found 0 / 1
Jul  9 04:03:22.408: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:03:22.408: INFO: Found 1 / 1
Jul  9 04:03:22.408: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  9 04:03:22.410: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:03:22.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul  9 04:03:22.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 logs redis-master-h6b7p redis-master --namespace=kubectl-4127'
Jul  9 04:03:22.486: INFO: stderr: ""
Jul  9 04:03:22.486: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Jul 04:03:21.677 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Jul 04:03:21.677 # Server started, Redis version 3.2.12\n1:M 09 Jul 04:03:21.677 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Jul 04:03:21.677 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul  9 04:03:22.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 log redis-master-h6b7p redis-master --namespace=kubectl-4127 --tail=1'
Jul  9 04:03:22.568: INFO: stderr: ""
Jul  9 04:03:22.568: INFO: stdout: "1:M 09 Jul 04:03:21.677 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul  9 04:03:22.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 log redis-master-h6b7p redis-master --namespace=kubectl-4127 --limit-bytes=1'
Jul  9 04:03:22.640: INFO: stderr: ""
Jul  9 04:03:22.640: INFO: stdout: " "
STEP: exposing timestamps
Jul  9 04:03:22.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 log redis-master-h6b7p redis-master --namespace=kubectl-4127 --tail=1 --timestamps'
Jul  9 04:03:22.705: INFO: stderr: ""
Jul  9 04:03:22.705: INFO: stdout: "2019-07-09T04:03:21.6814673Z 1:M 09 Jul 04:03:21.677 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul  9 04:03:25.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 log redis-master-h6b7p redis-master --namespace=kubectl-4127 --since=1s'
Jul  9 04:03:25.279: INFO: stderr: ""
Jul  9 04:03:25.280: INFO: stdout: ""
Jul  9 04:03:25.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 log redis-master-h6b7p redis-master --namespace=kubectl-4127 --since=24h'
Jul  9 04:03:25.368: INFO: stderr: ""
Jul  9 04:03:25.368: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Jul 04:03:21.677 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Jul 04:03:21.677 # Server started, Redis version 3.2.12\n1:M 09 Jul 04:03:21.677 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Jul 04:03:21.677 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul  9 04:03:25.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4127'
Jul  9 04:03:25.441: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:03:25.441: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul  9 04:03:25.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4127'
Jul  9 04:03:25.513: INFO: stderr: "No resources found.\n"
Jul  9 04:03:25.513: INFO: stdout: ""
Jul  9 04:03:25.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 get pods -l name=nginx --namespace=kubectl-4127 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  9 04:03:25.573: INFO: stderr: ""
Jul  9 04:03:25.573: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:25.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4127" for this suite.
Jul  9 04:03:31.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:31.720: INFO: namespace kubectl-4127 deletion completed in 6.143911s

• [SLOW TEST:11.529 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:31.720: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul  9 04:03:31.753: INFO: Waiting up to 5m0s for pod "client-containers-67dad903-a3fa-42b8-a250-e759079f1227" in namespace "containers-1573" to be "success or failure"
Jul  9 04:03:31.758: INFO: Pod "client-containers-67dad903-a3fa-42b8-a250-e759079f1227": Phase="Pending", Reason="", readiness=false. Elapsed: 5.0291ms
Jul  9 04:03:33.762: INFO: Pod "client-containers-67dad903-a3fa-42b8-a250-e759079f1227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0090637s
STEP: Saw pod success
Jul  9 04:03:33.762: INFO: Pod "client-containers-67dad903-a3fa-42b8-a250-e759079f1227" satisfied condition "success or failure"
Jul  9 04:03:33.764: INFO: Trying to get logs from node node2 pod client-containers-67dad903-a3fa-42b8-a250-e759079f1227 container test-container: <nil>
STEP: delete the pod
Jul  9 04:03:33.778: INFO: Waiting for pod client-containers-67dad903-a3fa-42b8-a250-e759079f1227 to disappear
Jul  9 04:03:33.781: INFO: Pod client-containers-67dad903-a3fa-42b8-a250-e759079f1227 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:33.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1573" for this suite.
Jul  9 04:03:39.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:39.879: INFO: namespace containers-1573 deletion completed in 6.0946748s

• [SLOW TEST:8.159 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:39.879: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-3a8cc5d2-5cbc-4822-86a5-6d8b3b61ea75
STEP: Creating a pod to test consume configMaps
Jul  9 04:03:39.921: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0" in namespace "projected-7431" to be "success or failure"
Jul  9 04:03:39.923: INFO: Pod "pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134ms
Jul  9 04:03:41.927: INFO: Pod "pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006101s
Jul  9 04:03:43.933: INFO: Pod "pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0116234s
STEP: Saw pod success
Jul  9 04:03:43.933: INFO: Pod "pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0" satisfied condition "success or failure"
Jul  9 04:03:43.935: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 04:03:43.952: INFO: Waiting for pod pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0 to disappear
Jul  9 04:03:43.956: INFO: Pod pod-projected-configmaps-e4794344-e909-413d-a4ad-6e859eac91e0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:43.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7431" for this suite.
Jul  9 04:03:49.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:03:50.105: INFO: namespace projected-7431 deletion completed in 6.1466474s

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:03:50.105: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  9 04:03:50.178: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:03:53.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8598" for this suite.
Jul  9 04:04:15.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:04:16.182: INFO: namespace init-container-8598 deletion completed in 22.2528462s

• [SLOW TEST:26.077 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:04:16.182: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  9 04:04:16.257: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  9 04:04:16.262: INFO: Waiting for terminating namespaces to be deleted...
Jul  9 04:04:16.263: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Jul  9 04:04:16.274: INFO: weave-net-wl5x5 from kube-system started at 2019-07-09 01:52:47 +0000 UTC (2 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container weave ready: true, restart count 3
Jul  9 04:04:16.274: INFO: 	Container weave-npc ready: true, restart count 2
Jul  9 04:04:16.274: INFO: coredns-7d66d4b8cd-bpcx9 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container coredns ready: true, restart count 2
Jul  9 04:04:16.274: INFO: sonobuoy-e2e-job-daa7340bb2c14627 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container e2e ready: true, restart count 0
Jul  9 04:04:16.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  9 04:04:16.274: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-gjrs2 from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  9 04:04:16.274: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  9 04:04:16.274: INFO: kube-proxy-dsfg4 from kube-system started at 2019-07-09 02:08:06 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 04:04:16.274: INFO: sdspaas-ns-controller-5bf85d9c4b-mrzhn from kube-system started at 2019-07-09 02:11:14 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.274: INFO: 	Container sdspaas-ns-controller ready: true, restart count 0
Jul  9 04:04:16.274: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Jul  9 04:04:16.280: INFO: weave-net-zfhsl from kube-system started at 2019-07-09 01:53:41 +0000 UTC (2 container statuses recorded)
Jul  9 04:04:16.280: INFO: 	Container weave ready: true, restart count 3
Jul  9 04:04:16.280: INFO: 	Container weave-npc ready: true, restart count 2
Jul  9 04:04:16.280: INFO: coredns-7d66d4b8cd-6wkt8 from kube-system started at 2019-07-09 02:07:51 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.280: INFO: 	Container coredns ready: true, restart count 2
Jul  9 04:04:16.280: INFO: kube-proxy-4bhgt from kube-system started at 2019-07-09 02:07:56 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.280: INFO: 	Container kube-proxy ready: true, restart count 1
Jul  9 04:04:16.280: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-09 02:39:33 +0000 UTC (1 container statuses recorded)
Jul  9 04:04:16.280: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  9 04:04:16.280: INFO: sonobuoy-systemd-logs-daemon-set-cc5f070898b84cda-zgdws from heptio-sonobuoy started at 2019-07-09 02:39:36 +0000 UTC (2 container statuses recorded)
Jul  9 04:04:16.280: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  9 04:04:16.280: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d78e46a3-3508-4709-8af2-e466e4ef6699 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d78e46a3-3508-4709-8af2-e466e4ef6699 off the node node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d78e46a3-3508-4709-8af2-e466e4ef6699
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:04:20.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4140" for this suite.
Jul  9 04:04:38.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:04:38.494: INFO: namespace sched-pred-4140 deletion completed in 18.1070769s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:04:38.495: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  9 04:04:42.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:42.581: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:44.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:44.587: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:46.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:46.587: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:48.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:48.583: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:50.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:50.583: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:52.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:52.583: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:54.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:54.583: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:56.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:56.589: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:04:58.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:04:58.586: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:05:00.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:05:00.585: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:05:02.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:05:02.583: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  9 04:05:04.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  9 04:05:04.586: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:05:04.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1076" for this suite.
Jul  9 04:05:26.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:05:26.774: INFO: namespace container-lifecycle-hook-1076 deletion completed in 22.1643416s

• [SLOW TEST:48.279 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:05:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  9 04:05:26.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-6935'
Jul  9 04:05:26.866: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  9 04:05:26.866: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul  9 04:05:28.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6935'
Jul  9 04:05:28.957: INFO: stderr: ""
Jul  9 04:05:28.957: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:05:28.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6935" for this suite.
Jul  9 04:05:34.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:05:35.183: INFO: namespace kubectl-6935 deletion completed in 6.2248045s

• [SLOW TEST:8.409 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:05:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-1b0a0189-1ad9-4226-af98-4d2c952d3a2b
STEP: Creating a pod to test consume configMaps
Jul  9 04:05:35.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6" in namespace "configmap-5062" to be "success or failure"
Jul  9 04:05:35.220: INFO: Pod "pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9039ms
Jul  9 04:05:37.222: INFO: Pod "pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0052094s
STEP: Saw pod success
Jul  9 04:05:37.222: INFO: Pod "pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6" satisfied condition "success or failure"
Jul  9 04:05:37.224: INFO: Trying to get logs from node node2 pod pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 04:05:37.246: INFO: Waiting for pod pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6 to disappear
Jul  9 04:05:37.249: INFO: Pod pod-configmaps-96c11fb5-af95-429c-8908-72a3b9e12bd6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:05:37.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5062" for this suite.
Jul  9 04:05:43.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:05:43.468: INFO: namespace configmap-5062 deletion completed in 6.2163147s

• [SLOW TEST:8.285 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:05:43.468: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e5fd580c-ce9b-48df-9dfb-06e18e515d6f
STEP: Creating a pod to test consume configMaps
Jul  9 04:05:43.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801" in namespace "projected-5337" to be "success or failure"
Jul  9 04:05:43.546: INFO: Pod "pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0312ms
Jul  9 04:05:45.553: INFO: Pod "pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0085744s
STEP: Saw pod success
Jul  9 04:05:45.553: INFO: Pod "pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801" satisfied condition "success or failure"
Jul  9 04:05:45.558: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 04:05:45.578: INFO: Waiting for pod pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801 to disappear
Jul  9 04:05:45.580: INFO: Pod pod-projected-configmaps-79f10add-2ec2-4a79-b58d-2b23e40e6801 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:05:45.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5337" for this suite.
Jul  9 04:05:51.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:05:51.808: INFO: namespace projected-5337 deletion completed in 6.2256528s

• [SLOW TEST:8.340 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:05:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  9 04:05:55.937: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:05:55.953: INFO: Pod pod-with-poststart-http-hook still exists
Jul  9 04:05:57.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:05:57.955: INFO: Pod pod-with-poststart-http-hook still exists
Jul  9 04:05:59.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:05:59.956: INFO: Pod pod-with-poststart-http-hook still exists
Jul  9 04:06:01.954: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:06:01.959: INFO: Pod pod-with-poststart-http-hook still exists
Jul  9 04:06:03.954: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:06:03.960: INFO: Pod pod-with-poststart-http-hook still exists
Jul  9 04:06:05.953: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  9 04:06:05.958: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:06:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4850" for this suite.
Jul  9 04:06:27.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:06:28.125: INFO: namespace container-lifecycle-hook-4850 deletion completed in 22.1644007s

• [SLOW TEST:36.317 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:06:28.125: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 04:06:28.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6" in namespace "downward-api-4375" to be "success or failure"
Jul  9 04:06:28.165: INFO: Pod "downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4377ms
Jul  9 04:06:30.171: INFO: Pod "downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0099113s
STEP: Saw pod success
Jul  9 04:06:30.171: INFO: Pod "downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6" satisfied condition "success or failure"
Jul  9 04:06:30.173: INFO: Trying to get logs from node node2 pod downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6 container client-container: <nil>
STEP: delete the pod
Jul  9 04:06:30.194: INFO: Waiting for pod downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6 to disappear
Jul  9 04:06:30.196: INFO: Pod downwardapi-volume-f98a9502-2f24-4cd7-b64c-65e2f6c237e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:06:30.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4375" for this suite.
Jul  9 04:06:36.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:06:36.445: INFO: namespace downward-api-4375 deletion completed in 6.2446802s

• [SLOW TEST:8.320 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:06:36.445: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 101.116.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.116.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.116.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.116.101_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 101.116.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.116.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.116.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.116.101_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  9 04:06:40.540: INFO: Unable to read wheezy_udp@dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.551: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.553: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.557: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.575: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.577: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.579: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.585: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.591: INFO: Unable to read 172.24.116.101_udp@PTR from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.594: INFO: Unable to read 172.24.116.101_tcp@PTR from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.597: INFO: Unable to read jessie_udp@dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.606: INFO: Unable to read jessie_tcp@dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.610: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.615: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.617: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.620: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.624: INFO: Unable to read jessie_udp@PodARecord from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.633: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.635: INFO: Unable to read 172.24.116.101_udp@PTR from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.638: INFO: Unable to read 172.24.116.101_tcp@PTR from pod dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9: the server could not find the requested resource (get pods dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9)
Jul  9 04:06:40.638: INFO: Lookups using dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9 failed for: [wheezy_udp@dns-test-service.dns-9262.svc.cluster.local wheezy_tcp@dns-test-service.dns-9262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 172.24.116.101_udp@PTR 172.24.116.101_tcp@PTR jessie_udp@dns-test-service.dns-9262.svc.cluster.local jessie_tcp@dns-test-service.dns-9262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9262.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-9262.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-9262.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 172.24.116.101_udp@PTR 172.24.116.101_tcp@PTR]

Jul  9 04:06:45.821: INFO: DNS probes using dns-9262/dns-test-8f223e93-852c-4528-9449-5ce418ca3bc9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:06:45.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9262" for this suite.
Jul  9 04:06:51.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:06:52.084: INFO: namespace dns-9262 deletion completed in 6.1864074s

• [SLOW TEST:15.639 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:06:52.084: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 04:06:52.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12" in namespace "projected-9015" to be "success or failure"
Jul  9 04:06:52.159: INFO: Pod "downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5208ms
Jul  9 04:06:54.162: INFO: Pod "downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0047833s
STEP: Saw pod success
Jul  9 04:06:54.162: INFO: Pod "downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12" satisfied condition "success or failure"
Jul  9 04:06:54.164: INFO: Trying to get logs from node node2 pod downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12 container client-container: <nil>
STEP: delete the pod
Jul  9 04:06:54.180: INFO: Waiting for pod downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12 to disappear
Jul  9 04:06:54.182: INFO: Pod downwardapi-volume-857a6215-c105-404f-a426-89fe68e8bb12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:06:54.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9015" for this suite.
Jul  9 04:07:00.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:07:00.382: INFO: namespace projected-9015 deletion completed in 6.1919378s

• [SLOW TEST:8.297 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:07:00.382: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 04:07:00.428: INFO: Create a RollingUpdate DaemonSet
Jul  9 04:07:00.435: INFO: Check that daemon pods launch on every node of the cluster
Jul  9 04:07:00.442: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:00.449: INFO: Number of nodes with available pods: 0
Jul  9 04:07:00.449: INFO: Node node1 is running more than one daemon pod
Jul  9 04:07:01.489: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:01.492: INFO: Number of nodes with available pods: 0
Jul  9 04:07:01.492: INFO: Node node1 is running more than one daemon pod
Jul  9 04:07:02.457: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:02.460: INFO: Number of nodes with available pods: 1
Jul  9 04:07:02.460: INFO: Node node1 is running more than one daemon pod
Jul  9 04:07:03.454: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:03.456: INFO: Number of nodes with available pods: 2
Jul  9 04:07:03.456: INFO: Number of running nodes: 2, number of available pods: 2
Jul  9 04:07:03.456: INFO: Update the DaemonSet to trigger a rollout
Jul  9 04:07:03.463: INFO: Updating DaemonSet daemon-set
Jul  9 04:07:07.507: INFO: Roll back the DaemonSet before rollout is complete
Jul  9 04:07:07.514: INFO: Updating DaemonSet daemon-set
Jul  9 04:07:07.514: INFO: Make sure DaemonSet rollback is complete
Jul  9 04:07:07.520: INFO: Wrong image for pod: daemon-set-xkj4s. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  9 04:07:07.520: INFO: Pod daemon-set-xkj4s is not available
Jul  9 04:07:07.524: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:08.533: INFO: Wrong image for pod: daemon-set-xkj4s. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  9 04:07:08.533: INFO: Pod daemon-set-xkj4s is not available
Jul  9 04:07:08.537: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  9 04:07:09.530: INFO: Pod daemon-set-8m7sb is not available
Jul  9 04:07:09.534: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9595, will wait for the garbage collector to delete the pods
Jul  9 04:07:09.601: INFO: Deleting DaemonSet.extensions daemon-set took: 5.3943ms
Jul  9 04:07:09.901: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.1975ms
Jul  9 04:07:14.108: INFO: Number of nodes with available pods: 0
Jul  9 04:07:14.108: INFO: Number of running nodes: 0, number of available pods: 0
Jul  9 04:07:14.110: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9595/daemonsets","resourceVersion":"24544"},"items":null}

Jul  9 04:07:14.112: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9595/pods","resourceVersion":"24544"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:07:14.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9595" for this suite.
Jul  9 04:07:20.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:07:20.407: INFO: namespace daemonsets-9595 deletion completed in 6.2815364s

• [SLOW TEST:20.025 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:07:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  9 04:10:02.514: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:02.529: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:04.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:04.533: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:06.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:06.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:08.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:08.534: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:10.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:10.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:12.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:12.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:14.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:14.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:16.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:16.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:18.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:18.533: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:20.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:20.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:22.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:22.533: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:24.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:24.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:26.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:26.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:28.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:28.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:30.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:30.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:32.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:32.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:34.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:34.533: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:36.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:36.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:38.532: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:38.538: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:40.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:40.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:42.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:42.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:44.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:44.533: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:46.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:46.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:48.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:48.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:50.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:50.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:52.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:52.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:54.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:54.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:56.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:56.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:10:58.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:10:58.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:00.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:00.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:02.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:02.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:04.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:04.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:06.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:06.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:08.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:08.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:10.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:10.534: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:12.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:12.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:14.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:14.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:16.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:16.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:18.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:18.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:20.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:20.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:22.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:22.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:24.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:24.540: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:26.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:26.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:28.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:28.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:30.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:30.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:32.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:32.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:34.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:34.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:36.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:36.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:38.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:38.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:40.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:40.534: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:42.529: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:42.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  9 04:11:44.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  9 04:11:44.534: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:11:44.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-230" for this suite.
Jul  9 04:12:06.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:12:06.696: INFO: namespace container-lifecycle-hook-230 deletion completed in 22.1592508s

• [SLOW TEST:286.288 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:12:06.696: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8226
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  9 04:12:06.726: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  9 04:12:26.812: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.64.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8226 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 04:12:26.812: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 04:12:27.971: INFO: Found all expected endpoints: [netserver-0]
Jul  9 04:12:27.975: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.192.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8226 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  9 04:12:27.975: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
Jul  9 04:12:29.118: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:12:29.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8226" for this suite.
Jul  9 04:12:51.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:12:51.305: INFO: namespace pod-network-test-8226 deletion completed in 22.1802059s

• [SLOW TEST:44.609 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:12:51.305: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 04:12:51.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4" in namespace "downward-api-400" to be "success or failure"
Jul  9 04:12:51.343: INFO: Pod "downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.958ms
Jul  9 04:12:53.355: INFO: Pod "downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0205269s
STEP: Saw pod success
Jul  9 04:12:53.355: INFO: Pod "downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4" satisfied condition "success or failure"
Jul  9 04:12:53.357: INFO: Trying to get logs from node node2 pod downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4 container client-container: <nil>
STEP: delete the pod
Jul  9 04:12:53.373: INFO: Waiting for pod downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4 to disappear
Jul  9 04:12:53.375: INFO: Pod downwardapi-volume-a8d79a5a-25da-4cc6-a228-676c6cab0ed4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:12:53.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-400" for this suite.
Jul  9 04:12:59.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:12:59.544: INFO: namespace downward-api-400 deletion completed in 6.1621733s

• [SLOW TEST:8.239 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:12:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-322
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul  9 04:12:59.595: INFO: Found 0 stateful pods, waiting for 3
Jul  9 04:13:09.607: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 04:13:09.607: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 04:13:09.607: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  9 04:13:09.652: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  9 04:13:19.698: INFO: Updating stateful set ss2
Jul  9 04:13:19.707: INFO: Waiting for Pod statefulset-322/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul  9 04:13:29.776: INFO: Found 2 stateful pods, waiting for 3
Jul  9 04:13:39.787: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 04:13:39.787: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  9 04:13:39.787: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  9 04:13:39.817: INFO: Updating stateful set ss2
Jul  9 04:13:39.828: INFO: Waiting for Pod statefulset-322/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  9 04:13:49.855: INFO: Updating stateful set ss2
Jul  9 04:13:49.873: INFO: Waiting for StatefulSet statefulset-322/ss2 to complete update
Jul  9 04:13:49.873: INFO: Waiting for Pod statefulset-322/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  9 04:13:59.893: INFO: Deleting all statefulset in ns statefulset-322
Jul  9 04:13:59.897: INFO: Scaling statefulset ss2 to 0
Jul  9 04:14:09.917: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 04:14:09.919: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:14:09.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-322" for this suite.
Jul  9 04:14:15.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:14:16.097: INFO: namespace statefulset-322 deletion completed in 6.1613918s

• [SLOW TEST:76.553 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:14:16.097: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul  9 04:14:46.691: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 165
	[quantile=0.99] = 165
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 211932
	[quantile=0.9] = 228400
	[quantile=0.99] = 228400
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 29853
	[quantile=0.99] = 29853
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 4622
	[quantile=0.9] = 30000
	[quantile=0.99] = 30000
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 3
	[quantile=0.9] = 7
	[quantile=0.99] = 31
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 8
	[quantile=0.9] = 22
	[quantile=0.99] = 65
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 29
	[quantile=0.99] = 36
For namespace_queue_latency_sum:
	[] = 3882
For namespace_queue_latency_count:
	[] = 347
For namespace_retries:
	[] = 357
For namespace_work_duration:
	[quantile=0.5] = 125114
	[quantile=0.9] = 219378
	[quantile=0.99] = 290808
For namespace_work_duration_sum:
	[] = 48242910
For namespace_work_duration_count:
	[] = 347
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:14:46.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4797" for this suite.
Jul  9 04:14:52.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:14:52.884: INFO: namespace gc-4797 deletion completed in 6.1897413s

• [SLOW TEST:36.786 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:14:52.884: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-905a3cb9-0d5d-486b-98b2-317da393f2f9
STEP: Creating a pod to test consume configMaps
Jul  9 04:14:52.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e" in namespace "projected-687" to be "success or failure"
Jul  9 04:14:52.948: INFO: Pod "pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858ms
Jul  9 04:14:54.950: INFO: Pod "pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0072672s
STEP: Saw pod success
Jul  9 04:14:54.950: INFO: Pod "pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e" satisfied condition "success or failure"
Jul  9 04:14:54.954: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 04:14:54.968: INFO: Waiting for pod pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e to disappear
Jul  9 04:14:54.970: INFO: Pod pod-projected-configmaps-ac0fbfdf-9355-4240-a585-ed6e7832f91e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:14:54.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-687" for this suite.
Jul  9 04:15:00.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:15:01.159: INFO: namespace projected-687 deletion completed in 6.1837337s

• [SLOW TEST:8.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:15:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:15:04.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6057" for this suite.
Jul  9 04:15:26.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:15:26.402: INFO: namespace replication-controller-6057 deletion completed in 22.158024s

• [SLOW TEST:25.243 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:15:26.402: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul  9 04:15:26.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 --namespace=kubectl-6588 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul  9 04:15:29.264: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul  9 04:15:29.264: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:15:31.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6588" for this suite.
Jul  9 04:15:37.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:15:37.425: INFO: namespace kubectl-6588 deletion completed in 6.1466688s

• [SLOW TEST:11.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:15:37.425: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-twq2
STEP: Creating a pod to test atomic-volume-subpath
Jul  9 04:15:37.466: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-twq2" in namespace "subpath-1961" to be "success or failure"
Jul  9 04:15:37.471: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6114ms
Jul  9 04:15:39.474: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008193s
Jul  9 04:15:41.480: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0137579s
Jul  9 04:15:43.484: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 6.0180239s
Jul  9 04:15:45.490: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 8.0241419s
Jul  9 04:15:47.500: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 10.0339065s
Jul  9 04:15:49.506: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 12.0404318s
Jul  9 04:15:51.516: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 14.050458s
Jul  9 04:15:53.525: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 16.058901s
Jul  9 04:15:55.529: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 18.063187s
Jul  9 04:15:57.542: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Running", Reason="", readiness=true. Elapsed: 20.0757338s
Jul  9 04:15:59.545: INFO: Pod "pod-subpath-test-secret-twq2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0788031s
STEP: Saw pod success
Jul  9 04:15:59.545: INFO: Pod "pod-subpath-test-secret-twq2" satisfied condition "success or failure"
Jul  9 04:15:59.546: INFO: Trying to get logs from node node2 pod pod-subpath-test-secret-twq2 container test-container-subpath-secret-twq2: <nil>
STEP: delete the pod
Jul  9 04:15:59.601: INFO: Waiting for pod pod-subpath-test-secret-twq2 to disappear
Jul  9 04:15:59.604: INFO: Pod pod-subpath-test-secret-twq2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-twq2
Jul  9 04:15:59.604: INFO: Deleting pod "pod-subpath-test-secret-twq2" in namespace "subpath-1961"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:15:59.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1961" for this suite.
Jul  9 04:16:05.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:16:05.787: INFO: namespace subpath-1961 deletion completed in 6.1783226s

• [SLOW TEST:28.362 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:16:05.788: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul  9 04:16:05.822: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul  9 04:16:05.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:05.942: INFO: stderr: ""
Jul  9 04:16:05.942: INFO: stdout: "service/redis-slave created\n"
Jul  9 04:16:05.942: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul  9 04:16:05.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:06.124: INFO: stderr: ""
Jul  9 04:16:06.125: INFO: stdout: "service/redis-master created\n"
Jul  9 04:16:06.125: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  9 04:16:06.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:06.258: INFO: stderr: ""
Jul  9 04:16:06.258: INFO: stdout: "service/frontend created\n"
Jul  9 04:16:06.258: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul  9 04:16:06.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:06.396: INFO: stderr: ""
Jul  9 04:16:06.396: INFO: stdout: "deployment.apps/frontend created\n"
Jul  9 04:16:06.396: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  9 04:16:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:06.539: INFO: stderr: ""
Jul  9 04:16:06.539: INFO: stdout: "deployment.apps/redis-master created\n"
Jul  9 04:16:06.539: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul  9 04:16:06.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-4066'
Jul  9 04:16:06.671: INFO: stderr: ""
Jul  9 04:16:06.671: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul  9 04:16:06.671: INFO: Waiting for all frontend pods to be Running.
Jul  9 04:17:41.731: INFO: Waiting for frontend to serve content.
Jul  9 04:17:42.764: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  9 04:17:48.790: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  9 04:17:54.805: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  9 04:18:00.822: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  9 04:18:06.835: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  9 04:18:11.854: INFO: Trying to add a new entry to the guestbook.
Jul  9 04:18:11.873: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul  9 04:18:11.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:11.977: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:11.977: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul  9 04:18:11.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:12.068: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:12.068: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  9 04:18:12.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:12.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:12.163: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  9 04:18:12.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:12.230: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:12.230: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  9 04:18:12.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:12.291: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:12.291: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  9 04:18:12.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jul  9 04:18:12.351: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  9 04:18:12.351: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:18:12.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4066" for this suite.
Jul  9 04:18:56.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:18:56.465: INFO: namespace kubectl-4066 deletion completed in 44.1117844s

• [SLOW TEST:170.678 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:18:56.466: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  9 04:18:59.527: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:18:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6863" for this suite.
Jul  9 04:19:05.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:19:05.666: INFO: namespace container-runtime-6863 deletion completed in 6.122712s

• [SLOW TEST:9.200 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:19:05.666: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul  9 04:19:05.694: INFO: namespace kubectl-5580
Jul  9 04:19:05.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 create -f - --namespace=kubectl-5580'
Jul  9 04:19:05.826: INFO: stderr: ""
Jul  9 04:19:05.826: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  9 04:19:06.831: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:19:06.831: INFO: Found 0 / 1
Jul  9 04:19:07.837: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:19:07.837: INFO: Found 1 / 1
Jul  9 04:19:07.837: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  9 04:19:07.839: INFO: Selector matched 1 pods for map[app:redis]
Jul  9 04:19:07.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  9 04:19:07.840: INFO: wait on redis-master startup in kubectl-5580 
Jul  9 04:19:07.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 logs redis-master-8h9x9 redis-master --namespace=kubectl-5580'
Jul  9 04:19:07.925: INFO: stderr: ""
Jul  9 04:19:07.925: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Jul 04:19:07.051 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Jul 04:19:07.051 # Server started, Redis version 3.2.12\n1:M 09 Jul 04:19:07.051 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Jul 04:19:07.051 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul  9 04:19:07.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5580'
Jul  9 04:19:08.005: INFO: stderr: ""
Jul  9 04:19:08.005: INFO: stdout: "service/rm2 exposed\n"
Jul  9 04:19:08.009: INFO: Service rm2 in namespace kubectl-5580 found.
STEP: exposing service
Jul  9 04:19:10.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5580'
Jul  9 04:19:10.092: INFO: stderr: ""
Jul  9 04:19:10.092: INFO: stdout: "service/rm3 exposed\n"
Jul  9 04:19:10.096: INFO: Service rm3 in namespace kubectl-5580 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:19:12.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5580" for this suite.
Jul  9 04:19:34.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:19:34.270: INFO: namespace kubectl-5580 deletion completed in 22.1645904s

• [SLOW TEST:28.603 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:19:34.270: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5485
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5485
STEP: Creating statefulset with conflicting port in namespace statefulset-5485
STEP: Waiting until pod test-pod will start running in namespace statefulset-5485
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5485
Jul  9 04:19:38.338: INFO: Observed stateful pod in namespace: statefulset-5485, name: ss-0, uid: 43cd0fdc-48eb-4845-9a2a-cfc20a56231d, status phase: Pending. Waiting for statefulset controller to delete.
Jul  9 04:19:44.075: INFO: Observed stateful pod in namespace: statefulset-5485, name: ss-0, uid: 43cd0fdc-48eb-4845-9a2a-cfc20a56231d, status phase: Failed. Waiting for statefulset controller to delete.
Jul  9 04:19:44.081: INFO: Observed stateful pod in namespace: statefulset-5485, name: ss-0, uid: 43cd0fdc-48eb-4845-9a2a-cfc20a56231d, status phase: Failed. Waiting for statefulset controller to delete.
Jul  9 04:19:44.087: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5485
STEP: Removing pod with conflicting port in namespace statefulset-5485
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5485 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  9 04:19:46.123: INFO: Deleting all statefulset in ns statefulset-5485
Jul  9 04:19:46.126: INFO: Scaling statefulset ss to 0
Jul  9 04:19:56.144: INFO: Waiting for statefulset status.replicas updated to 0
Jul  9 04:19:56.146: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:19:56.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5485" for this suite.
Jul  9 04:20:02.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:02.328: INFO: namespace statefulset-5485 deletion completed in 6.1615971s

• [SLOW TEST:28.059 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:02.328: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-694bcff9-b1ed-4cfc-83cf-2a6619b7fa82
STEP: Creating a pod to test consume configMaps
Jul  9 04:20:02.436: INFO: Waiting up to 5m0s for pod "pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14" in namespace "configmap-1598" to be "success or failure"
Jul  9 04:20:02.444: INFO: Pod "pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14": Phase="Pending", Reason="", readiness=false. Elapsed: 7.5778ms
Jul  9 04:20:04.449: INFO: Pod "pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0129567s
Jul  9 04:20:06.452: INFO: Pod "pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0155929s
STEP: Saw pod success
Jul  9 04:20:06.452: INFO: Pod "pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14" satisfied condition "success or failure"
Jul  9 04:20:06.454: INFO: Trying to get logs from node node2 pod pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  9 04:20:06.475: INFO: Waiting for pod pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14 to disappear
Jul  9 04:20:06.476: INFO: Pod pod-configmaps-7832079e-a4e0-4427-9b13-662285603e14 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:06.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1598" for this suite.
Jul  9 04:20:12.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:12.647: INFO: namespace configmap-1598 deletion completed in 6.1673594s

• [SLOW TEST:10.319 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:12.647: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  9 04:20:12.682: INFO: Waiting up to 5m0s for pod "pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972" in namespace "emptydir-4987" to be "success or failure"
Jul  9 04:20:12.687: INFO: Pod "pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972": Phase="Pending", Reason="", readiness=false. Elapsed: 5.4887ms
Jul  9 04:20:14.691: INFO: Pod "pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0085423s
STEP: Saw pod success
Jul  9 04:20:14.691: INFO: Pod "pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972" satisfied condition "success or failure"
Jul  9 04:20:14.692: INFO: Trying to get logs from node node2 pod pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972 container test-container: <nil>
STEP: delete the pod
Jul  9 04:20:14.709: INFO: Waiting for pod pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972 to disappear
Jul  9 04:20:14.713: INFO: Pod pod-d3753b94-58fe-4a32-9d93-1a0cbc35c972 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:14.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4987" for this suite.
Jul  9 04:20:20.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:20.881: INFO: namespace emptydir-4987 deletion completed in 6.1646016s

• [SLOW TEST:8.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4
Jul  9 04:20:20.922: INFO: Pod name my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4: Found 0 pods out of 1
Jul  9 04:20:25.925: INFO: Pod name my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4: Found 1 pods out of 1
Jul  9 04:20:25.925: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4" are running
Jul  9 04:20:25.927: INFO: Pod "my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4-75859" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 04:20:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 04:20:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 04:20:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-09 04:20:20 +0000 UTC Reason: Message:}])
Jul  9 04:20:25.927: INFO: Trying to dial the pod
Jul  9 04:20:30.955: INFO: Controller my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4: Got expected result from replica 1 [my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4-75859]: "my-hostname-basic-41a169f4-8830-4faa-9422-347ced73f2b4-75859", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:30.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3252" for this suite.
Jul  9 04:20:36.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:37.085: INFO: namespace replication-controller-3252 deletion completed in 6.1271024s

• [SLOW TEST:16.204 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:37.085: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 04:20:37.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a" in namespace "projected-106" to be "success or failure"
Jul  9 04:20:37.125: INFO: Pod "downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5135ms
Jul  9 04:20:39.128: INFO: Pod "downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0061688s
STEP: Saw pod success
Jul  9 04:20:39.128: INFO: Pod "downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a" satisfied condition "success or failure"
Jul  9 04:20:39.130: INFO: Trying to get logs from node node2 pod downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a container client-container: <nil>
STEP: delete the pod
Jul  9 04:20:39.158: INFO: Waiting for pod downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a to disappear
Jul  9 04:20:39.160: INFO: Pod downwardapi-volume-c921fa5c-a1ad-499e-93e9-a5f1143d949a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:39.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-106" for this suite.
Jul  9 04:20:45.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:45.288: INFO: namespace projected-106 deletion completed in 6.1248315s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  9 04:20:47.339: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:47.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8400" for this suite.
Jul  9 04:20:53.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:20:53.496: INFO: namespace container-runtime-8400 deletion completed in 6.1402679s

• [SLOW TEST:8.207 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:20:53.496: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-3037a7e8-ef05-4d90-8625-7a8303336585
STEP: Creating a pod to test consume secrets
Jul  9 04:20:53.532: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5" in namespace "projected-7559" to be "success or failure"
Jul  9 04:20:53.540: INFO: Pod "pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.7332ms
Jul  9 04:20:55.542: INFO: Pod "pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0096349s
STEP: Saw pod success
Jul  9 04:20:55.542: INFO: Pod "pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5" satisfied condition "success or failure"
Jul  9 04:20:55.544: INFO: Trying to get logs from node node2 pod pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  9 04:20:55.559: INFO: Waiting for pod pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5 to disappear
Jul  9 04:20:55.570: INFO: Pod pod-projected-secrets-114949b5-bee6-4e80-bb2a-b21bd9469fe5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:20:55.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7559" for this suite.
Jul  9 04:21:01.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:21:01.684: INFO: namespace projected-7559 deletion completed in 6.1113447s

• [SLOW TEST:8.189 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:21:01.685: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-93edf050-b447-4476-bcc9-508a1135c9cf
STEP: Creating a pod to test consume secrets
Jul  9 04:21:01.723: INFO: Waiting up to 5m0s for pod "pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da" in namespace "secrets-4685" to be "success or failure"
Jul  9 04:21:01.734: INFO: Pod "pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da": Phase="Pending", Reason="", readiness=false. Elapsed: 10.5905ms
Jul  9 04:21:03.737: INFO: Pod "pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0139255s
STEP: Saw pod success
Jul  9 04:21:03.737: INFO: Pod "pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da" satisfied condition "success or failure"
Jul  9 04:21:03.740: INFO: Trying to get logs from node node2 pod pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da container secret-volume-test: <nil>
STEP: delete the pod
Jul  9 04:21:03.757: INFO: Waiting for pod pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da to disappear
Jul  9 04:21:03.761: INFO: Pod pod-secrets-43a1aee6-fc35-42a5-a2e8-b764395734da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:21:03.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4685" for this suite.
Jul  9 04:21:09.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:21:09.896: INFO: namespace secrets-4685 deletion completed in 6.1322666s

• [SLOW TEST:8.211 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:21:09.896: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-705 to expose endpoints map[]
Jul  9 04:21:09.947: INFO: successfully validated that service endpoint-test2 in namespace services-705 exposes endpoints map[] (8.6031ms elapsed)
STEP: Creating pod pod1 in namespace services-705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-705 to expose endpoints map[pod1:[80]]
Jul  9 04:21:11.977: INFO: successfully validated that service endpoint-test2 in namespace services-705 exposes endpoints map[pod1:[80]] (2.0226034s elapsed)
STEP: Creating pod pod2 in namespace services-705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-705 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  9 04:21:14.053: INFO: successfully validated that service endpoint-test2 in namespace services-705 exposes endpoints map[pod1:[80] pod2:[80]] (2.0734399s elapsed)
STEP: Deleting pod pod1 in namespace services-705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-705 to expose endpoints map[pod2:[80]]
Jul  9 04:21:14.079: INFO: successfully validated that service endpoint-test2 in namespace services-705 exposes endpoints map[pod2:[80]] (21.1472ms elapsed)
STEP: Deleting pod pod2 in namespace services-705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-705 to expose endpoints map[]
Jul  9 04:21:15.091: INFO: successfully validated that service endpoint-test2 in namespace services-705 exposes endpoints map[] (1.0066449s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:21:15.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-705" for this suite.
Jul  9 04:21:37.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:21:37.273: INFO: namespace services-705 deletion completed in 22.1643209s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.377 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:21:37.273: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-16dc3eb2-2a41-45bd-871e-3289be1dc9cf
STEP: Creating a pod to test consume secrets
Jul  9 04:21:37.318: INFO: Waiting up to 5m0s for pod "pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317" in namespace "secrets-2576" to be "success or failure"
Jul  9 04:21:37.331: INFO: Pod "pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317": Phase="Pending", Reason="", readiness=false. Elapsed: 12.6282ms
Jul  9 04:21:39.337: INFO: Pod "pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018282s
STEP: Saw pod success
Jul  9 04:21:39.337: INFO: Pod "pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317" satisfied condition "success or failure"
Jul  9 04:21:39.338: INFO: Trying to get logs from node node2 pod pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317 container secret-env-test: <nil>
STEP: delete the pod
Jul  9 04:21:39.351: INFO: Waiting for pod pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317 to disappear
Jul  9 04:21:39.353: INFO: Pod pod-secrets-6a6ebc01-283a-4d8e-b035-d06117928317 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:21:39.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2576" for this suite.
Jul  9 04:21:45.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:21:45.485: INFO: namespace secrets-2576 deletion completed in 6.1284731s

• [SLOW TEST:8.212 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:21:45.486: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 04:21:45.522: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  9 04:21:45.528: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  9 04:21:50.534: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  9 04:21:50.535: INFO: Creating deployment "test-rolling-update-deployment"
Jul  9 04:21:50.537: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  9 04:21:50.553: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul  9 04:21:52.560: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  9 04:21:52.562: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  9 04:21:52.573: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9420,SelfLink:/apis/apps/v1/namespaces/deployment-9420/deployments/test-rolling-update-deployment,UID:aea0b277-52b0-4299-b3bc-47a731e92d79,ResourceVersion:27124,Generation:1,CreationTimestamp:2019-07-09 04:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-09 04:21:50 +0000 UTC 2019-07-09 04:21:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-09 04:21:51 +0000 UTC 2019-07-09 04:21:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  9 04:21:52.576: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9420,SelfLink:/apis/apps/v1/namespaces/deployment-9420/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:ea9fbc46-20b8-4064-8f4e-392cc70f3569,ResourceVersion:27113,Generation:1,CreationTimestamp:2019-07-09 04:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment aea0b277-52b0-4299-b3bc-47a731e92d79 0xc003e637d7 0xc003e637d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  9 04:21:52.576: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  9 04:21:52.576: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9420,SelfLink:/apis/apps/v1/namespaces/deployment-9420/replicasets/test-rolling-update-controller,UID:76a417d8-2337-46a8-b50a-f3ab7af08889,ResourceVersion:27122,Generation:2,CreationTimestamp:2019-07-09 04:21:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment aea0b277-52b0-4299-b3bc-47a731e92d79 0xc003e63707 0xc003e63708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 04:21:52.583: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-67lc7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-67lc7,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9420,SelfLink:/api/v1/namespaces/deployment-9420/pods/test-rolling-update-deployment-79f6b9d75c-67lc7,UID:5f75f3b3-cc23-45b2-b8a2-2cffe7c86ea0,ResourceVersion:27112,Generation:0,CreationTimestamp:2019-07-09 04:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c ea9fbc46-20b8-4064-8f4e-392cc70f3569 0xc003076547 0xc003076548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-plqn5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-plqn5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-plqn5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030765c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030765e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:21:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:21:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:21:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:21:50 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.4,StartTime:2019-07-09 04:21:50 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-09 04:21:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://085bd14f485df571fcb1885f0501fd41dcf56823887c8e356d37f0884a72c47d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:21:52.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9420" for this suite.
Jul  9 04:21:58.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:21:58.726: INFO: namespace deployment-9420 deletion completed in 6.1404478s

• [SLOW TEST:13.240 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:21:58.726: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  9 04:21:58.797: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:22:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-406" for this suite.
Jul  9 04:22:07.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:22:07.695: INFO: namespace init-container-406 deletion completed in 6.0913108s

• [SLOW TEST:8.969 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:22:07.695: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  9 04:22:07.722: INFO: Creating deployment "nginx-deployment"
Jul  9 04:22:07.726: INFO: Waiting for observed generation 1
Jul  9 04:22:09.797: INFO: Waiting for all required pods to come up
Jul  9 04:22:09.807: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  9 04:22:16.045: INFO: Waiting for deployment "nginx-deployment" to complete
Jul  9 04:22:16.049: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul  9 04:22:16.058: INFO: Updating deployment nginx-deployment
Jul  9 04:22:16.058: INFO: Waiting for observed generation 2
Jul  9 04:22:18.100: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  9 04:22:18.106: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  9 04:22:18.107: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  9 04:22:18.119: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  9 04:22:18.119: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  9 04:22:18.122: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  9 04:22:18.130: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul  9 04:22:18.130: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul  9 04:22:18.141: INFO: Updating deployment nginx-deployment
Jul  9 04:22:18.141: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul  9 04:22:18.157: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  9 04:22:20.175: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  9 04:22:20.191: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4397,SelfLink:/apis/apps/v1/namespaces/deployment-4397/deployments/nginx-deployment,UID:ee9c4c04-7b19-4c7e-a475-93636e2812cc,ResourceVersion:27462,Generation:3,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-09 04:22:17 +0000 UTC 2019-07-09 04:22:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-09 04:22:17 +0000 UTC 2019-07-09 04:22:07 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul  9 04:22:20.195: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-4397,SelfLink:/apis/apps/v1/namespaces/deployment-4397/replicasets/nginx-deployment-55fb7cb77f,UID:49233dc3-a4eb-4460-972e-1e0f1bc54e57,ResourceVersion:27461,Generation:3,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ee9c4c04-7b19-4c7e-a475-93636e2812cc 0xc0017c1047 0xc0017c1048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  9 04:22:20.195: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul  9 04:22:20.195: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-4397,SelfLink:/apis/apps/v1/namespaces/deployment-4397/replicasets/nginx-deployment-7b8c6f4498,UID:9179e929-dd06-4bde-803b-505e3eea1d95,ResourceVersion:27453,Generation:3,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ee9c4c04-7b19-4c7e-a475-93636e2812cc 0xc0017c1117 0xc0017c1118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul  9 04:22:20.212: INFO: Pod "nginx-deployment-55fb7cb77f-7wtxw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7wtxw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-7wtxw,UID:c348160f-3506-4e7a-aa09-69f81dddd7a5,ResourceVersion:27390,Generation:0,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc0032e99e7 0xc0032e99e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032e9a60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032e9a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:15 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.212: INFO: Pod "nginx-deployment-55fb7cb77f-8mmdm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8mmdm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-8mmdm,UID:c7937cb4-349e-4467-aed9-d80f1c15e232,ResourceVersion:27507,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc0032e9b50 0xc0032e9b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032e9bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032e9bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.212: INFO: Pod "nginx-deployment-55fb7cb77f-946mr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-946mr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-946mr,UID:c9fe19f7-e6e5-4671-8478-218195f45d6d,ResourceVersion:27384,Generation:0,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc0032e9cc0 0xc0032e9cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032e9d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032e9d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:15 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.212: INFO: Pod "nginx-deployment-55fb7cb77f-9hwcs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9hwcs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-9hwcs,UID:938c457d-ef98-4a18-8324-53a84e905c69,ResourceVersion:27495,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc0032e9e30 0xc0032e9e31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032e9eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032e9ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.212: INFO: Pod "nginx-deployment-55fb7cb77f-c8mcw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-c8mcw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-c8mcw,UID:aa7babd7-0701-4433-a314-c280bbd8e6d0,ResourceVersion:27476,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc0032e9fa0 0xc0032e9fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.213: INFO: Pod "nginx-deployment-55fb7cb77f-lgbmg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lgbmg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-lgbmg,UID:c74ebc44-07b9-4793-b322-4bdd49b7795d,ResourceVersion:27370,Generation:0,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e110 0xc00346e111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:15 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.213: INFO: Pod "nginx-deployment-55fb7cb77f-llghf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-llghf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-llghf,UID:49001196-d8df-4216-93ad-f931bdfcb7dd,ResourceVersion:27366,Generation:0,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e290 0xc00346e291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:15 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.213: INFO: Pod "nginx-deployment-55fb7cb77f-m42bg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m42bg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-m42bg,UID:c11159e7-53c2-4d89-aded-a8a43f6d6731,ResourceVersion:27466,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e400 0xc00346e401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.213: INFO: Pod "nginx-deployment-55fb7cb77f-m8mxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m8mxz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-m8mxz,UID:dcecaa5b-4259-4391-9d10-52f576b3b3d2,ResourceVersion:27514,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e570 0xc00346e571}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.213: INFO: Pod "nginx-deployment-55fb7cb77f-nvljl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nvljl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-nvljl,UID:973873bb-4258-465f-841d-ca65a22d8e2f,ResourceVersion:27388,Generation:0,CreationTimestamp:2019-07-09 04:22:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e6e0 0xc00346e6e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:15 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.214: INFO: Pod "nginx-deployment-55fb7cb77f-r2cm6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-r2cm6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-r2cm6,UID:991ceab4-cb5a-4782-8aa0-6b77d4c18c9c,ResourceVersion:27486,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e850 0xc00346e851}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346e8d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346e8f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.214: INFO: Pod "nginx-deployment-55fb7cb77f-r2vjn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-r2vjn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-r2vjn,UID:b49cb0de-a3f0-4578-b458-d255b6f163f8,ResourceVersion:27510,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346e9c0 0xc00346e9c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346ea40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346ea60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.214: INFO: Pod "nginx-deployment-55fb7cb77f-zlpfx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zlpfx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-55fb7cb77f-zlpfx,UID:290113b6-58ea-4a3c-a4cb-0d0d8dd18171,ResourceVersion:27493,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49233dc3-a4eb-4460-972e-1e0f1bc54e57 0xc00346eb40 0xc00346eb41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346ebc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346ebe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.214: INFO: Pod "nginx-deployment-7b8c6f4498-2g5hz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2g5hz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-2g5hz,UID:6cb8f5f2-8cb6-4d4d-8989-4b1f47fbf2fc,ResourceVersion:27320,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346ecb0 0xc00346ecb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346ed20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346ed40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.192.8,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3b9c78f10989a8e602005ab4596d0b5de1373309c848d74294de93eb0999d50f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.214: INFO: Pod "nginx-deployment-7b8c6f4498-4l6s4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4l6s4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-4l6s4,UID:7f6c9150-1c8b-43a2-af9e-aad2f0122ab8,ResourceVersion:27504,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346ee17 0xc00346ee18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346ee90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346eeb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.218: INFO: Pod "nginx-deployment-7b8c6f4498-4xhwd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4xhwd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-4xhwd,UID:42ce93ed-0575-4b61-a884-7a4ec141b1a8,ResourceVersion:27323,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346ef77 0xc00346ef78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346eff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.192.6,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e93d9cee72f7065d24a74f8955f37a7322e5244718be9b2e0177ffe392a63bda}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.218: INFO: Pod "nginx-deployment-7b8c6f4498-5k8d5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5k8d5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-5k8d5,UID:a644692a-67a9-4b50-a09f-ff74cdb5e0ad,ResourceVersion:27317,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f0e7 0xc00346f0e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.192.7,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b6852521d838341d87a63b6beb5ca439097a158a2762330347047e40ddb0a575}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.218: INFO: Pod "nginx-deployment-7b8c6f4498-6vgjv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6vgjv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-6vgjv,UID:ccf7d8d1-e0b8-43e5-94b0-0ccc015b6969,ResourceVersion:27330,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f257 0xc00346f258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.7,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://beaed39019668cfe184f0a2a543af542be469d4a045f28368a88abdcefef0eec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.218: INFO: Pod "nginx-deployment-7b8c6f4498-7nwzm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7nwzm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-7nwzm,UID:6149fcb4-c0ec-44c8-93a2-921bb47c99cd,ResourceVersion:27456,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f3c0 0xc00346f3c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.218: INFO: Pod "nginx-deployment-7b8c6f4498-7pm9l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7pm9l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-7pm9l,UID:27e2f14e-e4e4-4f78-8b0f-e9221952ffa4,ResourceVersion:27513,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f517 0xc00346f518}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.219: INFO: Pod "nginx-deployment-7b8c6f4498-7rcc2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7rcc2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-7rcc2,UID:c4445dff-86b7-4970-af22-59825fc710d4,ResourceVersion:27307,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f677 0xc00346f678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.3,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b8bad664378f4c617314089d02a1f50c96bc11b4dfd5786cd63f08a93c3ceaa9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.219: INFO: Pod "nginx-deployment-7b8c6f4498-887dq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-887dq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-887dq,UID:e442a12e-bb75-4f03-a124-660cc0457fd0,ResourceVersion:27488,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f810 0xc00346f811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f8a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.219: INFO: Pod "nginx-deployment-7b8c6f4498-9srxj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9srxj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-9srxj,UID:9e85aec1-1ec3-4fdb-8115-d8275baa402f,ResourceVersion:27438,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346f967 0xc00346f968}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fa00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.219: INFO: Pod "nginx-deployment-7b8c6f4498-b567s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-b567s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-b567s,UID:2d7819ac-0db7-4ea4-a6ef-c2d09adc54f4,ResourceVersion:27314,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346fac7 0xc00346fac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.192.4,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://88c4a395c3366700358dc497032a20945fd3124b2d83c11c5483c0671d2cf304}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.219: INFO: Pod "nginx-deployment-7b8c6f4498-f5d8z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f5d8z,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-f5d8z,UID:a60b9b49-3b67-4293-8e33-d68d78c3e5c0,ResourceVersion:27333,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346fc37 0xc00346fc38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fcb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fcd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.4,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d43fb5f3950da64c79b4191ec43bbfbbad505fc19d8f8bb7fc0272d746d349be}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.220: INFO: Pod "nginx-deployment-7b8c6f4498-kjm9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kjm9h,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-kjm9h,UID:e234ae41-0b16-4fcf-abf3-e9978a4c80b3,ResourceVersion:27489,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346fda0 0xc00346fda1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fe10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fe30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.220: INFO: Pod "nginx-deployment-7b8c6f4498-mbmn2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mbmn2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-mbmn2,UID:bb1021a7-53fe-436c-bb40-d6729f200d0b,ResourceVersion:27327,Generation:0,CreationTimestamp:2019-07-09 04:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00346fef7 0xc00346fef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346ff70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346ff90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:07 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.64.6,StartTime:2019-07-09 04:22:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-09 04:22:13 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c08b8ce4195bb67ca5e9ea5eb17ab646b507c23ef1c8fe775b9548198735f31e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.220: INFO: Pod "nginx-deployment-7b8c6f4498-swbp4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-swbp4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-swbp4,UID:36688da4-0eb5-4a7c-9bdb-390603495258,ResourceVersion:27463,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a060 0xc00391a061}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a0f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.220: INFO: Pod "nginx-deployment-7b8c6f4498-wjqlw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wjqlw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-wjqlw,UID:ef76f98a-3720-4351-87a2-4869bb7361c0,ResourceVersion:27509,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a1b7 0xc00391a1b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.225: INFO: Pod "nginx-deployment-7b8c6f4498-xh7tk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xh7tk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-xh7tk,UID:9c507c50-a53a-4674-af4d-d88d04d9dbab,ResourceVersion:27441,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a317 0xc00391a318}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.225: INFO: Pod "nginx-deployment-7b8c6f4498-xpzmg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xpzmg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-xpzmg,UID:c209cbeb-b0a0-4668-b538-39f943efb218,ResourceVersion:27511,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a477 0xc00391a478}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.225: INFO: Pod "nginx-deployment-7b8c6f4498-z425f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z425f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-z425f,UID:3c30b09a-b684-4879-8fab-06ac86c87a8e,ResourceVersion:27508,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a5e7 0xc00391a5e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  9 04:22:20.225: INFO: Pod "nginx-deployment-7b8c6f4498-zrbx6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zrbx6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4397,SelfLink:/api/v1/namespaces/deployment-4397/pods/nginx-deployment-7b8c6f4498-zrbx6,UID:e359f0c6-be73-479f-95e9-6a00cf02a33b,ResourceVersion:27471,Generation:0,CreationTimestamp:2019-07-09 04:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9179e929-dd06-4bde-803b-505e3eea1d95 0xc00391a747 0xc00391a748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wtc6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wtc6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wtc6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00391a7c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00391a7e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-09 04:22:17 +0000 UTC  }],Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2019-07-09 04:22:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:22:20.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4397" for this suite.
Jul  9 04:22:26.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:22:26.433: INFO: namespace deployment-4397 deletion completed in 6.2033219s

• [SLOW TEST:18.738 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:22:26.438: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  9 04:22:33.611: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:22:33.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5478" for this suite.
Jul  9 04:22:39.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:22:39.728: INFO: namespace container-runtime-5478 deletion completed in 6.0949831s

• [SLOW TEST:13.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:22:39.729: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  9 04:22:39.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22" in namespace "downward-api-9209" to be "success or failure"
Jul  9 04:22:39.773: INFO: Pod "downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953ms
Jul  9 04:22:41.776: INFO: Pod "downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008572s
STEP: Saw pod success
Jul  9 04:22:41.776: INFO: Pod "downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22" satisfied condition "success or failure"
Jul  9 04:22:41.778: INFO: Trying to get logs from node node2 pod downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22 container client-container: <nil>
STEP: delete the pod
Jul  9 04:22:41.793: INFO: Waiting for pod downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22 to disappear
Jul  9 04:22:41.796: INFO: Pod downwardapi-volume-d79198fe-02ac-409b-8179-de3a50b64c22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:22:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9209" for this suite.
Jul  9 04:22:47.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:22:47.906: INFO: namespace downward-api-9209 deletion completed in 6.1065598s

• [SLOW TEST:8.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:22:47.906: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-25r2
STEP: Creating a pod to test atomic-volume-subpath
Jul  9 04:22:47.969: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-25r2" in namespace "subpath-3039" to be "success or failure"
Jul  9 04:22:47.977: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.2069ms
Jul  9 04:22:49.979: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010205s
Jul  9 04:22:51.981: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0121589s
Jul  9 04:22:53.983: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 6.0141987s
Jul  9 04:22:55.986: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 8.0165573s
Jul  9 04:22:58.000: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 10.0307598s
Jul  9 04:23:00.002: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 12.0332831s
Jul  9 04:23:02.005: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 14.0357145s
Jul  9 04:23:04.008: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 16.0383544s
Jul  9 04:23:06.010: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 18.0408911s
Jul  9 04:23:08.013: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 20.0439023s
Jul  9 04:23:10.015: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Running", Reason="", readiness=true. Elapsed: 22.0461737s
Jul  9 04:23:12.018: INFO: Pod "pod-subpath-test-projected-25r2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0488869s
STEP: Saw pod success
Jul  9 04:23:12.018: INFO: Pod "pod-subpath-test-projected-25r2" satisfied condition "success or failure"
Jul  9 04:23:12.020: INFO: Trying to get logs from node node2 pod pod-subpath-test-projected-25r2 container test-container-subpath-projected-25r2: <nil>
STEP: delete the pod
Jul  9 04:23:12.042: INFO: Waiting for pod pod-subpath-test-projected-25r2 to disappear
Jul  9 04:23:12.047: INFO: Pod pod-subpath-test-projected-25r2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-25r2
Jul  9 04:23:12.047: INFO: Deleting pod "pod-subpath-test-projected-25r2" in namespace "subpath-3039"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:23:12.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3039" for this suite.
Jul  9 04:23:18.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:23:18.204: INFO: namespace subpath-3039 deletion completed in 6.1513341s

• [SLOW TEST:30.299 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:23:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:23:20.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6493" for this suite.
Jul  9 04:23:26.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:23:26.463: INFO: namespace emptydir-wrapper-6493 deletion completed in 6.1878399s

• [SLOW TEST:8.258 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:23:26.463: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul  9 04:23:28.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-630487638 exec pod-sharedvolume-80c6e674-a818-4d8f-a647-7d8fa639ed3c -c busybox-main-container --namespace=emptydir-7783 -- cat /usr/share/volumeshare/shareddata.txt'
Jul  9 04:23:28.773: INFO: stderr: ""
Jul  9 04:23:28.776: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:23:28.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7783" for this suite.
Jul  9 04:23:34.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:23:34.965: INFO: namespace emptydir-7783 deletion completed in 6.186736s

• [SLOW TEST:8.502 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  9 04:23:34.966: INFO: >>> kubeConfig: /tmp/kubeconfig-630487638
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  9 04:23:39.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3708" for this suite.
Jul  9 04:24:29.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  9 04:24:29.217: INFO: namespace kubelet-test-3708 deletion completed in 50.1741503s

• [SLOW TEST:54.252 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSJul  9 04:24:29.227: INFO: Running AfterSuite actions on all nodes
Jul  9 04:24:29.232: INFO: Running AfterSuite actions on node 1
Jul  9 04:24:29.234: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 6290.760 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h44m51.7975801s
Test Suite Passed
