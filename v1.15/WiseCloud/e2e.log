I0306 08:21:09.305098      17 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-201274422
I0306 08:21:09.305344      17 e2e.go:243] Starting e2e run "98e25c8e-6913-4318-a1ba-cbcca8066eb6" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583482866 - Will randomize all specs
Will run 215 of 4413 specs

Mar  6 08:21:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 08:21:09.719: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  6 08:21:09.761: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  6 08:21:09.881: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  6 08:21:09.881: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar  6 08:21:09.881: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  6 08:21:09.902: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  6 08:21:09.902: INFO: e2e test version: v1.15.5
Mar  6 08:21:09.904: INFO: kube-apiserver version: v1.15.5
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:21:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
Mar  6 08:21:10.072: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  6 08:21:10.110: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-f4a8c447-6b4c-47c4-b5fb-bb14fe9d8f2c
STEP: Creating a pod to test consume secrets
Mar  6 08:21:10.428: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9" in namespace "projected-9456" to be "success or failure"
Mar  6 08:21:10.513: INFO: Pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9": Phase="Pending", Reason="", readiness=false. Elapsed: 84.718139ms
Mar  6 08:21:12.540: INFO: Pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111632285s
Mar  6 08:21:14.551: INFO: Pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12259753s
Mar  6 08:21:16.557: INFO: Pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128821292s
STEP: Saw pod success
Mar  6 08:21:16.557: INFO: Pod "pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9" satisfied condition "success or failure"
Mar  6 08:21:16.562: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:21:16.666: INFO: Waiting for pod pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9 to disappear
Mar  6 08:21:16.671: INFO: Pod pod-projected-secrets-abf4e029-04a3-487a-94ea-0d31f319bdb9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:21:16.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9456" for this suite.
Mar  6 08:21:22.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:21:22.995: INFO: namespace projected-9456 deletion completed in 6.316059401s

• [SLOW TEST:13.090 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:21:22.996: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:21:23.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f" in namespace "projected-6534" to be "success or failure"
Mar  6 08:21:23.363: INFO: Pod "downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888557ms
Mar  6 08:21:25.384: INFO: Pod "downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027162543s
Mar  6 08:21:27.421: INFO: Pod "downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063838486s
STEP: Saw pod success
Mar  6 08:21:27.421: INFO: Pod "downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f" satisfied condition "success or failure"
Mar  6 08:21:27.426: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f container client-container: <nil>
STEP: delete the pod
Mar  6 08:21:27.699: INFO: Waiting for pod downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f to disappear
Mar  6 08:21:27.704: INFO: Pod downwardapi-volume-2b08da9a-3442-463e-8297-63ed6937dc1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:21:27.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6534" for this suite.
Mar  6 08:21:33.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:21:33.967: INFO: namespace projected-6534 deletion completed in 6.256042787s

• [SLOW TEST:10.971 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:21:33.967: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 08:21:34.574: INFO: Waiting up to 5m0s for pod "pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0" in namespace "emptydir-7635" to be "success or failure"
Mar  6 08:21:34.611: INFO: Pod "pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.346003ms
Mar  6 08:21:36.617: INFO: Pod "pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043398697s
Mar  6 08:21:38.629: INFO: Pod "pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055086553s
STEP: Saw pod success
Mar  6 08:21:38.629: INFO: Pod "pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0" satisfied condition "success or failure"
Mar  6 08:21:38.634: INFO: Trying to get logs from node wisecloud-worker02 pod pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0 container test-container: <nil>
STEP: delete the pod
Mar  6 08:21:38.764: INFO: Waiting for pod pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0 to disappear
Mar  6 08:21:38.769: INFO: Pod pod-4d370780-8d7d-4009-bacb-e6f17e7e44b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:21:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7635" for this suite.
Mar  6 08:21:44.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:21:45.085: INFO: namespace emptydir-7635 deletion completed in 6.30681793s

• [SLOW TEST:11.118 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:21:45.086: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar  6 08:21:51.445: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201274422 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  6 08:22:01.709: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:22:01.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7548" for this suite.
Mar  6 08:22:07.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:22:07.969: INFO: namespace pods-7548 deletion completed in 6.245887149s

• [SLOW TEST:22.884 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:22:07.969: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:22:08.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6" in namespace "projected-9023" to be "success or failure"
Mar  6 08:22:08.287: INFO: Pod "downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.14087ms
Mar  6 08:22:10.293: INFO: Pod "downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012164762s
Mar  6 08:22:12.304: INFO: Pod "downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022448337s
STEP: Saw pod success
Mar  6 08:22:12.304: INFO: Pod "downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6" satisfied condition "success or failure"
Mar  6 08:22:12.309: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6 container client-container: <nil>
STEP: delete the pod
Mar  6 08:22:12.403: INFO: Waiting for pod downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6 to disappear
Mar  6 08:22:12.408: INFO: Pod downwardapi-volume-0f7f8d59-fd0a-4ce6-8aea-775d340ae1a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:22:12.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9023" for this suite.
Mar  6 08:22:18.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:22:18.661: INFO: namespace projected-9023 deletion completed in 6.245446806s

• [SLOW TEST:10.692 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:22:18.662: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 08:22:19.199: INFO: Waiting up to 5m0s for pod "pod-1caef751-600c-456a-99fc-ae731a270817" in namespace "emptydir-7681" to be "success or failure"
Mar  6 08:22:19.205: INFO: Pod "pod-1caef751-600c-456a-99fc-ae731a270817": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953527ms
Mar  6 08:22:21.212: INFO: Pod "pod-1caef751-600c-456a-99fc-ae731a270817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012653516s
Mar  6 08:22:23.220: INFO: Pod "pod-1caef751-600c-456a-99fc-ae731a270817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020454864s
Mar  6 08:22:25.228: INFO: Pod "pod-1caef751-600c-456a-99fc-ae731a270817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028305246s
STEP: Saw pod success
Mar  6 08:22:25.228: INFO: Pod "pod-1caef751-600c-456a-99fc-ae731a270817" satisfied condition "success or failure"
Mar  6 08:22:25.233: INFO: Trying to get logs from node wisecloud-worker02 pod pod-1caef751-600c-456a-99fc-ae731a270817 container test-container: <nil>
STEP: delete the pod
Mar  6 08:22:25.334: INFO: Waiting for pod pod-1caef751-600c-456a-99fc-ae731a270817 to disappear
Mar  6 08:22:25.339: INFO: Pod pod-1caef751-600c-456a-99fc-ae731a270817 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:22:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7681" for this suite.
Mar  6 08:22:31.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:22:31.541: INFO: namespace emptydir-7681 deletion completed in 6.190088016s

• [SLOW TEST:12.880 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:22:31.543: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:22:31.849: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 08:22:31.889: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:31.889: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:31.889: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:31.934: INFO: Number of nodes with available pods: 0
Mar  6 08:22:31.934: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:22:32.944: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:32.944: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:32.944: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:32.950: INFO: Number of nodes with available pods: 0
Mar  6 08:22:32.950: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:22:33.953: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:33.953: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:33.953: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:33.960: INFO: Number of nodes with available pods: 0
Mar  6 08:22:33.960: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:22:34.953: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:34.953: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:34.953: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:34.959: INFO: Number of nodes with available pods: 2
Mar  6 08:22:34.959: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  6 08:22:35.076: INFO: Wrong image for pod: daemon-set-hccqr. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:35.076: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:35.086: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:35.086: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:35.086: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:36.093: INFO: Wrong image for pod: daemon-set-hccqr. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:36.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:36.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:36.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:36.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:37.094: INFO: Wrong image for pod: daemon-set-hccqr. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:37.094: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:37.102: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:37.103: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:37.103: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:38.094: INFO: Wrong image for pod: daemon-set-hccqr. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:38.095: INFO: Pod daemon-set-hccqr is not available
Mar  6 08:22:38.095: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:38.104: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:38.104: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:38.104: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:39.093: INFO: Pod daemon-set-fhjph is not available
Mar  6 08:22:39.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:39.102: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:39.102: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:39.102: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:40.093: INFO: Pod daemon-set-fhjph is not available
Mar  6 08:22:40.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:40.102: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:40.102: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:40.102: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:41.092: INFO: Pod daemon-set-fhjph is not available
Mar  6 08:22:41.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:41.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:41.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:41.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:42.093: INFO: Pod daemon-set-fhjph is not available
Mar  6 08:22:42.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:42.121: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:42.121: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:42.121: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:43.093: INFO: Pod daemon-set-fhjph is not available
Mar  6 08:22:43.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:43.168: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:43.168: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:43.168: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:44.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:44.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:44.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:44.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:45.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:45.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:45.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:45.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:46.094: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:46.094: INFO: Pod daemon-set-vjftz is not available
Mar  6 08:22:46.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:46.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:46.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:47.094: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:47.094: INFO: Pod daemon-set-vjftz is not available
Mar  6 08:22:47.125: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:47.125: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:47.125: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:48.093: INFO: Wrong image for pod: daemon-set-vjftz. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  6 08:22:48.093: INFO: Pod daemon-set-vjftz is not available
Mar  6 08:22:48.100: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:48.100: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:48.100: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.093: INFO: Pod daemon-set-kgzrq is not available
Mar  6 08:22:49.101: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.101: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.101: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  6 08:22:49.114: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.114: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.114: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:49.121: INFO: Number of nodes with available pods: 1
Mar  6 08:22:49.121: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 08:22:50.140: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:50.140: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:50.140: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:50.146: INFO: Number of nodes with available pods: 1
Mar  6 08:22:50.146: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 08:22:51.129: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:51.130: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:51.130: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:51.135: INFO: Number of nodes with available pods: 1
Mar  6 08:22:51.136: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 08:22:52.131: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:52.131: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:52.131: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:52.137: INFO: Number of nodes with available pods: 1
Mar  6 08:22:52.137: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 08:22:53.131: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:53.131: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:53.131: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:53.137: INFO: Number of nodes with available pods: 1
Mar  6 08:22:53.137: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 08:22:54.130: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:54.130: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:54.130: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:22:54.136: INFO: Number of nodes with available pods: 2
Mar  6 08:22:54.136: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2461, will wait for the garbage collector to delete the pods
Mar  6 08:22:54.242: INFO: Deleting DaemonSet.extensions daemon-set took: 24.195015ms
Mar  6 08:22:54.643: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.486203ms
Mar  6 08:23:08.550: INFO: Number of nodes with available pods: 0
Mar  6 08:23:08.550: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 08:23:08.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2461/daemonsets","resourceVersion":"8896"},"items":null}

Mar  6 08:23:08.562: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2461/pods","resourceVersion":"8896"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:08.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2461" for this suite.
Mar  6 08:23:16.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:23:16.802: INFO: namespace daemonsets-2461 deletion completed in 8.211296475s

• [SLOW TEST:45.259 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:23:16.802: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 08:23:17.176: INFO: Waiting up to 5m0s for pod "pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29" in namespace "emptydir-6687" to be "success or failure"
Mar  6 08:23:17.181: INFO: Pod "pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5136ms
Mar  6 08:23:19.188: INFO: Pod "pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012304052s
Mar  6 08:23:21.195: INFO: Pod "pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018791716s
STEP: Saw pod success
Mar  6 08:23:21.195: INFO: Pod "pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29" satisfied condition "success or failure"
Mar  6 08:23:21.199: INFO: Trying to get logs from node wisecloud-worker02 pod pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29 container test-container: <nil>
STEP: delete the pod
Mar  6 08:23:21.268: INFO: Waiting for pod pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29 to disappear
Mar  6 08:23:21.273: INFO: Pod pod-18aa06a1-f5d0-4efd-90e2-98e00aacdb29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:21.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6687" for this suite.
Mar  6 08:23:27.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:23:27.465: INFO: namespace emptydir-6687 deletion completed in 6.184429859s

• [SLOW TEST:10.663 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:23:27.465: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:23:27.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a" in namespace "downward-api-9610" to be "success or failure"
Mar  6 08:23:27.782: INFO: Pod "downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.20891ms
Mar  6 08:23:29.789: INFO: Pod "downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012984278s
Mar  6 08:23:31.835: INFO: Pod "downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058271506s
STEP: Saw pod success
Mar  6 08:23:31.835: INFO: Pod "downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a" satisfied condition "success or failure"
Mar  6 08:23:31.840: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a container client-container: <nil>
STEP: delete the pod
Mar  6 08:23:31.931: INFO: Waiting for pod downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a to disappear
Mar  6 08:23:31.936: INFO: Pod downwardapi-volume-580f4d96-b907-4108-ad97-54a34aa5270a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:31.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9610" for this suite.
Mar  6 08:23:38.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:23:38.164: INFO: namespace downward-api-9610 deletion completed in 6.218658474s

• [SLOW TEST:10.698 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:23:38.164: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  6 08:23:38.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9651,SelfLink:/api/v1/namespaces/watch-9651/configmaps/e2e-watch-test-watch-closed,UID:262598c2-72d8-4635-ba58-33564e250c2d,ResourceVersion:9039,Generation:0,CreationTimestamp:2020-03-06 08:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 08:23:38.497: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9651,SelfLink:/api/v1/namespaces/watch-9651/configmaps/e2e-watch-test-watch-closed,UID:262598c2-72d8-4635-ba58-33564e250c2d,ResourceVersion:9040,Generation:0,CreationTimestamp:2020-03-06 08:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  6 08:23:38.566: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9651,SelfLink:/api/v1/namespaces/watch-9651/configmaps/e2e-watch-test-watch-closed,UID:262598c2-72d8-4635-ba58-33564e250c2d,ResourceVersion:9041,Generation:0,CreationTimestamp:2020-03-06 08:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 08:23:38.566: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9651,SelfLink:/api/v1/namespaces/watch-9651/configmaps/e2e-watch-test-watch-closed,UID:262598c2-72d8-4635-ba58-33564e250c2d,ResourceVersion:9042,Generation:0,CreationTimestamp:2020-03-06 08:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:38.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9651" for this suite.
Mar  6 08:23:44.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:23:44.924: INFO: namespace watch-9651 deletion completed in 6.34701094s

• [SLOW TEST:6.760 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:23:44.924: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-4557f627-b73e-4555-b2df-8b70a195b861
STEP: Creating a pod to test consume secrets
Mar  6 08:23:45.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a" in namespace "projected-1914" to be "success or failure"
Mar  6 08:23:45.286: INFO: Pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.537637ms
Mar  6 08:23:47.293: INFO: Pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012793049s
Mar  6 08:23:49.300: INFO: Pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01958581s
Mar  6 08:23:51.308: INFO: Pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027775215s
STEP: Saw pod success
Mar  6 08:23:51.308: INFO: Pod "pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a" satisfied condition "success or failure"
Mar  6 08:23:51.313: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:23:51.394: INFO: Waiting for pod pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a to disappear
Mar  6 08:23:51.399: INFO: Pod pod-projected-secrets-8e3fc991-3a13-40d2-9966-76ebb1f7112a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:51.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1914" for this suite.
Mar  6 08:23:57.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:23:57.593: INFO: namespace projected-1914 deletion completed in 6.184973535s

• [SLOW TEST:12.668 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:23:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Mar  6 08:23:57.897: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201274422 proxy --unix-socket=/tmp/kubectl-proxy-unix440294429/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:23:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1611" for this suite.
Mar  6 08:24:04.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:24:04.244: INFO: namespace kubectl-1611 deletion completed in 6.19294405s

• [SLOW TEST:6.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:24:04.244: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-733eef8a-798e-4e8c-b6f5-436014a50112
STEP: Creating a pod to test consume configMaps
Mar  6 08:24:04.596: INFO: Waiting up to 5m0s for pod "pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537" in namespace "configmap-8778" to be "success or failure"
Mar  6 08:24:04.602: INFO: Pod "pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408617ms
Mar  6 08:24:06.608: INFO: Pod "pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012160962s
Mar  6 08:24:08.614: INFO: Pod "pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01763624s
STEP: Saw pod success
Mar  6 08:24:08.614: INFO: Pod "pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537" satisfied condition "success or failure"
Mar  6 08:24:08.619: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 08:24:08.698: INFO: Waiting for pod pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537 to disappear
Mar  6 08:24:08.704: INFO: Pod pod-configmaps-eeb5c3b6-0731-4ad0-9fd5-c98a13737537 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:24:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8778" for this suite.
Mar  6 08:24:14.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:24:15.014: INFO: namespace configmap-8778 deletion completed in 6.300575503s

• [SLOW TEST:10.769 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:24:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-727
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  6 08:24:15.515: INFO: Found 0 stateful pods, waiting for 3
Mar  6 08:24:25.523: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:24:25.523: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:24:25.523: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  6 08:24:35.522: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:24:35.522: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:24:35.522: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:24:35.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-727 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 08:24:37.373: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 08:24:37.373: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 08:24:37.373: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from 172.20.8.7/library/nginx:1.14-alpine to 172.20.8.7/library/nginx:1.15-alpine
Mar  6 08:24:47.451: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  6 08:24:57.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-727 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 08:24:57.921: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 08:24:57.921: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 08:24:57.921: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 08:24:58.143: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:24:58.143: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:24:58.143: INFO: Waiting for Pod statefulset-727/ss2-1 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:24:58.143: INFO: Waiting for Pod statefulset-727/ss2-2 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:25:08.213: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:25:08.213: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:25:08.213: INFO: Waiting for Pod statefulset-727/ss2-1 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:25:18.171: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:25:18.171: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:25:28.157: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:25:28.157: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
STEP: Rolling back to a previous revision
Mar  6 08:25:38.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-727 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 08:25:38.678: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 08:25:38.678: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 08:25:38.678: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 08:25:48.813: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  6 08:25:58.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-727 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 08:25:59.324: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 08:25:59.324: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 08:25:59.324: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 08:26:09.363: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:26:09.363: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
Mar  6 08:26:09.363: INFO: Waiting for Pod statefulset-727/ss2-1 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
Mar  6 08:26:19.377: INFO: Waiting for StatefulSet statefulset-727/ss2 to complete update
Mar  6 08:26:19.377: INFO: Waiting for Pod statefulset-727/ss2-0 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  6 08:26:29.378: INFO: Deleting all statefulset in ns statefulset-727
Mar  6 08:26:29.384: INFO: Scaling statefulset ss2 to 0
Mar  6 08:26:49.447: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 08:26:49.454: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:26:49.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-727" for this suite.
Mar  6 08:26:57.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:26:57.801: INFO: namespace statefulset-727 deletion completed in 8.293022317s

• [SLOW TEST:162.788 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:26:57.803: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  6 08:26:58.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-3755'
Mar  6 08:26:58.806: INFO: stderr: ""
Mar  6 08:26:58.806: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:26:58.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3755'
Mar  6 08:26:59.045: INFO: stderr: ""
Mar  6 08:26:59.045: INFO: stdout: "update-demo-nautilus-9flrk update-demo-nautilus-wd6hm "
Mar  6 08:26:59.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-9flrk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3755'
Mar  6 08:26:59.226: INFO: stderr: ""
Mar  6 08:26:59.226: INFO: stdout: ""
Mar  6 08:26:59.226: INFO: update-demo-nautilus-9flrk is created but not running
Mar  6 08:27:04.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3755'
Mar  6 08:27:04.403: INFO: stderr: ""
Mar  6 08:27:04.403: INFO: stdout: "update-demo-nautilus-9flrk update-demo-nautilus-wd6hm "
Mar  6 08:27:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-9flrk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3755'
Mar  6 08:27:04.574: INFO: stderr: ""
Mar  6 08:27:04.574: INFO: stdout: "true"
Mar  6 08:27:04.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-9flrk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3755'
Mar  6 08:27:04.748: INFO: stderr: ""
Mar  6 08:27:04.748: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:27:04.748: INFO: validating pod update-demo-nautilus-9flrk
Mar  6 08:27:04.801: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:27:04.801: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:27:04.801: INFO: update-demo-nautilus-9flrk is verified up and running
Mar  6 08:27:04.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-wd6hm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3755'
Mar  6 08:27:04.983: INFO: stderr: ""
Mar  6 08:27:04.983: INFO: stdout: "true"
Mar  6 08:27:04.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-wd6hm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3755'
Mar  6 08:27:05.153: INFO: stderr: ""
Mar  6 08:27:05.153: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:27:05.153: INFO: validating pod update-demo-nautilus-wd6hm
Mar  6 08:27:05.205: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:27:05.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:27:05.205: INFO: update-demo-nautilus-wd6hm is verified up and running
STEP: using delete to clean up resources
Mar  6 08:27:05.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-3755'
Mar  6 08:27:05.422: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 08:27:05.422: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 08:27:05.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3755'
Mar  6 08:27:05.677: INFO: stderr: "No resources found.\n"
Mar  6 08:27:05.677: INFO: stdout: ""
Mar  6 08:27:05.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=update-demo --namespace=kubectl-3755 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:27:05.866: INFO: stderr: ""
Mar  6 08:27:05.866: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:27:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3755" for this suite.
Mar  6 08:27:29.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:27:30.229: INFO: namespace kubectl-3755 deletion completed in 24.352533373s

• [SLOW TEST:32.427 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:27:30.230: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-d1082d2d-1d4e-4536-b396-a36d1e033ee5 in namespace container-probe-2398
Mar  6 08:27:36.552: INFO: Started pod liveness-d1082d2d-1d4e-4536-b396-a36d1e033ee5 in namespace container-probe-2398
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 08:27:36.557: INFO: Initial restart count of pod liveness-d1082d2d-1d4e-4536-b396-a36d1e033ee5 is 0
Mar  6 08:27:58.670: INFO: Restart count of pod container-probe-2398/liveness-d1082d2d-1d4e-4536-b396-a36d1e033ee5 is now 1 (22.113056696s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:27:58.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2398" for this suite.
Mar  6 08:28:04.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:28:04.966: INFO: namespace container-probe-2398 deletion completed in 6.211444984s

• [SLOW TEST:34.736 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:28:04.966: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-68f5aba3-32ac-4436-a0ca-48907ac875a9
STEP: Creating a pod to test consume secrets
Mar  6 08:28:05.539: INFO: Waiting up to 5m0s for pod "pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948" in namespace "secrets-4444" to be "success or failure"
Mar  6 08:28:05.545: INFO: Pod "pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247654ms
Mar  6 08:28:07.703: INFO: Pod "pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.163845889s
Mar  6 08:28:09.714: INFO: Pod "pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.174715437s
STEP: Saw pod success
Mar  6 08:28:09.714: INFO: Pod "pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948" satisfied condition "success or failure"
Mar  6 08:28:09.720: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:28:09.839: INFO: Waiting for pod pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948 to disappear
Mar  6 08:28:09.848: INFO: Pod pod-secrets-b05fae6a-33ea-43a7-8c55-2dfbb92a9948 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:28:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4444" for this suite.
Mar  6 08:28:15.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:28:16.186: INFO: namespace secrets-4444 deletion completed in 6.326729442s

• [SLOW TEST:11.220 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:28:16.187: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Mar  6 08:28:16.708: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2189" to be "success or failure"
Mar  6 08:28:16.715: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.818494ms
Mar  6 08:28:18.720: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012419712s
Mar  6 08:28:20.728: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019792s
Mar  6 08:28:22.734: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026700662s
STEP: Saw pod success
Mar  6 08:28:22.734: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  6 08:28:22.740: INFO: Trying to get logs from node wisecloud-worker02 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  6 08:28:22.808: INFO: Waiting for pod pod-host-path-test to disappear
Mar  6 08:28:22.813: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:28:22.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2189" for this suite.
Mar  6 08:28:28.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:28:29.117: INFO: namespace hostpath-2189 deletion completed in 6.295838836s

• [SLOW TEST:12.930 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:28:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-156
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fca0c4a9-683e-44db-ae33-f9c039567eb4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fca0c4a9-683e-44db-ae33-f9c039567eb4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:28:35.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-156" for this suite.
Mar  6 08:28:59.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:28:59.835: INFO: namespace projected-156 deletion completed in 24.214207238s

• [SLOW TEST:30.718 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:28:59.836: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8747
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8747
STEP: Creating statefulset with conflicting port in namespace statefulset-8747
STEP: Waiting until pod test-pod will start running in namespace statefulset-8747
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8747
Mar  6 08:29:04.641: INFO: Observed stateful pod in namespace: statefulset-8747, name: ss-0, uid: d1fbe65c-a760-4431-a134-6c835664f0ec, status phase: Pending. Waiting for statefulset controller to delete.
Mar  6 08:29:05.337: INFO: Observed stateful pod in namespace: statefulset-8747, name: ss-0, uid: d1fbe65c-a760-4431-a134-6c835664f0ec, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 08:29:05.434: INFO: Observed stateful pod in namespace: statefulset-8747, name: ss-0, uid: d1fbe65c-a760-4431-a134-6c835664f0ec, status phase: Failed. Waiting for statefulset controller to delete.
Mar  6 08:29:05.464: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8747
STEP: Removing pod with conflicting port in namespace statefulset-8747
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8747 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  6 08:29:09.610: INFO: Deleting all statefulset in ns statefulset-8747
Mar  6 08:29:09.745: INFO: Scaling statefulset ss to 0
Mar  6 08:29:19.793: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 08:29:19.820: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:29:19.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8747" for this suite.
Mar  6 08:29:27.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:29:28.111: INFO: namespace statefulset-8747 deletion completed in 8.227475706s

• [SLOW TEST:28.275 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:29:28.112: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  6 08:29:33.509: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:29:33.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8404" for this suite.
Mar  6 08:29:39.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:29:39.758: INFO: namespace container-runtime-8404 deletion completed in 6.179223499s

• [SLOW TEST:11.647 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:29:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  6 08:29:46.643: INFO: Successfully updated pod "labelsupdate04fcf8a4-c3f4-4060-82c4-af6a2d0183a8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:29:48.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-103" for this suite.
Mar  6 08:30:12.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:30:13.071: INFO: namespace downward-api-103 deletion completed in 24.388254149s

• [SLOW TEST:33.312 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:30:13.071: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-b73ac7c4-a875-47a6-8814-4fb116dd3dcd
STEP: Creating a pod to test consume configMaps
Mar  6 08:30:13.426: INFO: Waiting up to 5m0s for pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172" in namespace "configmap-932" to be "success or failure"
Mar  6 08:30:13.467: INFO: Pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172": Phase="Pending", Reason="", readiness=false. Elapsed: 41.209539ms
Mar  6 08:30:15.475: INFO: Pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049008844s
Mar  6 08:30:17.481: INFO: Pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055098503s
Mar  6 08:30:19.487: INFO: Pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061574294s
STEP: Saw pod success
Mar  6 08:30:19.487: INFO: Pod "pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172" satisfied condition "success or failure"
Mar  6 08:30:19.493: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 08:30:19.579: INFO: Waiting for pod pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172 to disappear
Mar  6 08:30:19.583: INFO: Pod pod-configmaps-387c3d7b-e257-4392-957e-56f8c0732172 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:30:19.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-932" for this suite.
Mar  6 08:30:27.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:30:28.790: INFO: namespace configmap-932 deletion completed in 9.19825794s

• [SLOW TEST:15.718 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:30:28.791: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:30:29.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f" in namespace "downward-api-8078" to be "success or failure"
Mar  6 08:30:29.126: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.403654ms
Mar  6 08:30:31.133: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012534669s
Mar  6 08:30:33.146: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025747891s
Mar  6 08:30:35.177: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056773831s
Mar  6 08:30:37.184: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.063878075s
STEP: Saw pod success
Mar  6 08:30:37.185: INFO: Pod "downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f" satisfied condition "success or failure"
Mar  6 08:30:37.190: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f container client-container: <nil>
STEP: delete the pod
Mar  6 08:30:37.292: INFO: Waiting for pod downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f to disappear
Mar  6 08:30:37.298: INFO: Pod downwardapi-volume-159bd2c4-76e6-46de-a8ad-660f6ea8d36f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:30:37.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8078" for this suite.
Mar  6 08:30:43.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:30:43.654: INFO: namespace downward-api-8078 deletion completed in 6.295817729s

• [SLOW TEST:14.863 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:30:43.654: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:30:43.914: INFO: Creating deployment "nginx-deployment"
Mar  6 08:30:43.958: INFO: Waiting for observed generation 1
Mar  6 08:30:45.972: INFO: Waiting for all required pods to come up
Mar  6 08:30:45.980: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  6 08:30:51.998: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  6 08:30:52.009: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  6 08:30:52.083: INFO: Updating deployment nginx-deployment
Mar  6 08:30:52.083: INFO: Waiting for observed generation 2
Mar  6 08:30:54.096: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  6 08:30:54.102: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  6 08:30:54.124: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 08:30:54.141: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  6 08:30:54.141: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  6 08:30:54.145: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  6 08:30:54.155: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  6 08:30:54.155: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  6 08:30:54.194: INFO: Updating deployment nginx-deployment
Mar  6 08:30:54.194: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  6 08:30:54.216: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  6 08:30:54.221: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  6 08:30:54.334: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-6458,SelfLink:/apis/apps/v1/namespaces/deployment-6458/deployments/nginx-deployment,UID:69e406a7-01f7-4207-a2a1-98bcb6124354,ResourceVersion:10894,Generation:3,CreationTimestamp:2020-03-06 08:30:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Available True 2020-03-06 08:30:50 +0000 UTC 2020-03-06 08:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-06 08:30:53 +0000 UTC 2020-03-06 08:30:43 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  6 08:30:54.421: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-6458,SelfLink:/apis/apps/v1/namespaces/deployment-6458/replicasets/nginx-deployment-55fb7cb77f,UID:1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce,ResourceVersion:10898,Generation:3,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 69e406a7-01f7-4207-a2a1-98bcb6124354 0xc002c41b37 0xc002c41b38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 08:30:54.421: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  6 08:30:54.422: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9,GenerateName:,Namespace:deployment-6458,SelfLink:/apis/apps/v1/namespaces/deployment-6458/replicasets/nginx-deployment-785f69f5d9,UID:3f5e261d-f541-4728-91fe-13e7a5651def,ResourceVersion:10895,Generation:3,CreationTimestamp:2020-03-06 08:30:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 69e406a7-01f7-4207-a2a1-98bcb6124354 0xc002c41c57 0xc002c41c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  6 08:30:54.573: INFO: Pod "nginx-deployment-55fb7cb77f-276mq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-276mq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-276mq,UID:0febfa94-454f-4d46-aa53-6566259d6c02,ResourceVersion:10869,Generation:0,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee8dc7 0xc001ee8dc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee8e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee8e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-06 08:30:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.573: INFO: Pod "nginx-deployment-55fb7cb77f-fnthw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fnthw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-fnthw,UID:884ffcf4-5c00-4bdd-8306-ab0694a3d37e,ResourceVersion:10887,Generation:0,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee8f20 0xc001ee8f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee8f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee8fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-06 08:30:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.573: INFO: Pod "nginx-deployment-55fb7cb77f-hkdrt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hkdrt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-hkdrt,UID:e768f068-222e-473b-b025-8c211d4e4a85,ResourceVersion:10904,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee9080 0xc001ee9081}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee90f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.574: INFO: Pod "nginx-deployment-55fb7cb77f-hl56d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hl56d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-hl56d,UID:61adacf5-30fa-4caa-a275-b8914c086b72,ResourceVersion:10874,Generation:0,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee9177 0xc001ee9178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee91e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-06 08:30:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.574: INFO: Pod "nginx-deployment-55fb7cb77f-sshcx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-sshcx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-sshcx,UID:4d5f1286-273d-4733-9e68-2ca9b469d787,ResourceVersion:10884,Generation:0,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee92d0 0xc001ee92d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-06 08:30:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.574: INFO: Pod "nginx-deployment-55fb7cb77f-tvj79" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tvj79,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-55fb7cb77f-tvj79,UID:f7d4716e-47f2-4095-b8ca-c97b88156576,ResourceVersion:10883,Generation:0,CreationTimestamp:2020-03-06 08:30:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1e4a42d8-25f2-42b1-b9c4-6e13f8cb9bce 0xc001ee9430 0xc001ee9431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee94a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee94c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:52 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-06 08:30:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.574: INFO: Pod "nginx-deployment-785f69f5d9-6jrrs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-6jrrs,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-6jrrs,UID:bd3e0552-361b-420c-8f30-a8b97b6993c2,ResourceVersion:10912,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9590 0xc001ee9591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee95f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.575: INFO: Pod "nginx-deployment-785f69f5d9-75drt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-75drt,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-75drt,UID:ee8e6697-dc4a-4f61-83eb-1506d5687e93,ResourceVersion:10901,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9677 0xc001ee9678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee96e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:54 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.575: INFO: Pod "nginx-deployment-785f69f5d9-7w2f6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-7w2f6,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-7w2f6,UID:65499f20-cdfc-4fc6-8098-fa9ed4e3f4a4,ResourceVersion:10906,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9780 0xc001ee9781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee97e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:54 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.575: INFO: Pod "nginx-deployment-785f69f5d9-8qddc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-8qddc,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-8qddc,UID:a504ad17-c7e8-4938-96f6-69dedc437ce9,ResourceVersion:10822,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9880 0xc001ee9881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee98e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.37,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:49 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://d6e1bb9a07e0fe04626e4110f690646a01391c03495f7045662a8d5a60fa03e0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.576: INFO: Pod "nginx-deployment-785f69f5d9-bszn6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-bszn6,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-bszn6,UID:86172614-7a8a-4651-8788-719e7d7d6343,ResourceVersion:10788,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee99d0 0xc001ee99d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9a30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9a50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.15,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:46 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://14209435fb52e70c68645b42b6c23aa4f728e6a338cc2d1ae044fd181a610a13}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.576: INFO: Pod "nginx-deployment-785f69f5d9-fxrgv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-fxrgv,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-fxrgv,UID:4f8e3646-bace-4333-a261-a4e97d76bc81,ResourceVersion:10911,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9b30 0xc001ee9b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:54 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.576: INFO: Pod "nginx-deployment-785f69f5d9-gbc57" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-gbc57,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-gbc57,UID:477e8b77-0e59-4ae3-a6b8-d1af80efa30c,ResourceVersion:10909,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9c80 0xc001ee9c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.577: INFO: Pod "nginx-deployment-785f69f5d9-k866b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-k866b,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-k866b,UID:2c5e86c4-8ce5-48bd-ab54-71cd060df235,ResourceVersion:10796,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9d67 0xc001ee9d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.19,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:47 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://0849aa8e45ad56b96458317f905a456e973bffdac255034712182efae8aa7dba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.577: INFO: Pod "nginx-deployment-785f69f5d9-kt5tp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-kt5tp,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-kt5tp,UID:b4f52a06-f837-41b3-8b17-017d29aa4c05,ResourceVersion:10910,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9ec0 0xc001ee9ec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ee9f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ee9f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.577: INFO: Pod "nginx-deployment-785f69f5d9-ln4tr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-ln4tr,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-ln4tr,UID:7f444486-87ec-4320-b5b7-f4fe8af61829,ResourceVersion:10785,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc001ee9fa7 0xc001ee9fa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.17,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:47 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://72c977349282d1abfe602d973e6bf1d94897fb419ad663c2d13584adcbdfb0e3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.577: INFO: Pod "nginx-deployment-785f69f5d9-pq9ng" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-pq9ng,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-pq9ng,UID:b85dc47d-a792-46ea-8dc8-88a0a5bc84cc,ResourceVersion:10824,Generation:0,CreationTimestamp:2020-03-06 08:30:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc002b3e280 0xc002b3e281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.35,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:48 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://f8c54c6797f57f5acc2dc86279ef564acb7aa028b267a8bc66facc5cfbe1ab2c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.578: INFO: Pod "nginx-deployment-785f69f5d9-qktgv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-qktgv,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-qktgv,UID:a4c063c3-447a-4bd9-99ef-6559d89c09ee,ResourceVersion:10783,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc002b3e440 0xc002b3e441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.16,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:47 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://c4b8079859ab494208bd87b5f4acdbd3cb11f076b6ea7548454a1f150b34a01f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.578: INFO: Pod "nginx-deployment-785f69f5d9-rcbzh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-rcbzh,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-rcbzh,UID:5a215555-f2b7-45e4-b255-16442649c845,ResourceVersion:10793,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc002b3e590 0xc002b3e591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.18,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:47 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://43d721601f2855e467de3f4c4eadc7d901d112e4cd65028c8aaa3bd49ce070ef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.578: INFO: Pod "nginx-deployment-785f69f5d9-rld2t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-rld2t,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-rld2t,UID:0df150a0-4b5b-4417-9852-8a16ee810f9b,ResourceVersion:10809,Generation:0,CreationTimestamp:2020-03-06 08:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc002b3e6f0 0xc002b3e6f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:30:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.34,StartTime:2020-03-06 08:30:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:30:48 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://951ae3edae89df4950e30bd464a58294cdd0f7e8b6e5594b2a89958b8797084a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  6 08:30:54.579: INFO: Pod "nginx-deployment-785f69f5d9-s555x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-s555x,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-6458,SelfLink:/api/v1/namespaces/deployment-6458/pods/nginx-deployment-785f69f5d9-s555x,UID:4dc89db3-2e9c-4c0b-a100-433dfc3679f8,ResourceVersion:10908,Generation:0,CreationTimestamp:2020-03-06 08:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 3f5e261d-f541-4728-91fe-13e7a5651def 0xc002b3e840 0xc002b3e841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8xjl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8xjl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8xjl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b3e8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b3e8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:30:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6458" for this suite.
Mar  6 08:31:04.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:31:05.056: INFO: namespace deployment-6458 deletion completed in 10.391119041s

• [SLOW TEST:21.402 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:31:05.056: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:31:05.415: INFO: Creating deployment "test-recreate-deployment"
Mar  6 08:31:05.431: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  6 08:31:05.458: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  6 08:31:07.472: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  6 08:31:07.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719080265, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719080265, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719080265, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719080265, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68487ddfcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 08:31:09.510: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  6 08:31:09.554: INFO: Updating deployment test-recreate-deployment
Mar  6 08:31:09.554: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  6 08:31:09.964: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7982,SelfLink:/apis/apps/v1/namespaces/deployment-7982/deployments/test-recreate-deployment,UID:8c3a81e1-064f-4381-91fa-28113426a6e8,ResourceVersion:11214,Generation:2,CreationTimestamp:2020-03-06 08:31:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-03-06 08:31:09 +0000 UTC 2020-03-06 08:31:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-03-06 08:31:09 +0000 UTC 2020-03-06 08:31:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-677f89d4b5" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 08:31:09.973: INFO: New ReplicaSet "test-recreate-deployment-677f89d4b5" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-677f89d4b5,GenerateName:,Namespace:deployment-7982,SelfLink:/apis/apps/v1/namespaces/deployment-7982/replicasets/test-recreate-deployment-677f89d4b5,UID:a7c08d80-f338-4afd-b917-9a718de49bec,ResourceVersion:11213,Generation:1,CreationTimestamp:2020-03-06 08:31:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8c3a81e1-064f-4381-91fa-28113426a6e8 0xc002b2e8d7 0xc002b2e8d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 08:31:09.973: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  6 08:31:09.973: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-68487ddfcd,GenerateName:,Namespace:deployment-7982,SelfLink:/apis/apps/v1/namespaces/deployment-7982/replicasets/test-recreate-deployment-68487ddfcd,UID:1f9542ba-cb13-4405-9ffa-5759427d1051,ResourceVersion:11203,Generation:2,CreationTimestamp:2020-03-06 08:31:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8c3a81e1-064f-4381-91fa-28113426a6e8 0xc002b2ebe7 0xc002b2ebe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 08:31:09.980: INFO: Pod "test-recreate-deployment-677f89d4b5-sxqhl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-677f89d4b5-sxqhl,GenerateName:test-recreate-deployment-677f89d4b5-,Namespace:deployment-7982,SelfLink:/api/v1/namespaces/deployment-7982/pods/test-recreate-deployment-677f89d4b5-sxqhl,UID:78989987-74c4-47cf-b7c5-724126635698,ResourceVersion:11210,Generation:0,CreationTimestamp:2020-03-06 08:31:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-677f89d4b5 a7c08d80-f338-4afd-b917-9a718de49bec 0xc002b2ffb7 0xc002b2ffb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tl985 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tl985,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tl985 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002526030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002526050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:31:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:31:09.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7982" for this suite.
Mar  6 08:31:18.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:31:18.163: INFO: namespace deployment-7982 deletion completed in 8.174173296s

• [SLOW TEST:13.107 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:31:18.163: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Mar  6 08:31:18.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 api-versions'
Mar  6 08:31:18.835: INFO: stderr: ""
Mar  6 08:31:18.835: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:31:18.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4740" for this suite.
Mar  6 08:31:25.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:31:25.321: INFO: namespace kubectl-4740 deletion completed in 6.472631154s

• [SLOW TEST:7.158 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:31:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 08:31:45.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:45.785: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:47.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:47.792: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:49.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:49.792: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:51.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:51.795: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:53.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:53.792: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:55.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:55.809: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:57.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:57.809: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:31:59.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:31:59.791: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:32:01.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:32:01.802: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:32:03.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:32:03.793: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  6 08:32:05.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  6 08:32:05.791: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:32:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9597" for this suite.
Mar  6 08:32:29.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:32:30.312: INFO: namespace container-lifecycle-hook-9597 deletion completed in 24.497734936s

• [SLOW TEST:64.990 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:32:30.312: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3787, will wait for the garbage collector to delete the pods
Mar  6 08:32:34.933: INFO: Deleting Job.batch foo took: 199.369626ms
Mar  6 08:32:35.333: INFO: Terminating Job.batch foo pods took: 400.326583ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:33:09.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3787" for this suite.
Mar  6 08:33:15.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:33:15.335: INFO: namespace job-3787 deletion completed in 6.284952512s

• [SLOW TEST:45.022 seconds]
[sig-apps] Job
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:33:15.335: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 08:33:15.729: INFO: Waiting up to 5m0s for pod "pod-31910c86-7216-491c-b9b8-8d03233d3591" in namespace "emptydir-9553" to be "success or failure"
Mar  6 08:33:15.736: INFO: Pod "pod-31910c86-7216-491c-b9b8-8d03233d3591": Phase="Pending", Reason="", readiness=false. Elapsed: 6.742594ms
Mar  6 08:33:17.742: INFO: Pod "pod-31910c86-7216-491c-b9b8-8d03233d3591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012788552s
Mar  6 08:33:19.779: INFO: Pod "pod-31910c86-7216-491c-b9b8-8d03233d3591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049810302s
STEP: Saw pod success
Mar  6 08:33:19.779: INFO: Pod "pod-31910c86-7216-491c-b9b8-8d03233d3591" satisfied condition "success or failure"
Mar  6 08:33:19.784: INFO: Trying to get logs from node wisecloud-worker02 pod pod-31910c86-7216-491c-b9b8-8d03233d3591 container test-container: <nil>
STEP: delete the pod
Mar  6 08:33:19.894: INFO: Waiting for pod pod-31910c86-7216-491c-b9b8-8d03233d3591 to disappear
Mar  6 08:33:19.901: INFO: Pod pod-31910c86-7216-491c-b9b8-8d03233d3591 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:33:19.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9553" for this suite.
Mar  6 08:33:25.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:33:26.181: INFO: namespace emptydir-9553 deletion completed in 6.271878351s

• [SLOW TEST:10.846 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:33:26.182: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Mar  6 08:33:26.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-8734'
Mar  6 08:33:27.037: INFO: stderr: ""
Mar  6 08:33:27.037: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:33:27.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8734'
Mar  6 08:33:27.308: INFO: stderr: ""
Mar  6 08:33:27.308: INFO: stdout: "update-demo-nautilus-hzf7c update-demo-nautilus-ppg4t "
Mar  6 08:33:27.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-hzf7c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:27.495: INFO: stderr: ""
Mar  6 08:33:27.495: INFO: stdout: ""
Mar  6 08:33:27.495: INFO: update-demo-nautilus-hzf7c is created but not running
Mar  6 08:33:32.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8734'
Mar  6 08:33:32.675: INFO: stderr: ""
Mar  6 08:33:32.675: INFO: stdout: "update-demo-nautilus-hzf7c update-demo-nautilus-ppg4t "
Mar  6 08:33:32.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-hzf7c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:32.850: INFO: stderr: ""
Mar  6 08:33:32.850: INFO: stdout: "true"
Mar  6 08:33:32.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-hzf7c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:33.019: INFO: stderr: ""
Mar  6 08:33:33.020: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:33:33.020: INFO: validating pod update-demo-nautilus-hzf7c
Mar  6 08:33:33.028: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:33:33.028: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:33:33.028: INFO: update-demo-nautilus-hzf7c is verified up and running
Mar  6 08:33:33.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-ppg4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:33.206: INFO: stderr: ""
Mar  6 08:33:33.206: INFO: stdout: "true"
Mar  6 08:33:33.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-ppg4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:33.392: INFO: stderr: ""
Mar  6 08:33:33.392: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:33:33.392: INFO: validating pod update-demo-nautilus-ppg4t
Mar  6 08:33:33.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:33:33.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:33:33.401: INFO: update-demo-nautilus-ppg4t is verified up and running
STEP: rolling-update to new replication controller
Mar  6 08:33:33.404: INFO: scanned /root for discovery docs: <nil>
Mar  6 08:33:33.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8734'
Mar  6 08:33:58.185: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 08:33:58.185: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:33:58.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8734'
Mar  6 08:33:58.569: INFO: stderr: ""
Mar  6 08:33:58.569: INFO: stdout: "update-demo-kitten-4k26f update-demo-kitten-b8j49 "
Mar  6 08:33:58.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-kitten-4k26f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:58.739: INFO: stderr: ""
Mar  6 08:33:58.739: INFO: stdout: "true"
Mar  6 08:33:58.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-kitten-4k26f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:58.909: INFO: stderr: ""
Mar  6 08:33:58.909: INFO: stdout: "172.20.8.7/library/kitten:1.0"
Mar  6 08:33:58.909: INFO: validating pod update-demo-kitten-4k26f
Mar  6 08:33:58.919: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 08:33:58.919: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 08:33:58.919: INFO: update-demo-kitten-4k26f is verified up and running
Mar  6 08:33:58.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-kitten-b8j49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:59.096: INFO: stderr: ""
Mar  6 08:33:59.096: INFO: stdout: "true"
Mar  6 08:33:59.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-kitten-b8j49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8734'
Mar  6 08:33:59.278: INFO: stderr: ""
Mar  6 08:33:59.279: INFO: stdout: "172.20.8.7/library/kitten:1.0"
Mar  6 08:33:59.279: INFO: validating pod update-demo-kitten-b8j49
Mar  6 08:33:59.310: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  6 08:33:59.310: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  6 08:33:59.310: INFO: update-demo-kitten-b8j49 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:33:59.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8734" for this suite.
Mar  6 08:34:23.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:34:23.589: INFO: namespace kubectl-8734 deletion completed in 24.270961102s

• [SLOW TEST:57.407 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:34:23.589: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:34:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5807" for this suite.
Mar  6 08:34:30.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:34:30.480: INFO: namespace kubelet-test-5807 deletion completed in 6.385658952s

• [SLOW TEST:6.891 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:34:30.481: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Mar  6 08:34:30.896: INFO: Waiting up to 5m0s for pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f" in namespace "containers-6824" to be "success or failure"
Mar  6 08:34:30.902: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.660127ms
Mar  6 08:34:32.911: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015117632s
Mar  6 08:34:34.920: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023454257s
Mar  6 08:34:36.927: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030470409s
Mar  6 08:34:38.934: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038227564s
STEP: Saw pod success
Mar  6 08:34:38.935: INFO: Pod "client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f" satisfied condition "success or failure"
Mar  6 08:34:38.940: INFO: Trying to get logs from node wisecloud-worker02 pod client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f container test-container: <nil>
STEP: delete the pod
Mar  6 08:34:39.007: INFO: Waiting for pod client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f to disappear
Mar  6 08:34:39.013: INFO: Pod client-containers-975928c0-29c1-40b5-b3fa-ce5956123e8f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:34:39.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6824" for this suite.
Mar  6 08:34:45.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:34:45.235: INFO: namespace containers-6824 deletion completed in 6.214112504s

• [SLOW TEST:14.755 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:34:45.236: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  6 08:35:26.365: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:35:26.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 08:35:26.364949      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7512" for this suite.
Mar  6 08:35:34.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:35:34.808: INFO: namespace gc-7512 deletion completed in 8.43524687s

• [SLOW TEST:49.572 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:35:34.809: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-8660/secret-test-25393781-81d7-484b-a55a-bcbab7ca1bd7
STEP: Creating a pod to test consume secrets
Mar  6 08:35:35.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a" in namespace "secrets-8660" to be "success or failure"
Mar  6 08:35:35.234: INFO: Pod "pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380611ms
Mar  6 08:35:37.252: INFO: Pod "pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024174763s
Mar  6 08:35:39.259: INFO: Pod "pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030966141s
STEP: Saw pod success
Mar  6 08:35:39.259: INFO: Pod "pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a" satisfied condition "success or failure"
Mar  6 08:35:39.264: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a container env-test: <nil>
STEP: delete the pod
Mar  6 08:35:39.381: INFO: Waiting for pod pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a to disappear
Mar  6 08:35:39.386: INFO: Pod pod-configmaps-2a12912d-e4b5-438c-b89b-f01b15717f4a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:35:39.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8660" for this suite.
Mar  6 08:35:45.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:35:45.741: INFO: namespace secrets-8660 deletion completed in 6.327267542s

• [SLOW TEST:10.933 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:35:45.742: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  6 08:35:54.407: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:35:54.412: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:35:56.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:35:56.420: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:35:58.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:35:58.420: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:00.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:00.441: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:02.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:02.419: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:04.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:04.420: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:06.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:06.419: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:08.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:08.419: INFO: Pod pod-with-prestop-http-hook still exists
Mar  6 08:36:10.413: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  6 08:36:10.421: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:36:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7013" for this suite.
Mar  6 08:36:34.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:36:34.730: INFO: namespace container-lifecycle-hook-7013 deletion completed in 24.285726486s

• [SLOW TEST:48.988 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:36:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-0a2ef738-0015-4061-aebd-9982c04c9334
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:36:35.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6436" for this suite.
Mar  6 08:36:41.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:36:41.300: INFO: namespace secrets-6436 deletion completed in 6.239198066s

• [SLOW TEST:6.568 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:36:41.300: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 08:36:41.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-rc --image=172.20.8.7/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2470'
Mar  6 08:36:42.987: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 08:36:42.987: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  6 08:36:43.006: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-swdl5]
Mar  6 08:36:43.007: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-swdl5" in namespace "kubectl-2470" to be "running and ready"
Mar  6 08:36:43.048: INFO: Pod "e2e-test-nginx-rc-swdl5": Phase="Pending", Reason="", readiness=false. Elapsed: 41.128919ms
Mar  6 08:36:45.056: INFO: Pod "e2e-test-nginx-rc-swdl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048883534s
Mar  6 08:36:47.063: INFO: Pod "e2e-test-nginx-rc-swdl5": Phase="Running", Reason="", readiness=true. Elapsed: 4.056002099s
Mar  6 08:36:47.063: INFO: Pod "e2e-test-nginx-rc-swdl5" satisfied condition "running and ready"
Mar  6 08:36:47.063: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-swdl5]
Mar  6 08:36:47.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs rc/e2e-test-nginx-rc --namespace=kubectl-2470'
Mar  6 08:36:47.303: INFO: stderr: ""
Mar  6 08:36:47.303: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Mar  6 08:36:47.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete rc e2e-test-nginx-rc --namespace=kubectl-2470'
Mar  6 08:36:47.508: INFO: stderr: ""
Mar  6 08:36:47.508: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:36:47.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2470" for this suite.
Mar  6 08:36:53.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:36:53.715: INFO: namespace kubectl-2470 deletion completed in 6.197633046s

• [SLOW TEST:12.415 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:36:53.716: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-6b6db3d9-4a6f-46fa-b25a-450e7953cc63
STEP: Creating a pod to test consume configMaps
Mar  6 08:36:54.043: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a" in namespace "projected-5741" to be "success or failure"
Mar  6 08:36:54.070: INFO: Pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a": Phase="Pending", Reason="", readiness=false. Elapsed: 27.576222ms
Mar  6 08:36:56.077: INFO: Pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034254624s
Mar  6 08:36:58.084: INFO: Pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041146672s
Mar  6 08:37:00.090: INFO: Pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047648907s
STEP: Saw pod success
Mar  6 08:37:00.091: INFO: Pod "pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a" satisfied condition "success or failure"
Mar  6 08:37:00.096: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 08:37:00.210: INFO: Waiting for pod pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a to disappear
Mar  6 08:37:00.215: INFO: Pod pod-projected-configmaps-59e3b549-d580-4349-91ca-585fcb64b04a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:37:00.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5741" for this suite.
Mar  6 08:37:06.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:37:06.397: INFO: namespace projected-5741 deletion completed in 6.173595882s

• [SLOW TEST:12.682 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:37:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 08:37:14.978: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:14.984: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:16.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:16.991: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:18.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:18.992: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:20.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:20.991: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:22.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:22.990: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:24.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:24.993: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:26.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:26.992: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:28.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:28.991: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:30.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:30.992: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  6 08:37:32.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  6 08:37:33.007: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:37:33.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8261" for this suite.
Mar  6 08:37:57.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:37:57.230: INFO: namespace container-lifecycle-hook-8261 deletion completed in 24.214579851s

• [SLOW TEST:50.833 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:37:57.233: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4357
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-32f3979c-df42-4c55-a34d-67582e7f4c85
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:38:03.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4357" for this suite.
Mar  6 08:38:27.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:38:27.984: INFO: namespace configmap-4357 deletion completed in 24.223724678s

• [SLOW TEST:30.751 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:38:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Mar  6 08:38:28.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 cluster-info'
Mar  6 08:38:28.459: INFO: stderr: ""
Mar  6 08:38:28.459: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:38:28.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2171" for this suite.
Mar  6 08:38:34.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:38:34.660: INFO: namespace kubectl-2171 deletion completed in 6.190512033s

• [SLOW TEST:6.676 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:38:34.661: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4768
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:38:34.963: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:38:41.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4768" for this suite.
Mar  6 08:38:47.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:38:47.447: INFO: namespace custom-resource-definition-4768 deletion completed in 6.239277609s

• [SLOW TEST:12.786 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:38:47.447: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar  6 08:39:18.116: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:39:18.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 08:39:18.116053      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4687" for this suite.
Mar  6 08:39:26.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:39:26.331: INFO: namespace gc-4687 deletion completed in 8.206340934s

• [SLOW TEST:38.883 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:39:26.332: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8822/configmap-test-5a5267a2-e47a-4a29-9e81-332fa1a9b9a6
STEP: Creating a pod to test consume configMaps
Mar  6 08:39:26.753: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93" in namespace "configmap-8822" to be "success or failure"
Mar  6 08:39:26.759: INFO: Pod "pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605687ms
Mar  6 08:39:28.766: INFO: Pod "pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012599662s
Mar  6 08:39:30.773: INFO: Pod "pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019265723s
STEP: Saw pod success
Mar  6 08:39:30.773: INFO: Pod "pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93" satisfied condition "success or failure"
Mar  6 08:39:30.778: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93 container env-test: <nil>
STEP: delete the pod
Mar  6 08:39:30.868: INFO: Waiting for pod pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93 to disappear
Mar  6 08:39:30.873: INFO: Pod pod-configmaps-9b451ebf-5d98-487a-9c12-8f5337ae9f93 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:39:30.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8822" for this suite.
Mar  6 08:39:36.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:39:37.135: INFO: namespace configmap-8822 deletion completed in 6.25453784s

• [SLOW TEST:10.804 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:39:37.136: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:39:37.421: INFO: Creating ReplicaSet my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e
Mar  6 08:39:37.455: INFO: Pod name my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e: Found 0 pods out of 1
Mar  6 08:39:42.501: INFO: Pod name my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e: Found 1 pods out of 1
Mar  6 08:39:42.501: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e" is running
Mar  6 08:39:42.527: INFO: Pod "my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e-v2djd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:39:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:39:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:39:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:39:37 +0000 UTC Reason: Message:}])
Mar  6 08:39:42.527: INFO: Trying to dial the pod
Mar  6 08:39:47.548: INFO: Controller my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e: Got expected result from replica 1 [my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e-v2djd]: "my-hostname-basic-6624b3ec-4365-4359-949c-e961a11a281e-v2djd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:39:47.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2480" for this suite.
Mar  6 08:39:53.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:39:53.878: INFO: namespace replicaset-2480 deletion completed in 6.323058294s

• [SLOW TEST:16.743 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:39:53.879: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f24b97a7-5dcf-4438-9de3-bc1591259147
STEP: Creating a pod to test consume configMaps
Mar  6 08:39:54.264: INFO: Waiting up to 5m0s for pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a" in namespace "configmap-2949" to be "success or failure"
Mar  6 08:39:54.284: INFO: Pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.386601ms
Mar  6 08:39:56.290: INFO: Pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025895236s
Mar  6 08:39:58.297: INFO: Pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032432041s
Mar  6 08:40:00.303: INFO: Pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038734572s
STEP: Saw pod success
Mar  6 08:40:00.303: INFO: Pod "pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a" satisfied condition "success or failure"
Mar  6 08:40:00.331: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 08:40:00.497: INFO: Waiting for pod pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a to disappear
Mar  6 08:40:00.502: INFO: Pod pod-configmaps-03215584-ec12-4521-ae3e-656016bf870a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:40:00.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2949" for this suite.
Mar  6 08:40:06.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:40:06.702: INFO: namespace configmap-2949 deletion completed in 6.192673912s

• [SLOW TEST:12.823 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:40:06.702: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:40:06.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa" in namespace "projected-9340" to be "success or failure"
Mar  6 08:40:07.004: INFO: Pod "downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.473714ms
Mar  6 08:40:09.017: INFO: Pod "downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017906979s
Mar  6 08:40:11.023: INFO: Pod "downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024150947s
STEP: Saw pod success
Mar  6 08:40:11.023: INFO: Pod "downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa" satisfied condition "success or failure"
Mar  6 08:40:11.028: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa container client-container: <nil>
STEP: delete the pod
Mar  6 08:40:11.110: INFO: Waiting for pod downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa to disappear
Mar  6 08:40:11.115: INFO: Pod downwardapi-volume-8ef0f359-89d8-418c-a060-db118ec483fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:40:11.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9340" for this suite.
Mar  6 08:40:17.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:40:17.330: INFO: namespace projected-9340 deletion completed in 6.207151564s

• [SLOW TEST:10.628 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:40:17.330: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:40:53.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2387" for this suite.
Mar  6 08:40:59.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:40:59.930: INFO: namespace container-runtime-2387 deletion completed in 6.308335386s

• [SLOW TEST:42.600 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:40:59.931: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  6 08:41:04.840: INFO: Successfully updated pod "annotationupdate2390acb1-22d2-45c4-b62c-b9ef14415068"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:41:06.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7718" for this suite.
Mar  6 08:41:30.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:41:31.107: INFO: namespace downward-api-7718 deletion completed in 24.220251104s

• [SLOW TEST:31.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:41:31.108: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  6 08:41:31.368: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:41:39.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8817" for this suite.
Mar  6 08:42:03.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:42:03.373: INFO: namespace init-container-8817 deletion completed in 24.299210386s

• [SLOW TEST:32.265 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:42:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4400
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e061f5ba-265e-4f22-8a15-eb2434be6fcc
STEP: Creating configMap with name cm-test-opt-upd-579e499f-64ec-4995-b7c0-c8b189e71ce0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e061f5ba-265e-4f22-8a15-eb2434be6fcc
STEP: Updating configmap cm-test-opt-upd-579e499f-64ec-4995-b7c0-c8b189e71ce0
STEP: Creating configMap with name cm-test-opt-create-41aa707a-c810-4d6e-9dc2-3000ff566811
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:42:10.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4400" for this suite.
Mar  6 08:42:34.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:42:34.232: INFO: namespace configmap-4400 deletion completed in 24.185796019s

• [SLOW TEST:30.858 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:42:34.232: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:42:34.566: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  6 08:42:37.005: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:42:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7542" for this suite.
Mar  6 08:42:46.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:42:46.261: INFO: namespace replication-controller-7542 deletion completed in 8.232121356s

• [SLOW TEST:12.029 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:42:46.262: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Mar  6 08:42:46.605: INFO: Waiting up to 5m0s for pod "var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063" in namespace "var-expansion-1086" to be "success or failure"
Mar  6 08:42:46.670: INFO: Pod "var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063": Phase="Pending", Reason="", readiness=false. Elapsed: 65.615031ms
Mar  6 08:42:48.679: INFO: Pod "var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073851896s
Mar  6 08:42:50.685: INFO: Pod "var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080649714s
STEP: Saw pod success
Mar  6 08:42:50.686: INFO: Pod "var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063" satisfied condition "success or failure"
Mar  6 08:42:50.691: INFO: Trying to get logs from node wisecloud-worker02 pod var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063 container dapi-container: <nil>
STEP: delete the pod
Mar  6 08:42:50.772: INFO: Waiting for pod var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063 to disappear
Mar  6 08:42:50.777: INFO: Pod var-expansion-35a94130-f82b-478e-8e21-d1a43e8b4063 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:42:50.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1086" for this suite.
Mar  6 08:42:56.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:42:56.964: INFO: namespace var-expansion-1086 deletion completed in 6.178341259s

• [SLOW TEST:10.702 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:42:56.965: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:43:01.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2365" for this suite.
Mar  6 08:43:49.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:43:49.572: INFO: namespace kubelet-test-2365 deletion completed in 48.227208492s

• [SLOW TEST:52.607 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:43:49.573: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6018
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7680
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:44:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2528" for this suite.
Mar  6 08:44:22.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:44:22.908: INFO: namespace namespaces-2528 deletion completed in 6.175609208s
STEP: Destroying namespace "nsdeletetest-6018" for this suite.
Mar  6 08:44:22.913: INFO: Namespace nsdeletetest-6018 was already deleted
STEP: Destroying namespace "nsdeletetest-7680" for this suite.
Mar  6 08:44:28.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:44:29.102: INFO: namespace nsdeletetest-7680 deletion completed in 6.188995279s

• [SLOW TEST:39.529 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:44:29.102: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  6 08:44:29.679: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14024,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 08:44:29.679: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14025,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 08:44:29.680: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14026,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  6 08:44:39.809: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14049,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 08:44:39.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14050,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  6 08:44:39.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6233,SelfLink:/api/v1/namespaces/watch-6233/configmaps/e2e-watch-test-label-changed,UID:c97c42b2-f920-4929-8c8e-3b03ac7daf41,ResourceVersion:14051,Generation:0,CreationTimestamp:2020-03-06 08:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:44:39.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6233" for this suite.
Mar  6 08:44:45.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:44:46.017: INFO: namespace watch-6233 deletion completed in 6.19946774s

• [SLOW TEST:16.915 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:44:46.018: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3654
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  6 08:44:46.536: INFO: Found 0 stateful pods, waiting for 3
Mar  6 08:44:56.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:44:56.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:44:56.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Mar  6 08:45:06.543: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:45:06.544: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:45:06.544: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from 172.20.8.7/library/nginx:1.14-alpine to 172.20.8.7/library/nginx:1.15-alpine
Mar  6 08:45:06.606: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  6 08:45:16.722: INFO: Updating stateful set ss2
Mar  6 08:45:16.789: INFO: Waiting for Pod statefulset-3654/ss2-2 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:45:26.805: INFO: Waiting for Pod statefulset-3654/ss2-2 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
STEP: Restoring Pods to the correct revision when they are deleted
Mar  6 08:45:37.099: INFO: Found 2 stateful pods, waiting for 3
Mar  6 08:45:47.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:45:47.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 08:45:47.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  6 08:45:47.166: INFO: Updating stateful set ss2
Mar  6 08:45:47.205: INFO: Waiting for Pod statefulset-3654/ss2-1 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:45:57.267: INFO: Updating stateful set ss2
Mar  6 08:45:57.279: INFO: Waiting for StatefulSet statefulset-3654/ss2 to complete update
Mar  6 08:45:57.279: INFO: Waiting for Pod statefulset-3654/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  6 08:46:07.292: INFO: Waiting for StatefulSet statefulset-3654/ss2 to complete update
Mar  6 08:46:07.292: INFO: Waiting for Pod statefulset-3654/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  6 08:46:17.293: INFO: Deleting all statefulset in ns statefulset-3654
Mar  6 08:46:17.300: INFO: Scaling statefulset ss2 to 0
Mar  6 08:46:47.368: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 08:46:47.375: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:46:47.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3654" for this suite.
Mar  6 08:46:55.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:46:55.701: INFO: namespace statefulset-3654 deletion completed in 8.245021467s

• [SLOW TEST:129.683 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:46:55.701: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Mar  6 08:46:56.005: INFO: Waiting up to 5m0s for pod "var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b" in namespace "var-expansion-6114" to be "success or failure"
Mar  6 08:46:56.011: INFO: Pod "var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248403ms
Mar  6 08:46:58.019: INFO: Pod "var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014492555s
Mar  6 08:47:00.044: INFO: Pod "var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038911541s
STEP: Saw pod success
Mar  6 08:47:00.044: INFO: Pod "var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b" satisfied condition "success or failure"
Mar  6 08:47:00.050: INFO: Trying to get logs from node wisecloud-worker02 pod var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b container dapi-container: <nil>
STEP: delete the pod
Mar  6 08:47:00.145: INFO: Waiting for pod var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b to disappear
Mar  6 08:47:00.151: INFO: Pod var-expansion-6a351ca8-4ba0-444e-bcda-a987ac560a9b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:47:00.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6114" for this suite.
Mar  6 08:47:06.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:47:06.374: INFO: namespace var-expansion-6114 deletion completed in 6.215332434s

• [SLOW TEST:10.673 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:47:06.374: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:47:06.662: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  6 08:47:11.670: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 08:47:11.670: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  6 08:47:11.721: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5634,SelfLink:/apis/apps/v1/namespaces/deployment-5634/deployments/test-cleanup-deployment,UID:ff155075-b8ea-4685-9e2f-35abffc28bca,ResourceVersion:14643,Generation:1,CreationTimestamp:2020-03-06 08:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  6 08:47:11.742: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar  6 08:47:11.742: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  6 08:47:11.742: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-5634,SelfLink:/apis/apps/v1/namespaces/deployment-5634/replicasets/test-cleanup-controller,UID:3d43df26-1b4a-4283-ba67-2340b23fb781,ResourceVersion:14644,Generation:1,CreationTimestamp:2020-03-06 08:47:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment ff155075-b8ea-4685-9e2f-35abffc28bca 0xc002924d77 0xc002924d78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 08:47:11.751: INFO: Pod "test-cleanup-controller-lt6vg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-lt6vg,GenerateName:test-cleanup-controller-,Namespace:deployment-5634,SelfLink:/api/v1/namespaces/deployment-5634/pods/test-cleanup-controller-lt6vg,UID:7ceaa637-bcf2-4e83-9040-30e52ccf4a92,ResourceVersion:14635,Generation:0,CreationTimestamp:2020-03-06 08:47:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 3d43df26-1b4a-4283-ba67-2340b23fb781 0xc0029252f7 0xc0029252f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5t7fc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5t7fc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5t7fc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002925360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:47:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:47:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:47:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 08:47:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.97,StartTime:2020-03-06 08:47:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-06 08:47:09 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://55b1258ccbb552ec12b023dabdb2acb4466b8bb3624d5acf77f455b06df4f899}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:47:11.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5634" for this suite.
Mar  6 08:47:17.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:47:18.116: INFO: namespace deployment-5634 deletion completed in 6.330570282s

• [SLOW TEST:11.741 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:47:18.116: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:47:23.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6407" for this suite.
Mar  6 08:47:32.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:47:32.230: INFO: namespace watch-6407 deletion completed in 8.263368711s

• [SLOW TEST:14.114 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:47:32.231: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:47:32.554: INFO: (0) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.488514ms)
Mar  6 08:47:32.560: INFO: (1) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.542517ms)
Mar  6 08:47:32.567: INFO: (2) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.794067ms)
Mar  6 08:47:32.574: INFO: (3) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.454234ms)
Mar  6 08:47:32.581: INFO: (4) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.363907ms)
Mar  6 08:47:32.588: INFO: (5) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.13144ms)
Mar  6 08:47:32.602: INFO: (6) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 13.800134ms)
Mar  6 08:47:32.609: INFO: (7) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.468808ms)
Mar  6 08:47:32.615: INFO: (8) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.079713ms)
Mar  6 08:47:32.621: INFO: (9) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.759684ms)
Mar  6 08:47:32.648: INFO: (10) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 26.960825ms)
Mar  6 08:47:32.655: INFO: (11) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.961344ms)
Mar  6 08:47:32.661: INFO: (12) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.005167ms)
Mar  6 08:47:32.667: INFO: (13) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.30222ms)
Mar  6 08:47:32.674: INFO: (14) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.66679ms)
Mar  6 08:47:32.680: INFO: (15) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.983751ms)
Mar  6 08:47:32.686: INFO: (16) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.301031ms)
Mar  6 08:47:32.692: INFO: (17) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.15099ms)
Mar  6 08:47:32.699: INFO: (18) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.259254ms)
Mar  6 08:47:32.705: INFO: (19) /api/v1/nodes/wisecloud-worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.305417ms)
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:47:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2163" for this suite.
Mar  6 08:47:38.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:47:38.986: INFO: namespace proxy-2163 deletion completed in 6.272845491s

• [SLOW TEST:6.756 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:47:38.987: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3095.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3095.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 08:47:47.376: INFO: DNS probes using dns-3095/dns-test-306d536e-a78c-4cf3-a541-33063b7210f8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:47:47.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3095" for this suite.
Mar  6 08:47:53.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:47:53.686: INFO: namespace dns-3095 deletion completed in 6.231648082s

• [SLOW TEST:14.699 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:47:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 121.213.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.213.121_udp@PTR;check="$$(dig +tcp +noall +answer +search 121.213.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.213.121_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 121.213.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.213.121_udp@PTR;check="$$(dig +tcp +noall +answer +search 121.213.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.213.121_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 08:48:00.311: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.317: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.322: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.329: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.369: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.374: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.380: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.386: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:00.422: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:05.429: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.436: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.442: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.448: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.488: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.494: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.499: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.531: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:05.568: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:10.430: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.443: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.492: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.499: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.505: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.511: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:10.553: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:15.431: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.508: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.517: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.566: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.572: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.607: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.628: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:15.666: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:20.430: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.443: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.488: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.493: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.500: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.506: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:20.542: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:25.430: INFO: Unable to read wheezy_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.436: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.442: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.450: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.490: INFO: Unable to read jessie_udp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.502: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.508: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local from pod dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886: the server could not find the requested resource (get pods dns-test-1bf65091-c04f-48b8-825f-667a00423886)
Mar  6 08:48:25.580: INFO: Lookups using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 failed for: [wheezy_udp@dns-test-service.dns-4.svc.cluster.local wheezy_tcp@dns-test-service.dns-4.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_udp@dns-test-service.dns-4.svc.cluster.local jessie_tcp@dns-test-service.dns-4.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4.svc.cluster.local]

Mar  6 08:48:30.609: INFO: DNS probes using dns-4/dns-test-1bf65091-c04f-48b8-825f-667a00423886 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:48:31.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4" for this suite.
Mar  6 08:48:39.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:48:39.363: INFO: namespace dns-4 deletion completed in 8.188191993s

• [SLOW TEST:45.677 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:48:39.363: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  6 08:48:39.677: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15087,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 08:48:39.677: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15087,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  6 08:48:49.722: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15106,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  6 08:48:49.722: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15106,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  6 08:48:59.780: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15126,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 08:48:59.781: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15126,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  6 08:49:09.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15147,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 08:49:09.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-a,UID:d09f81f1-8319-49c0-9654-3c6cb010a476,ResourceVersion:15147,Generation:0,CreationTimestamp:2020-03-06 08:48:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  6 08:49:19.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-b,UID:6deae4c0-7fba-43bc-937c-1a13d20f74b0,ResourceVersion:15167,Generation:0,CreationTimestamp:2020-03-06 08:49:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 08:49:19.940: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-b,UID:6deae4c0-7fba-43bc-937c-1a13d20f74b0,ResourceVersion:15167,Generation:0,CreationTimestamp:2020-03-06 08:49:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  6 08:49:29.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-b,UID:6deae4c0-7fba-43bc-937c-1a13d20f74b0,ResourceVersion:15187,Generation:0,CreationTimestamp:2020-03-06 08:49:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  6 08:49:29.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1878,SelfLink:/api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-configmap-b,UID:6deae4c0-7fba-43bc-937c-1a13d20f74b0,ResourceVersion:15187,Generation:0,CreationTimestamp:2020-03-06 08:49:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:49:39.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1878" for this suite.
Mar  6 08:49:46.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:49:46.179: INFO: namespace watch-1878 deletion completed in 6.20513958s

• [SLOW TEST:66.815 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:49:46.179: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-80bd85a9-1f8d-454e-8b5b-9def98b7e4e8
STEP: Creating a pod to test consume secrets
Mar  6 08:49:46.500: INFO: Waiting up to 5m0s for pod "pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f" in namespace "secrets-7943" to be "success or failure"
Mar  6 08:49:46.506: INFO: Pod "pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91381ms
Mar  6 08:49:48.520: INFO: Pod "pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019933962s
Mar  6 08:49:50.528: INFO: Pod "pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027978937s
STEP: Saw pod success
Mar  6 08:49:50.528: INFO: Pod "pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f" satisfied condition "success or failure"
Mar  6 08:49:50.533: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:49:50.588: INFO: Waiting for pod pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f to disappear
Mar  6 08:49:50.594: INFO: Pod pod-secrets-0a3d9b0e-8a74-48ae-b1aa-3cf58595d00f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:49:50.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7943" for this suite.
Mar  6 08:49:56.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:49:57.004: INFO: namespace secrets-7943 deletion completed in 6.401999643s

• [SLOW TEST:10.824 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:49:57.004: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  6 08:50:02.415: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:50:03.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3312" for this suite.
Mar  6 08:50:27.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:50:27.735: INFO: namespace replicaset-3312 deletion completed in 24.221212048s

• [SLOW TEST:30.731 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:50:27.735: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9390
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9141
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:50:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6955" for this suite.
Mar  6 08:50:40.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:50:41.078: INFO: namespace namespaces-6955 deletion completed in 6.294373176s
STEP: Destroying namespace "nsdeletetest-9390" for this suite.
Mar  6 08:50:41.082: INFO: Namespace nsdeletetest-9390 was already deleted
STEP: Destroying namespace "nsdeletetest-9141" for this suite.
Mar  6 08:50:47.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:50:47.325: INFO: namespace nsdeletetest-9141 deletion completed in 6.24274887s

• [SLOW TEST:19.590 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:50:47.325: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Mar  6 08:50:47.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 --namespace=kubectl-1345 run e2e-test-rm-busybox-job --image=172.20.8.7/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  6 08:50:52.317: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  6 08:50:52.317: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:50:54.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1345" for this suite.
Mar  6 08:51:00.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:51:00.613: INFO: namespace kubectl-1345 deletion completed in 6.276085378s

• [SLOW TEST:13.288 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:51:00.614: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7359
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  6 08:51:06.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec pod-sharedvolume-81d7f321-d04c-499f-859e-c3d2e2e53f66 -c busybox-main-container --namespace=emptydir-7359 -- cat /usr/share/volumeshare/shareddata.txt'
Mar  6 08:51:07.389: INFO: stderr: ""
Mar  6 08:51:07.389: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:51:07.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7359" for this suite.
Mar  6 08:51:13.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:51:13.600: INFO: namespace emptydir-7359 deletion completed in 6.200732823s

• [SLOW TEST:12.986 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:51:13.600: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  6 08:51:24.331: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:51:24.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 08:51:24.331043      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3653" for this suite.
Mar  6 08:51:32.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:51:32.772: INFO: namespace gc-3653 deletion completed in 8.431680786s

• [SLOW TEST:19.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:51:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e
Mar  6 08:51:33.276: INFO: Pod name my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e: Found 0 pods out of 1
Mar  6 08:51:38.305: INFO: Pod name my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e: Found 1 pods out of 1
Mar  6 08:51:38.305: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e" are running
Mar  6 08:51:38.311: INFO: Pod "my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e-7ns9c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:51:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:51:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:51:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-06 08:51:33 +0000 UTC Reason: Message:}])
Mar  6 08:51:38.311: INFO: Trying to dial the pod
Mar  6 08:51:43.332: INFO: Controller my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e: Got expected result from replica 1 [my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e-7ns9c]: "my-hostname-basic-23be1bed-c447-4add-8620-cfa48ce6471e-7ns9c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:51:43.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2169" for this suite.
Mar  6 08:51:49.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:51:49.546: INFO: namespace replication-controller-2169 deletion completed in 6.205373817s

• [SLOW TEST:16.774 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:51:49.547: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 in namespace container-probe-4125
Mar  6 08:51:53.927: INFO: Started pod liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 in namespace container-probe-4125
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 08:51:53.933: INFO: Initial restart count of pod liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is 0
Mar  6 08:52:10.013: INFO: Restart count of pod container-probe-4125/liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is now 1 (16.079507045s elapsed)
Mar  6 08:52:30.083: INFO: Restart count of pod container-probe-4125/liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is now 2 (36.149986254s elapsed)
Mar  6 08:52:50.163: INFO: Restart count of pod container-probe-4125/liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is now 3 (56.22956202s elapsed)
Mar  6 08:53:10.234: INFO: Restart count of pod container-probe-4125/liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is now 4 (1m16.30038788s elapsed)
Mar  6 08:54:14.455: INFO: Restart count of pod container-probe-4125/liveness-a881bbcd-9dca-49ca-9f90-2c93ff0d8e26 is now 5 (2m20.52142394s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:54:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4125" for this suite.
Mar  6 08:54:20.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:54:20.827: INFO: namespace container-probe-4125 deletion completed in 6.297010713s

• [SLOW TEST:151.280 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:54:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8851 to expose endpoints map[]
Mar  6 08:54:21.277: INFO: Get endpoints failed (36.75261ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  6 08:54:22.284: INFO: successfully validated that service multi-endpoint-test in namespace services-8851 exposes endpoints map[] (1.043537772s elapsed)
STEP: Creating pod pod1 in namespace services-8851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8851 to expose endpoints map[pod1:[100]]
Mar  6 08:54:25.381: INFO: successfully validated that service multi-endpoint-test in namespace services-8851 exposes endpoints map[pod1:[100]] (3.055405237s elapsed)
STEP: Creating pod pod2 in namespace services-8851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8851 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  6 08:54:28.472: INFO: successfully validated that service multi-endpoint-test in namespace services-8851 exposes endpoints map[pod1:[100] pod2:[101]] (3.070449025s elapsed)
STEP: Deleting pod pod1 in namespace services-8851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8851 to expose endpoints map[pod2:[101]]
Mar  6 08:54:29.589: INFO: successfully validated that service multi-endpoint-test in namespace services-8851 exposes endpoints map[pod2:[101]] (1.078658717s elapsed)
STEP: Deleting pod pod2 in namespace services-8851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8851 to expose endpoints map[]
Mar  6 08:54:30.665: INFO: successfully validated that service multi-endpoint-test in namespace services-8851 exposes endpoints map[] (1.0112427s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:54:30.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8851" for this suite.
Mar  6 08:54:54.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:54:55.102: INFO: namespace services-8851 deletion completed in 24.223305135s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:34.275 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:54:55.103: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Mar  6 08:54:55.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-8608'
Mar  6 08:54:56.145: INFO: stderr: ""
Mar  6 08:54:56.145: INFO: stdout: "pod/pause created\n"
Mar  6 08:54:56.145: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  6 08:54:56.145: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8608" to be "running and ready"
Mar  6 08:54:56.164: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.894031ms
Mar  6 08:54:58.170: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025006879s
Mar  6 08:55:00.183: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.038006731s
Mar  6 08:55:00.183: INFO: Pod "pause" satisfied condition "running and ready"
Mar  6 08:55:00.183: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  6 08:55:00.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 label pods pause testing-label=testing-label-value --namespace=kubectl-8608'
Mar  6 08:55:00.409: INFO: stderr: ""
Mar  6 08:55:00.409: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  6 08:55:00.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pod pause -L testing-label --namespace=kubectl-8608'
Mar  6 08:55:00.593: INFO: stderr: ""
Mar  6 08:55:00.593: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  6 08:55:00.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 label pods pause testing-label- --namespace=kubectl-8608'
Mar  6 08:55:00.798: INFO: stderr: ""
Mar  6 08:55:00.798: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  6 08:55:00.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pod pause -L testing-label --namespace=kubectl-8608'
Mar  6 08:55:00.979: INFO: stderr: ""
Mar  6 08:55:00.980: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Mar  6 08:55:00.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-8608'
Mar  6 08:55:01.271: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 08:55:01.271: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  6 08:55:01.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=pause --no-headers --namespace=kubectl-8608'
Mar  6 08:55:01.456: INFO: stderr: "No resources found.\n"
Mar  6 08:55:01.456: INFO: stdout: ""
Mar  6 08:55:01.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=pause --namespace=kubectl-8608 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:55:01.630: INFO: stderr: ""
Mar  6 08:55:01.631: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:55:01.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8608" for this suite.
Mar  6 08:55:07.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:55:07.908: INFO: namespace kubectl-8608 deletion completed in 6.268620208s

• [SLOW TEST:12.806 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:55:07.909: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  6 08:55:08.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-6589'
Mar  6 08:55:08.941: INFO: stderr: ""
Mar  6 08:55:08.942: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:55:08.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:09.255: INFO: stderr: ""
Mar  6 08:55:09.255: INFO: stdout: "update-demo-nautilus-t74vb update-demo-nautilus-vb2f6 "
Mar  6 08:55:09.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-t74vb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:09.492: INFO: stderr: ""
Mar  6 08:55:09.492: INFO: stdout: ""
Mar  6 08:55:09.492: INFO: update-demo-nautilus-t74vb is created but not running
Mar  6 08:55:14.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:14.675: INFO: stderr: ""
Mar  6 08:55:14.675: INFO: stdout: "update-demo-nautilus-t74vb update-demo-nautilus-vb2f6 "
Mar  6 08:55:14.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-t74vb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:14.859: INFO: stderr: ""
Mar  6 08:55:14.859: INFO: stdout: "true"
Mar  6 08:55:14.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-t74vb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:15.041: INFO: stderr: ""
Mar  6 08:55:15.041: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:55:15.041: INFO: validating pod update-demo-nautilus-t74vb
Mar  6 08:55:15.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:55:15.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:55:15.052: INFO: update-demo-nautilus-t74vb is verified up and running
Mar  6 08:55:15.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:15.230: INFO: stderr: ""
Mar  6 08:55:15.230: INFO: stdout: "true"
Mar  6 08:55:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:15.404: INFO: stderr: ""
Mar  6 08:55:15.404: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:55:15.404: INFO: validating pod update-demo-nautilus-vb2f6
Mar  6 08:55:15.414: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:55:15.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:55:15.414: INFO: update-demo-nautilus-vb2f6 is verified up and running
STEP: scaling down the replication controller
Mar  6 08:55:15.417: INFO: scanned /root for discovery docs: <nil>
Mar  6 08:55:15.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6589'
Mar  6 08:55:16.702: INFO: stderr: ""
Mar  6 08:55:16.702: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:55:16.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:16.892: INFO: stderr: ""
Mar  6 08:55:16.892: INFO: stdout: "update-demo-nautilus-t74vb update-demo-nautilus-vb2f6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  6 08:55:21.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:22.081: INFO: stderr: ""
Mar  6 08:55:22.081: INFO: stdout: "update-demo-nautilus-t74vb update-demo-nautilus-vb2f6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  6 08:55:27.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:27.275: INFO: stderr: ""
Mar  6 08:55:27.275: INFO: stdout: "update-demo-nautilus-t74vb update-demo-nautilus-vb2f6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  6 08:55:32.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:32.450: INFO: stderr: ""
Mar  6 08:55:32.450: INFO: stdout: "update-demo-nautilus-vb2f6 "
Mar  6 08:55:32.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:32.627: INFO: stderr: ""
Mar  6 08:55:32.627: INFO: stdout: "true"
Mar  6 08:55:32.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:32.797: INFO: stderr: ""
Mar  6 08:55:32.797: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:55:32.797: INFO: validating pod update-demo-nautilus-vb2f6
Mar  6 08:55:32.805: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:55:32.805: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:55:32.805: INFO: update-demo-nautilus-vb2f6 is verified up and running
STEP: scaling up the replication controller
Mar  6 08:55:32.808: INFO: scanned /root for discovery docs: <nil>
Mar  6 08:55:32.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6589'
Mar  6 08:55:34.077: INFO: stderr: ""
Mar  6 08:55:34.077: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  6 08:55:34.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:34.256: INFO: stderr: ""
Mar  6 08:55:34.256: INFO: stdout: "update-demo-nautilus-kdt59 update-demo-nautilus-vb2f6 "
Mar  6 08:55:34.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-kdt59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:34.435: INFO: stderr: ""
Mar  6 08:55:34.435: INFO: stdout: ""
Mar  6 08:55:34.435: INFO: update-demo-nautilus-kdt59 is created but not running
Mar  6 08:55:39.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6589'
Mar  6 08:55:39.619: INFO: stderr: ""
Mar  6 08:55:39.619: INFO: stdout: "update-demo-nautilus-kdt59 update-demo-nautilus-vb2f6 "
Mar  6 08:55:39.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-kdt59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:39.836: INFO: stderr: ""
Mar  6 08:55:39.836: INFO: stdout: "true"
Mar  6 08:55:39.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-kdt59 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:40.060: INFO: stderr: ""
Mar  6 08:55:40.060: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:55:40.060: INFO: validating pod update-demo-nautilus-kdt59
Mar  6 08:55:40.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:55:40.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:55:40.069: INFO: update-demo-nautilus-kdt59 is verified up and running
Mar  6 08:55:40.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:40.243: INFO: stderr: ""
Mar  6 08:55:40.243: INFO: stdout: "true"
Mar  6 08:55:40.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods update-demo-nautilus-vb2f6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6589'
Mar  6 08:55:40.445: INFO: stderr: ""
Mar  6 08:55:40.446: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  6 08:55:40.446: INFO: validating pod update-demo-nautilus-vb2f6
Mar  6 08:55:40.453: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  6 08:55:40.453: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  6 08:55:40.453: INFO: update-demo-nautilus-vb2f6 is verified up and running
STEP: using delete to clean up resources
Mar  6 08:55:40.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-6589'
Mar  6 08:55:40.652: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 08:55:40.652: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  6 08:55:40.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6589'
Mar  6 08:55:40.839: INFO: stderr: "No resources found.\n"
Mar  6 08:55:40.839: INFO: stdout: ""
Mar  6 08:55:40.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=update-demo --namespace=kubectl-6589 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:55:41.022: INFO: stderr: ""
Mar  6 08:55:41.022: INFO: stdout: "update-demo-nautilus-kdt59\nupdate-demo-nautilus-vb2f6\n"
Mar  6 08:55:41.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6589'
Mar  6 08:55:41.709: INFO: stderr: "No resources found.\n"
Mar  6 08:55:41.709: INFO: stdout: ""
Mar  6 08:55:41.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=update-demo --namespace=kubectl-6589 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:55:41.898: INFO: stderr: ""
Mar  6 08:55:41.898: INFO: stdout: "update-demo-nautilus-kdt59\nupdate-demo-nautilus-vb2f6\n"
Mar  6 08:55:42.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6589'
Mar  6 08:55:42.219: INFO: stderr: "No resources found.\n"
Mar  6 08:55:42.219: INFO: stdout: ""
Mar  6 08:55:42.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=update-demo --namespace=kubectl-6589 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:55:42.422: INFO: stderr: ""
Mar  6 08:55:42.422: INFO: stdout: "update-demo-nautilus-kdt59\nupdate-demo-nautilus-vb2f6\n"
Mar  6 08:55:42.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6589'
Mar  6 08:55:42.853: INFO: stderr: "No resources found.\n"
Mar  6 08:55:42.853: INFO: stdout: ""
Mar  6 08:55:42.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=update-demo --namespace=kubectl-6589 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 08:55:43.047: INFO: stderr: ""
Mar  6 08:55:43.047: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:55:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6589" for this suite.
Mar  6 08:55:51.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:55:51.281: INFO: namespace kubectl-6589 deletion completed in 8.222406509s

• [SLOW TEST:43.372 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:55:51.282: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-51e1f640-cce7-441a-a100-101b9fc7f608
STEP: Creating a pod to test consume secrets
Mar  6 08:55:51.657: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671" in namespace "projected-4274" to be "success or failure"
Mar  6 08:55:51.663: INFO: Pod "pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671": Phase="Pending", Reason="", readiness=false. Elapsed: 6.723334ms
Mar  6 08:55:53.671: INFO: Pod "pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014215886s
Mar  6 08:55:55.692: INFO: Pod "pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035010672s
STEP: Saw pod success
Mar  6 08:55:55.692: INFO: Pod "pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671" satisfied condition "success or failure"
Mar  6 08:55:55.697: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:55:55.797: INFO: Waiting for pod pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671 to disappear
Mar  6 08:55:55.802: INFO: Pod pod-projected-secrets-4aaf80ff-4058-4ec1-8bba-c4abc52aa671 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:55:55.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4274" for this suite.
Mar  6 08:56:01.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:56:01.994: INFO: namespace projected-4274 deletion completed in 6.183391258s

• [SLOW TEST:10.712 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:56:01.995: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-a769ed35-9763-41ab-a9ec-4af2f910b007
STEP: Creating a pod to test consume secrets
Mar  6 08:56:02.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17" in namespace "projected-6707" to be "success or failure"
Mar  6 08:56:02.295: INFO: Pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408797ms
Mar  6 08:56:04.302: INFO: Pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012745856s
Mar  6 08:56:06.309: INFO: Pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019479791s
Mar  6 08:56:08.319: INFO: Pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029549279s
STEP: Saw pod success
Mar  6 08:56:08.319: INFO: Pod "pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17" satisfied condition "success or failure"
Mar  6 08:56:08.325: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 08:56:08.395: INFO: Waiting for pod pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17 to disappear
Mar  6 08:56:08.401: INFO: Pod pod-projected-secrets-ddee30a4-09d1-409a-9fee-7836d7fc5c17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:56:08.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6707" for this suite.
Mar  6 08:56:14.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:56:14.637: INFO: namespace projected-6707 deletion completed in 6.227251425s

• [SLOW TEST:12.642 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:56:14.637: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  6 08:56:14.953: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 08:56:14.968: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 08:56:14.980: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker01 before test
Mar  6 08:56:15.054: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-4sk55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 08:56:15.054: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 08:56:15.054: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  6 08:56:15.054: INFO: kube-proxy-7b546 from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 08:56:15.054: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 08:56:15.054: INFO: kube-flannel-ds-amd64-dp8qs from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 08:56:15.054: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 08:56:15.055: INFO: sonobuoy-e2e-job-2be1ed9b02254e55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 08:56:15.055: INFO: 	Container e2e ready: true, restart count 0
Mar  6 08:56:15.055: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 08:56:15.055: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker02 before test
Mar  6 08:56:15.078: INFO: kube-flannel-ds-amd64-f5d4f from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 08:56:15.078: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 08:56:15.078: INFO: kube-proxy-hb4sp from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 08:56:15.078: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 08:56:15.078: INFO: sonobuoy from sonobuoy started at 2020-03-06 08:20:17 +0000 UTC (1 container statuses recorded)
Mar  6 08:56:15.078: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 08:56:15.078: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-p6r2j from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 08:56:15.078: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 08:56:15.078: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node wisecloud-worker01
STEP: verifying the node has the label node wisecloud-worker02
Mar  6 08:56:15.257: INFO: Pod kube-flannel-ds-amd64-dp8qs requesting resource cpu=100m on Node wisecloud-worker01
Mar  6 08:56:15.257: INFO: Pod kube-flannel-ds-amd64-f5d4f requesting resource cpu=100m on Node wisecloud-worker02
Mar  6 08:56:15.257: INFO: Pod kube-proxy-7b546 requesting resource cpu=0m on Node wisecloud-worker01
Mar  6 08:56:15.257: INFO: Pod kube-proxy-hb4sp requesting resource cpu=0m on Node wisecloud-worker02
Mar  6 08:56:15.257: INFO: Pod sonobuoy requesting resource cpu=0m on Node wisecloud-worker02
Mar  6 08:56:15.257: INFO: Pod sonobuoy-e2e-job-2be1ed9b02254e55 requesting resource cpu=0m on Node wisecloud-worker01
Mar  6 08:56:15.257: INFO: Pod sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-4sk55 requesting resource cpu=0m on Node wisecloud-worker01
Mar  6 08:56:15.257: INFO: Pod sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-p6r2j requesting resource cpu=0m on Node wisecloud-worker02
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d.15f9ab3208a9159c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4441/filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d to wisecloud-worker02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d.15f9ab32784f9e51], Reason = [Pulled], Message = [Container image "172.20.8.7/library/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d.15f9ab3288010496], Reason = [Created], Message = [Created container filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d.15f9ab32a5f199ef], Reason = [Started], Message = [Started container filler-pod-2b68704c-4ffc-4fe6-bda0-60145287bf4d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f52f667-213e-4e98-8b40-166a358e0371.15f9ab3205383175], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4441/filler-pod-8f52f667-213e-4e98-8b40-166a358e0371 to wisecloud-worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f52f667-213e-4e98-8b40-166a358e0371.15f9ab3258d512e9], Reason = [Pulled], Message = [Container image "172.20.8.7/library/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f52f667-213e-4e98-8b40-166a358e0371.15f9ab3260c06293], Reason = [Created], Message = [Created container filler-pod-8f52f667-213e-4e98-8b40-166a358e0371]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f52f667-213e-4e98-8b40-166a358e0371.15f9ab327509da54], Reason = [Started], Message = [Started container filler-pod-8f52f667-213e-4e98-8b40-166a358e0371]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f9ab32f8ad51c7], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node wisecloud-worker01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node wisecloud-worker02
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:56:20.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4441" for this suite.
Mar  6 08:56:28.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:56:28.841: INFO: namespace sched-pred-4441 deletion completed in 8.282765016s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:14.204 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:56:28.841: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:56:29.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8212" for this suite.
Mar  6 08:56:51.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:56:51.504: INFO: namespace pods-8212 deletion completed in 22.260038007s

• [SLOW TEST:22.662 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:56:51.505: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  6 08:56:55.939: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:56:56.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2656" for this suite.
Mar  6 08:57:02.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:57:02.294: INFO: namespace container-runtime-2656 deletion completed in 6.276291308s

• [SLOW TEST:10.790 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:57:02.295: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:57:02.654: INFO: Create a RollingUpdate DaemonSet
Mar  6 08:57:02.682: INFO: Check that daemon pods launch on every node of the cluster
Mar  6 08:57:02.709: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:02.709: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:02.709: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:02.714: INFO: Number of nodes with available pods: 0
Mar  6 08:57:02.714: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:57:03.724: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:03.724: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:03.725: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:03.731: INFO: Number of nodes with available pods: 0
Mar  6 08:57:03.731: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:57:04.725: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:04.725: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:04.725: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:04.731: INFO: Number of nodes with available pods: 0
Mar  6 08:57:04.731: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 08:57:05.738: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:05.738: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:05.738: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:05.762: INFO: Number of nodes with available pods: 2
Mar  6 08:57:05.762: INFO: Number of running nodes: 2, number of available pods: 2
Mar  6 08:57:05.763: INFO: Update the DaemonSet to trigger a rollout
Mar  6 08:57:05.784: INFO: Updating DaemonSet daemon-set
Mar  6 08:57:18.870: INFO: Roll back the DaemonSet before rollout is complete
Mar  6 08:57:18.897: INFO: Updating DaemonSet daemon-set
Mar  6 08:57:18.897: INFO: Make sure DaemonSet rollback is complete
Mar  6 08:57:18.904: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:18.904: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:18.942: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:18.943: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:18.943: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:19.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:19.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:19.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:19.959: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:19.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:20.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:20.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:20.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:20.958: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:20.958: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:21.949: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:21.949: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:21.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:21.958: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:21.958: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:22.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:22.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:22.959: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:22.959: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:22.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:23.949: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:23.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:23.959: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:23.959: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:23.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:24.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:24.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:24.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:24.958: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:24.958: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:25.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:25.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:25.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:25.958: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:25.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:26.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:26.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:26.958: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:26.958: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:26.958: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:27.950: INFO: Wrong image for pod: daemon-set-lnjl7. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  6 08:57:27.950: INFO: Pod daemon-set-lnjl7 is not available
Mar  6 08:57:27.959: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:27.959: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:27.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:28.950: INFO: Pod daemon-set-lxwgh is not available
Mar  6 08:57:28.959: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:28.959: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 08:57:28.959: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7646, will wait for the garbage collector to delete the pods
Mar  6 08:57:29.063: INFO: Deleting DaemonSet.extensions daemon-set took: 35.255989ms
Mar  6 08:57:29.463: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.375213ms
Mar  6 08:57:32.669: INFO: Number of nodes with available pods: 0
Mar  6 08:57:32.669: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 08:57:32.674: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7646/daemonsets","resourceVersion":"16972"},"items":null}

Mar  6 08:57:32.679: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7646/pods","resourceVersion":"16972"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:57:32.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7646" for this suite.
Mar  6 08:57:40.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:57:41.200: INFO: namespace daemonsets-7646 deletion completed in 8.495048593s

• [SLOW TEST:38.905 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:57:41.201: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 08:57:45.759: INFO: Waiting up to 5m0s for pod "client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c" in namespace "pods-7194" to be "success or failure"
Mar  6 08:57:45.766: INFO: Pod "client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.71028ms
Mar  6 08:57:47.773: INFO: Pod "client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014257902s
Mar  6 08:57:49.780: INFO: Pod "client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02147616s
STEP: Saw pod success
Mar  6 08:57:49.780: INFO: Pod "client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c" satisfied condition "success or failure"
Mar  6 08:57:49.786: INFO: Trying to get logs from node wisecloud-worker02 pod client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c container env3cont: <nil>
STEP: delete the pod
Mar  6 08:57:49.864: INFO: Waiting for pod client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c to disappear
Mar  6 08:57:49.870: INFO: Pod client-envvars-7d8589cf-d8fc-46b2-b5d9-0273c6a0067c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:57:49.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7194" for this suite.
Mar  6 08:58:43.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:58:44.085: INFO: namespace pods-7194 deletion completed in 54.202729368s

• [SLOW TEST:62.885 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:58:44.086: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-dcd39c81-4ed6-4d61-9e49-13d4c320fdd2
STEP: Creating secret with name secret-projected-all-test-volume-cac374a3-04e7-4a72-aeb3-f1b202f6e1cf
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  6 08:58:44.475: INFO: Waiting up to 5m0s for pod "projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe" in namespace "projected-8423" to be "success or failure"
Mar  6 08:58:44.480: INFO: Pod "projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.355127ms
Mar  6 08:58:46.488: INFO: Pod "projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012975269s
Mar  6 08:58:48.496: INFO: Pod "projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020803127s
STEP: Saw pod success
Mar  6 08:58:48.496: INFO: Pod "projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe" satisfied condition "success or failure"
Mar  6 08:58:48.501: INFO: Trying to get logs from node wisecloud-worker02 pod projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  6 08:58:48.606: INFO: Waiting for pod projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe to disappear
Mar  6 08:58:48.612: INFO: Pod projected-volume-6096b429-0876-43de-8f9b-b3eed13f9dfe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:58:48.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8423" for this suite.
Mar  6 08:58:54.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:58:55.001: INFO: namespace projected-8423 deletion completed in 6.370249281s

• [SLOW TEST:10.916 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:58:55.002: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:58:59.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9148" for this suite.
Mar  6 08:59:39.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:59:39.695: INFO: namespace kubelet-test-9148 deletion completed in 40.297788399s

• [SLOW TEST:44.693 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:59:39.696: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 08:59:40.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97" in namespace "projected-6111" to be "success or failure"
Mar  6 08:59:40.182: INFO: Pod "downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97": Phase="Pending", Reason="", readiness=false. Elapsed: 56.203117ms
Mar  6 08:59:42.188: INFO: Pod "downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062673232s
Mar  6 08:59:44.195: INFO: Pod "downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069962117s
STEP: Saw pod success
Mar  6 08:59:44.195: INFO: Pod "downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97" satisfied condition "success or failure"
Mar  6 08:59:44.201: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97 container client-container: <nil>
STEP: delete the pod
Mar  6 08:59:44.287: INFO: Waiting for pod downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97 to disappear
Mar  6 08:59:44.293: INFO: Pod downwardapi-volume-7ff17021-143c-419b-b982-f0f710ef5a97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:59:44.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6111" for this suite.
Mar  6 08:59:52.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 08:59:52.504: INFO: namespace projected-6111 deletion completed in 8.203083845s

• [SLOW TEST:12.808 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 08:59:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  6 08:59:52.832: INFO: Waiting up to 5m0s for pod "downward-api-d04f8683-771c-4f44-b23d-bc906fecf527" in namespace "downward-api-9830" to be "success or failure"
Mar  6 08:59:52.838: INFO: Pod "downward-api-d04f8683-771c-4f44-b23d-bc906fecf527": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990477ms
Mar  6 08:59:54.845: INFO: Pod "downward-api-d04f8683-771c-4f44-b23d-bc906fecf527": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012932762s
Mar  6 08:59:56.852: INFO: Pod "downward-api-d04f8683-771c-4f44-b23d-bc906fecf527": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02007931s
STEP: Saw pod success
Mar  6 08:59:56.852: INFO: Pod "downward-api-d04f8683-771c-4f44-b23d-bc906fecf527" satisfied condition "success or failure"
Mar  6 08:59:56.858: INFO: Trying to get logs from node wisecloud-worker02 pod downward-api-d04f8683-771c-4f44-b23d-bc906fecf527 container dapi-container: <nil>
STEP: delete the pod
Mar  6 08:59:56.915: INFO: Waiting for pod downward-api-d04f8683-771c-4f44-b23d-bc906fecf527 to disappear
Mar  6 08:59:56.920: INFO: Pod downward-api-d04f8683-771c-4f44-b23d-bc906fecf527 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 08:59:56.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9830" for this suite.
Mar  6 09:00:02.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:00:03.117: INFO: namespace downward-api-9830 deletion completed in 6.189195339s

• [SLOW TEST:10.612 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:00:03.118: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  6 09:00:03.526: INFO: Waiting up to 5m0s for pod "pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6" in namespace "emptydir-5292" to be "success or failure"
Mar  6 09:00:03.532: INFO: Pod "pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854497ms
Mar  6 09:00:05.558: INFO: Pod "pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0314335s
Mar  6 09:00:07.583: INFO: Pod "pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056197779s
STEP: Saw pod success
Mar  6 09:00:07.583: INFO: Pod "pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6" satisfied condition "success or failure"
Mar  6 09:00:07.588: INFO: Trying to get logs from node wisecloud-worker02 pod pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6 container test-container: <nil>
STEP: delete the pod
Mar  6 09:00:07.639: INFO: Waiting for pod pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6 to disappear
Mar  6 09:00:07.644: INFO: Pod pod-7fc35d89-95f8-499a-9fc1-567b8c452fe6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:00:07.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5292" for this suite.
Mar  6 09:00:13.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:00:13.877: INFO: namespace emptydir-5292 deletion completed in 6.224343218s

• [SLOW TEST:10.759 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:00:13.877: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  6 09:00:14.215: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 09:00:14.231: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 09:00:14.235: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker01 before test
Mar  6 09:00:14.245: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-4sk55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:00:14.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 09:00:14.245: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  6 09:00:14.245: INFO: kube-proxy-7b546 from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 09:00:14.245: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 09:00:14.245: INFO: kube-flannel-ds-amd64-dp8qs from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 09:00:14.246: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 09:00:14.246: INFO: sonobuoy-e2e-job-2be1ed9b02254e55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:00:14.246: INFO: 	Container e2e ready: true, restart count 0
Mar  6 09:00:14.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 09:00:14.246: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker02 before test
Mar  6 09:00:14.263: INFO: kube-proxy-hb4sp from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 09:00:14.263: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 09:00:14.263: INFO: sonobuoy from sonobuoy started at 2020-03-06 08:20:17 +0000 UTC (1 container statuses recorded)
Mar  6 09:00:14.263: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 09:00:14.263: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-p6r2j from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:00:14.263: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 09:00:14.263: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  6 09:00:14.263: INFO: kube-flannel-ds-amd64-f5d4f from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 09:00:14.263: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-326c93a2-7cd4-4430-974d-0aaa3a0a161a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-326c93a2-7cd4-4430-974d-0aaa3a0a161a off the node wisecloud-worker02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-326c93a2-7cd4-4430-974d-0aaa3a0a161a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:00:22.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4109" for this suite.
Mar  6 09:00:32.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:00:33.154: INFO: namespace sched-pred-4109 deletion completed in 10.397692749s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:19.277 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:00:33.155: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:00:33.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76" in namespace "projected-5479" to be "success or failure"
Mar  6 09:00:33.498: INFO: Pod "downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.926207ms
Mar  6 09:00:35.523: INFO: Pod "downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03144891s
Mar  6 09:00:37.530: INFO: Pod "downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038492195s
STEP: Saw pod success
Mar  6 09:00:37.531: INFO: Pod "downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76" satisfied condition "success or failure"
Mar  6 09:00:37.536: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76 container client-container: <nil>
STEP: delete the pod
Mar  6 09:00:37.640: INFO: Waiting for pod downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76 to disappear
Mar  6 09:00:37.646: INFO: Pod downwardapi-volume-35917e4a-af7f-4dc0-ab2d-d89d4eadab76 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:00:37.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5479" for this suite.
Mar  6 09:00:43.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:00:43.846: INFO: namespace projected-5479 deletion completed in 6.190380079s

• [SLOW TEST:10.691 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:00:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:00:44.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a" in namespace "projected-7659" to be "success or failure"
Mar  6 09:00:44.201: INFO: Pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.070831ms
Mar  6 09:00:46.208: INFO: Pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014622046s
Mar  6 09:00:48.216: INFO: Pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022064671s
Mar  6 09:00:50.235: INFO: Pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041687023s
STEP: Saw pod success
Mar  6 09:00:50.235: INFO: Pod "downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a" satisfied condition "success or failure"
Mar  6 09:00:50.264: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a container client-container: <nil>
STEP: delete the pod
Mar  6 09:00:50.327: INFO: Waiting for pod downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a to disappear
Mar  6 09:00:50.333: INFO: Pod downwardapi-volume-6c78e3ed-a45f-4c40-98fe-3420d4b4c47a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:00:50.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7659" for this suite.
Mar  6 09:00:56.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:00:56.631: INFO: namespace projected-7659 deletion completed in 6.268132925s

• [SLOW TEST:12.785 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:00:56.632: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fe89283f-3f24-4080-aafa-9fa3d4c1bd28
STEP: Creating a pod to test consume secrets
Mar  6 09:00:57.012: INFO: Waiting up to 5m0s for pod "pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c" in namespace "secrets-4014" to be "success or failure"
Mar  6 09:00:57.017: INFO: Pod "pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.447473ms
Mar  6 09:00:59.023: INFO: Pod "pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011587882s
Mar  6 09:01:01.030: INFO: Pod "pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018558043s
STEP: Saw pod success
Mar  6 09:01:01.031: INFO: Pod "pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c" satisfied condition "success or failure"
Mar  6 09:01:01.036: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:01:01.110: INFO: Waiting for pod pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c to disappear
Mar  6 09:01:01.116: INFO: Pod pod-secrets-553ab3a4-ffce-492f-a8d2-c6f1fc8ac78c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:01:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4014" for this suite.
Mar  6 09:01:07.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:01:07.336: INFO: namespace secrets-4014 deletion completed in 6.211446904s

• [SLOW TEST:10.704 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:01:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  6 09:01:17.999: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:18.005: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:20.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:20.053: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:22.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:22.012: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:24.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:24.013: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:26.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:26.012: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:28.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:28.013: INFO: Pod pod-with-poststart-http-hook still exists
Mar  6 09:01:30.005: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  6 09:01:30.025: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:01:30.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-107" for this suite.
Mar  6 09:01:54.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:01:54.247: INFO: namespace container-lifecycle-hook-107 deletion completed in 24.204147944s

• [SLOW TEST:46.910 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:01:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:01:54.932: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"59736f59-20af-4332-b80f-881fd9d587b2", Controller:(*bool)(0xc001972696), BlockOwnerDeletion:(*bool)(0xc001972697)}}
Mar  6 09:01:54.989: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"86993c9d-25fe-4f33-b5b6-bcf661cd47ef", Controller:(*bool)(0xc00287e3f6), BlockOwnerDeletion:(*bool)(0xc00287e3f7)}}
Mar  6 09:01:55.043: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0ace3ef4-56d9-499e-9831-46a6a6ca22f1", Controller:(*bool)(0xc001972856), BlockOwnerDeletion:(*bool)(0xc001972857)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:00.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6805" for this suite.
Mar  6 09:02:06.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:02:06.360: INFO: namespace gc-6805 deletion completed in 6.2076623s

• [SLOW TEST:12.111 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:02:06.360: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:06.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7147" for this suite.
Mar  6 09:02:12.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:02:12.861: INFO: namespace services-7147 deletion completed in 6.187792273s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.502 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:02:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 09:02:13.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-6710'
Mar  6 09:02:14.727: INFO: stderr: ""
Mar  6 09:02:14.727: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Mar  6 09:02:14.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete pods e2e-test-nginx-pod --namespace=kubectl-6710'
Mar  6 09:02:18.481: INFO: stderr: ""
Mar  6 09:02:18.481: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:18.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6710" for this suite.
Mar  6 09:02:24.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:02:24.686: INFO: namespace kubectl-6710 deletion completed in 6.196643299s

• [SLOW TEST:11.824 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:02:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  6 09:02:25.103: INFO: Waiting up to 5m0s for pod "pod-cfeba534-26b9-4752-bde9-af11921010a2" in namespace "emptydir-574" to be "success or failure"
Mar  6 09:02:25.108: INFO: Pod "pod-cfeba534-26b9-4752-bde9-af11921010a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019603ms
Mar  6 09:02:27.119: INFO: Pod "pod-cfeba534-26b9-4752-bde9-af11921010a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015958578s
Mar  6 09:02:29.126: INFO: Pod "pod-cfeba534-26b9-4752-bde9-af11921010a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02321802s
Mar  6 09:02:31.133: INFO: Pod "pod-cfeba534-26b9-4752-bde9-af11921010a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029991138s
STEP: Saw pod success
Mar  6 09:02:31.133: INFO: Pod "pod-cfeba534-26b9-4752-bde9-af11921010a2" satisfied condition "success or failure"
Mar  6 09:02:31.138: INFO: Trying to get logs from node wisecloud-worker02 pod pod-cfeba534-26b9-4752-bde9-af11921010a2 container test-container: <nil>
STEP: delete the pod
Mar  6 09:02:31.217: INFO: Waiting for pod pod-cfeba534-26b9-4752-bde9-af11921010a2 to disappear
Mar  6 09:02:31.223: INFO: Pod pod-cfeba534-26b9-4752-bde9-af11921010a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:31.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-574" for this suite.
Mar  6 09:02:37.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:02:37.515: INFO: namespace emptydir-574 deletion completed in 6.282935119s

• [SLOW TEST:12.829 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:02:37.515: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:02:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 version'
Mar  6 09:02:38.012: INFO: stderr: ""
Mar  6 09:02:38.012: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.5\", GitCommit:\"20c265fef0741dd71a66480e35bd69f18351daea\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:16:51Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.5\", GitCommit:\"20c265fef0741dd71a66480e35bd69f18351daea\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:07:57Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:38.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5798" for this suite.
Mar  6 09:02:44.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:02:44.244: INFO: namespace kubectl-5798 deletion completed in 6.19786609s

• [SLOW TEST:6.729 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:02:44.244: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  6 09:02:48.558: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-48b2600d-7e99-4b2b-b4a5-74f8566502d9,GenerateName:,Namespace:events-424,SelfLink:/api/v1/namespaces/events-424/pods/send-events-48b2600d-7e99-4b2b-b4a5-74f8566502d9,UID:d7e52613-32c6-483b-8616-88831c08aeb1,ResourceVersion:18058,Generation:0,CreationTimestamp:2020-03-06 09:02:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 518441302,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-g8scg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g8scg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p 172.20.8.7/library/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-g8scg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003192a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003192a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:02:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:02:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:02:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:02:44 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.138,StartTime:2020-03-06 09:02:44 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-03-06 09:02:47 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/serve-hostname:1.1 docker-pullable://172.20.8.7/library/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 docker://4addaeaf1092b575e4bbcac9ad9614be59134dca8a87c6e158c38b0f898cf15d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  6 09:02:50.566: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  6 09:02:52.573: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:02:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-424" for this suite.
Mar  6 09:03:32.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:03:32.921: INFO: namespace events-424 deletion completed in 40.271848214s

• [SLOW TEST:48.677 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:03:32.922: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-j8tr
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 09:03:33.399: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j8tr" in namespace "subpath-7881" to be "success or failure"
Mar  6 09:03:33.406: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714393ms
Mar  6 09:03:35.412: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012981785s
Mar  6 09:03:37.441: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 4.042369635s
Mar  6 09:03:39.448: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 6.048562736s
Mar  6 09:03:41.467: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 8.068204062s
Mar  6 09:03:43.474: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 10.07474652s
Mar  6 09:03:45.480: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 12.080800315s
Mar  6 09:03:47.486: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 14.087126766s
Mar  6 09:03:49.493: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 16.094094725s
Mar  6 09:03:51.596: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 18.197278169s
Mar  6 09:03:53.603: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 20.204299914s
Mar  6 09:03:55.612: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Running", Reason="", readiness=true. Elapsed: 22.213204909s
Mar  6 09:03:57.619: INFO: Pod "pod-subpath-test-downwardapi-j8tr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.219970454s
STEP: Saw pod success
Mar  6 09:03:57.619: INFO: Pod "pod-subpath-test-downwardapi-j8tr" satisfied condition "success or failure"
Mar  6 09:03:57.625: INFO: Trying to get logs from node wisecloud-worker02 pod pod-subpath-test-downwardapi-j8tr container test-container-subpath-downwardapi-j8tr: <nil>
STEP: delete the pod
Mar  6 09:03:57.683: INFO: Waiting for pod pod-subpath-test-downwardapi-j8tr to disappear
Mar  6 09:03:57.688: INFO: Pod pod-subpath-test-downwardapi-j8tr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j8tr
Mar  6 09:03:57.688: INFO: Deleting pod "pod-subpath-test-downwardapi-j8tr" in namespace "subpath-7881"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:03:57.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7881" for this suite.
Mar  6 09:04:03.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:04:03.926: INFO: namespace subpath-7881 deletion completed in 6.223527888s

• [SLOW TEST:31.004 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:04:03.926: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-fr8h
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 09:04:04.346: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fr8h" in namespace "subpath-6579" to be "success or failure"
Mar  6 09:04:04.381: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Pending", Reason="", readiness=false. Elapsed: 34.646879ms
Mar  6 09:04:06.387: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041186034s
Mar  6 09:04:08.394: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 4.048223019s
Mar  6 09:04:10.402: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 6.055813214s
Mar  6 09:04:12.410: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 8.063543636s
Mar  6 09:04:14.416: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.070446607s
Mar  6 09:04:16.423: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.077115905s
Mar  6 09:04:18.430: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.083904277s
Mar  6 09:04:20.444: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.097553876s
Mar  6 09:04:22.451: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.105076354s
Mar  6 09:04:24.477: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.130764307s
Mar  6 09:04:26.495: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Running", Reason="", readiness=true. Elapsed: 22.148646009s
Mar  6 09:04:28.501: INFO: Pod "pod-subpath-test-secret-fr8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.154810911s
STEP: Saw pod success
Mar  6 09:04:28.501: INFO: Pod "pod-subpath-test-secret-fr8h" satisfied condition "success or failure"
Mar  6 09:04:28.506: INFO: Trying to get logs from node wisecloud-worker02 pod pod-subpath-test-secret-fr8h container test-container-subpath-secret-fr8h: <nil>
STEP: delete the pod
Mar  6 09:04:28.813: INFO: Waiting for pod pod-subpath-test-secret-fr8h to disappear
Mar  6 09:04:28.819: INFO: Pod pod-subpath-test-secret-fr8h no longer exists
STEP: Deleting pod pod-subpath-test-secret-fr8h
Mar  6 09:04:28.819: INFO: Deleting pod "pod-subpath-test-secret-fr8h" in namespace "subpath-6579"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:04:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6579" for this suite.
Mar  6 09:04:34.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:04:35.200: INFO: namespace subpath-6579 deletion completed in 6.359060983s

• [SLOW TEST:31.274 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:04:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  6 09:04:35.521: INFO: Waiting up to 5m0s for pod "pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b" in namespace "emptydir-2919" to be "success or failure"
Mar  6 09:04:35.594: INFO: Pod "pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 72.904395ms
Mar  6 09:04:37.600: INFO: Pod "pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079294847s
Mar  6 09:04:39.607: INFO: Pod "pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085343588s
STEP: Saw pod success
Mar  6 09:04:39.607: INFO: Pod "pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b" satisfied condition "success or failure"
Mar  6 09:04:39.612: INFO: Trying to get logs from node wisecloud-worker02 pod pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b container test-container: <nil>
STEP: delete the pod
Mar  6 09:04:39.691: INFO: Waiting for pod pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b to disappear
Mar  6 09:04:39.696: INFO: Pod pod-9a2f8c66-7a3b-4237-bad9-225a051e9b3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:04:39.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2919" for this suite.
Mar  6 09:04:45.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:04:45.895: INFO: namespace emptydir-2919 deletion completed in 6.189136312s

• [SLOW TEST:10.695 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:04:45.896: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2024.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2024.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2024.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2024.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 09:04:50.523: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-2024/dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca: the server could not find the requested resource (get pods dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca)
Mar  6 09:04:50.528: INFO: Unable to read jessie_udp@PodARecord from pod dns-2024/dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca: the server could not find the requested resource (get pods dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca)
Mar  6 09:04:50.533: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2024/dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca: the server could not find the requested resource (get pods dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca)
Mar  6 09:04:50.533: INFO: Lookups using dns-2024/dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  6 09:04:55.636: INFO: DNS probes using dns-2024/dns-test-2d929fe7-d14d-4c3a-99ee-20e0f44203ca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:04:55.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2024" for this suite.
Mar  6 09:05:01.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:05:02.114: INFO: namespace dns-2024 deletion completed in 6.238907238s

• [SLOW TEST:16.218 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:05:02.114: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7967
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-58468ed2-30ca-4acd-a60d-69c186295275
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-58468ed2-30ca-4acd-a60d-69c186295275
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:05:08.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7967" for this suite.
Mar  6 09:05:30.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:05:30.681: INFO: namespace configmap-7967 deletion completed in 22.135776985s

• [SLOW TEST:28.567 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:05:30.682: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4f47d01b-33bb-47da-a814-410b33c37ccf
STEP: Creating a pod to test consume secrets
Mar  6 09:05:30.966: INFO: Waiting up to 5m0s for pod "pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b" in namespace "secrets-7484" to be "success or failure"
Mar  6 09:05:30.970: INFO: Pod "pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4071ms
Mar  6 09:05:32.975: INFO: Pod "pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0086477s
Mar  6 09:05:34.984: INFO: Pod "pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017531272s
STEP: Saw pod success
Mar  6 09:05:34.984: INFO: Pod "pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b" satisfied condition "success or failure"
Mar  6 09:05:34.989: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b container secret-env-test: <nil>
STEP: delete the pod
Mar  6 09:05:35.079: INFO: Waiting for pod pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b to disappear
Mar  6 09:05:35.088: INFO: Pod pod-secrets-cc325aca-83a4-41e5-81f4-88d936a6882b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:05:35.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7484" for this suite.
Mar  6 09:05:41.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:05:41.313: INFO: namespace secrets-7484 deletion completed in 6.218520847s

• [SLOW TEST:10.631 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:05:41.313: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-158cbc05-d7cb-4d31-81f8-3c968e850e82 in namespace container-probe-6635
Mar  6 09:05:45.659: INFO: Started pod busybox-158cbc05-d7cb-4d31-81f8-3c968e850e82 in namespace container-probe-6635
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 09:05:45.662: INFO: Initial restart count of pod busybox-158cbc05-d7cb-4d31-81f8-3c968e850e82 is 0
Mar  6 09:06:39.804: INFO: Restart count of pod container-probe-6635/busybox-158cbc05-d7cb-4d31-81f8-3c968e850e82 is now 1 (54.141312821s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:06:39.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6635" for this suite.
Mar  6 09:06:45.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:06:46.067: INFO: namespace container-probe-6635 deletion completed in 6.165052918s

• [SLOW TEST:64.753 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:06:46.067: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8947
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-d5f51e97-fbf1-40b9-82db-ede7757b2531
STEP: Creating secret with name s-test-opt-upd-5081a065-6bb7-4cc0-9e3a-d3e0e0fba39f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d5f51e97-fbf1-40b9-82db-ede7757b2531
STEP: Updating secret s-test-opt-upd-5081a065-6bb7-4cc0-9e3a-d3e0e0fba39f
STEP: Creating secret with name s-test-opt-create-739f0ca8-563c-4b7c-bd43-a246eb98044b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:06:52.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8947" for this suite.
Mar  6 09:07:16.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:07:16.966: INFO: namespace projected-8947 deletion completed in 24.179200182s

• [SLOW TEST:30.898 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:07:16.966: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:07:17.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63" in namespace "downward-api-5013" to be "success or failure"
Mar  6 09:07:17.457: INFO: Pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63": Phase="Pending", Reason="", readiness=false. Elapsed: 34.199424ms
Mar  6 09:07:19.461: INFO: Pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038773685s
Mar  6 09:07:21.467: INFO: Pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04445794s
Mar  6 09:07:23.472: INFO: Pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049468168s
STEP: Saw pod success
Mar  6 09:07:23.472: INFO: Pod "downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63" satisfied condition "success or failure"
Mar  6 09:07:23.475: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63 container client-container: <nil>
STEP: delete the pod
Mar  6 09:07:23.542: INFO: Waiting for pod downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63 to disappear
Mar  6 09:07:23.546: INFO: Pod downwardapi-volume-9bdf48f6-238d-40e3-a08c-8b3c93254a63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:07:23.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5013" for this suite.
Mar  6 09:07:29.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:07:29.733: INFO: namespace downward-api-5013 deletion completed in 6.181545335s

• [SLOW TEST:12.767 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:07:29.734: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-92509b9d-4758-4411-9b2f-ac23fe0ea628
STEP: Creating a pod to test consume configMaps
Mar  6 09:07:30.332: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22" in namespace "projected-8559" to be "success or failure"
Mar  6 09:07:30.354: INFO: Pod "pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22": Phase="Pending", Reason="", readiness=false. Elapsed: 21.552869ms
Mar  6 09:07:32.359: INFO: Pod "pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02642913s
Mar  6 09:07:34.379: INFO: Pod "pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046454281s
STEP: Saw pod success
Mar  6 09:07:34.379: INFO: Pod "pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22" satisfied condition "success or failure"
Mar  6 09:07:34.382: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:07:34.450: INFO: Waiting for pod pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22 to disappear
Mar  6 09:07:34.468: INFO: Pod pod-projected-configmaps-ea041ff7-599c-439f-aed5-8b16d8a66e22 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:07:34.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8559" for this suite.
Mar  6 09:07:40.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:07:40.731: INFO: namespace projected-8559 deletion completed in 6.256171666s

• [SLOW TEST:10.997 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:07:40.731: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  6 09:07:53.028: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:53.251: INFO: Exec stderr: ""
Mar  6 09:07:53.251: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:53.472: INFO: Exec stderr: ""
Mar  6 09:07:53.472: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:53.698: INFO: Exec stderr: ""
Mar  6 09:07:53.698: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:53.914: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  6 09:07:53.914: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:54.134: INFO: Exec stderr: ""
Mar  6 09:07:54.134: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:54.134: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:54.396: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  6 09:07:54.396: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:54.396: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:54.604: INFO: Exec stderr: ""
Mar  6 09:07:54.604: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:54.605: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:54.798: INFO: Exec stderr: ""
Mar  6 09:07:54.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:54.798: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:54.988: INFO: Exec stderr: ""
Mar  6 09:07:54.988: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4450 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:07:54.988: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:07:55.186: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:07:55.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4450" for this suite.
Mar  6 09:08:41.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:08:41.368: INFO: namespace e2e-kubelet-etc-hosts-4450 deletion completed in 46.168844535s

• [SLOW TEST:60.637 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:08:41.368: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7449
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 09:08:41.615: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 09:09:03.828: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7449 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:09:03.828: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:09:04.985: INFO: Found all expected endpoints: [netserver-0]
Mar  6 09:09:05.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.56 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7449 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:09:05.011: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:09:06.785: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:09:06.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7449" for this suite.
Mar  6 09:09:30.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:09:31.009: INFO: namespace pod-network-test-7449 deletion completed in 24.216640894s

• [SLOW TEST:49.641 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:09:31.009: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 09:09:31.579: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:31.579: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:31.579: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:31.589: INFO: Number of nodes with available pods: 0
Mar  6 09:09:31.589: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:09:32.597: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:32.597: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:32.597: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:32.601: INFO: Number of nodes with available pods: 0
Mar  6 09:09:32.601: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:09:33.620: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:33.620: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:33.620: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:33.624: INFO: Number of nodes with available pods: 0
Mar  6 09:09:33.624: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:09:34.596: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:34.596: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:34.596: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:34.602: INFO: Number of nodes with available pods: 1
Mar  6 09:09:34.602: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:35.596: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.596: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.596: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.600: INFO: Number of nodes with available pods: 2
Mar  6 09:09:35.600: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  6 09:09:35.655: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.655: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.655: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:35.659: INFO: Number of nodes with available pods: 1
Mar  6 09:09:35.659: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:36.666: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:36.666: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:36.666: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:36.669: INFO: Number of nodes with available pods: 1
Mar  6 09:09:36.670: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:37.667: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:37.667: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:37.667: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:37.671: INFO: Number of nodes with available pods: 1
Mar  6 09:09:37.671: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:38.667: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:38.667: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:38.667: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:38.671: INFO: Number of nodes with available pods: 1
Mar  6 09:09:38.671: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:39.667: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:39.667: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:39.667: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:39.672: INFO: Number of nodes with available pods: 1
Mar  6 09:09:39.672: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:40.668: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:40.668: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:40.668: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:40.673: INFO: Number of nodes with available pods: 1
Mar  6 09:09:40.673: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:41.666: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:41.667: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:41.667: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:41.670: INFO: Number of nodes with available pods: 1
Mar  6 09:09:41.670: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:09:42.667: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:42.667: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:42.667: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:09:42.672: INFO: Number of nodes with available pods: 2
Mar  6 09:09:42.672: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3833, will wait for the garbage collector to delete the pods
Mar  6 09:09:42.755: INFO: Deleting DaemonSet.extensions daemon-set took: 25.559305ms
Mar  6 09:09:43.156: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.364873ms
Mar  6 09:09:47.260: INFO: Number of nodes with available pods: 0
Mar  6 09:09:47.260: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 09:09:47.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3833/daemonsets","resourceVersion":"19368"},"items":null}

Mar  6 09:09:47.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3833/pods","resourceVersion":"19368"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:09:47.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3833" for this suite.
Mar  6 09:09:53.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:09:53.527: INFO: namespace daemonsets-3833 deletion completed in 6.243882506s

• [SLOW TEST:22.518 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:09:53.528: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:09:53.847: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071" in namespace "projected-6838" to be "success or failure"
Mar  6 09:09:53.894: INFO: Pod "downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071": Phase="Pending", Reason="", readiness=false. Elapsed: 46.987011ms
Mar  6 09:09:55.899: INFO: Pod "downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052064272s
Mar  6 09:09:57.906: INFO: Pod "downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058790087s
STEP: Saw pod success
Mar  6 09:09:57.906: INFO: Pod "downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071" satisfied condition "success or failure"
Mar  6 09:09:57.910: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071 container client-container: <nil>
STEP: delete the pod
Mar  6 09:09:58.013: INFO: Waiting for pod downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071 to disappear
Mar  6 09:09:58.039: INFO: Pod downwardapi-volume-2154f3fb-b7f9-4c8c-816e-cb64caca1071 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:09:58.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6838" for this suite.
Mar  6 09:10:06.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:10:06.187: INFO: namespace projected-6838 deletion completed in 8.139926427s

• [SLOW TEST:12.659 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:10:06.188: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2223
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9b8b7e85-1c7c-4f0a-b558-eab8ed52925b
STEP: Creating secret with name s-test-opt-upd-15ba6eeb-9eef-486e-a0c3-02a3e30277b2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9b8b7e85-1c7c-4f0a-b558-eab8ed52925b
STEP: Updating secret s-test-opt-upd-15ba6eeb-9eef-486e-a0c3-02a3e30277b2
STEP: Creating secret with name s-test-opt-create-3e594e79-99e1-40ce-9fb3-d9389351962f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:10:12.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2223" for this suite.
Mar  6 09:10:32.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:10:33.500: INFO: namespace secrets-2223 deletion completed in 20.52213327s

• [SLOW TEST:27.313 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:10:33.501: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:10:33.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73" in namespace "downward-api-7530" to be "success or failure"
Mar  6 09:10:33.863: INFO: Pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73": Phase="Pending", Reason="", readiness=false. Elapsed: 36.202369ms
Mar  6 09:10:35.867: INFO: Pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04056741s
Mar  6 09:10:37.874: INFO: Pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047730459s
Mar  6 09:10:39.883: INFO: Pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0563768s
STEP: Saw pod success
Mar  6 09:10:39.883: INFO: Pod "downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73" satisfied condition "success or failure"
Mar  6 09:10:39.888: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73 container client-container: <nil>
STEP: delete the pod
Mar  6 09:10:39.941: INFO: Waiting for pod downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73 to disappear
Mar  6 09:10:39.944: INFO: Pod downwardapi-volume-40190083-d9c5-497a-b043-0861c3576e73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:10:39.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7530" for this suite.
Mar  6 09:10:46.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:10:46.186: INFO: namespace downward-api-7530 deletion completed in 6.236793439s

• [SLOW TEST:12.685 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:10:46.187: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7031
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  6 09:10:46.515: INFO: Waiting up to 5m0s for pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7" in namespace "emptydir-7031" to be "success or failure"
Mar  6 09:10:46.519: INFO: Pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53676ms
Mar  6 09:10:48.523: INFO: Pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007612695s
Mar  6 09:10:50.527: INFO: Pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01237217s
Mar  6 09:10:52.533: INFO: Pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01765196s
STEP: Saw pod success
Mar  6 09:10:52.533: INFO: Pod "pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7" satisfied condition "success or failure"
Mar  6 09:10:52.537: INFO: Trying to get logs from node wisecloud-worker02 pod pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7 container test-container: <nil>
STEP: delete the pod
Mar  6 09:10:52.600: INFO: Waiting for pod pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7 to disappear
Mar  6 09:10:52.606: INFO: Pod pod-a27b4490-c7ba-41e4-bf19-5e6770a1cbe7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:10:52.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7031" for this suite.
Mar  6 09:10:58.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:10:59.336: INFO: namespace emptydir-7031 deletion completed in 6.72267816s

• [SLOW TEST:13.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:10:59.337: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5166
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 09:10:59.584: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 09:11:19.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.156:8080/dial?request=hostName&protocol=http&host=10.244.3.59&port=8080&tries=1'] Namespace:pod-network-test-5166 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:11:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:11:20.055: INFO: Waiting for endpoints: map[]
Mar  6 09:11:20.084: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.156:8080/dial?request=hostName&protocol=http&host=10.244.4.155&port=8080&tries=1'] Namespace:pod-network-test-5166 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:11:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:11:20.263: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:11:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5166" for this suite.
Mar  6 09:11:44.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:11:44.539: INFO: namespace pod-network-test-5166 deletion completed in 24.26883721s

• [SLOW TEST:45.202 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:11:44.540: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Mar  6 09:11:44.848: INFO: Waiting up to 5m0s for pod "client-containers-14575bad-56a1-4658-875b-dd166fafac74" in namespace "containers-8993" to be "success or failure"
Mar  6 09:11:44.881: INFO: Pod "client-containers-14575bad-56a1-4658-875b-dd166fafac74": Phase="Pending", Reason="", readiness=false. Elapsed: 32.808902ms
Mar  6 09:11:46.886: INFO: Pod "client-containers-14575bad-56a1-4658-875b-dd166fafac74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037856843s
Mar  6 09:11:48.891: INFO: Pod "client-containers-14575bad-56a1-4658-875b-dd166fafac74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042470577s
STEP: Saw pod success
Mar  6 09:11:48.891: INFO: Pod "client-containers-14575bad-56a1-4658-875b-dd166fafac74" satisfied condition "success or failure"
Mar  6 09:11:48.894: INFO: Trying to get logs from node wisecloud-worker02 pod client-containers-14575bad-56a1-4658-875b-dd166fafac74 container test-container: <nil>
STEP: delete the pod
Mar  6 09:11:48.964: INFO: Waiting for pod client-containers-14575bad-56a1-4658-875b-dd166fafac74 to disappear
Mar  6 09:11:48.990: INFO: Pod client-containers-14575bad-56a1-4658-875b-dd166fafac74 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:11:48.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8993" for this suite.
Mar  6 09:11:57.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:11:57.126: INFO: namespace containers-8993 deletion completed in 8.129982975s

• [SLOW TEST:12.586 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:11:57.126: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b81f7606-4daf-449f-b8bf-2c6f87fca2de
STEP: Creating a pod to test consume configMaps
Mar  6 09:11:57.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685" in namespace "projected-645" to be "success or failure"
Mar  6 09:11:57.507: INFO: Pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536484ms
Mar  6 09:11:59.512: INFO: Pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008854052s
Mar  6 09:12:01.517: INFO: Pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014585668s
Mar  6 09:12:03.523: INFO: Pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020329236s
STEP: Saw pod success
Mar  6 09:12:03.523: INFO: Pod "pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685" satisfied condition "success or failure"
Mar  6 09:12:03.527: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:12:03.620: INFO: Waiting for pod pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685 to disappear
Mar  6 09:12:03.624: INFO: Pod pod-projected-configmaps-cab02f67-0d02-43f7-8d6e-49ca6cb08685 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:12:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-645" for this suite.
Mar  6 09:12:09.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:12:09.779: INFO: namespace projected-645 deletion completed in 6.147705029s

• [SLOW TEST:12.652 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:12:09.779: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:12:10.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a" in namespace "downward-api-7503" to be "success or failure"
Mar  6 09:12:10.121: INFO: Pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188763ms
Mar  6 09:12:12.125: INFO: Pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008197481s
Mar  6 09:12:14.130: INFO: Pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01390458s
Mar  6 09:12:16.136: INFO: Pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019156212s
STEP: Saw pod success
Mar  6 09:12:16.136: INFO: Pod "downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a" satisfied condition "success or failure"
Mar  6 09:12:16.140: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a container client-container: <nil>
STEP: delete the pod
Mar  6 09:12:16.196: INFO: Waiting for pod downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a to disappear
Mar  6 09:12:16.200: INFO: Pod downwardapi-volume-7fb4cfc6-3b11-47fa-93be-5f673a3ca87a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:12:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7503" for this suite.
Mar  6 09:12:22.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:12:22.406: INFO: namespace downward-api-7503 deletion completed in 6.201395158s

• [SLOW TEST:12.627 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:12:22.407: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:12:44.721: INFO: Container started at 2020-03-06 09:12:26 +0000 UTC, pod became ready at 2020-03-06 09:12:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:12:44.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-129" for this suite.
Mar  6 09:13:08.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:13:08.875: INFO: namespace container-probe-129 deletion completed in 24.148912536s

• [SLOW TEST:46.469 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:13:08.876: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:13:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4333" for this suite.
Mar  6 09:13:36.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:13:36.577: INFO: namespace replication-controller-4333 deletion completed in 22.338732404s

• [SLOW TEST:27.702 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:13:36.578: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-827e37d7-84ad-4588-bce2-33eedd8e1702
STEP: Creating a pod to test consume configMaps
Mar  6 09:13:37.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379" in namespace "projected-5612" to be "success or failure"
Mar  6 09:13:37.156: INFO: Pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.646638ms
Mar  6 09:13:39.161: INFO: Pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010023987s
Mar  6 09:13:41.166: INFO: Pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015573682s
Mar  6 09:13:43.171: INFO: Pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020532725s
STEP: Saw pod success
Mar  6 09:13:43.171: INFO: Pod "pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379" satisfied condition "success or failure"
Mar  6 09:13:43.175: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:13:43.233: INFO: Waiting for pod pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379 to disappear
Mar  6 09:13:43.236: INFO: Pod pod-projected-configmaps-caaf5bf4-4e7d-4ff8-a9f0-0594ed024379 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:13:43.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5612" for this suite.
Mar  6 09:13:49.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:13:49.423: INFO: namespace projected-5612 deletion completed in 6.181255142s

• [SLOW TEST:12.846 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:13:49.424: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-3d160f6a-3774-4126-a064-759a4b3e1a39
STEP: Creating a pod to test consume configMaps
Mar  6 09:13:49.722: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101" in namespace "projected-2370" to be "success or failure"
Mar  6 09:13:49.737: INFO: Pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101": Phase="Pending", Reason="", readiness=false. Elapsed: 14.144011ms
Mar  6 09:13:51.753: INFO: Pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030149274s
Mar  6 09:13:53.757: INFO: Pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034254356s
Mar  6 09:13:55.762: INFO: Pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03993791s
STEP: Saw pod success
Mar  6 09:13:55.763: INFO: Pod "pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101" satisfied condition "success or failure"
Mar  6 09:13:55.766: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:13:55.839: INFO: Waiting for pod pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101 to disappear
Mar  6 09:13:55.844: INFO: Pod pod-projected-configmaps-d5af7434-a772-4876-9898-faf21a819101 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:13:55.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2370" for this suite.
Mar  6 09:14:03.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:14:04.025: INFO: namespace projected-2370 deletion completed in 8.174481403s

• [SLOW TEST:14.601 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:14:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:14:04.330: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  6 09:14:09.335: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 09:14:09.335: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  6 09:14:11.340: INFO: Creating deployment "test-rollover-deployment"
Mar  6 09:14:11.387: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  6 09:14:13.430: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  6 09:14:13.438: INFO: Ensure that both replica sets have 1 created replica
Mar  6 09:14:13.446: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  6 09:14:13.469: INFO: Updating deployment test-rollover-deployment
Mar  6 09:14:13.469: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  6 09:14:15.478: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  6 09:14:15.489: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  6 09:14:15.500: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:15.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082853, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:17.508: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:17.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082856, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:19.510: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:19.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082856, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:21.509: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:21.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082856, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:23.675: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:23.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082856, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:25.509: INFO: all replica sets need to contain the pod-template-hash label
Mar  6 09:14:25.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082856, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719082851, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:14:27.510: INFO: 
Mar  6 09:14:27.510: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  6 09:14:27.522: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-2596,SelfLink:/apis/apps/v1/namespaces/deployment-2596/deployments/test-rollover-deployment,UID:94ae2452-d755-4066-ba74-762c7b86a83f,ResourceVersion:20363,Generation:2,CreationTimestamp:2020-03-06 09:14:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-06 09:14:11 +0000 UTC 2020-03-06 09:14:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-06 09:14:26 +0000 UTC 2020-03-06 09:14:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-65ff5b8f77" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 09:14:27.529: INFO: New ReplicaSet "test-rollover-deployment-65ff5b8f77" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-65ff5b8f77,GenerateName:,Namespace:deployment-2596,SelfLink:/apis/apps/v1/namespaces/deployment-2596/replicasets/test-rollover-deployment-65ff5b8f77,UID:0c7535ad-8ffd-4c72-86af-0204dc9d442e,ResourceVersion:20352,Generation:2,CreationTimestamp:2020-03-06 09:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94ae2452-d755-4066-ba74-762c7b86a83f 0xc00384e847 0xc00384e848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 09:14:27.529: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  6 09:14:27.529: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-2596,SelfLink:/apis/apps/v1/namespaces/deployment-2596/replicasets/test-rollover-controller,UID:2de04ae8-edea-4191-ad25-49cf45cf5236,ResourceVersion:20362,Generation:2,CreationTimestamp:2020-03-06 09:14:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94ae2452-d755-4066-ba74-762c7b86a83f 0xc00384e777 0xc00384e778}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 09:14:27.530: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-2596,SelfLink:/apis/apps/v1/namespaces/deployment-2596/replicasets/test-rollover-deployment-9b8b997cf,UID:4a14adce-060e-418f-96e5-11d01af551d1,ResourceVersion:20322,Generation:2,CreationTimestamp:2020-03-06 09:14:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94ae2452-d755-4066-ba74-762c7b86a83f 0xc00384e910 0xc00384e911}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 09:14:27.536: INFO: Pod "test-rollover-deployment-65ff5b8f77-rl5mk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-65ff5b8f77-rl5mk,GenerateName:test-rollover-deployment-65ff5b8f77-,Namespace:deployment-2596,SelfLink:/api/v1/namespaces/deployment-2596/pods/test-rollover-deployment-65ff5b8f77-rl5mk,UID:551aa106-78f0-48ff-a5f2-c6e6ad712bc3,ResourceVersion:20330,Generation:0,CreationTimestamp:2020-03-06 09:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-65ff5b8f77 0c7535ad-8ffd-4c72-86af-0204dc9d442e 0xc00384f4f7 0xc00384f4f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-w44rv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w44rv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-w44rv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00384f560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00384f580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:14:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:14:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:14:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:14:13 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.60,StartTime:2020-03-06 09:14:13 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-06 09:14:15 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/redis:1.0 docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://493405f6f5cd3544eabe925e49cfc61fc9cf9e0d25f26831346f5720ed6a1843}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:14:27.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2596" for this suite.
Mar  6 09:14:35.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:14:35.800: INFO: namespace deployment-2596 deletion completed in 8.256027339s

• [SLOW TEST:31.775 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:14:35.800: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:14:42.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6932" for this suite.
Mar  6 09:14:48.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:14:48.682: INFO: namespace emptydir-wrapper-6932 deletion completed in 6.284743845s

• [SLOW TEST:12.882 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:14:48.683: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:14:48.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82" in namespace "downward-api-9997" to be "success or failure"
Mar  6 09:14:48.988: INFO: Pod "downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609514ms
Mar  6 09:14:50.994: INFO: Pod "downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010429451s
Mar  6 09:14:53.000: INFO: Pod "downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016051263s
STEP: Saw pod success
Mar  6 09:14:53.000: INFO: Pod "downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82" satisfied condition "success or failure"
Mar  6 09:14:53.006: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82 container client-container: <nil>
STEP: delete the pod
Mar  6 09:14:53.077: INFO: Waiting for pod downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82 to disappear
Mar  6 09:14:53.081: INFO: Pod downwardapi-volume-54275e08-83da-4253-a76a-97a5ce578e82 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:14:53.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9997" for this suite.
Mar  6 09:14:59.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:14:59.242: INFO: namespace downward-api-9997 deletion completed in 6.153782628s

• [SLOW TEST:10.559 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:14:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 09:14:59.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-deployment --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-2919'
Mar  6 09:15:01.185: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 09:15:01.185: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Mar  6 09:15:01.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2919'
Mar  6 09:15:01.409: INFO: stderr: ""
Mar  6 09:15:01.409: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:15:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2919" for this suite.
Mar  6 09:15:09.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:15:09.563: INFO: namespace kubectl-2919 deletion completed in 8.148107953s

• [SLOW TEST:10.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:15:09.564: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  6 09:15:09.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-5717'
Mar  6 09:15:10.321: INFO: stderr: ""
Mar  6 09:15:10.321: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 09:15:11.328: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:11.328: INFO: Found 0 / 1
Mar  6 09:15:12.326: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:12.326: INFO: Found 0 / 1
Mar  6 09:15:13.348: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:13.348: INFO: Found 0 / 1
Mar  6 09:15:14.327: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:14.327: INFO: Found 1 / 1
Mar  6 09:15:14.327: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  6 09:15:14.331: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:14.331: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 09:15:14.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 patch pod redis-master-qdv46 --namespace=kubectl-5717 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  6 09:15:14.537: INFO: stderr: ""
Mar  6 09:15:14.538: INFO: stdout: "pod/redis-master-qdv46 patched\n"
STEP: checking annotations
Mar  6 09:15:14.542: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:15:14.542: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:15:14.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5717" for this suite.
Mar  6 09:15:34.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:15:34.756: INFO: namespace kubectl-5717 deletion completed in 20.207874061s

• [SLOW TEST:25.192 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:15:34.756: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3054
STEP: Creating secret with name secret-test-8691e5d7-5c47-482c-b0d5-cbd657bde173
STEP: Creating a pod to test consume secrets
Mar  6 09:15:35.379: INFO: Waiting up to 5m0s for pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5" in namespace "secrets-2801" to be "success or failure"
Mar  6 09:15:35.382: INFO: Pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.484794ms
Mar  6 09:15:37.387: INFO: Pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008362287s
Mar  6 09:15:39.392: INFO: Pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012982382s
Mar  6 09:15:41.396: INFO: Pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017841398s
STEP: Saw pod success
Mar  6 09:15:41.397: INFO: Pod "pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5" satisfied condition "success or failure"
Mar  6 09:15:41.400: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:15:41.450: INFO: Waiting for pod pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5 to disappear
Mar  6 09:15:41.454: INFO: Pod pod-secrets-579335ea-684b-4207-bc64-acdacc3849e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:15:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2801" for this suite.
Mar  6 09:15:47.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:15:48.787: INFO: namespace secrets-2801 deletion completed in 7.326235084s
STEP: Destroying namespace "secret-namespace-3054" for this suite.
Mar  6 09:15:54.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:15:54.925: INFO: namespace secret-namespace-3054 deletion completed in 6.137419364s

• [SLOW TEST:20.168 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:15:54.925: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  6 09:15:55.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2532,SelfLink:/api/v1/namespaces/watch-2532/configmaps/e2e-watch-test-resource-version,UID:53b4ad9b-efd7-4a8f-b4c2-c10e3c6ad287,ResourceVersion:20729,Generation:0,CreationTimestamp:2020-03-06 09:15:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  6 09:15:55.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2532,SelfLink:/api/v1/namespaces/watch-2532/configmaps/e2e-watch-test-resource-version,UID:53b4ad9b-efd7-4a8f-b4c2-c10e3c6ad287,ResourceVersion:20730,Generation:0,CreationTimestamp:2020-03-06 09:15:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:15:55.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2532" for this suite.
Mar  6 09:16:01.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:16:01.884: INFO: namespace watch-2532 deletion completed in 6.356790497s

• [SLOW TEST:6.960 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:16:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 09:16:06.270: INFO: DNS probes using dns-test-c42893eb-e4f1-4ee2-b7cb-beade52a14ff succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 09:16:12.547: INFO: File wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:12.552: INFO: File jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:12.552: INFO: Lookups using dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 failed for: [wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local]

Mar  6 09:16:17.558: INFO: File wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:17.563: INFO: File jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:17.563: INFO: Lookups using dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 failed for: [wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local]

Mar  6 09:16:22.558: INFO: File wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:22.563: INFO: File jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:22.563: INFO: Lookups using dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 failed for: [wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local]

Mar  6 09:16:27.580: INFO: File wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:27.585: INFO: File jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:27.585: INFO: Lookups using dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 failed for: [wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local]

Mar  6 09:16:32.558: INFO: File wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:32.563: INFO: File jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local from pod  dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  6 09:16:32.563: INFO: Lookups using dns-6814/dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 failed for: [wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local]

Mar  6 09:16:37.565: INFO: DNS probes using dns-test-8110c0a1-58e2-4a19-b7ea-053240958fc6 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6814.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6814.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  6 09:16:42.092: INFO: DNS probes using dns-test-2034d93a-1f0b-44ae-8f1a-11b27913a9e1 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:16:42.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6814" for this suite.
Mar  6 09:16:50.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:16:51.056: INFO: namespace dns-6814 deletion completed in 8.145919178s

• [SLOW TEST:49.171 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:16:51.057: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 09:16:51.369: INFO: Waiting up to 5m0s for pod "pod-f25dff8e-3008-47c4-b38c-df63b82fef02" in namespace "emptydir-7383" to be "success or failure"
Mar  6 09:16:51.372: INFO: Pod "pod-f25dff8e-3008-47c4-b38c-df63b82fef02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769317ms
Mar  6 09:16:53.390: INFO: Pod "pod-f25dff8e-3008-47c4-b38c-df63b82fef02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021696465s
Mar  6 09:16:55.435: INFO: Pod "pod-f25dff8e-3008-47c4-b38c-df63b82fef02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066772738s
STEP: Saw pod success
Mar  6 09:16:55.435: INFO: Pod "pod-f25dff8e-3008-47c4-b38c-df63b82fef02" satisfied condition "success or failure"
Mar  6 09:16:55.440: INFO: Trying to get logs from node wisecloud-worker02 pod pod-f25dff8e-3008-47c4-b38c-df63b82fef02 container test-container: <nil>
STEP: delete the pod
Mar  6 09:16:55.508: INFO: Waiting for pod pod-f25dff8e-3008-47c4-b38c-df63b82fef02 to disappear
Mar  6 09:16:55.512: INFO: Pod pod-f25dff8e-3008-47c4-b38c-df63b82fef02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:16:55.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7383" for this suite.
Mar  6 09:17:01.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:17:01.777: INFO: namespace emptydir-7383 deletion completed in 6.258447535s

• [SLOW TEST:10.720 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:17:01.778: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-z5wx
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 09:17:02.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z5wx" in namespace "subpath-2778" to be "success or failure"
Mar  6 09:17:02.101: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056104ms
Mar  6 09:17:04.108: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011483084s
Mar  6 09:17:06.114: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017362614s
Mar  6 09:17:08.120: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 6.022952625s
Mar  6 09:17:10.125: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 8.028411545s
Mar  6 09:17:12.130: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 10.033073711s
Mar  6 09:17:14.136: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 12.038642522s
Mar  6 09:17:16.141: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 14.043806939s
Mar  6 09:17:18.145: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 16.048057553s
Mar  6 09:17:20.149: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 18.052455001s
Mar  6 09:17:22.154: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 20.057125657s
Mar  6 09:17:24.159: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 22.061785081s
Mar  6 09:17:26.164: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Running", Reason="", readiness=true. Elapsed: 24.067400757s
Mar  6 09:17:28.169: INFO: Pod "pod-subpath-test-configmap-z5wx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.07240468s
STEP: Saw pod success
Mar  6 09:17:28.169: INFO: Pod "pod-subpath-test-configmap-z5wx" satisfied condition "success or failure"
Mar  6 09:17:28.174: INFO: Trying to get logs from node wisecloud-worker02 pod pod-subpath-test-configmap-z5wx container test-container-subpath-configmap-z5wx: <nil>
STEP: delete the pod
Mar  6 09:17:28.226: INFO: Waiting for pod pod-subpath-test-configmap-z5wx to disappear
Mar  6 09:17:28.232: INFO: Pod pod-subpath-test-configmap-z5wx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z5wx
Mar  6 09:17:28.232: INFO: Deleting pod "pod-subpath-test-configmap-z5wx" in namespace "subpath-2778"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:17:28.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2778" for this suite.
Mar  6 09:17:36.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:17:36.703: INFO: namespace subpath-2778 deletion completed in 8.453673712s

• [SLOW TEST:34.926 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:17:36.704: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  6 09:17:36.961: INFO: PodSpec: initContainers in spec.initContainers
Mar  6 09:18:23.407: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-66387079-d1a0-4f0e-bf65-f0b2398ecc5d", GenerateName:"", Namespace:"init-container-7994", SelfLink:"/api/v1/namespaces/init-container-7994/pods/pod-init-66387079-d1a0-4f0e-bf65-f0b2398ecc5d", UID:"b7c30cde-839e-4ace-8d7f-bef4eebfca3b", ResourceVersion:"21215", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63719083056, loc:(*time.Location)(0x7ed0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"961425385"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rgbkh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00269c240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"172.20.8.7/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rgbkh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"172.20.8.7/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rgbkh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"172.20.8.7/library/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rgbkh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002310198), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wisecloud-worker02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ef4540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002310210)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002310230)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002310238), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00231023c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083057, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083057, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083057, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083057, loc:(*time.Location)(0x7ed0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.20.8.6", PodIP:"10.244.4.174", StartTime:(*v1.Time)(0xc00265af20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00226d8f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00226d960)}, Ready:false, RestartCount:3, Image:"172.20.8.7/library/busybox:1.29", ImageID:"docker-pullable://172.20.8.7/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://4d7a2006a7bc04bb6e927092872ecbe448cb23af64efe45f8bce5ed6e7ea980c"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00265af60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"172.20.8.7/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00265af40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"172.20.8.7/library/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:18:23.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7994" for this suite.
Mar  6 09:18:47.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:18:47.570: INFO: namespace init-container-7994 deletion completed in 24.155107545s

• [SLOW TEST:70.866 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:18:47.570: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-963
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Mar  6 09:18:48.504: INFO: created pod pod-service-account-defaultsa
Mar  6 09:18:48.504: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  6 09:18:48.556: INFO: created pod pod-service-account-mountsa
Mar  6 09:18:48.556: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  6 09:18:48.608: INFO: created pod pod-service-account-nomountsa
Mar  6 09:18:48.608: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  6 09:18:48.657: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  6 09:18:48.657: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  6 09:18:48.733: INFO: created pod pod-service-account-mountsa-mountspec
Mar  6 09:18:48.733: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  6 09:18:48.822: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  6 09:18:48.822: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  6 09:18:48.868: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  6 09:18:48.868: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  6 09:18:48.940: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  6 09:18:48.940: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  6 09:18:49.014: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  6 09:18:49.014: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:18:49.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-963" for this suite.
Mar  6 09:19:13.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:19:13.405: INFO: namespace svcaccounts-963 deletion completed in 24.277534094s

• [SLOW TEST:25.834 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:19:13.405: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  6 09:19:13.714: INFO: Waiting up to 5m0s for pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9" in namespace "downward-api-9503" to be "success or failure"
Mar  6 09:19:13.718: INFO: Pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091243ms
Mar  6 09:19:15.730: INFO: Pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015951694s
Mar  6 09:19:17.737: INFO: Pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022846652s
Mar  6 09:19:19.798: INFO: Pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083498677s
STEP: Saw pod success
Mar  6 09:19:19.798: INFO: Pod "downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9" satisfied condition "success or failure"
Mar  6 09:19:19.802: INFO: Trying to get logs from node wisecloud-worker02 pod downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9 container dapi-container: <nil>
STEP: delete the pod
Mar  6 09:19:19.880: INFO: Waiting for pod downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9 to disappear
Mar  6 09:19:19.884: INFO: Pod downward-api-0c2f0eff-3b8a-4505-b0d2-af0e684e0fe9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:19:19.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9503" for this suite.
Mar  6 09:19:25.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:19:26.220: INFO: namespace downward-api-9503 deletion completed in 6.328100881s

• [SLOW TEST:12.815 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:19:26.220: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Mar  6 09:19:26.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4158'
Mar  6 09:19:26.915: INFO: stderr: ""
Mar  6 09:19:26.915: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Mar  6 09:19:27.933: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:27.933: INFO: Found 0 / 1
Mar  6 09:19:28.921: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:28.921: INFO: Found 0 / 1
Mar  6 09:19:29.920: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:29.920: INFO: Found 0 / 1
Mar  6 09:19:30.920: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:30.920: INFO: Found 0 / 1
Mar  6 09:19:31.943: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:31.943: INFO: Found 1 / 1
Mar  6 09:19:31.943: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 09:19:31.946: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:19:31.946: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  6 09:19:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158'
Mar  6 09:19:32.110: INFO: stderr: ""
Mar  6 09:19:32.110: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 09:19:30.509 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 09:19:30.510 # Server started, Redis version 3.2.12\n1:M 06 Mar 09:19:30.510 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 09:19:30.510 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  6 09:19:32.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158 --tail=1'
Mar  6 09:19:32.280: INFO: stderr: ""
Mar  6 09:19:32.280: INFO: stdout: "1:M 06 Mar 09:19:30.510 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  6 09:19:32.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158 --limit-bytes=1'
Mar  6 09:19:32.426: INFO: stderr: ""
Mar  6 09:19:32.426: INFO: stdout: " "
STEP: exposing timestamps
Mar  6 09:19:32.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158 --tail=1 --timestamps'
Mar  6 09:19:32.571: INFO: stderr: ""
Mar  6 09:19:32.571: INFO: stdout: "2020-03-06T09:19:30.510482015Z 1:M 06 Mar 09:19:30.510 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  6 09:19:35.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158 --since=1s'
Mar  6 09:19:35.396: INFO: stderr: ""
Mar  6 09:19:35.396: INFO: stdout: ""
Mar  6 09:19:35.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-h6czp redis-master --namespace=kubectl-4158 --since=24h'
Mar  6 09:19:35.523: INFO: stderr: ""
Mar  6 09:19:35.523: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 09:19:30.509 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 09:19:30.510 # Server started, Redis version 3.2.12\n1:M 06 Mar 09:19:30.510 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 09:19:30.510 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Mar  6 09:19:35.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4158'
Mar  6 09:19:35.878: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:19:35.878: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  6 09:19:35.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4158'
Mar  6 09:19:36.036: INFO: stderr: "No resources found.\n"
Mar  6 09:19:36.036: INFO: stdout: ""
Mar  6 09:19:36.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -l name=nginx --namespace=kubectl-4158 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  6 09:19:36.172: INFO: stderr: ""
Mar  6 09:19:36.172: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:19:36.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4158" for this suite.
Mar  6 09:20:00.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:20:00.504: INFO: namespace kubectl-4158 deletion completed in 24.324406967s

• [SLOW TEST:34.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:20:00.505: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  6 09:20:07.477: INFO: Successfully updated pod "annotationupdate368765e8-a244-4999-85c1-76fcd904c21a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:20:09.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9180" for this suite.
Mar  6 09:20:33.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:20:33.658: INFO: namespace projected-9180 deletion completed in 24.138994997s

• [SLOW TEST:33.153 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:20:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  6 09:20:34.084: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  6 09:20:39.090: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:20:40.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6476" for this suite.
Mar  6 09:20:46.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:20:46.388: INFO: namespace replication-controller-6476 deletion completed in 6.249530631s

• [SLOW TEST:12.730 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:20:46.388: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-q4n8
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 09:20:46.734: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q4n8" in namespace "subpath-1272" to be "success or failure"
Mar  6 09:20:46.738: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.799728ms
Mar  6 09:20:48.742: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008186009s
Mar  6 09:20:50.747: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013490584s
Mar  6 09:20:52.752: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 6.018582188s
Mar  6 09:20:54.758: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 8.024264852s
Mar  6 09:20:56.763: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 10.029609463s
Mar  6 09:20:58.768: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 12.03431619s
Mar  6 09:21:00.773: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 14.038993535s
Mar  6 09:21:02.778: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 16.044632688s
Mar  6 09:21:04.835: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 18.100900167s
Mar  6 09:21:06.851: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 20.117653015s
Mar  6 09:21:08.855: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 22.121609574s
Mar  6 09:21:10.860: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Running", Reason="", readiness=true. Elapsed: 24.12630004s
Mar  6 09:21:12.865: INFO: Pod "pod-subpath-test-projected-q4n8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.131217616s
STEP: Saw pod success
Mar  6 09:21:12.865: INFO: Pod "pod-subpath-test-projected-q4n8" satisfied condition "success or failure"
Mar  6 09:21:12.868: INFO: Trying to get logs from node wisecloud-worker02 pod pod-subpath-test-projected-q4n8 container test-container-subpath-projected-q4n8: <nil>
STEP: delete the pod
Mar  6 09:21:12.923: INFO: Waiting for pod pod-subpath-test-projected-q4n8 to disappear
Mar  6 09:21:12.927: INFO: Pod pod-subpath-test-projected-q4n8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-q4n8
Mar  6 09:21:12.927: INFO: Deleting pod "pod-subpath-test-projected-q4n8" in namespace "subpath-1272"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:21:12.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1272" for this suite.
Mar  6 09:21:19.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:21:19.115: INFO: namespace subpath-1272 deletion completed in 6.146457893s

• [SLOW TEST:32.727 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:21:19.116: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 09:21:19.395: INFO: Waiting up to 5m0s for pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17" in namespace "emptydir-8145" to be "success or failure"
Mar  6 09:21:19.400: INFO: Pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333903ms
Mar  6 09:21:21.404: INFO: Pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898006s
Mar  6 09:21:23.410: INFO: Pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014526465s
Mar  6 09:21:25.420: INFO: Pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024663476s
STEP: Saw pod success
Mar  6 09:21:25.420: INFO: Pod "pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17" satisfied condition "success or failure"
Mar  6 09:21:25.425: INFO: Trying to get logs from node wisecloud-worker02 pod pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17 container test-container: <nil>
STEP: delete the pod
Mar  6 09:21:25.537: INFO: Waiting for pod pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17 to disappear
Mar  6 09:21:25.541: INFO: Pod pod-4db9b80d-9529-41c2-aea8-ff4c1de09a17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:21:25.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8145" for this suite.
Mar  6 09:21:31.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:21:31.768: INFO: namespace emptydir-8145 deletion completed in 6.220932141s

• [SLOW TEST:12.652 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:21:31.768: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  6 09:21:33.734: INFO: Pod name wrapped-volume-race-4e88039f-9eaf-4572-9eb8-5535dcfb720f: Found 0 pods out of 5
Mar  6 09:21:38.757: INFO: Pod name wrapped-volume-race-4e88039f-9eaf-4572-9eb8-5535dcfb720f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4e88039f-9eaf-4572-9eb8-5535dcfb720f in namespace emptydir-wrapper-9461, will wait for the garbage collector to delete the pods
Mar  6 09:21:49.316: INFO: Deleting ReplicationController wrapped-volume-race-4e88039f-9eaf-4572-9eb8-5535dcfb720f took: 82.557887ms
Mar  6 09:21:49.716: INFO: Terminating ReplicationController wrapped-volume-race-4e88039f-9eaf-4572-9eb8-5535dcfb720f pods took: 400.280696ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 09:22:30.549: INFO: Pod name wrapped-volume-race-d6496a72-d8e9-47f2-bf0e-47ae6028daee: Found 0 pods out of 5
Mar  6 09:22:35.590: INFO: Pod name wrapped-volume-race-d6496a72-d8e9-47f2-bf0e-47ae6028daee: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d6496a72-d8e9-47f2-bf0e-47ae6028daee in namespace emptydir-wrapper-9461, will wait for the garbage collector to delete the pods
Mar  6 09:22:47.999: INFO: Deleting ReplicationController wrapped-volume-race-d6496a72-d8e9-47f2-bf0e-47ae6028daee took: 39.466455ms
Mar  6 09:22:48.401: INFO: Terminating ReplicationController wrapped-volume-race-d6496a72-d8e9-47f2-bf0e-47ae6028daee pods took: 401.98885ms
STEP: Creating RC which spawns configmap-volume pods
Mar  6 09:23:28.681: INFO: Pod name wrapped-volume-race-e285f86c-eada-4ac5-a5d5-129fe734bdd8: Found 0 pods out of 5
Mar  6 09:23:33.696: INFO: Pod name wrapped-volume-race-e285f86c-eada-4ac5-a5d5-129fe734bdd8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e285f86c-eada-4ac5-a5d5-129fe734bdd8 in namespace emptydir-wrapper-9461, will wait for the garbage collector to delete the pods
Mar  6 09:23:45.863: INFO: Deleting ReplicationController wrapped-volume-race-e285f86c-eada-4ac5-a5d5-129fe734bdd8 took: 36.587277ms
Mar  6 09:23:46.264: INFO: Terminating ReplicationController wrapped-volume-race-e285f86c-eada-4ac5-a5d5-129fe734bdd8 pods took: 400.288952ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:24:30.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9461" for this suite.
Mar  6 09:24:42.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:24:42.664: INFO: namespace emptydir-wrapper-9461 deletion completed in 12.226379143s

• [SLOW TEST:190.896 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:24:42.664: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  6 09:24:49.236: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:24:49.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 09:24:49.236554      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4951" for this suite.
Mar  6 09:24:57.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:24:57.427: INFO: namespace gc-4951 deletion completed in 8.179879022s

• [SLOW TEST:14.763 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:24:57.427: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-08201bd8-76aa-4415-b528-9b8d9049925c
STEP: Creating a pod to test consume configMaps
Mar  6 09:24:58.017: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060" in namespace "projected-6054" to be "success or failure"
Mar  6 09:24:58.022: INFO: Pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312329ms
Mar  6 09:25:00.027: INFO: Pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009588826s
Mar  6 09:25:02.033: INFO: Pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015305165s
Mar  6 09:25:04.038: INFO: Pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020281389s
STEP: Saw pod success
Mar  6 09:25:04.038: INFO: Pod "pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060" satisfied condition "success or failure"
Mar  6 09:25:04.041: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:25:04.143: INFO: Waiting for pod pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060 to disappear
Mar  6 09:25:04.147: INFO: Pod pod-projected-configmaps-4a356577-1629-47e0-b391-61e50e6d5060 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:25:04.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6054" for this suite.
Mar  6 09:25:10.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:25:10.520: INFO: namespace projected-6054 deletion completed in 6.367415971s

• [SLOW TEST:13.093 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:25:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  6 09:25:10.805: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Mar  6 09:25:11.976: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  6 09:25:14.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:16.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:18.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:20.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:22.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:24.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:26.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:28.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:30.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:32.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083512, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719083511, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:25:35.567: INFO: Waited 1.345745717s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:25:36.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2347" for this suite.
Mar  6 09:25:42.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:25:42.828: INFO: namespace aggregator-2347 deletion completed in 6.199325913s

• [SLOW TEST:32.307 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:25:42.828: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d9bb6c55-37d2-42ed-ac8a-7c1df52f0dc1
STEP: Creating a pod to test consume configMaps
Mar  6 09:25:43.146: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9" in namespace "projected-8652" to be "success or failure"
Mar  6 09:25:43.150: INFO: Pod "pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08566ms
Mar  6 09:25:45.157: INFO: Pod "pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01069818s
Mar  6 09:25:47.162: INFO: Pod "pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015441282s
STEP: Saw pod success
Mar  6 09:25:47.162: INFO: Pod "pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9" satisfied condition "success or failure"
Mar  6 09:25:47.165: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:25:47.235: INFO: Waiting for pod pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9 to disappear
Mar  6 09:25:47.240: INFO: Pod pod-projected-configmaps-3345ca3d-933e-4646-8f04-23a5ccc4d5a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:25:47.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8652" for this suite.
Mar  6 09:25:53.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:25:53.366: INFO: namespace projected-8652 deletion completed in 6.117088337s

• [SLOW TEST:10.538 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:25:53.368: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-9ad0f765-a0f0-4645-8ae5-e4a8bea6b870
STEP: Creating a pod to test consume secrets
Mar  6 09:25:53.675: INFO: Waiting up to 5m0s for pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77" in namespace "secrets-4467" to be "success or failure"
Mar  6 09:25:53.692: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 16.772016ms
Mar  6 09:25:55.698: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023092846s
Mar  6 09:25:57.740: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064717378s
Mar  6 09:25:59.768: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09276009s
Mar  6 09:26:01.772: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096610241s
Mar  6 09:26:03.776: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100763546s
Mar  6 09:26:05.782: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.106968258s
STEP: Saw pod success
Mar  6 09:26:05.782: INFO: Pod "pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77" satisfied condition "success or failure"
Mar  6 09:26:05.786: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:26:05.904: INFO: Waiting for pod pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77 to disappear
Mar  6 09:26:05.909: INFO: Pod pod-secrets-f2abbc57-8721-4c96-bf09-eaaa1fde4a77 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:26:05.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4467" for this suite.
Mar  6 09:26:11.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:26:12.198: INFO: namespace secrets-4467 deletion completed in 6.283035467s

• [SLOW TEST:18.830 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:26:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-69396ace-604a-436f-b91c-8089d3b97f7b
STEP: Creating a pod to test consume secrets
Mar  6 09:26:12.502: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502" in namespace "projected-7898" to be "success or failure"
Mar  6 09:26:12.507: INFO: Pod "pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38004ms
Mar  6 09:26:14.511: INFO: Pod "pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008817091s
Mar  6 09:26:16.517: INFO: Pod "pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014377584s
STEP: Saw pod success
Mar  6 09:26:16.517: INFO: Pod "pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502" satisfied condition "success or failure"
Mar  6 09:26:16.520: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:26:16.593: INFO: Waiting for pod pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502 to disappear
Mar  6 09:26:16.597: INFO: Pod pod-projected-secrets-f41cda8e-6b9b-4269-b6b5-7d884d0e5502 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:26:16.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7898" for this suite.
Mar  6 09:26:22.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:26:22.776: INFO: namespace projected-7898 deletion completed in 6.173625452s

• [SLOW TEST:10.579 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:26:22.777: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  6 09:26:23.053: INFO: namespace kubectl-3599
Mar  6 09:26:23.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-3599'
Mar  6 09:26:24.912: INFO: stderr: ""
Mar  6 09:26:24.912: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 09:26:25.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:26:25.917: INFO: Found 0 / 1
Mar  6 09:26:26.919: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:26:26.919: INFO: Found 0 / 1
Mar  6 09:26:27.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:26:27.917: INFO: Found 1 / 1
Mar  6 09:26:27.917: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 09:26:27.921: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 09:26:27.921: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 09:26:27.921: INFO: wait on redis-master startup in kubectl-3599 
Mar  6 09:26:27.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 logs redis-master-wrs9b redis-master --namespace=kubectl-3599'
Mar  6 09:26:28.090: INFO: stderr: ""
Mar  6 09:26:28.090: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Mar 09:26:26.837 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Mar 09:26:26.837 # Server started, Redis version 3.2.12\n1:M 06 Mar 09:26:26.837 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Mar 09:26:26.837 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  6 09:26:28.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3599'
Mar  6 09:26:28.343: INFO: stderr: ""
Mar  6 09:26:28.343: INFO: stdout: "service/rm2 exposed\n"
Mar  6 09:26:28.349: INFO: Service rm2 in namespace kubectl-3599 found.
STEP: exposing service
Mar  6 09:26:30.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3599'
Mar  6 09:26:30.729: INFO: stderr: ""
Mar  6 09:26:30.729: INFO: stdout: "service/rm3 exposed\n"
Mar  6 09:26:30.734: INFO: Service rm3 in namespace kubectl-3599 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:26:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3599" for this suite.
Mar  6 09:26:56.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:26:57.099: INFO: namespace kubectl-3599 deletion completed in 24.348925313s

• [SLOW TEST:34.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:26:57.100: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Mar  6 09:26:57.373: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  6 09:26:57.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:26:58.047: INFO: stderr: ""
Mar  6 09:26:58.047: INFO: stdout: "service/redis-slave created\n"
Mar  6 09:26:58.047: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  6 09:26:58.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:26:58.575: INFO: stderr: ""
Mar  6 09:26:58.575: INFO: stdout: "service/redis-master created\n"
Mar  6 09:26:58.575: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  6 09:26:58.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:26:59.123: INFO: stderr: ""
Mar  6 09:26:59.124: INFO: stdout: "service/frontend created\n"
Mar  6 09:26:59.124: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: 172.20.8.7/library/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  6 09:26:59.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:26:59.697: INFO: stderr: ""
Mar  6 09:26:59.697: INFO: stdout: "deployment.apps/frontend created\n"
Mar  6 09:26:59.698: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: 172.20.8.7/library/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  6 09:26:59.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:27:00.285: INFO: stderr: ""
Mar  6 09:27:00.285: INFO: stdout: "deployment.apps/redis-master created\n"
Mar  6 09:27:00.285: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: 172.20.8.7/library/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  6 09:27:00.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-4346'
Mar  6 09:27:00.977: INFO: stderr: ""
Mar  6 09:27:00.977: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar  6 09:27:00.977: INFO: Waiting for all frontend pods to be Running.
Mar  6 09:27:51.029: INFO: Waiting for frontend to serve content.
Mar  6 09:27:52.077: INFO: Trying to add a new entry to the guestbook.
Mar  6 09:27:52.103: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  6 09:27:52.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:52.581: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:52.581: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 09:27:52.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:52.990: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:52.990: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 09:27:52.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:53.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:53.485: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 09:27:53.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:53.727: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:53.727: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 09:27:53.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:53.858: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:53.858: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  6 09:27:53.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete --grace-period=0 --force -f - --namespace=kubectl-4346'
Mar  6 09:27:54.109: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  6 09:27:54.109: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:27:54.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4346" for this suite.
Mar  6 09:28:34.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:28:34.333: INFO: namespace kubectl-4346 deletion completed in 40.218340596s

• [SLOW TEST:97.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:28:34.334: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-6605
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6605
STEP: Deleting pre-stop pod
Mar  6 09:28:49.721: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:28:49.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6605" for this suite.
Mar  6 09:29:29.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:29:30.227: INFO: namespace prestop-6605 deletion completed in 40.480855968s

• [SLOW TEST:55.893 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:29:30.228: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9219
I0306 09:29:30.522272      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9219, replica count: 1
I0306 09:29:31.572955      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:29:32.573340      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:29:33.573709      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:29:34.574155      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 09:29:34.780: INFO: Created: latency-svc-n7q67
Mar  6 09:29:34.798: INFO: Got endpoints: latency-svc-n7q67 [124.007482ms]
Mar  6 09:29:34.890: INFO: Created: latency-svc-zm2q6
Mar  6 09:29:35.020: INFO: Got endpoints: latency-svc-zm2q6 [221.803078ms]
Mar  6 09:29:35.034: INFO: Created: latency-svc-9sbps
Mar  6 09:29:35.065: INFO: Got endpoints: latency-svc-9sbps [266.731683ms]
Mar  6 09:29:35.446: INFO: Created: latency-svc-6n7hf
Mar  6 09:29:35.446: INFO: Created: latency-svc-6blm7
Mar  6 09:29:35.512: INFO: Created: latency-svc-92b49
Mar  6 09:29:35.512: INFO: Created: latency-svc-4ln8j
Mar  6 09:29:35.525: INFO: Got endpoints: latency-svc-6n7hf [726.811104ms]
Mar  6 09:29:35.540: INFO: Got endpoints: latency-svc-6blm7 [741.888017ms]
Mar  6 09:29:35.540: INFO: Created: latency-svc-mjh4k
Mar  6 09:29:35.617: INFO: Got endpoints: latency-svc-4ln8j [818.495936ms]
Mar  6 09:29:35.617: INFO: Got endpoints: latency-svc-92b49 [818.565327ms]
Mar  6 09:29:35.634: INFO: Created: latency-svc-6w99v
Mar  6 09:29:35.637: INFO: Got endpoints: latency-svc-mjh4k [837.815865ms]
Mar  6 09:29:35.658: INFO: Got endpoints: latency-svc-6w99v [859.382168ms]
Mar  6 09:29:35.834: INFO: Created: latency-svc-g97th
Mar  6 09:29:35.834: INFO: Created: latency-svc-p2cgj
Mar  6 09:29:35.886: INFO: Got endpoints: latency-svc-p2cgj [1.087706795s]
Mar  6 09:29:35.887: INFO: Got endpoints: latency-svc-g97th [1.087934827s]
Mar  6 09:29:36.055: INFO: Created: latency-svc-dfl62
Mar  6 09:29:36.113: INFO: Created: latency-svc-tcq4j
Mar  6 09:29:36.137: INFO: Got endpoints: latency-svc-dfl62 [1.33826293s]
Mar  6 09:29:36.163: INFO: Created: latency-svc-7sfd7
Mar  6 09:29:36.164: INFO: Created: latency-svc-j7gft
Mar  6 09:29:36.182: INFO: Created: latency-svc-tppnm
Mar  6 09:29:36.194: INFO: Got endpoints: latency-svc-tcq4j [1.395301756s]
Mar  6 09:29:36.199: INFO: Got endpoints: latency-svc-j7gft [1.400347088s]
Mar  6 09:29:36.201: INFO: Got endpoints: latency-svc-7sfd7 [1.402517158s]
Mar  6 09:29:36.219: INFO: Got endpoints: latency-svc-tppnm [1.420160985s]
Mar  6 09:29:36.559: INFO: Created: latency-svc-2jvbp
Mar  6 09:29:36.559: INFO: Created: latency-svc-kx4cl
Mar  6 09:29:36.559: INFO: Created: latency-svc-pbwph
Mar  6 09:29:36.595: INFO: Got endpoints: latency-svc-pbwph [1.574537856s]
Mar  6 09:29:36.653: INFO: Got endpoints: latency-svc-2jvbp [1.587184056s]
Mar  6 09:29:36.653: INFO: Got endpoints: latency-svc-kx4cl [1.127075762s]
Mar  6 09:29:36.747: INFO: Created: latency-svc-qjxw6
Mar  6 09:29:36.764: INFO: Created: latency-svc-sh6b7
Mar  6 09:29:36.764: INFO: Created: latency-svc-plp8r
Mar  6 09:29:36.851: INFO: Got endpoints: latency-svc-qjxw6 [1.310259287s]
Mar  6 09:29:36.884: INFO: Got endpoints: latency-svc-plp8r [1.266300211s]
Mar  6 09:29:36.898: INFO: Got endpoints: latency-svc-sh6b7 [1.280480677s]
Mar  6 09:29:36.927: INFO: Created: latency-svc-gtlgp
Mar  6 09:29:36.927: INFO: Created: latency-svc-ckfnl
Mar  6 09:29:36.927: INFO: Created: latency-svc-p6jx5
Mar  6 09:29:36.969: INFO: Got endpoints: latency-svc-gtlgp [1.310924338s]
Mar  6 09:29:36.997: INFO: Got endpoints: latency-svc-p6jx5 [1.110293305s]
Mar  6 09:29:36.997: INFO: Got endpoints: latency-svc-ckfnl [1.360260789s]
Mar  6 09:29:37.290: INFO: Created: latency-svc-s76s5
Mar  6 09:29:37.290: INFO: Created: latency-svc-pb9bl
Mar  6 09:29:37.340: INFO: Created: latency-svc-nsbr9
Mar  6 09:29:37.341: INFO: Created: latency-svc-wsrs4
Mar  6 09:29:37.365: INFO: Created: latency-svc-d2gk9
Mar  6 09:29:37.367: INFO: Created: latency-svc-hsp6z
Mar  6 09:29:37.379: INFO: Got endpoints: latency-svc-s76s5 [1.491684967s]
Mar  6 09:29:37.379: INFO: Got endpoints: latency-svc-pb9bl [1.241450877s]
Mar  6 09:29:37.388: INFO: Created: latency-svc-jn6fq
Mar  6 09:29:37.420: INFO: Got endpoints: latency-svc-d2gk9 [1.201267415s]
Mar  6 09:29:37.421: INFO: Got endpoints: latency-svc-wsrs4 [1.221290949s]
Mar  6 09:29:37.421: INFO: Got endpoints: latency-svc-nsbr9 [1.226448383s]
Mar  6 09:29:37.421: INFO: Got endpoints: latency-svc-hsp6z [1.219158688s]
Mar  6 09:29:37.492: INFO: Got endpoints: latency-svc-jn6fq [897.425734ms]
Mar  6 09:29:37.509: INFO: Created: latency-svc-xdz9c
Mar  6 09:29:37.584: INFO: Got endpoints: latency-svc-xdz9c [931.232662ms]
Mar  6 09:29:37.649: INFO: Created: latency-svc-sgh6z
Mar  6 09:29:37.675: INFO: Created: latency-svc-m6ppx
Mar  6 09:29:37.704: INFO: Got endpoints: latency-svc-sgh6z [1.051222438s]
Mar  6 09:29:37.725: INFO: Got endpoints: latency-svc-m6ppx [874.265118ms]
Mar  6 09:29:38.064: INFO: Created: latency-svc-g99t4
Mar  6 09:29:38.064: INFO: Created: latency-svc-fkjz2
Mar  6 09:29:38.122: INFO: Got endpoints: latency-svc-g99t4 [1.223847253s]
Mar  6 09:29:38.123: INFO: Got endpoints: latency-svc-fkjz2 [1.23877598s]
Mar  6 09:29:38.290: INFO: Created: latency-svc-d4vjl
Mar  6 09:29:38.290: INFO: Created: latency-svc-4f6k2
Mar  6 09:29:38.290: INFO: Created: latency-svc-4gb77
Mar  6 09:29:38.346: INFO: Got endpoints: latency-svc-4f6k2 [1.34935382s]
Mar  6 09:29:38.349: INFO: Got endpoints: latency-svc-d4vjl [1.380310276s]
Mar  6 09:29:38.350: INFO: Got endpoints: latency-svc-4gb77 [1.352702867s]
Mar  6 09:29:38.465: INFO: Created: latency-svc-c6spn
Mar  6 09:29:38.466: INFO: Created: latency-svc-96m88
Mar  6 09:29:38.466: INFO: Created: latency-svc-6t2wr
Mar  6 09:29:38.518: INFO: Got endpoints: latency-svc-6t2wr [1.139336966s]
Mar  6 09:29:38.554: INFO: Got endpoints: latency-svc-96m88 [1.175086172s]
Mar  6 09:29:38.554: INFO: Got endpoints: latency-svc-c6spn [1.132964263s]
Mar  6 09:29:38.629: INFO: Created: latency-svc-q542q
Mar  6 09:29:38.639: INFO: Created: latency-svc-8z797
Mar  6 09:29:38.680: INFO: Created: latency-svc-ph42k
Mar  6 09:29:38.712: INFO: Got endpoints: latency-svc-ph42k [1.291192535s]
Mar  6 09:29:38.712: INFO: Got endpoints: latency-svc-8z797 [1.291287464s]
Mar  6 09:29:38.712: INFO: Got endpoints: latency-svc-q542q [1.291494553s]
Mar  6 09:29:38.823: INFO: Created: latency-svc-d8s94
Mar  6 09:29:38.856: INFO: Created: latency-svc-q4vkz
Mar  6 09:29:39.074: INFO: Got endpoints: latency-svc-d8s94 [1.582051009s]
Mar  6 09:29:39.098: INFO: Got endpoints: latency-svc-q4vkz [1.51445305s]
Mar  6 09:29:39.285: INFO: Created: latency-svc-rvlt7
Mar  6 09:29:39.287: INFO: Created: latency-svc-fnf9k
Mar  6 09:29:39.287: INFO: Created: latency-svc-frqwh
Mar  6 09:29:39.314: INFO: Got endpoints: latency-svc-frqwh [1.609755437s]
Mar  6 09:29:39.330: INFO: Got endpoints: latency-svc-rvlt7 [1.604636745s]
Mar  6 09:29:39.331: INFO: Got endpoints: latency-svc-fnf9k [1.209429564s]
Mar  6 09:29:39.595: INFO: Created: latency-svc-7ss7v
Mar  6 09:29:39.595: INFO: Created: latency-svc-7r5vq
Mar  6 09:29:39.597: INFO: Created: latency-svc-w62tl
Mar  6 09:29:39.661: INFO: Got endpoints: latency-svc-7ss7v [1.538179751s]
Mar  6 09:29:39.661: INFO: Got endpoints: latency-svc-7r5vq [1.314542198s]
Mar  6 09:29:39.661: INFO: Got endpoints: latency-svc-w62tl [1.31175382s]
Mar  6 09:29:40.023: INFO: Created: latency-svc-w74gm
Mar  6 09:29:40.045: INFO: Created: latency-svc-djt2v
Mar  6 09:29:40.045: INFO: Created: latency-svc-gpgsb
Mar  6 09:29:40.046: INFO: Created: latency-svc-wls4r
Mar  6 09:29:40.081: INFO: Got endpoints: latency-svc-w74gm [1.731597994s]
Mar  6 09:29:40.171: INFO: Got endpoints: latency-svc-wls4r [1.617457982s]
Mar  6 09:29:40.171: INFO: Got endpoints: latency-svc-djt2v [1.617567378s]
Mar  6 09:29:40.172: INFO: Got endpoints: latency-svc-gpgsb [1.653578815s]
Mar  6 09:29:40.237: INFO: Created: latency-svc-59tlx
Mar  6 09:29:40.237: INFO: Created: latency-svc-jmzqq
Mar  6 09:29:40.270: INFO: Got endpoints: latency-svc-59tlx [1.55769462s]
Mar  6 09:29:40.270: INFO: Got endpoints: latency-svc-jmzqq [1.557813615s]
Mar  6 09:29:40.294: INFO: Created: latency-svc-wlzdg
Mar  6 09:29:40.294: INFO: Created: latency-svc-2tbcv
Mar  6 09:29:40.294: INFO: Created: latency-svc-78tdv
Mar  6 09:29:40.294: INFO: Created: latency-svc-2ktg2
Mar  6 09:29:40.367: INFO: Got endpoints: latency-svc-2tbcv [1.292245139s]
Mar  6 09:29:40.367: INFO: Got endpoints: latency-svc-wlzdg [1.053295101s]
Mar  6 09:29:40.411: INFO: Got endpoints: latency-svc-2ktg2 [1.698624926s]
Mar  6 09:29:40.414: INFO: Got endpoints: latency-svc-78tdv [1.315539742s]
Mar  6 09:29:40.418: INFO: Created: latency-svc-vcdb6
Mar  6 09:29:40.434: INFO: Created: latency-svc-2r9gs
Mar  6 09:29:40.496: INFO: Got endpoints: latency-svc-vcdb6 [1.164574243s]
Mar  6 09:29:40.508: INFO: Got endpoints: latency-svc-2r9gs [1.177920264s]
Mar  6 09:29:40.570: INFO: Created: latency-svc-n544j
Mar  6 09:29:40.643: INFO: Got endpoints: latency-svc-n544j [982.052827ms]
Mar  6 09:29:40.661: INFO: Created: latency-svc-rw4md
Mar  6 09:29:40.693: INFO: Got endpoints: latency-svc-rw4md [278.618715ms]
Mar  6 09:29:40.829: INFO: Created: latency-svc-r4zc7
Mar  6 09:29:40.887: INFO: Got endpoints: latency-svc-r4zc7 [1.225679129s]
Mar  6 09:29:40.998: INFO: Created: latency-svc-m8ppw
Mar  6 09:29:41.004: INFO: Created: latency-svc-7mshp
Mar  6 09:29:41.073: INFO: Created: latency-svc-zrb2g
Mar  6 09:29:41.105: INFO: Got endpoints: latency-svc-zrb2g [933.198028ms]
Mar  6 09:29:41.105: INFO: Got endpoints: latency-svc-m8ppw [1.443477238s]
Mar  6 09:29:41.105: INFO: Got endpoints: latency-svc-7mshp [1.023459887s]
Mar  6 09:29:41.202: INFO: Created: latency-svc-vjkxz
Mar  6 09:29:41.202: INFO: Created: latency-svc-5n6sq
Mar  6 09:29:41.276: INFO: Got endpoints: latency-svc-vjkxz [1.103761724s]
Mar  6 09:29:41.277: INFO: Got endpoints: latency-svc-5n6sq [1.105759891s]
Mar  6 09:29:41.283: INFO: Created: latency-svc-zx2wr
Mar  6 09:29:41.361: INFO: Created: latency-svc-pxwt8
Mar  6 09:29:41.368: INFO: Got endpoints: latency-svc-zx2wr [1.09870265s]
Mar  6 09:29:41.401: INFO: Created: latency-svc-6b8rk
Mar  6 09:29:41.443: INFO: Got endpoints: latency-svc-pxwt8 [1.173072808s]
Mar  6 09:29:41.481: INFO: Got endpoints: latency-svc-6b8rk [1.114070905s]
Mar  6 09:29:41.595: INFO: Created: latency-svc-g24fq
Mar  6 09:29:41.643: INFO: Got endpoints: latency-svc-g24fq [1.275786251s]
Mar  6 09:29:41.715: INFO: Created: latency-svc-zpfc7
Mar  6 09:29:41.743: INFO: Created: latency-svc-47697
Mar  6 09:29:41.753: INFO: Got endpoints: latency-svc-zpfc7 [1.342162252s]
Mar  6 09:29:41.764: INFO: Got endpoints: latency-svc-47697 [1.26850016s]
Mar  6 09:29:42.135: INFO: Created: latency-svc-pscp5
Mar  6 09:29:42.157: INFO: Created: latency-svc-x6xjr
Mar  6 09:29:42.157: INFO: Created: latency-svc-r5gps
Mar  6 09:29:42.157: INFO: Created: latency-svc-b5tqh
Mar  6 09:29:42.221: INFO: Got endpoints: latency-svc-pscp5 [1.713103334s]
Mar  6 09:29:42.222: INFO: Got endpoints: latency-svc-b5tqh [1.334883329s]
Mar  6 09:29:42.222: INFO: Got endpoints: latency-svc-r5gps [1.578796641s]
Mar  6 09:29:42.223: INFO: Got endpoints: latency-svc-x6xjr [1.529934829s]
Mar  6 09:29:42.366: INFO: Created: latency-svc-gzhq4
Mar  6 09:29:42.367: INFO: Created: latency-svc-dx9md
Mar  6 09:29:42.410: INFO: Got endpoints: latency-svc-dx9md [1.305397958s]
Mar  6 09:29:42.421: INFO: Got endpoints: latency-svc-gzhq4 [1.316456717s]
Mar  6 09:29:42.426: INFO: Created: latency-svc-wbnvt
Mar  6 09:29:42.545: INFO: Got endpoints: latency-svc-wbnvt [1.440384927s]
Mar  6 09:29:42.557: INFO: Created: latency-svc-sstxr
Mar  6 09:29:42.566: INFO: Created: latency-svc-x5d4r
Mar  6 09:29:42.566: INFO: Created: latency-svc-chq9r
Mar  6 09:29:42.608: INFO: Got endpoints: latency-svc-sstxr [1.332640635s]
Mar  6 09:29:42.610: INFO: Created: latency-svc-z8zgr
Mar  6 09:29:42.696: INFO: Got endpoints: latency-svc-z8zgr [1.252723521s]
Mar  6 09:29:42.696: INFO: Got endpoints: latency-svc-x5d4r [1.327470159s]
Mar  6 09:29:42.696: INFO: Got endpoints: latency-svc-chq9r [1.419051102s]
Mar  6 09:29:42.730: INFO: Created: latency-svc-nnk6l
Mar  6 09:29:42.734: INFO: Created: latency-svc-gvc6v
Mar  6 09:29:42.777: INFO: Got endpoints: latency-svc-nnk6l [1.295796908s]
Mar  6 09:29:42.839: INFO: Got endpoints: latency-svc-gvc6v [1.195916418s]
Mar  6 09:29:42.915: INFO: Created: latency-svc-f9sdr
Mar  6 09:29:42.971: INFO: Got endpoints: latency-svc-f9sdr [1.217653135s]
Mar  6 09:29:43.086: INFO: Created: latency-svc-zltf5
Mar  6 09:29:43.108: INFO: Created: latency-svc-thjdn
Mar  6 09:29:43.154: INFO: Created: latency-svc-2rhrw
Mar  6 09:29:43.201: INFO: Got endpoints: latency-svc-zltf5 [1.436298567s]
Mar  6 09:29:43.211: INFO: Got endpoints: latency-svc-thjdn [989.64479ms]
Mar  6 09:29:43.219: INFO: Got endpoints: latency-svc-2rhrw [997.487398ms]
Mar  6 09:29:43.229: INFO: Created: latency-svc-sjzq5
Mar  6 09:29:43.296: INFO: Got endpoints: latency-svc-sjzq5 [1.073326374s]
Mar  6 09:29:43.313: INFO: Created: latency-svc-2qrkp
Mar  6 09:29:43.371: INFO: Got endpoints: latency-svc-2qrkp [1.148995508s]
Mar  6 09:29:43.625: INFO: Created: latency-svc-sv7dp
Mar  6 09:29:43.625: INFO: Created: latency-svc-px7dd
Mar  6 09:29:43.626: INFO: Created: latency-svc-kfq44
Mar  6 09:29:43.738: INFO: Got endpoints: latency-svc-sv7dp [1.327815518s]
Mar  6 09:29:43.738: INFO: Got endpoints: latency-svc-kfq44 [1.19282051s]
Mar  6 09:29:43.739: INFO: Got endpoints: latency-svc-px7dd [1.31730784s]
Mar  6 09:29:43.782: INFO: Created: latency-svc-r28pg
Mar  6 09:29:43.814: INFO: Created: latency-svc-rshfc
Mar  6 09:29:43.872: INFO: Got endpoints: latency-svc-rshfc [1.175641217s]
Mar  6 09:29:43.872: INFO: Got endpoints: latency-svc-r28pg [1.263924481s]
Mar  6 09:29:44.008: INFO: Created: latency-svc-sttgn
Mar  6 09:29:44.083: INFO: Got endpoints: latency-svc-sttgn [1.38653265s]
Mar  6 09:29:44.116: INFO: Created: latency-svc-p6g9v
Mar  6 09:29:44.175: INFO: Created: latency-svc-m7686
Mar  6 09:29:44.230: INFO: Got endpoints: latency-svc-p6g9v [1.534173433s]
Mar  6 09:29:44.245: INFO: Got endpoints: latency-svc-m7686 [1.468638802s]
Mar  6 09:29:44.249: INFO: Created: latency-svc-jf7mp
Mar  6 09:29:44.313: INFO: Got endpoints: latency-svc-jf7mp [1.473953881s]
Mar  6 09:29:44.373: INFO: Created: latency-svc-p59h9
Mar  6 09:29:44.386: INFO: Created: latency-svc-d8lss
Mar  6 09:29:44.456: INFO: Got endpoints: latency-svc-d8lss [1.255502943s]
Mar  6 09:29:44.457: INFO: Got endpoints: latency-svc-p59h9 [1.486225163s]
Mar  6 09:29:44.468: INFO: Created: latency-svc-w8h5s
Mar  6 09:29:44.487: INFO: Got endpoints: latency-svc-w8h5s [1.276464052s]
Mar  6 09:29:44.608: INFO: Created: latency-svc-lknwt
Mar  6 09:29:44.700: INFO: Created: latency-svc-fpr6l
Mar  6 09:29:44.764: INFO: Got endpoints: latency-svc-fpr6l [1.467744567s]
Mar  6 09:29:44.764: INFO: Got endpoints: latency-svc-lknwt [1.545017915s]
Mar  6 09:29:44.910: INFO: Created: latency-svc-8q7pc
Mar  6 09:29:44.976: INFO: Created: latency-svc-xmzs6
Mar  6 09:29:44.976: INFO: Created: latency-svc-qlmgm
Mar  6 09:29:44.993: INFO: Got endpoints: latency-svc-8q7pc [1.622186158s]
Mar  6 09:29:45.002: INFO: Got endpoints: latency-svc-xmzs6 [1.2638963s]
Mar  6 09:29:45.002: INFO: Got endpoints: latency-svc-qlmgm [1.263924511s]
Mar  6 09:29:45.279: INFO: Created: latency-svc-mvvtz
Mar  6 09:29:45.292: INFO: Created: latency-svc-r2xqk
Mar  6 09:29:45.305: INFO: Got endpoints: latency-svc-mvvtz [1.565741701s]
Mar  6 09:29:45.331: INFO: Got endpoints: latency-svc-r2xqk [1.459697248s]
Mar  6 09:29:45.366: INFO: Created: latency-svc-lfmkl
Mar  6 09:29:45.476: INFO: Created: latency-svc-5vsxc
Mar  6 09:29:45.476: INFO: Created: latency-svc-nzz9b
Mar  6 09:29:45.476: INFO: Created: latency-svc-5lm9p
Mar  6 09:29:45.477: INFO: Got endpoints: latency-svc-lfmkl [1.605052351s]
Mar  6 09:29:45.480: INFO: Created: latency-svc-zkvnl
Mar  6 09:29:45.564: INFO: Got endpoints: latency-svc-5lm9p [1.48106232s]
Mar  6 09:29:45.580: INFO: Got endpoints: latency-svc-zkvnl [1.267383115s]
Mar  6 09:29:45.581: INFO: Got endpoints: latency-svc-nzz9b [1.350815388s]
Mar  6 09:29:45.581: INFO: Got endpoints: latency-svc-5vsxc [1.335484908s]
Mar  6 09:29:45.623: INFO: Created: latency-svc-qt4zh
Mar  6 09:29:45.805: INFO: Got endpoints: latency-svc-qt4zh [1.348549941s]
Mar  6 09:29:45.814: INFO: Created: latency-svc-g7wjs
Mar  6 09:29:45.921: INFO: Got endpoints: latency-svc-g7wjs [1.46385152s]
Mar  6 09:29:45.938: INFO: Created: latency-svc-spbbz
Mar  6 09:29:46.009: INFO: Got endpoints: latency-svc-spbbz [1.521417512s]
Mar  6 09:29:46.051: INFO: Created: latency-svc-glrzd
Mar  6 09:29:46.088: INFO: Got endpoints: latency-svc-glrzd [1.323641129s]
Mar  6 09:29:46.427: INFO: Created: latency-svc-2vr6s
Mar  6 09:29:46.428: INFO: Created: latency-svc-x9xkv
Mar  6 09:29:46.432: INFO: Created: latency-svc-fbbmg
Mar  6 09:29:46.509: INFO: Created: latency-svc-ftff9
Mar  6 09:29:46.529: INFO: Got endpoints: latency-svc-x9xkv [1.765517393s]
Mar  6 09:29:46.530: INFO: Got endpoints: latency-svc-fbbmg [1.528328228s]
Mar  6 09:29:46.531: INFO: Got endpoints: latency-svc-2vr6s [1.537576998s]
Mar  6 09:29:46.560: INFO: Got endpoints: latency-svc-ftff9 [1.558145686s]
Mar  6 09:29:46.617: INFO: Created: latency-svc-xdshg
Mar  6 09:29:46.617: INFO: Created: latency-svc-snn6z
Mar  6 09:29:46.688: INFO: Created: latency-svc-5hjln
Mar  6 09:29:46.689: INFO: Created: latency-svc-5b7g5
Mar  6 09:29:46.711: INFO: Got endpoints: latency-svc-snn6z [1.405975567s]
Mar  6 09:29:46.714: INFO: Got endpoints: latency-svc-xdshg [1.38246679s]
Mar  6 09:29:46.787: INFO: Got endpoints: latency-svc-5hjln [1.309731108s]
Mar  6 09:29:46.838: INFO: Got endpoints: latency-svc-5b7g5 [1.27366703s]
Mar  6 09:29:46.852: INFO: Created: latency-svc-zc2q2
Mar  6 09:29:46.923: INFO: Got endpoints: latency-svc-zc2q2 [1.341811028s]
Mar  6 09:29:46.993: INFO: Created: latency-svc-sflhm
Mar  6 09:29:47.031: INFO: Got endpoints: latency-svc-sflhm [1.450762023s]
Mar  6 09:29:47.081: INFO: Created: latency-svc-kf6pz
Mar  6 09:29:47.089: INFO: Created: latency-svc-swdb8
Mar  6 09:29:47.233: INFO: Got endpoints: latency-svc-kf6pz [1.651461651s]
Mar  6 09:29:47.257: INFO: Created: latency-svc-9r9tb
Mar  6 09:29:47.265: INFO: Got endpoints: latency-svc-swdb8 [1.459902862s]
Mar  6 09:29:47.275: INFO: Got endpoints: latency-svc-9r9tb [1.353588238s]
Mar  6 09:29:47.317: INFO: Created: latency-svc-l94jn
Mar  6 09:29:47.393: INFO: Got endpoints: latency-svc-l94jn [1.38434296s]
Mar  6 09:29:47.554: INFO: Created: latency-svc-484j2
Mar  6 09:29:47.586: INFO: Got endpoints: latency-svc-484j2 [1.498196138s]
Mar  6 09:29:47.765: INFO: Created: latency-svc-hrdwf
Mar  6 09:29:47.836: INFO: Created: latency-svc-5jwg4
Mar  6 09:29:47.836: INFO: Created: latency-svc-k6vvl
Mar  6 09:29:47.839: INFO: Got endpoints: latency-svc-hrdwf [1.309130694s]
Mar  6 09:29:47.865: INFO: Got endpoints: latency-svc-5jwg4 [1.33405948s]
Mar  6 09:29:47.865: INFO: Got endpoints: latency-svc-k6vvl [1.334832608s]
Mar  6 09:29:47.946: INFO: Created: latency-svc-qwgmr
Mar  6 09:29:47.947: INFO: Created: latency-svc-25tzt
Mar  6 09:29:47.971: INFO: Created: latency-svc-ltzlx
Mar  6 09:29:47.980: INFO: Created: latency-svc-hjvkt
Mar  6 09:29:47.989: INFO: Got endpoints: latency-svc-qwgmr [1.428446686s]
Mar  6 09:29:48.023: INFO: Got endpoints: latency-svc-25tzt [1.311932517s]
Mar  6 09:29:48.039: INFO: Got endpoints: latency-svc-ltzlx [1.325019542s]
Mar  6 09:29:48.073: INFO: Got endpoints: latency-svc-hjvkt [1.285330318s]
Mar  6 09:29:48.140: INFO: Created: latency-svc-xs6sn
Mar  6 09:29:48.175: INFO: Got endpoints: latency-svc-xs6sn [1.336822385s]
Mar  6 09:29:48.215: INFO: Created: latency-svc-7nksh
Mar  6 09:29:48.300: INFO: Got endpoints: latency-svc-7nksh [1.377200176s]
Mar  6 09:29:48.302: INFO: Created: latency-svc-z45qc
Mar  6 09:29:48.324: INFO: Got endpoints: latency-svc-z45qc [1.292434829s]
Mar  6 09:29:48.452: INFO: Created: latency-svc-pqvcp
Mar  6 09:29:48.534: INFO: Created: latency-svc-m82r6
Mar  6 09:29:48.540: INFO: Created: latency-svc-tn959
Mar  6 09:29:48.553: INFO: Got endpoints: latency-svc-pqvcp [1.320289933s]
Mar  6 09:29:48.560: INFO: Got endpoints: latency-svc-m82r6 [1.295026132s]
Mar  6 09:29:48.577: INFO: Got endpoints: latency-svc-tn959 [1.301976376s]
Mar  6 09:29:48.687: INFO: Created: latency-svc-hl7p9
Mar  6 09:29:48.716: INFO: Got endpoints: latency-svc-hl7p9 [1.322569465s]
Mar  6 09:29:48.733: INFO: Created: latency-svc-v5pzj
Mar  6 09:29:48.783: INFO: Got endpoints: latency-svc-v5pzj [1.196118131s]
Mar  6 09:29:49.178: INFO: Created: latency-svc-wjzvx
Mar  6 09:29:49.203: INFO: Got endpoints: latency-svc-wjzvx [1.364815886s]
Mar  6 09:29:49.261: INFO: Created: latency-svc-r9lxx
Mar  6 09:29:49.261: INFO: Created: latency-svc-bdgrf
Mar  6 09:29:49.261: INFO: Created: latency-svc-mbnrb
Mar  6 09:29:49.261: INFO: Created: latency-svc-f5x6t
Mar  6 09:29:49.261: INFO: Created: latency-svc-85nfr
Mar  6 09:29:49.261: INFO: Created: latency-svc-drhgj
Mar  6 09:29:49.261: INFO: Created: latency-svc-jm4j4
Mar  6 09:29:49.274: INFO: Created: latency-svc-2qckc
Mar  6 09:29:49.294: INFO: Got endpoints: latency-svc-85nfr [1.429533598s]
Mar  6 09:29:49.295: INFO: Got endpoints: latency-svc-r9lxx [1.42928046s]
Mar  6 09:29:49.295: INFO: Got endpoints: latency-svc-mbnrb [1.255692584s]
Mar  6 09:29:49.295: INFO: Got endpoints: latency-svc-f5x6t [1.272197625s]
Mar  6 09:29:49.295: INFO: Got endpoints: latency-svc-jm4j4 [1.306434431s]
Mar  6 09:29:49.392: INFO: Got endpoints: latency-svc-drhgj [1.217048993s]
Mar  6 09:29:49.392: INFO: Got endpoints: latency-svc-bdgrf [1.319180939s]
Mar  6 09:29:49.392: INFO: Got endpoints: latency-svc-2qckc [1.091757169s]
Mar  6 09:29:49.407: INFO: Created: latency-svc-cmkp9
Mar  6 09:29:49.464: INFO: Got endpoints: latency-svc-cmkp9 [1.140099739s]
Mar  6 09:29:49.585: INFO: Created: latency-svc-sxwz2
Mar  6 09:29:49.652: INFO: Created: latency-svc-2lc6p
Mar  6 09:29:49.670: INFO: Got endpoints: latency-svc-sxwz2 [1.117129126s]
Mar  6 09:29:49.687: INFO: Got endpoints: latency-svc-2lc6p [1.126391366s]
Mar  6 09:29:49.750: INFO: Created: latency-svc-cl48w
Mar  6 09:29:49.837: INFO: Created: latency-svc-p8hlj
Mar  6 09:29:49.856: INFO: Got endpoints: latency-svc-cl48w [1.279339826s]
Mar  6 09:29:49.875: INFO: Got endpoints: latency-svc-p8hlj [1.15935199s]
Mar  6 09:29:50.052: INFO: Created: latency-svc-ptt8w
Mar  6 09:29:50.052: INFO: Created: latency-svc-ftpdk
Mar  6 09:29:50.135: INFO: Created: latency-svc-vr4x5
Mar  6 09:29:50.140: INFO: Got endpoints: latency-svc-ftpdk [936.489344ms]
Mar  6 09:29:50.141: INFO: Got endpoints: latency-svc-ptt8w [1.358453116s]
Mar  6 09:29:50.152: INFO: Created: latency-svc-jzq6d
Mar  6 09:29:50.198: INFO: Got endpoints: latency-svc-vr4x5 [903.765928ms]
Mar  6 09:29:50.272: INFO: Got endpoints: latency-svc-jzq6d [977.262772ms]
Mar  6 09:29:50.636: INFO: Created: latency-svc-qgkvq
Mar  6 09:29:50.636: INFO: Created: latency-svc-vl5tz
Mar  6 09:29:50.687: INFO: Got endpoints: latency-svc-qgkvq [1.39244986s]
Mar  6 09:29:50.773: INFO: Got endpoints: latency-svc-vl5tz [1.47857964s]
Mar  6 09:29:50.833: INFO: Created: latency-svc-fbt9p
Mar  6 09:29:50.833: INFO: Created: latency-svc-tqbtt
Mar  6 09:29:50.834: INFO: Created: latency-svc-dhrkc
Mar  6 09:29:50.836: INFO: Created: latency-svc-l5z9x
Mar  6 09:29:51.107: INFO: Got endpoints: latency-svc-fbt9p [1.811714658s]
Mar  6 09:29:51.107: INFO: Got endpoints: latency-svc-tqbtt [1.71555164s]
Mar  6 09:29:51.108: INFO: Got endpoints: latency-svc-dhrkc [1.71579861s]
Mar  6 09:29:51.108: INFO: Got endpoints: latency-svc-l5z9x [1.716207704s]
Mar  6 09:29:51.109: INFO: Created: latency-svc-4gtm5
Mar  6 09:29:51.134: INFO: Created: latency-svc-485jc
Mar  6 09:29:51.189: INFO: Got endpoints: latency-svc-4gtm5 [1.725312539s]
Mar  6 09:29:51.202: INFO: Got endpoints: latency-svc-485jc [1.531894674s]
Mar  6 09:29:51.327: INFO: Created: latency-svc-k9g5t
Mar  6 09:29:51.372: INFO: Got endpoints: latency-svc-k9g5t [1.685101157s]
Mar  6 09:29:51.842: INFO: Created: latency-svc-b7scs
Mar  6 09:29:51.862: INFO: Created: latency-svc-rsckh
Mar  6 09:29:51.862: INFO: Created: latency-svc-kbhzt
Mar  6 09:29:51.862: INFO: Created: latency-svc-nl7xh
Mar  6 09:29:51.862: INFO: Created: latency-svc-qgmk2
Mar  6 09:29:51.862: INFO: Created: latency-svc-hlvdj
Mar  6 09:29:51.880: INFO: Got endpoints: latency-svc-b7scs [2.024078643s]
Mar  6 09:29:51.889: INFO: Created: latency-svc-rjbvg
Mar  6 09:29:51.917: INFO: Got endpoints: latency-svc-nl7xh [1.718405415s]
Mar  6 09:29:51.917: INFO: Got endpoints: latency-svc-hlvdj [1.77721292s]
Mar  6 09:29:51.917: INFO: Got endpoints: latency-svc-qgmk2 [2.042118405s]
Mar  6 09:29:51.940: INFO: Got endpoints: latency-svc-rsckh [1.798724232s]
Mar  6 09:29:51.954: INFO: Got endpoints: latency-svc-rjbvg [1.266569328s]
Mar  6 09:29:51.954: INFO: Got endpoints: latency-svc-kbhzt [1.681423428s]
Mar  6 09:29:52.059: INFO: Created: latency-svc-j5jdj
Mar  6 09:29:52.117: INFO: Got endpoints: latency-svc-j5jdj [1.344003373s]
Mar  6 09:29:52.214: INFO: Created: latency-svc-88kcw
Mar  6 09:29:52.231: INFO: Got endpoints: latency-svc-88kcw [1.123262308s]
Mar  6 09:29:52.236: INFO: Created: latency-svc-fflpm
Mar  6 09:29:52.292: INFO: Created: latency-svc-wm55k
Mar  6 09:29:52.294: INFO: Created: latency-svc-8hhzm
Mar  6 09:29:52.365: INFO: Got endpoints: latency-svc-fflpm [1.25729517s]
Mar  6 09:29:52.378: INFO: Got endpoints: latency-svc-wm55k [1.270589832s]
Mar  6 09:29:52.404: INFO: Created: latency-svc-hcq2k
Mar  6 09:29:52.409: INFO: Got endpoints: latency-svc-8hhzm [1.300358467s]
Mar  6 09:29:52.431: INFO: Got endpoints: latency-svc-hcq2k [1.242119729s]
Mar  6 09:29:52.538: INFO: Created: latency-svc-zr8rp
Mar  6 09:29:52.596: INFO: Created: latency-svc-ttpvk
Mar  6 09:29:52.621: INFO: Got endpoints: latency-svc-zr8rp [1.418684271s]
Mar  6 09:29:52.634: INFO: Got endpoints: latency-svc-ttpvk [1.26260485s]
Mar  6 09:29:52.779: INFO: Created: latency-svc-fhg8h
Mar  6 09:29:52.841: INFO: Got endpoints: latency-svc-fhg8h [960.969425ms]
Mar  6 09:29:52.855: INFO: Created: latency-svc-j2wdg
Mar  6 09:29:52.907: INFO: Got endpoints: latency-svc-j2wdg [990.029007ms]
Mar  6 09:29:52.907: INFO: Latencies: [221.803078ms 266.731683ms 278.618715ms 726.811104ms 741.888017ms 818.495936ms 818.565327ms 837.815865ms 859.382168ms 874.265118ms 897.425734ms 903.765928ms 931.232662ms 933.198028ms 936.489344ms 960.969425ms 977.262772ms 982.052827ms 989.64479ms 990.029007ms 997.487398ms 1.023459887s 1.051222438s 1.053295101s 1.073326374s 1.087706795s 1.087934827s 1.091757169s 1.09870265s 1.103761724s 1.105759891s 1.110293305s 1.114070905s 1.117129126s 1.123262308s 1.126391366s 1.127075762s 1.132964263s 1.139336966s 1.140099739s 1.148995508s 1.15935199s 1.164574243s 1.173072808s 1.175086172s 1.175641217s 1.177920264s 1.19282051s 1.195916418s 1.196118131s 1.201267415s 1.209429564s 1.217048993s 1.217653135s 1.219158688s 1.221290949s 1.223847253s 1.225679129s 1.226448383s 1.23877598s 1.241450877s 1.242119729s 1.252723521s 1.255502943s 1.255692584s 1.25729517s 1.26260485s 1.2638963s 1.263924481s 1.263924511s 1.266300211s 1.266569328s 1.267383115s 1.26850016s 1.270589832s 1.272197625s 1.27366703s 1.275786251s 1.276464052s 1.279339826s 1.280480677s 1.285330318s 1.291192535s 1.291287464s 1.291494553s 1.292245139s 1.292434829s 1.295026132s 1.295796908s 1.300358467s 1.301976376s 1.305397958s 1.306434431s 1.309130694s 1.309731108s 1.310259287s 1.310924338s 1.31175382s 1.311932517s 1.314542198s 1.315539742s 1.316456717s 1.31730784s 1.319180939s 1.320289933s 1.322569465s 1.323641129s 1.325019542s 1.327470159s 1.327815518s 1.332640635s 1.33405948s 1.334832608s 1.334883329s 1.335484908s 1.336822385s 1.33826293s 1.341811028s 1.342162252s 1.344003373s 1.348549941s 1.34935382s 1.350815388s 1.352702867s 1.353588238s 1.358453116s 1.360260789s 1.364815886s 1.377200176s 1.380310276s 1.38246679s 1.38434296s 1.38653265s 1.39244986s 1.395301756s 1.400347088s 1.402517158s 1.405975567s 1.418684271s 1.419051102s 1.420160985s 1.428446686s 1.42928046s 1.429533598s 1.436298567s 1.440384927s 1.443477238s 1.450762023s 1.459697248s 1.459902862s 1.46385152s 1.467744567s 1.468638802s 1.473953881s 1.47857964s 1.48106232s 1.486225163s 1.491684967s 1.498196138s 1.51445305s 1.521417512s 1.528328228s 1.529934829s 1.531894674s 1.534173433s 1.537576998s 1.538179751s 1.545017915s 1.55769462s 1.557813615s 1.558145686s 1.565741701s 1.574537856s 1.578796641s 1.582051009s 1.587184056s 1.604636745s 1.605052351s 1.609755437s 1.617457982s 1.617567378s 1.622186158s 1.651461651s 1.653578815s 1.681423428s 1.685101157s 1.698624926s 1.713103334s 1.71555164s 1.71579861s 1.716207704s 1.718405415s 1.725312539s 1.731597994s 1.765517393s 1.77721292s 1.798724232s 1.811714658s 2.024078643s 2.042118405s]
Mar  6 09:29:52.907: INFO: 50 %ile: 1.315539742s
Mar  6 09:29:52.907: INFO: 90 %ile: 1.617567378s
Mar  6 09:29:52.907: INFO: 99 %ile: 2.024078643s
Mar  6 09:29:52.907: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:29:52.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9219" for this suite.
Mar  6 09:30:46.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:30:47.064: INFO: namespace svc-latency-9219 deletion completed in 54.147125847s

• [SLOW TEST:76.837 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:30:47.065: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  6 09:30:47.366: INFO: Waiting up to 5m0s for pod "pod-bf985136-9d92-47b9-9421-a33c2cbae5f4" in namespace "emptydir-8027" to be "success or failure"
Mar  6 09:30:47.392: INFO: Pod "pod-bf985136-9d92-47b9-9421-a33c2cbae5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.928258ms
Mar  6 09:30:49.398: INFO: Pod "pod-bf985136-9d92-47b9-9421-a33c2cbae5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031739242s
Mar  6 09:30:51.409: INFO: Pod "pod-bf985136-9d92-47b9-9421-a33c2cbae5f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043180296s
STEP: Saw pod success
Mar  6 09:30:51.409: INFO: Pod "pod-bf985136-9d92-47b9-9421-a33c2cbae5f4" satisfied condition "success or failure"
Mar  6 09:30:51.413: INFO: Trying to get logs from node wisecloud-worker02 pod pod-bf985136-9d92-47b9-9421-a33c2cbae5f4 container test-container: <nil>
STEP: delete the pod
Mar  6 09:30:51.537: INFO: Waiting for pod pod-bf985136-9d92-47b9-9421-a33c2cbae5f4 to disappear
Mar  6 09:30:51.542: INFO: Pod pod-bf985136-9d92-47b9-9421-a33c2cbae5f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:30:51.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8027" for this suite.
Mar  6 09:30:57.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:30:57.718: INFO: namespace emptydir-8027 deletion completed in 6.168792222s

• [SLOW TEST:10.653 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:30:57.718: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f6299f84-b1c1-4e20-9452-fd88961338b4
STEP: Creating a pod to test consume secrets
Mar  6 09:30:58.105: INFO: Waiting up to 5m0s for pod "pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23" in namespace "secrets-7122" to be "success or failure"
Mar  6 09:30:58.111: INFO: Pod "pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23": Phase="Pending", Reason="", readiness=false. Elapsed: 5.875467ms
Mar  6 09:31:00.120: INFO: Pod "pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014092241s
Mar  6 09:31:02.125: INFO: Pod "pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019416677s
STEP: Saw pod success
Mar  6 09:31:02.125: INFO: Pod "pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23" satisfied condition "success or failure"
Mar  6 09:31:02.128: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:31:02.186: INFO: Waiting for pod pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23 to disappear
Mar  6 09:31:02.189: INFO: Pod pod-secrets-5a0b53c8-0ed6-497d-b5c4-0244079c9b23 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:31:02.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7122" for this suite.
Mar  6 09:31:08.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:31:08.319: INFO: namespace secrets-7122 deletion completed in 6.124992387s

• [SLOW TEST:10.601 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:31:08.319: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  6 09:31:08.621: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  6 09:31:17.695: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:31:17.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8288" for this suite.
Mar  6 09:31:23.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:31:24.070: INFO: namespace pods-8288 deletion completed in 6.363363458s

• [SLOW TEST:15.750 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:31:24.070: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:31:24.306: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:31:28.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-431" for this suite.
Mar  6 09:32:08.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:32:08.801: INFO: namespace pods-431 deletion completed in 40.170683252s

• [SLOW TEST:44.731 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:32:08.801: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 09:32:09.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-deployment --image=172.20.8.7/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1731'
Mar  6 09:32:09.218: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 09:32:09.218: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Mar  6 09:32:13.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1731'
Mar  6 09:32:13.502: INFO: stderr: ""
Mar  6 09:32:13.503: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:32:13.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1731" for this suite.
Mar  6 09:32:19.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:32:19.673: INFO: namespace kubectl-1731 deletion completed in 6.165585416s

• [SLOW TEST:10.872 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:32:19.674: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:32:19.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a" in namespace "projected-6613" to be "success or failure"
Mar  6 09:32:19.995: INFO: Pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172767ms
Mar  6 09:32:21.999: INFO: Pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0084093s
Mar  6 09:32:24.007: INFO: Pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01551802s
Mar  6 09:32:26.012: INFO: Pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021444264s
STEP: Saw pod success
Mar  6 09:32:26.013: INFO: Pod "downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a" satisfied condition "success or failure"
Mar  6 09:32:26.018: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a container client-container: <nil>
STEP: delete the pod
Mar  6 09:32:26.116: INFO: Waiting for pod downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a to disappear
Mar  6 09:32:26.142: INFO: Pod downwardapi-volume-5ba27be1-6170-4d8d-afd2-a7b8d7c4d56a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:32:26.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6613" for this suite.
Mar  6 09:32:32.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:32:32.323: INFO: namespace projected-6613 deletion completed in 6.175100433s

• [SLOW TEST:12.650 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:32:32.324: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f7c47e8e-9e62-4f55-b06d-05683f3d1c66
STEP: Creating a pod to test consume configMaps
Mar  6 09:32:32.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551" in namespace "configmap-6199" to be "success or failure"
Mar  6 09:32:32.700: INFO: Pod "pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616788ms
Mar  6 09:32:34.705: INFO: Pod "pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00940784s
Mar  6 09:32:36.708: INFO: Pod "pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012867939s
STEP: Saw pod success
Mar  6 09:32:36.709: INFO: Pod "pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551" satisfied condition "success or failure"
Mar  6 09:32:36.711: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:32:36.761: INFO: Waiting for pod pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551 to disappear
Mar  6 09:32:36.764: INFO: Pod pod-configmaps-4897ecf9-7459-492f-aea9-d16ccd620551 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:32:36.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6199" for this suite.
Mar  6 09:32:42.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:32:42.964: INFO: namespace configmap-6199 deletion completed in 6.194991813s

• [SLOW TEST:10.641 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:32:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:32:43.382: INFO: (0) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.704358ms)
Mar  6 09:32:43.388: INFO: (1) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.997677ms)
Mar  6 09:32:43.440: INFO: (2) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 51.624667ms)
Mar  6 09:32:43.458: INFO: (3) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 18.214924ms)
Mar  6 09:32:43.464: INFO: (4) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.096205ms)
Mar  6 09:32:43.470: INFO: (5) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.350767ms)
Mar  6 09:32:43.475: INFO: (6) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.686244ms)
Mar  6 09:32:43.481: INFO: (7) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.46652ms)
Mar  6 09:32:43.486: INFO: (8) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.448662ms)
Mar  6 09:32:43.491: INFO: (9) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.5419ms)
Mar  6 09:32:43.496: INFO: (10) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.092681ms)
Mar  6 09:32:43.501: INFO: (11) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.322514ms)
Mar  6 09:32:43.506: INFO: (12) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.414356ms)
Mar  6 09:32:43.512: INFO: (13) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.77095ms)
Mar  6 09:32:43.518: INFO: (14) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.99972ms)
Mar  6 09:32:43.524: INFO: (15) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.849428ms)
Mar  6 09:32:43.530: INFO: (16) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.072907ms)
Mar  6 09:32:43.535: INFO: (17) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.526645ms)
Mar  6 09:32:43.564: INFO: (18) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 28.866053ms)
Mar  6 09:32:43.570: INFO: (19) /api/v1/nodes/wisecloud-worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.375929ms)
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:32:43.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3341" for this suite.
Mar  6 09:32:49.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:32:49.747: INFO: namespace proxy-3341 deletion completed in 6.170536362s

• [SLOW TEST:6.783 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:32:49.748: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5e9a2621-157d-440f-9af3-297cf53f8f63
STEP: Creating a pod to test consume configMaps
Mar  6 09:32:50.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8" in namespace "configmap-7493" to be "success or failure"
Mar  6 09:32:50.053: INFO: Pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.839175ms
Mar  6 09:32:52.057: INFO: Pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007285193s
Mar  6 09:32:54.063: INFO: Pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013100771s
Mar  6 09:32:56.069: INFO: Pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019049598s
STEP: Saw pod success
Mar  6 09:32:56.069: INFO: Pod "pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8" satisfied condition "success or failure"
Mar  6 09:32:56.073: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 09:32:56.134: INFO: Waiting for pod pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8 to disappear
Mar  6 09:32:56.138: INFO: Pod pod-configmaps-354e66e4-02ad-4da2-b431-e6f746ccd3d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:32:56.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7493" for this suite.
Mar  6 09:33:02.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:33:02.303: INFO: namespace configmap-7493 deletion completed in 6.157755017s

• [SLOW TEST:12.555 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:33:02.303: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Mar  6 09:33:02.555: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201274422 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:33:02.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8193" for this suite.
Mar  6 09:33:08.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:33:08.801: INFO: namespace kubectl-8193 deletion completed in 6.130473142s

• [SLOW TEST:6.498 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:33:08.802: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  6 09:33:13.854: INFO: Successfully updated pod "labelsupdate235b061b-658a-40e2-ba64-ea995adbc6f7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:33:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8704" for this suite.
Mar  6 09:33:41.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:33:42.067: INFO: namespace projected-8704 deletion completed in 24.162834356s

• [SLOW TEST:33.265 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:33:42.067: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  6 09:33:45.548: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:33:45.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1601" for this suite.
Mar  6 09:33:51.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:33:51.832: INFO: namespace container-runtime-1601 deletion completed in 6.222226378s

• [SLOW TEST:9.765 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:33:51.832: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  6 09:33:52.139: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:52.139: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:52.139: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:52.189: INFO: Number of nodes with available pods: 0
Mar  6 09:33:52.189: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:33:53.198: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:53.198: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:53.198: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:53.203: INFO: Number of nodes with available pods: 0
Mar  6 09:33:53.203: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:33:54.509: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:54.509: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:54.509: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:54.535: INFO: Number of nodes with available pods: 0
Mar  6 09:33:54.535: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 09:33:55.202: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:55.202: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:55.202: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:55.226: INFO: Number of nodes with available pods: 1
Mar  6 09:33:55.226: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:33:56.196: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.196: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.196: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.200: INFO: Number of nodes with available pods: 2
Mar  6 09:33:56.200: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  6 09:33:56.280: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.280: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.280: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:56.289: INFO: Number of nodes with available pods: 1
Mar  6 09:33:56.289: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:33:57.297: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:57.297: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:57.297: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:57.302: INFO: Number of nodes with available pods: 1
Mar  6 09:33:57.302: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:33:58.306: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:58.306: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:58.306: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:58.310: INFO: Number of nodes with available pods: 1
Mar  6 09:33:58.310: INFO: Node wisecloud-worker02 is running more than one daemon pod
Mar  6 09:33:59.298: INFO: DaemonSet pods can't tolerate node wisecloud-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:59.298: INFO: DaemonSet pods can't tolerate node wisecloud-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:59.298: INFO: DaemonSet pods can't tolerate node wisecloud-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  6 09:33:59.304: INFO: Number of nodes with available pods: 2
Mar  6 09:33:59.304: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-984, will wait for the garbage collector to delete the pods
Mar  6 09:33:59.393: INFO: Deleting DaemonSet.extensions daemon-set took: 24.641415ms
Mar  6 09:33:59.793: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.207885ms
Mar  6 09:34:08.498: INFO: Number of nodes with available pods: 0
Mar  6 09:34:08.498: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 09:34:08.524: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-984/daemonsets","resourceVersion":"26651"},"items":null}

Mar  6 09:34:08.528: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-984/pods","resourceVersion":"26651"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:34:08.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-984" for this suite.
Mar  6 09:34:14.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:34:14.768: INFO: namespace daemonsets-984 deletion completed in 6.218199659s

• [SLOW TEST:22.936 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:34:14.769: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-38eaceac-7d24-4a93-9281-46975b30c4b0
STEP: Creating a pod to test consume secrets
Mar  6 09:34:15.105: INFO: Waiting up to 5m0s for pod "pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2" in namespace "secrets-410" to be "success or failure"
Mar  6 09:34:15.109: INFO: Pod "pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195889ms
Mar  6 09:34:17.115: INFO: Pod "pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009596306s
Mar  6 09:34:19.121: INFO: Pod "pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016285227s
STEP: Saw pod success
Mar  6 09:34:19.121: INFO: Pod "pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2" satisfied condition "success or failure"
Mar  6 09:34:19.156: INFO: Trying to get logs from node wisecloud-worker02 pod pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2 container secret-volume-test: <nil>
STEP: delete the pod
Mar  6 09:34:19.248: INFO: Waiting for pod pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2 to disappear
Mar  6 09:34:19.252: INFO: Pod pod-secrets-f37cc848-3681-4656-9063-4b193cfc33c2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:34:19.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-410" for this suite.
Mar  6 09:34:25.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:34:25.430: INFO: namespace secrets-410 deletion completed in 6.171700584s

• [SLOW TEST:10.661 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:34:25.430: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  6 09:34:25.692: INFO: Waiting up to 5m0s for pod "pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21" in namespace "emptydir-5421" to be "success or failure"
Mar  6 09:34:25.695: INFO: Pod "pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21": Phase="Pending", Reason="", readiness=false. Elapsed: 3.322051ms
Mar  6 09:34:27.700: INFO: Pod "pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007995105s
Mar  6 09:34:29.705: INFO: Pod "pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013264106s
STEP: Saw pod success
Mar  6 09:34:29.705: INFO: Pod "pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21" satisfied condition "success or failure"
Mar  6 09:34:29.710: INFO: Trying to get logs from node wisecloud-worker02 pod pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21 container test-container: <nil>
STEP: delete the pod
Mar  6 09:34:29.784: INFO: Waiting for pod pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21 to disappear
Mar  6 09:34:29.788: INFO: Pod pod-6fe34eb8-efb6-4f8b-a924-f9d16f394d21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:34:29.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5421" for this suite.
Mar  6 09:34:35.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:34:36.085: INFO: namespace emptydir-5421 deletion completed in 6.288898389s

• [SLOW TEST:10.655 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:34:36.085: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Mar  6 09:34:40.407: INFO: Pod pod-hostip-7320571c-c476-4043-8870-e1cef60fc7e1 has hostIP: 172.20.8.6
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:34:40.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2901" for this suite.
Mar  6 09:35:06.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:35:06.554: INFO: namespace pods-2901 deletion completed in 26.142112782s

• [SLOW TEST:30.469 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:35:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:35:06.795: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  6 09:35:06.832: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  6 09:35:11.839: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  6 09:35:11.839: INFO: Creating deployment "test-rolling-update-deployment"
Mar  6 09:35:11.867: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  6 09:35:11.887: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  6 09:35:13.896: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  6 09:35:13.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719084111, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719084111, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719084112, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719084111, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5b5b44df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  6 09:35:15.906: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  6 09:35:15.921: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4259,SelfLink:/apis/apps/v1/namespaces/deployment-4259/deployments/test-rolling-update-deployment,UID:d66e0444-1499-40ec-b530-82c9a46bdcbb,ResourceVersion:26926,Generation:1,CreationTimestamp:2020-03-06 09:35:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-06 09:35:11 +0000 UTC 2020-03-06 09:35:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-06 09:35:15 +0000 UTC 2020-03-06 09:35:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-5b5b44df46" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  6 09:35:15.927: INFO: New ReplicaSet "test-rolling-update-deployment-5b5b44df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-5b5b44df46,GenerateName:,Namespace:deployment-4259,SelfLink:/apis/apps/v1/namespaces/deployment-4259/replicasets/test-rolling-update-deployment-5b5b44df46,UID:821c7dc2-d263-4efd-9fae-904a761e9826,ResourceVersion:26916,Generation:1,CreationTimestamp:2020-03-06 09:35:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d66e0444-1499-40ec-b530-82c9a46bdcbb 0xc000c06757 0xc000c06758}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  6 09:35:15.927: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  6 09:35:15.927: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4259,SelfLink:/apis/apps/v1/namespaces/deployment-4259/replicasets/test-rolling-update-controller,UID:8a9cf45f-19be-43fb-ac90-c181025af79a,ResourceVersion:26924,Generation:2,CreationTimestamp:2020-03-06 09:35:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d66e0444-1499-40ec-b530-82c9a46bdcbb 0xc000c06687 0xc000c06688}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  6 09:35:15.935: INFO: Pod "test-rolling-update-deployment-5b5b44df46-qvcjf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-5b5b44df46-qvcjf,GenerateName:test-rolling-update-deployment-5b5b44df46-,Namespace:deployment-4259,SelfLink:/api/v1/namespaces/deployment-4259/pods/test-rolling-update-deployment-5b5b44df46-qvcjf,UID:2c3324cd-ba9f-4e6d-9dd2-c534d4e8e625,ResourceVersion:26915,Generation:0,CreationTimestamp:2020-03-06 09:35:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-5b5b44df46 821c7dc2-d263-4efd-9fae-904a761e9826 0xc000c07037 0xc000c07038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b79lv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b79lv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-b79lv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wisecloud-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c070b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c070d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:11 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.222,StartTime:2020-03-06 09:35:12 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-06 09:35:14 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/redis:1.0 docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://083cf6422d161a581782c43693e9021cddb2d2a06ad2da3cfb20fdcff78f5534}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:35:15.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4259" for this suite.
Mar  6 09:35:21.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:35:22.199: INFO: namespace deployment-4259 deletion completed in 6.25509308s

• [SLOW TEST:15.644 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:35:22.199: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7051
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-7051
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7051
Mar  6 09:35:22.541: INFO: Found 0 stateful pods, waiting for 1
Mar  6 09:35:32.553: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  6 09:35:32.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 09:35:33.090: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 09:35:33.090: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 09:35:33.090: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 09:35:33.097: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 09:35:43.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 09:35:43.105: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 09:35:43.162: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:35:43.162: INFO: ss-0  wisecloud-worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  }]
Mar  6 09:35:43.162: INFO: 
Mar  6 09:35:43.162: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  6 09:35:44.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981186222s
Mar  6 09:35:45.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974775805s
Mar  6 09:35:46.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968535997s
Mar  6 09:35:47.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962041225s
Mar  6 09:35:48.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95352766s
Mar  6 09:35:49.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946841395s
Mar  6 09:35:50.216: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940409463s
Mar  6 09:35:51.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.926881981s
Mar  6 09:35:52.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 916.695353ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7051
Mar  6 09:35:53.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:35:53.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 09:35:53.621: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 09:35:53.621: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 09:35:53.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:35:53.953: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  6 09:35:53.954: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 09:35:53.954: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 09:35:53.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:35:54.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  6 09:35:54.338: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 09:35:54.338: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 09:35:54.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 09:35:54.342: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 09:35:54.342: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  6 09:35:54.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 09:35:54.739: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 09:35:54.739: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 09:35:54.739: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 09:35:54.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 09:35:55.179: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 09:35:55.179: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 09:35:55.179: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 09:35:55.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 09:35:55.589: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 09:35:55.589: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 09:35:55.589: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 09:35:55.589: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 09:35:55.595: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  6 09:36:05.609: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 09:36:05.609: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 09:36:05.609: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 09:36:05.701: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:05.701: INFO: ss-0  wisecloud-worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  }]
Mar  6 09:36:05.702: INFO: ss-1  wisecloud-worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:05.702: INFO: ss-2  wisecloud-worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:05.702: INFO: 
Mar  6 09:36:05.702: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 09:36:06.709: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:06.709: INFO: ss-0  wisecloud-worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  }]
Mar  6 09:36:06.709: INFO: ss-1  wisecloud-worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:06.709: INFO: ss-2  wisecloud-worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:06.709: INFO: 
Mar  6 09:36:06.709: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 09:36:07.716: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:07.716: INFO: ss-0  wisecloud-worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  }]
Mar  6 09:36:07.716: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:07.716: INFO: ss-2  wisecloud-worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:07.716: INFO: 
Mar  6 09:36:07.716: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 09:36:08.721: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:08.721: INFO: ss-0  wisecloud-worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:22 +0000 UTC  }]
Mar  6 09:36:08.721: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:08.721: INFO: ss-2  wisecloud-worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:08.721: INFO: 
Mar  6 09:36:08.721: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  6 09:36:09.726: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:09.726: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:09.726: INFO: 
Mar  6 09:36:09.726: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  6 09:36:10.733: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:10.733: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:10.733: INFO: 
Mar  6 09:36:10.733: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  6 09:36:11.738: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:11.739: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:11.739: INFO: 
Mar  6 09:36:11.739: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  6 09:36:12.743: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:12.743: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:12.744: INFO: 
Mar  6 09:36:12.744: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  6 09:36:13.763: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:13.763: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:13.763: INFO: 
Mar  6 09:36:13.763: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  6 09:36:14.769: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Mar  6 09:36:14.769: INFO: ss-1  wisecloud-worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-06 09:35:43 +0000 UTC  }]
Mar  6 09:36:14.769: INFO: 
Mar  6 09:36:14.769: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7051
Mar  6 09:36:15.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:36:15.962: INFO: rc: 1
Mar  6 09:36:15.962: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002c46660 exit status 1 <nil> <nil> true [0xc002bb1f98 0xc002bb1fb0 0xc002bb1fc8] [0xc002bb1f98 0xc002bb1fb0 0xc002bb1fc8] [0xc002bb1fa8 0xc002bb1fc0] [0xba6ac0 0xba6ac0] 0xc001fdc240 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Mar  6 09:36:25.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:36:27.445: INFO: rc: 1
Mar  6 09:36:27.445: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0027d2e10 exit status 1 <nil> <nil> true [0xc0006825e0 0xc0006825f8 0xc000682610] [0xc0006825e0 0xc0006825f8 0xc000682610] [0xc0006825f0 0xc000682608] [0xba6ac0 0xba6ac0] 0xc0028355c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:36:37.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:36:37.571: INFO: rc: 1
Mar  6 09:36:37.571: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c469f0 exit status 1 <nil> <nil> true [0xc002bb1fd0 0xc002bb1fe8 0xc001ab8000] [0xc002bb1fd0 0xc002bb1fe8 0xc001ab8000] [0xc002bb1fe0 0xc002bb1ff8] [0xba6ac0 0xba6ac0] 0xc001fdcea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:36:47.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:36:47.701: INFO: rc: 1
Mar  6 09:36:47.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c46d80 exit status 1 <nil> <nil> true [0xc001ab8008 0xc001ab8020 0xc001ab8038] [0xc001ab8008 0xc001ab8020 0xc001ab8038] [0xc001ab8018 0xc001ab8030] [0xba6ac0 0xba6ac0] 0xc003000180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:36:57.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:36:57.863: INFO: rc: 1
Mar  6 09:36:57.863: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da2330 exit status 1 <nil> <nil> true [0xc0024900a8 0xc002490158 0xc002490190] [0xc0024900a8 0xc002490158 0xc002490190] [0xc002490128 0xc002490178] [0xba6ac0 0xba6ac0] 0xc001fdca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:07.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:08.013: INFO: rc: 1
Mar  6 09:37:08.014: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c8a2d0 exit status 1 <nil> <nil> true [0xc000c1e010 0xc000c1e5e0 0xc000c1e748] [0xc000c1e010 0xc000c1e5e0 0xc000c1e748] [0xc000c1e570 0xc000c1e690] [0xba6ac0 0xba6ac0] 0xc002748d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:18.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:18.141: INFO: rc: 1
Mar  6 09:37:18.141: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c8a660 exit status 1 <nil> <nil> true [0xc000c1e8a0 0xc000c1efe0 0xc000c1f268] [0xc000c1e8a0 0xc000c1efe0 0xc000c1f268] [0xc000c1ef10 0xc000c1f120] [0xba6ac0 0xba6ac0] 0xc0027495c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:28.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:28.278: INFO: rc: 1
Mar  6 09:37:28.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da2690 exit status 1 <nil> <nil> true [0xc0024901b0 0xc0024901f8 0xc002490258] [0xc0024901b0 0xc0024901f8 0xc002490258] [0xc0024901d8 0xc002490240] [0xba6ac0 0xba6ac0] 0xc001fddb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:38.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:38.436: INFO: rc: 1
Mar  6 09:37:38.436: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732330 exit status 1 <nil> <nil> true [0xc0000d80a8 0xc0000d8c08 0xc0000d8d68] [0xc0000d80a8 0xc0000d8c08 0xc0000d8d68] [0xc0000d89f8 0xc0000d8c70] [0xba6ac0 0xba6ac0] 0xc002085380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:48.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:48.565: INFO: rc: 1
Mar  6 09:37:48.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732690 exit status 1 <nil> <nil> true [0xc0000d8f20 0xc0000d8fd0 0xc0000d9170] [0xc0000d8f20 0xc0000d8fd0 0xc0000d9170] [0xc0000d8fb0 0xc0000d9150] [0xba6ac0 0xba6ac0] 0xc00036b080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:37:58.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:37:58.699: INFO: rc: 1
Mar  6 09:37:58.699: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c8ab40 exit status 1 <nil> <nil> true [0xc000c1f310 0xc000c1f478 0xc000c1f5e8] [0xc000c1f310 0xc000c1f478 0xc000c1f5e8] [0xc000c1f428 0xc000c1f580] [0xba6ac0 0xba6ac0] 0xc002749ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:08.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:08.838: INFO: rc: 1
Mar  6 09:38:08.838: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732a20 exit status 1 <nil> <nil> true [0xc0000d9180 0xc0000d93b0 0xc0000d93e0] [0xc0000d9180 0xc0000d93b0 0xc0000d93e0] [0xc0000d9378 0xc0000d93d0] [0xba6ac0 0xba6ac0] 0xc001f6cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:18.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:18.981: INFO: rc: 1
Mar  6 09:38:18.981: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da2a20 exit status 1 <nil> <nil> true [0xc002490278 0xc0024902c8 0xc002490328] [0xc002490278 0xc0024902c8 0xc002490328] [0xc0024902b8 0xc002490308] [0xba6ac0 0xba6ac0] 0xc001f015c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:28.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:29.081: INFO: rc: 1
Mar  6 09:38:29.081: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da2db0 exit status 1 <nil> <nil> true [0xc002490340 0xc0024903a8 0xc0024903d8] [0xc002490340 0xc0024903a8 0xc0024903d8] [0xc002490378 0xc0024903d0] [0xba6ac0 0xba6ac0] 0xc001d16240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:39.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:39.241: INFO: rc: 1
Mar  6 09:38:39.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732db0 exit status 1 <nil> <nil> true [0xc0000d9478 0xc0000d9548 0xc0000d9600] [0xc0000d9478 0xc0000d9548 0xc0000d9600] [0xc0000d9520 0xc0000d95f0] [0xba6ac0 0xba6ac0] 0xc0012e9b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:49.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:49.387: INFO: rc: 1
Mar  6 09:38:49.387: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da3140 exit status 1 <nil> <nil> true [0xc0024904b8 0xc0024904e8 0xc002490510] [0xc0024904b8 0xc0024904e8 0xc002490510] [0xc0024904d8 0xc002490500] [0xba6ac0 0xba6ac0] 0xc0022d4d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:38:59.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:38:59.504: INFO: rc: 1
Mar  6 09:38:59.504: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da22a0 exit status 1 <nil> <nil> true [0xc0024900a8 0xc002490158 0xc002490190] [0xc0024900a8 0xc002490158 0xc002490190] [0xc002490128 0xc002490178] [0xba6ac0 0xba6ac0] 0xc001d93ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:39:09.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:39:09.672: INFO: rc: 1
Mar  6 09:39:09.672: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002438360 exit status 1 <nil> <nil> true [0xc0000d80a8 0xc0000d8c08 0xc0000d8d68] [0xc0000d80a8 0xc0000d8c08 0xc0000d8d68] [0xc0000d89f8 0xc0000d8c70] [0xba6ac0 0xba6ac0] 0xc001f00000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:39:19.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:39:19.794: INFO: rc: 1
Mar  6 09:39:19.794: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732390 exit status 1 <nil> <nil> true [0xc000c1e010 0xc000c1e5e0 0xc000c1e748] [0xc000c1e010 0xc000c1e5e0 0xc000c1e748] [0xc000c1e570 0xc000c1e690] [0xba6ac0 0xba6ac0] 0xc001f6d800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:39:29.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:39:29.942: INFO: rc: 1
Mar  6 09:39:29.942: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732720 exit status 1 <nil> <nil> true [0xc000c1e8a0 0xc000c1efe0 0xc000c1f268] [0xc000c1e8a0 0xc000c1efe0 0xc000c1f268] [0xc000c1ef10 0xc000c1f120] [0xba6ac0 0xba6ac0] 0xc00036a4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:39:39.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:39:40.163: INFO: rc: 1
Mar  6 09:39:40.163: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732ae0 exit status 1 <nil> <nil> true [0xc000c1f310 0xc000c1f478 0xc000c1f5e8] [0xc000c1f310 0xc000c1f478 0xc000c1f5e8] [0xc000c1f428 0xc000c1f580] [0xba6ac0 0xba6ac0] 0xc002084ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:39:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:39:50.322: INFO: rc: 1
Mar  6 09:39:50.322: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001732ea0 exit status 1 <nil> <nil> true [0xc000c1f630 0xc000c1f760 0xc000c1f8c8] [0xc000c1f630 0xc000c1f760 0xc000c1f8c8] [0xc000c1f6b8 0xc000c1f8c0] [0xba6ac0 0xba6ac0] 0xc001fdc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:00.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:00.471: INFO: rc: 1
Mar  6 09:40:00.471: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0024387b0 exit status 1 <nil> <nil> true [0xc0000d8f20 0xc0000d8fd0 0xc0000d9170] [0xc0000d8f20 0xc0000d8fd0 0xc0000d9170] [0xc0000d8fb0 0xc0000d9150] [0xba6ac0 0xba6ac0] 0xc0022d5da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:10.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:10.592: INFO: rc: 1
Mar  6 09:40:10.592: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001733200 exit status 1 <nil> <nil> true [0xc000c1f8d8 0xc000c1f978 0xc000c1fa50] [0xc000c1f8d8 0xc000c1f978 0xc000c1fa50] [0xc000c1f930 0xc000c1f9f8] [0xba6ac0 0xba6ac0] 0xc001fdcfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:20.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:20.745: INFO: rc: 1
Mar  6 09:40:20.745: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001733590 exit status 1 <nil> <nil> true [0xc000c1fab8 0xc000c1fb28 0xc000c1fbe0] [0xc000c1fab8 0xc000c1fb28 0xc000c1fbe0] [0xc000c1fb18 0xc000c1fb78] [0xba6ac0 0xba6ac0] 0xc0027484e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:30.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:30.850: INFO: rc: 1
Mar  6 09:40:30.850: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c8a3c0 exit status 1 <nil> <nil> true [0xc000686008 0xc000687618 0xc000687770] [0xc000686008 0xc000687618 0xc000687770] [0xc000687478 0xc000687728] [0xba6ac0 0xba6ac0] 0xc001a50600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:40.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:40.959: INFO: rc: 1
Mar  6 09:40:40.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0017338f0 exit status 1 <nil> <nil> true [0xc000c1fd30 0xc00035e0b0 0xc00035e1d8] [0xc000c1fd30 0xc00035e0b0 0xc00035e1d8] [0xc000010010 0xc00035e1b8] [0xba6ac0 0xba6ac0] 0xc002749140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:40:50.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:40:51.089: INFO: rc: 1
Mar  6 09:40:51.089: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002da2720 exit status 1 <nil> <nil> true [0xc0024901b0 0xc0024901f8 0xc002490258] [0xc0024901b0 0xc0024901f8 0xc002490258] [0xc0024901d8 0xc002490240] [0xba6ac0 0xba6ac0] 0xc001736840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:41:01.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:41:01.220: INFO: rc: 1
Mar  6 09:41:01.220: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0024382d0 exit status 1 <nil> <nil> true [0xc000c1e3f0 0xc000c1e660 0xc000c1e8a0] [0xc000c1e3f0 0xc000c1e660 0xc000c1e8a0] [0xc000c1e5e0 0xc000c1e748] [0xba6ac0 0xba6ac0] 0xc001fdc240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:41:11.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:41:11.383: INFO: rc: 1
Mar  6 09:41:11.383: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c8a360 exit status 1 <nil> <nil> true [0xc000010010 0xc0000d89f8 0xc0000d8c70] [0xc000010010 0xc0000d89f8 0xc0000d8c70] [0xc0000d88c0 0xc0000d8c40] [0xba6ac0 0xba6ac0] 0xc002085380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  6 09:41:21.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-7051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 09:41:21.507: INFO: rc: 1
Mar  6 09:41:21.507: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Mar  6 09:41:21.507: INFO: Scaling statefulset ss to 0
Mar  6 09:41:21.520: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  6 09:41:21.524: INFO: Deleting all statefulset in ns statefulset-7051
Mar  6 09:41:21.528: INFO: Scaling statefulset ss to 0
Mar  6 09:41:21.539: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 09:41:21.542: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:41:21.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7051" for this suite.
Mar  6 09:41:29.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:41:29.758: INFO: namespace statefulset-7051 deletion completed in 8.156050515s

• [SLOW TEST:367.559 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:41:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  6 09:41:30.028: INFO: Waiting up to 5m0s for pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15" in namespace "downward-api-2775" to be "success or failure"
Mar  6 09:41:30.033: INFO: Pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.226274ms
Mar  6 09:41:32.039: INFO: Pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010990784s
Mar  6 09:41:34.044: INFO: Pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015926973s
Mar  6 09:41:36.049: INFO: Pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020910912s
STEP: Saw pod success
Mar  6 09:41:36.049: INFO: Pod "downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15" satisfied condition "success or failure"
Mar  6 09:41:36.053: INFO: Trying to get logs from node wisecloud-worker02 pod downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15 container dapi-container: <nil>
STEP: delete the pod
Mar  6 09:41:36.132: INFO: Waiting for pod downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15 to disappear
Mar  6 09:41:36.136: INFO: Pod downward-api-aea6e85d-cbe4-4680-8064-44a4fc994a15 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:41:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2775" for this suite.
Mar  6 09:41:42.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:41:42.281: INFO: namespace downward-api-2775 deletion completed in 6.139347803s

• [SLOW TEST:12.522 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:41:42.281: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3485
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 09:41:42.611: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 09:42:08.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.226:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3485 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:42:08.827: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:42:09.074: INFO: Found all expected endpoints: [netserver-0]
Mar  6 09:42:09.080: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.91:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3485 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:42:09.080: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:42:09.319: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:42:09.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3485" for this suite.
Mar  6 09:42:33.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:42:33.546: INFO: namespace pod-network-test-3485 deletion completed in 24.219306483s

• [SLOW TEST:51.265 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:42:33.547: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  6 09:42:34.028: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:42:37.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-246" for this suite.
Mar  6 09:42:43.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:42:43.751: INFO: namespace init-container-246 deletion completed in 6.253896395s

• [SLOW TEST:10.204 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:42:43.751: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ff9f4c9c-1980-4796-a6ff-bf4fc8507811 in namespace container-probe-177
Mar  6 09:42:48.152: INFO: Started pod busybox-ff9f4c9c-1980-4796-a6ff-bf4fc8507811 in namespace container-probe-177
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 09:42:48.157: INFO: Initial restart count of pod busybox-ff9f4c9c-1980-4796-a6ff-bf4fc8507811 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:46:49.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-177" for this suite.
Mar  6 09:46:55.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:46:55.659: INFO: namespace container-probe-177 deletion completed in 6.519354782s

• [SLOW TEST:251.908 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:46:55.659: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  6 09:46:56.982: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:46:56.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 09:46:56.982029      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5017" for this suite.
Mar  6 09:47:05.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:47:05.181: INFO: namespace gc-5017 deletion completed in 8.192861584s

• [SLOW TEST:9.522 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:47:05.182: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 09:47:05.436: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:47:11.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9222" for this suite.
Mar  6 09:48:05.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:48:05.658: INFO: namespace pods-9222 deletion completed in 54.137532579s

• [SLOW TEST:60.477 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:48:05.659: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  6 09:48:05.913: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  6 09:48:05.927: INFO: Waiting for terminating namespaces to be deleted...
Mar  6 09:48:05.930: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker01 before test
Mar  6 09:48:05.943: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-4sk55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:48:05.944: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 09:48:05.944: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  6 09:48:05.944: INFO: kube-proxy-7b546 from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 09:48:05.944: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 09:48:05.944: INFO: kube-flannel-ds-amd64-dp8qs from kube-system started at 2020-03-06 07:21:56 +0000 UTC (1 container statuses recorded)
Mar  6 09:48:05.944: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  6 09:48:05.944: INFO: sonobuoy-e2e-job-2be1ed9b02254e55 from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:48:05.944: INFO: 	Container e2e ready: true, restart count 0
Mar  6 09:48:05.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  6 09:48:05.944: INFO: 
Logging pods the kubelet thinks is on node wisecloud-worker02 before test
Mar  6 09:48:05.954: INFO: kube-proxy-hb4sp from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 09:48:05.954: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  6 09:48:05.954: INFO: sonobuoy from sonobuoy started at 2020-03-06 08:20:17 +0000 UTC (1 container statuses recorded)
Mar  6 09:48:05.954: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  6 09:48:05.954: INFO: sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-p6r2j from sonobuoy started at 2020-03-06 08:20:21 +0000 UTC (2 container statuses recorded)
Mar  6 09:48:05.954: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  6 09:48:05.954: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  6 09:48:05.954: INFO: kube-flannel-ds-amd64-f5d4f from kube-system started at 2020-03-06 07:21:58 +0000 UTC (1 container statuses recorded)
Mar  6 09:48:05.954: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f9ae064a6bb236], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:48:07.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-553" for this suite.
Mar  6 09:48:13.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:48:13.430: INFO: namespace sched-pred-553 deletion completed in 6.357450657s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.772 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:48:13.431: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Mar  6 09:48:13.745: INFO: Waiting up to 5m0s for pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d" in namespace "containers-6196" to be "success or failure"
Mar  6 09:48:13.748: INFO: Pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682073ms
Mar  6 09:48:15.753: INFO: Pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008502519s
Mar  6 09:48:17.758: INFO: Pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013114906s
Mar  6 09:48:19.763: INFO: Pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017956849s
STEP: Saw pod success
Mar  6 09:48:19.763: INFO: Pod "client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d" satisfied condition "success or failure"
Mar  6 09:48:19.766: INFO: Trying to get logs from node wisecloud-worker02 pod client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d container test-container: <nil>
STEP: delete the pod
Mar  6 09:48:19.840: INFO: Waiting for pod client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d to disappear
Mar  6 09:48:19.844: INFO: Pod client-containers-1afbec9f-7287-44fd-bbf3-59a62887c14d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:48:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6196" for this suite.
Mar  6 09:48:25.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:48:26.078: INFO: namespace containers-6196 deletion completed in 6.227724085s

• [SLOW TEST:12.647 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:48:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  6 09:48:32.489: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:48:32.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4481" for this suite.
Mar  6 09:48:40.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:48:40.792: INFO: namespace container-runtime-4481 deletion completed in 8.166538936s

• [SLOW TEST:14.714 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:48:40.793: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4990
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  6 09:48:41.080: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  6 09:49:07.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.234:8080/dial?request=hostName&protocol=udp&host=10.244.3.94&port=8081&tries=1'] Namespace:pod-network-test-4990 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:49:07.483: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:49:07.701: INFO: Waiting for endpoints: map[]
Mar  6 09:49:07.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.234:8080/dial?request=hostName&protocol=udp&host=10.244.4.233&port=8081&tries=1'] Namespace:pod-network-test-4990 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  6 09:49:07.706: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
Mar  6 09:49:07.942: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:49:07.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4990" for this suite.
Mar  6 09:49:31.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:49:32.151: INFO: namespace pod-network-test-4990 deletion completed in 24.186204466s

• [SLOW TEST:51.358 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:49:32.151: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8693
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8693 to expose endpoints map[]
Mar  6 09:49:33.055: INFO: successfully validated that service endpoint-test2 in namespace services-8693 exposes endpoints map[] (270.830991ms elapsed)
STEP: Creating pod pod1 in namespace services-8693
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8693 to expose endpoints map[pod1:[80]]
Mar  6 09:49:37.266: INFO: successfully validated that service endpoint-test2 in namespace services-8693 exposes endpoints map[pod1:[80]] (4.172192027s elapsed)
STEP: Creating pod pod2 in namespace services-8693
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8693 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  6 09:49:40.384: INFO: successfully validated that service endpoint-test2 in namespace services-8693 exposes endpoints map[pod1:[80] pod2:[80]] (3.071309674s elapsed)
STEP: Deleting pod pod1 in namespace services-8693
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8693 to expose endpoints map[pod2:[80]]
Mar  6 09:49:41.504: INFO: successfully validated that service endpoint-test2 in namespace services-8693 exposes endpoints map[pod2:[80]] (1.099224749s elapsed)
STEP: Deleting pod pod2 in namespace services-8693
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8693 to expose endpoints map[]
Mar  6 09:49:42.535: INFO: successfully validated that service endpoint-test2 in namespace services-8693 exposes endpoints map[] (1.009592359s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:49:43.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8693" for this suite.
Mar  6 09:50:07.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:50:07.285: INFO: namespace services-8693 deletion completed in 24.24660838s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:35.134 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:50:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:51:07.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-174" for this suite.
Mar  6 09:51:29.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:51:29.766: INFO: namespace container-probe-174 deletion completed in 22.201352151s

• [SLOW TEST:82.481 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:51:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  6 09:51:30.011: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:51:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9308" for this suite.
Mar  6 09:51:42.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:51:42.502: INFO: namespace init-container-9308 deletion completed in 6.168766807s

• [SLOW TEST:12.735 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:51:42.502: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6tdg
STEP: Creating a pod to test atomic-volume-subpath
Mar  6 09:51:42.876: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6tdg" in namespace "subpath-3285" to be "success or failure"
Mar  6 09:51:42.881: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700337ms
Mar  6 09:51:44.886: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765319s
Mar  6 09:51:46.892: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 4.015896328s
Mar  6 09:51:48.896: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 6.020352572s
Mar  6 09:51:50.902: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 8.025640333s
Mar  6 09:51:52.906: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 10.030495921s
Mar  6 09:51:54.911: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 12.035051798s
Mar  6 09:51:56.917: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 14.040689291s
Mar  6 09:51:58.922: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 16.045603356s
Mar  6 09:52:00.927: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 18.050533876s
Mar  6 09:52:02.932: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 20.056001362s
Mar  6 09:52:04.956: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Running", Reason="", readiness=true. Elapsed: 22.079758266s
Mar  6 09:52:06.961: INFO: Pod "pod-subpath-test-configmap-6tdg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.085112158s
STEP: Saw pod success
Mar  6 09:52:06.961: INFO: Pod "pod-subpath-test-configmap-6tdg" satisfied condition "success or failure"
Mar  6 09:52:06.964: INFO: Trying to get logs from node wisecloud-worker02 pod pod-subpath-test-configmap-6tdg container test-container-subpath-configmap-6tdg: <nil>
STEP: delete the pod
Mar  6 09:52:07.041: INFO: Waiting for pod pod-subpath-test-configmap-6tdg to disappear
Mar  6 09:52:07.044: INFO: Pod pod-subpath-test-configmap-6tdg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6tdg
Mar  6 09:52:07.044: INFO: Deleting pod "pod-subpath-test-configmap-6tdg" in namespace "subpath-3285"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:52:07.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3285" for this suite.
Mar  6 09:52:13.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:52:13.245: INFO: namespace subpath-3285 deletion completed in 6.193215303s

• [SLOW TEST:30.743 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:52:13.246: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  6 09:52:18.110: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8770 pod-service-account-094bd96f-c14b-4a06-b1f5-15071880dbbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  6 09:52:19.703: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8770 pod-service-account-094bd96f-c14b-4a06-b1f5-15071880dbbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  6 09:52:20.135: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8770 pod-service-account-094bd96f-c14b-4a06-b1f5-15071880dbbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:52:20.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8770" for this suite.
Mar  6 09:52:26.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:52:26.958: INFO: namespace svcaccounts-8770 deletion completed in 6.222103007s

• [SLOW TEST:13.712 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:52:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-e32b2764-b4b7-4f51-986e-6d6200be029c
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:52:27.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8157" for this suite.
Mar  6 09:52:33.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:52:33.584: INFO: namespace configmap-8157 deletion completed in 6.315921954s

• [SLOW TEST:6.625 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:52:33.584: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:52:37.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7671" for this suite.
Mar  6 09:53:20.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:53:20.285: INFO: namespace kubelet-test-7671 deletion completed in 42.301207441s

• [SLOW TEST:46.701 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:53:20.285: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-466r6 in namespace proxy-6986
I0306 09:53:20.675479      17 runners.go:180] Created replication controller with name: proxy-service-466r6, namespace: proxy-6986, replica count: 1
I0306 09:53:21.726013      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:53:22.726231      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:53:23.726486      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:53:24.726705      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0306 09:53:25.726930      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:26.727198      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:27.727437      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:28.727671      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:29.727859      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:30.728086      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:31.728310      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:32.728637      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:33.728911      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0306 09:53:34.729172      17 runners.go:180] proxy-service-466r6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  6 09:53:34.735: INFO: setup took 14.212855739s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 14.378661ms)
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 13.986124ms)
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 14.514014ms)
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 15.514398ms)
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 15.639515ms)
Mar  6 09:53:34.751: INFO: (0) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 15.112457ms)
Mar  6 09:53:34.753: INFO: (0) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 17.227328ms)
Mar  6 09:53:34.883: INFO: (0) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 146.925717ms)
Mar  6 09:53:34.883: INFO: (0) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 146.322287ms)
Mar  6 09:53:34.883: INFO: (0) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 147.225333ms)
Mar  6 09:53:34.883: INFO: (0) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 146.225223ms)
Mar  6 09:53:34.883: INFO: (0) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 146.527986ms)
Mar  6 09:53:34.884: INFO: (0) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 148.043373ms)
Mar  6 09:53:34.884: INFO: (0) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 147.840949ms)
Mar  6 09:53:34.888: INFO: (0) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 152.178934ms)
Mar  6 09:53:34.895: INFO: (0) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 158.849673ms)
Mar  6 09:53:34.905: INFO: (1) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.167254ms)
Mar  6 09:53:34.906: INFO: (1) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 9.688711ms)
Mar  6 09:53:34.906: INFO: (1) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.542944ms)
Mar  6 09:53:34.906: INFO: (1) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 10.409494ms)
Mar  6 09:53:34.907: INFO: (1) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 10.508467ms)
Mar  6 09:53:34.908: INFO: (1) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 12.362221ms)
Mar  6 09:53:34.908: INFO: (1) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 11.975384ms)
Mar  6 09:53:34.908: INFO: (1) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 12.929744ms)
Mar  6 09:53:34.908: INFO: (1) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 12.443471ms)
Mar  6 09:53:34.908: INFO: (1) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 13.194174ms)
Mar  6 09:53:34.911: INFO: (1) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 15.380264ms)
Mar  6 09:53:34.911: INFO: (1) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 15.303468ms)
Mar  6 09:53:34.913: INFO: (1) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 17.643381ms)
Mar  6 09:53:34.914: INFO: (1) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 18.571538ms)
Mar  6 09:53:34.916: INFO: (1) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 19.201944ms)
Mar  6 09:53:34.916: INFO: (1) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 19.292458ms)
Mar  6 09:53:34.924: INFO: (2) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 8.258241ms)
Mar  6 09:53:34.926: INFO: (2) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 9.507027ms)
Mar  6 09:53:34.926: INFO: (2) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.083524ms)
Mar  6 09:53:34.926: INFO: (2) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.256288ms)
Mar  6 09:53:34.927: INFO: (2) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.843208ms)
Mar  6 09:53:34.927: INFO: (2) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 10.057101ms)
Mar  6 09:53:34.927: INFO: (2) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 10.364234ms)
Mar  6 09:53:34.928: INFO: (2) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 10.672714ms)
Mar  6 09:53:34.928: INFO: (2) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 11.207538ms)
Mar  6 09:53:34.928: INFO: (2) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 10.972097ms)
Mar  6 09:53:34.929: INFO: (2) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 12.252471ms)
Mar  6 09:53:34.929: INFO: (2) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 13.255511ms)
Mar  6 09:53:34.929: INFO: (2) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 12.440387ms)
Mar  6 09:53:34.929: INFO: (2) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 12.527528ms)
Mar  6 09:53:34.930: INFO: (2) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 12.915761ms)
Mar  6 09:53:34.930: INFO: (2) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 13.486918ms)
Mar  6 09:53:34.937: INFO: (3) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 6.387944ms)
Mar  6 09:53:34.937: INFO: (3) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 5.680196ms)
Mar  6 09:53:34.937: INFO: (3) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 5.86186ms)
Mar  6 09:53:34.938: INFO: (3) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 6.637131ms)
Mar  6 09:53:34.938: INFO: (3) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 7.211834ms)
Mar  6 09:53:34.938: INFO: (3) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.248287ms)
Mar  6 09:53:34.938: INFO: (3) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 7.718878ms)
Mar  6 09:53:34.939: INFO: (3) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.186493ms)
Mar  6 09:53:34.940: INFO: (3) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 8.359138ms)
Mar  6 09:53:34.940: INFO: (3) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.836384ms)
Mar  6 09:53:34.940: INFO: (3) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 8.322695ms)
Mar  6 09:53:34.941: INFO: (3) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 10.292842ms)
Mar  6 09:53:34.941: INFO: (3) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.902483ms)
Mar  6 09:53:34.942: INFO: (3) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 10.032994ms)
Mar  6 09:53:34.943: INFO: (3) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 11.66591ms)
Mar  6 09:53:34.943: INFO: (3) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.546868ms)
Mar  6 09:53:34.949: INFO: (4) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 5.996222ms)
Mar  6 09:53:34.950: INFO: (4) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.460457ms)
Mar  6 09:53:34.951: INFO: (4) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 6.999923ms)
Mar  6 09:53:34.951: INFO: (4) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.856207ms)
Mar  6 09:53:34.951: INFO: (4) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 7.530382ms)
Mar  6 09:53:34.952: INFO: (4) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 6.594581ms)
Mar  6 09:53:34.952: INFO: (4) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.625084ms)
Mar  6 09:53:34.953: INFO: (4) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 8.282513ms)
Mar  6 09:53:34.953: INFO: (4) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.587964ms)
Mar  6 09:53:34.953: INFO: (4) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 8.043513ms)
Mar  6 09:53:34.953: INFO: (4) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 8.580811ms)
Mar  6 09:53:34.955: INFO: (4) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 11.135754ms)
Mar  6 09:53:34.955: INFO: (4) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.058067ms)
Mar  6 09:53:34.955: INFO: (4) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 11.77751ms)
Mar  6 09:53:34.955: INFO: (4) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 10.560852ms)
Mar  6 09:53:34.957: INFO: (4) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 11.698752ms)
Mar  6 09:53:34.964: INFO: (5) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 6.474501ms)
Mar  6 09:53:34.964: INFO: (5) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.113164ms)
Mar  6 09:53:34.967: INFO: (5) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.882194ms)
Mar  6 09:53:34.967: INFO: (5) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 9.04442ms)
Mar  6 09:53:34.967: INFO: (5) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 9.235674ms)
Mar  6 09:53:34.967: INFO: (5) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.202073ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 10.147036ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 10.653094ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 10.307707ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 11.03581ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 10.549131ms)
Mar  6 09:53:34.968: INFO: (5) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 10.627646ms)
Mar  6 09:53:34.970: INFO: (5) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 12.144843ms)
Mar  6 09:53:34.971: INFO: (5) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 13.004103ms)
Mar  6 09:53:34.971: INFO: (5) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 13.187543ms)
Mar  6 09:53:34.972: INFO: (5) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 14.480631ms)
Mar  6 09:53:34.980: INFO: (6) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.778851ms)
Mar  6 09:53:34.982: INFO: (6) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.206487ms)
Mar  6 09:53:34.982: INFO: (6) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 9.040513ms)
Mar  6 09:53:34.982: INFO: (6) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 9.364854ms)
Mar  6 09:53:34.982: INFO: (6) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 9.55712ms)
Mar  6 09:53:34.983: INFO: (6) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.709861ms)
Mar  6 09:53:34.983: INFO: (6) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.915801ms)
Mar  6 09:53:34.983: INFO: (6) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 10.163007ms)
Mar  6 09:53:34.983: INFO: (6) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 10.348313ms)
Mar  6 09:53:34.983: INFO: (6) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 10.275421ms)
Mar  6 09:53:34.984: INFO: (6) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 11.914957ms)
Mar  6 09:53:34.986: INFO: (6) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 12.93342ms)
Mar  6 09:53:34.988: INFO: (6) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 14.829893ms)
Mar  6 09:53:34.988: INFO: (6) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 14.900246ms)
Mar  6 09:53:34.988: INFO: (6) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 15.146726ms)
Mar  6 09:53:34.988: INFO: (6) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 14.974032ms)
Mar  6 09:53:34.993: INFO: (7) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 4.646214ms)
Mar  6 09:53:34.995: INFO: (7) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 5.847484ms)
Mar  6 09:53:34.995: INFO: (7) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 6.563918ms)
Mar  6 09:53:34.995: INFO: (7) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.728804ms)
Mar  6 09:53:34.995: INFO: (7) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 7.031187ms)
Mar  6 09:53:34.995: INFO: (7) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 5.845737ms)
Mar  6 09:53:34.996: INFO: (7) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.862444ms)
Mar  6 09:53:34.996: INFO: (7) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.174197ms)
Mar  6 09:53:34.997: INFO: (7) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 7.034837ms)
Mar  6 09:53:34.997: INFO: (7) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.449497ms)
Mar  6 09:53:34.998: INFO: (7) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 8.69021ms)
Mar  6 09:53:34.999: INFO: (7) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 10.751501ms)
Mar  6 09:53:34.999: INFO: (7) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.977053ms)
Mar  6 09:53:35.000: INFO: (7) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 10.404331ms)
Mar  6 09:53:35.000: INFO: (7) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 11.047027ms)
Mar  6 09:53:35.001: INFO: (7) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 12.585521ms)
Mar  6 09:53:35.010: INFO: (8) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.16736ms)
Mar  6 09:53:35.010: INFO: (8) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.348804ms)
Mar  6 09:53:35.010: INFO: (8) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 8.550724ms)
Mar  6 09:53:35.011: INFO: (8) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 8.374158ms)
Mar  6 09:53:35.011: INFO: (8) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 8.418124ms)
Mar  6 09:53:35.012: INFO: (8) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.83056ms)
Mar  6 09:53:35.012: INFO: (8) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 11.036778ms)
Mar  6 09:53:35.013: INFO: (8) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.817397ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 11.363984ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 10.850694ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 11.305728ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 12.169818ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 9.557097ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 10.838387ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 10.707631ms)
Mar  6 09:53:35.014: INFO: (8) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 10.339201ms)
Mar  6 09:53:35.065: INFO: (9) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 49.644906ms)
Mar  6 09:53:35.065: INFO: (9) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 51.00334ms)
Mar  6 09:53:35.066: INFO: (9) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 50.223111ms)
Mar  6 09:53:35.066: INFO: (9) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 50.440435ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 51.054689ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 51.82842ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 51.683673ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 51.552726ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 51.480216ms)
Mar  6 09:53:35.067: INFO: (9) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 51.540991ms)
Mar  6 09:53:35.069: INFO: (9) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 53.430321ms)
Mar  6 09:53:35.070: INFO: (9) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 55.638717ms)
Mar  6 09:53:35.071: INFO: (9) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 55.602482ms)
Mar  6 09:53:35.071: INFO: (9) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 55.81169ms)
Mar  6 09:53:35.072: INFO: (9) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 56.182499ms)
Mar  6 09:53:35.072: INFO: (9) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 56.191366ms)
Mar  6 09:53:35.078: INFO: (10) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 5.932362ms)
Mar  6 09:53:35.103: INFO: (10) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 30.273945ms)
Mar  6 09:53:35.103: INFO: (10) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 30.583504ms)
Mar  6 09:53:35.103: INFO: (10) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 30.194631ms)
Mar  6 09:53:35.103: INFO: (10) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 30.937712ms)
Mar  6 09:53:35.104: INFO: (10) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 30.188ms)
Mar  6 09:53:35.104: INFO: (10) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 29.986499ms)
Mar  6 09:53:35.104: INFO: (10) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 30.487503ms)
Mar  6 09:53:35.104: INFO: (10) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 30.602398ms)
Mar  6 09:53:35.104: INFO: (10) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 31.431738ms)
Mar  6 09:53:35.105: INFO: (10) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 31.667235ms)
Mar  6 09:53:35.106: INFO: (10) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 32.904546ms)
Mar  6 09:53:35.106: INFO: (10) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 33.588641ms)
Mar  6 09:53:35.106: INFO: (10) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 33.130854ms)
Mar  6 09:53:35.106: INFO: (10) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 32.979931ms)
Mar  6 09:53:35.106: INFO: (10) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 33.812107ms)
Mar  6 09:53:35.112: INFO: (11) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 5.861605ms)
Mar  6 09:53:35.114: INFO: (11) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.339062ms)
Mar  6 09:53:35.115: INFO: (11) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 6.791723ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 8.295762ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.422794ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.75574ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 7.652294ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 8.912298ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.354974ms)
Mar  6 09:53:35.116: INFO: (11) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 8.56999ms)
Mar  6 09:53:35.117: INFO: (11) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 9.412658ms)
Mar  6 09:53:35.117: INFO: (11) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.982153ms)
Mar  6 09:53:35.119: INFO: (11) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 10.872505ms)
Mar  6 09:53:35.121: INFO: (11) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 13.101722ms)
Mar  6 09:53:35.121: INFO: (11) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 13.570424ms)
Mar  6 09:53:35.122: INFO: (11) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 14.003664ms)
Mar  6 09:53:35.130: INFO: (12) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.956149ms)
Mar  6 09:53:35.131: INFO: (12) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 8.615718ms)
Mar  6 09:53:35.132: INFO: (12) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.264497ms)
Mar  6 09:53:35.132: INFO: (12) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.152246ms)
Mar  6 09:53:35.132: INFO: (12) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 9.349873ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.70972ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 11.803499ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 12.945698ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 12.512493ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 11.813996ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 12.052243ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 12.233927ms)
Mar  6 09:53:35.135: INFO: (12) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 12.636091ms)
Mar  6 09:53:35.136: INFO: (12) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 13.244962ms)
Mar  6 09:53:35.136: INFO: (12) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 13.20145ms)
Mar  6 09:53:35.136: INFO: (12) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 13.601338ms)
Mar  6 09:53:35.141: INFO: (13) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 4.664001ms)
Mar  6 09:53:35.143: INFO: (13) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.01502ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.162344ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 7.068891ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 6.885159ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.955786ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.025984ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.377375ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 7.39712ms)
Mar  6 09:53:35.144: INFO: (13) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.527231ms)
Mar  6 09:53:35.146: INFO: (13) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 9.205536ms)
Mar  6 09:53:35.146: INFO: (13) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.742957ms)
Mar  6 09:53:35.146: INFO: (13) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 10.008315ms)
Mar  6 09:53:35.147: INFO: (13) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 9.94657ms)
Mar  6 09:53:35.147: INFO: (13) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 10.113829ms)
Mar  6 09:53:35.147: INFO: (13) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 9.561249ms)
Mar  6 09:53:35.152: INFO: (14) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 5.456391ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.781324ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.432295ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 9.015773ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 8.845995ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 8.430551ms)
Mar  6 09:53:35.156: INFO: (14) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 8.851597ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.921121ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.673732ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 9.591685ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 9.600155ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 9.572914ms)
Mar  6 09:53:35.157: INFO: (14) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 9.777457ms)
Mar  6 09:53:35.158: INFO: (14) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 10.896545ms)
Mar  6 09:53:35.158: INFO: (14) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 10.861598ms)
Mar  6 09:53:35.159: INFO: (14) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 10.9461ms)
Mar  6 09:53:35.164: INFO: (15) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 5.37773ms)
Mar  6 09:53:35.165: INFO: (15) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 5.859267ms)
Mar  6 09:53:35.166: INFO: (15) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.104444ms)
Mar  6 09:53:35.166: INFO: (15) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 7.002644ms)
Mar  6 09:53:35.166: INFO: (15) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.161396ms)
Mar  6 09:53:35.166: INFO: (15) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.281789ms)
Mar  6 09:53:35.166: INFO: (15) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.48179ms)
Mar  6 09:53:35.167: INFO: (15) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.82979ms)
Mar  6 09:53:35.167: INFO: (15) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 8.578962ms)
Mar  6 09:53:35.167: INFO: (15) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 8.522307ms)
Mar  6 09:53:35.169: INFO: (15) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 10.209615ms)
Mar  6 09:53:35.169: INFO: (15) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 10.556558ms)
Mar  6 09:53:35.170: INFO: (15) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.018215ms)
Mar  6 09:53:35.171: INFO: (15) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 11.64904ms)
Mar  6 09:53:35.171: INFO: (15) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 11.9862ms)
Mar  6 09:53:35.171: INFO: (15) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 11.939778ms)
Mar  6 09:53:35.177: INFO: (16) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.290587ms)
Mar  6 09:53:35.179: INFO: (16) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.404704ms)
Mar  6 09:53:35.180: INFO: (16) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 7.865467ms)
Mar  6 09:53:35.180: INFO: (16) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.077507ms)
Mar  6 09:53:35.180: INFO: (16) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 7.478643ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 8.482007ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.39475ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 8.887797ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 8.183761ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 8.130453ms)
Mar  6 09:53:35.181: INFO: (16) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 8.869514ms)
Mar  6 09:53:35.183: INFO: (16) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 12.008108ms)
Mar  6 09:53:35.185: INFO: (16) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 12.64255ms)
Mar  6 09:53:35.185: INFO: (16) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 13.143251ms)
Mar  6 09:53:35.186: INFO: (16) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 12.991474ms)
Mar  6 09:53:35.186: INFO: (16) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 14.330855ms)
Mar  6 09:53:35.194: INFO: (17) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.413831ms)
Mar  6 09:53:35.194: INFO: (17) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 6.46192ms)
Mar  6 09:53:35.194: INFO: (17) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 7.433664ms)
Mar  6 09:53:35.194: INFO: (17) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 8.100984ms)
Mar  6 09:53:35.195: INFO: (17) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.890194ms)
Mar  6 09:53:35.195: INFO: (17) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 7.99138ms)
Mar  6 09:53:35.195: INFO: (17) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 8.437266ms)
Mar  6 09:53:35.196: INFO: (17) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 8.117867ms)
Mar  6 09:53:35.196: INFO: (17) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 9.129814ms)
Mar  6 09:53:35.196: INFO: (17) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 8.949487ms)
Mar  6 09:53:35.197: INFO: (17) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 10.803007ms)
Mar  6 09:53:35.197: INFO: (17) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 11.046914ms)
Mar  6 09:53:35.197: INFO: (17) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 10.299719ms)
Mar  6 09:53:35.198: INFO: (17) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 10.767171ms)
Mar  6 09:53:35.198: INFO: (17) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 10.883015ms)
Mar  6 09:53:35.198: INFO: (17) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.126582ms)
Mar  6 09:53:35.206: INFO: (18) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.281324ms)
Mar  6 09:53:35.206: INFO: (18) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.664507ms)
Mar  6 09:53:35.206: INFO: (18) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 6.309643ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 7.89401ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 8.186884ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 7.6425ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 8.451407ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 7.913749ms)
Mar  6 09:53:35.207: INFO: (18) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 8.867692ms)
Mar  6 09:53:35.208: INFO: (18) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 9.855029ms)
Mar  6 09:53:35.208: INFO: (18) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 8.891481ms)
Mar  6 09:53:35.210: INFO: (18) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 11.416523ms)
Mar  6 09:53:35.210: INFO: (18) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 11.37195ms)
Mar  6 09:53:35.210: INFO: (18) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 11.857341ms)
Mar  6 09:53:35.211: INFO: (18) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 11.179059ms)
Mar  6 09:53:35.211: INFO: (18) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 11.979608ms)
Mar  6 09:53:35.218: INFO: (19) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:162/proxy/: bar (200; 6.116125ms)
Mar  6 09:53:35.218: INFO: (19) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:162/proxy/: bar (200; 5.600211ms)
Mar  6 09:53:35.218: INFO: (19) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg/proxy/rewriteme">test</a> (200; 5.477298ms)
Mar  6 09:53:35.218: INFO: (19) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:1080/proxy/rewriteme">... (200; 6.405378ms)
Mar  6 09:53:35.218: INFO: (19) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:1080/proxy/rewriteme">test<... (200; 5.585536ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:462/proxy/: tls qux (200; 7.658156ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:460/proxy/: tls baz (200; 8.443058ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/pods/proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.251859ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/: <a href="/api/v1/namespaces/proxy-6986/pods/https:proxy-service-466r6-hthjg:443/proxy/tlsrewritem... (200; 7.499799ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/pods/http:proxy-service-466r6-hthjg:160/proxy/: foo (200; 7.883153ms)
Mar  6 09:53:35.220: INFO: (19) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname2/proxy/: bar (200; 8.955686ms)
Mar  6 09:53:35.222: INFO: (19) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname2/proxy/: tls qux (200; 9.79893ms)
Mar  6 09:53:35.223: INFO: (19) /api/v1/namespaces/proxy-6986/services/https:proxy-service-466r6:tlsportname1/proxy/: tls baz (200; 9.925725ms)
Mar  6 09:53:35.223: INFO: (19) /api/v1/namespaces/proxy-6986/services/http:proxy-service-466r6:portname1/proxy/: foo (200; 10.687359ms)
Mar  6 09:53:35.223: INFO: (19) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname1/proxy/: foo (200; 10.525902ms)
Mar  6 09:53:35.223: INFO: (19) /api/v1/namespaces/proxy-6986/services/proxy-service-466r6:portname2/proxy/: bar (200; 10.90363ms)
STEP: deleting ReplicationController proxy-service-466r6 in namespace proxy-6986, will wait for the garbage collector to delete the pods
Mar  6 09:53:35.354: INFO: Deleting ReplicationController proxy-service-466r6 took: 75.054915ms
Mar  6 09:53:35.855: INFO: Terminating ReplicationController proxy-service-466r6 pods took: 500.588146ms
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:53:38.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6986" for this suite.
Mar  6 09:53:45.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:53:45.342: INFO: namespace proxy-6986 deletion completed in 6.379357943s

• [SLOW TEST:25.057 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:53:45.342: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  6 09:53:45.639: INFO: Waiting up to 5m0s for pod "pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f" in namespace "emptydir-7786" to be "success or failure"
Mar  6 09:53:45.643: INFO: Pod "pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586462ms
Mar  6 09:53:47.648: INFO: Pod "pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008681112s
Mar  6 09:53:49.652: INFO: Pod "pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0133639s
STEP: Saw pod success
Mar  6 09:53:49.652: INFO: Pod "pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f" satisfied condition "success or failure"
Mar  6 09:53:49.656: INFO: Trying to get logs from node wisecloud-worker02 pod pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f container test-container: <nil>
STEP: delete the pod
Mar  6 09:53:49.718: INFO: Waiting for pod pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f to disappear
Mar  6 09:53:49.746: INFO: Pod pod-43787f85-b28e-4ea2-b1ea-2e0d1f30708f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:53:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7786" for this suite.
Mar  6 09:53:55.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:53:56.006: INFO: namespace emptydir-7786 deletion completed in 6.25360855s

• [SLOW TEST:10.664 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:53:56.007: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6880
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-19e8cce5-eb72-4fea-b5a2-38a3f66e6231
STEP: Creating configMap with name cm-test-opt-upd-1218138f-ae95-444d-97d9-5fe328a244ac
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-19e8cce5-eb72-4fea-b5a2-38a3f66e6231
STEP: Updating configmap cm-test-opt-upd-1218138f-ae95-444d-97d9-5fe328a244ac
STEP: Creating configMap with name cm-test-opt-create-820d5d6b-badf-4686-844f-aa42a0381070
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:54:02.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6880" for this suite.
Mar  6 09:54:26.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:54:26.796: INFO: namespace projected-6880 deletion completed in 24.243481247s

• [SLOW TEST:30.789 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:54:26.796: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 09:54:27.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b" in namespace "downward-api-260" to be "success or failure"
Mar  6 09:54:27.105: INFO: Pod "downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060491ms
Mar  6 09:54:29.111: INFO: Pod "downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010407416s
Mar  6 09:54:31.146: INFO: Pod "downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045419326s
STEP: Saw pod success
Mar  6 09:54:31.146: INFO: Pod "downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b" satisfied condition "success or failure"
Mar  6 09:54:31.151: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b container client-container: <nil>
STEP: delete the pod
Mar  6 09:54:31.207: INFO: Waiting for pod downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b to disappear
Mar  6 09:54:31.211: INFO: Pod downwardapi-volume-c2304268-fca8-43db-99fc-de61add0652b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:54:31.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-260" for this suite.
Mar  6 09:54:37.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:54:37.481: INFO: namespace downward-api-260 deletion completed in 6.264306306s

• [SLOW TEST:10.684 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:54:37.481: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Mar  6 09:54:37.756: INFO: Waiting up to 5m0s for pod "client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9" in namespace "containers-7749" to be "success or failure"
Mar  6 09:54:37.761: INFO: Pod "client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62696ms
Mar  6 09:54:39.765: INFO: Pod "client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009327875s
Mar  6 09:54:41.770: INFO: Pod "client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014044889s
STEP: Saw pod success
Mar  6 09:54:41.770: INFO: Pod "client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9" satisfied condition "success or failure"
Mar  6 09:54:41.774: INFO: Trying to get logs from node wisecloud-worker02 pod client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9 container test-container: <nil>
STEP: delete the pod
Mar  6 09:54:41.836: INFO: Waiting for pod client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9 to disappear
Mar  6 09:54:41.839: INFO: Pod client-containers-fee6728d-fb74-4257-a46d-50f9bc564da9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:54:41.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7749" for this suite.
Mar  6 09:54:47.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:54:48.004: INFO: namespace containers-7749 deletion completed in 6.158632528s

• [SLOW TEST:10.523 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:54:48.004: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 09:54:48.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-5358'
Mar  6 09:54:48.427: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 09:54:48.427: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Mar  6 09:54:48.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete jobs e2e-test-nginx-job --namespace=kubectl-5358'
Mar  6 09:54:48.605: INFO: stderr: ""
Mar  6 09:54:48.605: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:54:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5358" for this suite.
Mar  6 09:55:12.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:55:12.789: INFO: namespace kubectl-5358 deletion completed in 24.176363458s

• [SLOW TEST:24.784 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:55:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 09:55:13.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-pod --generator=run-pod/v1 --image=172.20.8.7/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4793'
Mar  6 09:55:13.224: INFO: stderr: ""
Mar  6 09:55:13.224: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  6 09:55:18.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pod e2e-test-nginx-pod --namespace=kubectl-4793 -o json'
Mar  6 09:55:18.399: INFO: stderr: ""
Mar  6 09:55:18.400: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-06T09:55:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4793\",\n        \"resourceVersion\": \"30154\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4793/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8012868e-2f3f-4cbd-9707-8a210f6fb83a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"172.20.8.7/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-55ddw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wisecloud-worker02\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-55ddw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-55ddw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-06T09:55:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-06T09:55:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-06T09:55:17Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-06T09:55:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://07d6043276e402884b2f7ce0b0de02f4edfa25155db9013ff23315f260254bc5\",\n                \"image\": \"172.20.8.7/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-06T09:55:16Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.20.8.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.246\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-06T09:55:13Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  6 09:55:18.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 replace -f - --namespace=kubectl-4793'
Mar  6 09:55:18.798: INFO: stderr: ""
Mar  6 09:55:18.798: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image 172.20.8.7/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Mar  6 09:55:18.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete pods e2e-test-nginx-pod --namespace=kubectl-4793'
Mar  6 09:55:23.201: INFO: stderr: ""
Mar  6 09:55:23.201: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:55:23.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4793" for this suite.
Mar  6 09:55:29.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:55:29.440: INFO: namespace kubectl-4793 deletion completed in 6.231544375s

• [SLOW TEST:16.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:55:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1218
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  6 09:55:30.054: INFO: Waiting up to 5m0s for pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09" in namespace "emptydir-1218" to be "success or failure"
Mar  6 09:55:30.059: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.890466ms
Mar  6 09:55:32.064: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010011579s
Mar  6 09:55:34.070: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015786046s
Mar  6 09:55:36.088: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034586152s
Mar  6 09:55:38.093: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039202206s
STEP: Saw pod success
Mar  6 09:55:38.093: INFO: Pod "pod-da1b8276-f29d-4a7f-a10a-e386f5821d09" satisfied condition "success or failure"
Mar  6 09:55:38.097: INFO: Trying to get logs from node wisecloud-worker02 pod pod-da1b8276-f29d-4a7f-a10a-e386f5821d09 container test-container: <nil>
STEP: delete the pod
Mar  6 09:55:38.161: INFO: Waiting for pod pod-da1b8276-f29d-4a7f-a10a-e386f5821d09 to disappear
Mar  6 09:55:38.189: INFO: Pod pod-da1b8276-f29d-4a7f-a10a-e386f5821d09 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:55:38.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1218" for this suite.
Mar  6 09:55:44.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:55:45.036: INFO: namespace emptydir-1218 deletion completed in 6.84130622s

• [SLOW TEST:15.595 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:55:45.037: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 09:55:51.888: INFO: Successfully updated pod "pod-update-4b46eeb8-0d29-4b94-94ba-d2828caa4dfe"
STEP: verifying the updated pod is in kubernetes
Mar  6 09:55:51.894: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 09:55:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1018" for this suite.
Mar  6 09:56:15.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 09:56:16.030: INFO: namespace pods-1018 deletion completed in 24.130779144s

• [SLOW TEST:30.993 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 09:56:16.031: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-4f9534e0-4d41-4aa0-86d7-d6a5b02c4aff in namespace container-probe-2278
Mar  6 09:56:22.321: INFO: Started pod test-webserver-4f9534e0-4d41-4aa0-86d7-d6a5b02c4aff in namespace container-probe-2278
STEP: checking the pod's current state and verifying that restartCount is present
Mar  6 09:56:22.325: INFO: Initial restart count of pod test-webserver-4f9534e0-4d41-4aa0-86d7-d6a5b02c4aff is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:00:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2278" for this suite.
Mar  6 10:00:29.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:00:30.237: INFO: namespace container-probe-2278 deletion completed in 6.689270425s

• [SLOW TEST:254.206 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:00:30.238: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f26112a7-2111-41e7-8696-c9b817e723c7
STEP: Creating a pod to test consume secrets
Mar  6 10:00:30.579: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e" in namespace "projected-8238" to be "success or failure"
Mar  6 10:00:30.584: INFO: Pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.987666ms
Mar  6 10:00:32.601: INFO: Pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022197734s
Mar  6 10:00:34.606: INFO: Pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027185098s
Mar  6 10:00:36.611: INFO: Pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032222679s
STEP: Saw pod success
Mar  6 10:00:36.611: INFO: Pod "pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e" satisfied condition "success or failure"
Mar  6 10:00:36.616: INFO: Trying to get logs from node wisecloud-worker02 pod pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  6 10:00:36.719: INFO: Waiting for pod pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e to disappear
Mar  6 10:00:36.759: INFO: Pod pod-projected-secrets-87a4bdc0-9331-47af-9c8c-0536292c444e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:00:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8238" for this suite.
Mar  6 10:00:42.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:00:42.905: INFO: namespace projected-8238 deletion completed in 6.13964498s

• [SLOW TEST:12.667 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:00:42.906: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3db95e73-03cf-476a-9489-6e4e653797e6
STEP: Creating a pod to test consume configMaps
Mar  6 10:00:43.212: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba" in namespace "configmap-2621" to be "success or failure"
Mar  6 10:00:43.216: INFO: Pod "pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.727084ms
Mar  6 10:00:45.221: INFO: Pod "pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008680925s
Mar  6 10:00:47.242: INFO: Pod "pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029984712s
STEP: Saw pod success
Mar  6 10:00:47.242: INFO: Pod "pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba" satisfied condition "success or failure"
Mar  6 10:00:47.246: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 10:00:47.297: INFO: Waiting for pod pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba to disappear
Mar  6 10:00:47.301: INFO: Pod pod-configmaps-0e6223c8-ffcc-4c45-88a4-c3f24b21acba no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:00:47.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2621" for this suite.
Mar  6 10:00:53.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:00:53.517: INFO: namespace configmap-2621 deletion completed in 6.210816636s

• [SLOW TEST:10.611 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:00:53.517: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4116
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4116
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4116
Mar  6 10:00:53.989: INFO: Found 0 stateful pods, waiting for 1
Mar  6 10:01:03.995: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  6 10:01:04.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 10:01:04.442: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 10:01:04.442: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 10:01:04.442: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 10:01:04.447: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  6 10:01:14.452: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 10:01:14.452: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 10:01:14.556: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999543s
Mar  6 10:01:15.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.929627173s
Mar  6 10:01:16.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.92421152s
Mar  6 10:01:17.572: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.918881212s
Mar  6 10:01:18.578: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.913377179s
Mar  6 10:01:19.583: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.907541511s
Mar  6 10:01:20.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.902319524s
Mar  6 10:01:21.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.876625436s
Mar  6 10:01:22.622: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.869386557s
Mar  6 10:01:23.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 863.502461ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4116
Mar  6 10:01:24.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 10:01:25.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 10:01:25.040: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 10:01:25.040: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 10:01:25.045: INFO: Found 1 stateful pods, waiting for 3
Mar  6 10:01:35.051: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 10:01:35.051: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  6 10:01:35.051: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  6 10:01:35.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 10:01:35.464: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 10:01:35.464: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 10:01:35.464: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 10:01:35.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 10:01:35.851: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 10:01:35.851: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 10:01:35.851: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 10:01:35.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  6 10:01:36.303: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  6 10:01:36.303: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  6 10:01:36.303: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  6 10:01:36.303: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 10:01:36.310: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  6 10:01:46.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 10:01:46.322: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 10:01:46.322: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  6 10:01:46.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999841s
Mar  6 10:01:47.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.945571146s
Mar  6 10:01:48.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.940048015s
Mar  6 10:01:49.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.934212232s
Mar  6 10:01:50.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.927244813s
Mar  6 10:01:51.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.92131183s
Mar  6 10:01:52.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914210307s
Mar  6 10:01:53.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.908250278s
Mar  6 10:01:54.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.901799022s
Mar  6 10:01:55.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 896.265763ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4116
Mar  6 10:01:56.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 10:01:56.814: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 10:01:56.814: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 10:01:56.814: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 10:01:56.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 10:01:57.156: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 10:01:57.156: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 10:01:57.156: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 10:01:57.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 exec --namespace=statefulset-4116 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  6 10:01:57.529: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  6 10:01:57.529: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  6 10:01:57.529: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  6 10:01:57.529: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  6 10:02:27.553: INFO: Deleting all statefulset in ns statefulset-4116
Mar  6 10:02:27.559: INFO: Scaling statefulset ss to 0
Mar  6 10:02:27.572: INFO: Waiting for statefulset status.replicas updated to 0
Mar  6 10:02:27.576: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:02:27.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4116" for this suite.
Mar  6 10:02:35.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:02:35.784: INFO: namespace statefulset-4116 deletion completed in 8.142960987s

• [SLOW TEST:102.267 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:02:35.785: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:02:44.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3518" for this suite.
Mar  6 10:02:50.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:02:50.364: INFO: namespace kubelet-test-3518 deletion completed in 6.167847996s

• [SLOW TEST:14.579 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:02:50.364: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  6 10:02:55.223: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f25eac64-76d5-4c10-b0e6-a1be785d038c"
Mar  6 10:02:55.223: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f25eac64-76d5-4c10-b0e6-a1be785d038c" in namespace "pods-1576" to be "terminated due to deadline exceeded"
Mar  6 10:02:55.227: INFO: Pod "pod-update-activedeadlineseconds-f25eac64-76d5-4c10-b0e6-a1be785d038c": Phase="Running", Reason="", readiness=true. Elapsed: 4.080927ms
Mar  6 10:02:57.232: INFO: Pod "pod-update-activedeadlineseconds-f25eac64-76d5-4c10-b0e6-a1be785d038c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009403465s
Mar  6 10:02:57.232: INFO: Pod "pod-update-activedeadlineseconds-f25eac64-76d5-4c10-b0e6-a1be785d038c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:02:57.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1576" for this suite.
Mar  6 10:03:03.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:03:03.457: INFO: namespace pods-1576 deletion completed in 6.218272248s

• [SLOW TEST:13.093 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:03:03.457: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  6 10:03:03.790: INFO: Waiting up to 5m0s for pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432" in namespace "downward-api-5425" to be "success or failure"
Mar  6 10:03:03.795: INFO: Pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458404ms
Mar  6 10:03:05.800: INFO: Pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009814122s
Mar  6 10:03:07.805: INFO: Pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014883924s
Mar  6 10:03:09.810: INFO: Pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019784982s
STEP: Saw pod success
Mar  6 10:03:09.810: INFO: Pod "downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432" satisfied condition "success or failure"
Mar  6 10:03:09.814: INFO: Trying to get logs from node wisecloud-worker02 pod downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432 container dapi-container: <nil>
STEP: delete the pod
Mar  6 10:03:09.888: INFO: Waiting for pod downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432 to disappear
Mar  6 10:03:09.893: INFO: Pod downward-api-ed4fed19-6995-445a-b98a-32ecbbc25432 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:03:09.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5425" for this suite.
Mar  6 10:03:15.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:03:16.034: INFO: namespace downward-api-5425 deletion completed in 6.134850421s

• [SLOW TEST:12.576 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:03:16.034: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  6 10:03:16.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 run e2e-test-nginx-rc --image=172.20.8.7/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3735'
Mar  6 10:03:17.825: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  6 10:03:17.825: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  6 10:03:17.885: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar  6 10:03:18.015: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  6 10:03:18.018: INFO: scanned /root for discovery docs: <nil>
Mar  6 10:03:18.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 rolling-update e2e-test-nginx-rc --update-period=1s --image=172.20.8.7/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3735'
Mar  6 10:03:33.990: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  6 10:03:33.990: INFO: stdout: "Created e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e\nScaling up e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  6 10:03:33.990: INFO: stdout: "Created e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e\nScaling up e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  6 10:03:33.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3735'
Mar  6 10:03:34.105: INFO: stderr: ""
Mar  6 10:03:34.105: INFO: stdout: "e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e-nz6qz "
Mar  6 10:03:34.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e-nz6qz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3735'
Mar  6 10:03:34.231: INFO: stderr: ""
Mar  6 10:03:34.231: INFO: stdout: "true"
Mar  6 10:03:34.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 get pods e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e-nz6qz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3735'
Mar  6 10:03:34.376: INFO: stderr: ""
Mar  6 10:03:34.376: INFO: stdout: "172.20.8.7/library/nginx:1.14-alpine"
Mar  6 10:03:34.376: INFO: e2e-test-nginx-rc-c55778cadd55682259090d1f098c6f2e-nz6qz is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Mar  6 10:03:34.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 delete rc e2e-test-nginx-rc --namespace=kubectl-3735'
Mar  6 10:03:34.552: INFO: stderr: ""
Mar  6 10:03:34.552: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:03:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3735" for this suite.
Mar  6 10:03:58.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:03:58.768: INFO: namespace kubectl-3735 deletion completed in 24.198384876s

• [SLOW TEST:42.734 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:03:58.769: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9439025a-251b-4950-96f9-32727120f388
STEP: Creating a pod to test consume configMaps
Mar  6 10:03:59.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010" in namespace "configmap-4076" to be "success or failure"
Mar  6 10:03:59.312: INFO: Pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211656ms
Mar  6 10:04:01.318: INFO: Pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009574938s
Mar  6 10:04:03.324: INFO: Pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015845816s
Mar  6 10:04:05.330: INFO: Pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021242728s
STEP: Saw pod success
Mar  6 10:04:05.330: INFO: Pod "pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010" satisfied condition "success or failure"
Mar  6 10:04:05.335: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  6 10:04:05.475: INFO: Waiting for pod pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010 to disappear
Mar  6 10:04:05.479: INFO: Pod pod-configmaps-9584e40f-276a-4e71-97cb-9858c8ac1010 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:04:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4076" for this suite.
Mar  6 10:04:11.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:04:11.723: INFO: namespace configmap-4076 deletion completed in 6.235244501s

• [SLOW TEST:12.954 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:04:11.723: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 10:04:12.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c" in namespace "downward-api-165" to be "success or failure"
Mar  6 10:04:12.057: INFO: Pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.230878ms
Mar  6 10:04:14.071: INFO: Pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029254922s
Mar  6 10:04:16.076: INFO: Pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034745811s
Mar  6 10:04:18.081: INFO: Pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039890021s
STEP: Saw pod success
Mar  6 10:04:18.081: INFO: Pod "downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c" satisfied condition "success or failure"
Mar  6 10:04:18.084: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c container client-container: <nil>
STEP: delete the pod
Mar  6 10:04:18.142: INFO: Waiting for pod downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c to disappear
Mar  6 10:04:18.146: INFO: Pod downwardapi-volume-4b6f1154-b2ba-4f90-9a75-54027d9ad99c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:04:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-165" for this suite.
Mar  6 10:04:24.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:04:24.336: INFO: namespace downward-api-165 deletion completed in 6.18284116s

• [SLOW TEST:12.613 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:04:24.336: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar  6 10:04:34.729: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:04:34.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0306 10:04:34.729628      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9301" for this suite.
Mar  6 10:04:40.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:04:41.005: INFO: namespace gc-9301 deletion completed in 6.269271221s

• [SLOW TEST:16.669 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:04:41.006: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 10:04:41.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-570'
Mar  6 10:04:41.792: INFO: stderr: ""
Mar  6 10:04:41.792: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  6 10:04:41.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 create -f - --namespace=kubectl-570'
Mar  6 10:04:42.568: INFO: stderr: ""
Mar  6 10:04:42.568: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  6 10:04:43.573: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 10:04:43.573: INFO: Found 0 / 1
Mar  6 10:04:44.573: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 10:04:44.573: INFO: Found 0 / 1
Mar  6 10:04:45.587: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 10:04:45.587: INFO: Found 1 / 1
Mar  6 10:04:45.587: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  6 10:04:45.593: INFO: Selector matched 1 pods for map[app:redis]
Mar  6 10:04:45.593: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  6 10:04:45.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 describe pod redis-master-p4xpc --namespace=kubectl-570'
Mar  6 10:04:45.741: INFO: stderr: ""
Mar  6 10:04:45.741: INFO: stdout: "Name:           redis-master-p4xpc\nNamespace:      kubectl-570\nPriority:       0\nNode:           wisecloud-worker02/172.20.8.6\nStart Time:     Fri, 06 Mar 2020 10:04:42 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.4.10\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://54fb9db15f275e30dd0c5f032798152a721149cbcfe3b3f716a3d8db2b1280e8\n    Image:          172.20.8.7/library/redis:1.0\n    Image ID:       docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 06 Mar 2020 10:04:44 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n9mg2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n9mg2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n9mg2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                         Message\n  ----    ------     ----  ----                         -------\n  Normal  Scheduled  4s    default-scheduler            Successfully assigned kubectl-570/redis-master-p4xpc to wisecloud-worker02\n  Normal  Pulled     2s    kubelet, wisecloud-worker02  Container image \"172.20.8.7/library/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, wisecloud-worker02  Created container redis-master\n  Normal  Started    1s    kubelet, wisecloud-worker02  Started container redis-master\n"
Mar  6 10:04:45.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 describe rc redis-master --namespace=kubectl-570'
Mar  6 10:04:45.913: INFO: stderr: ""
Mar  6 10:04:45.913: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-570\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        172.20.8.7/library/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-p4xpc\n"
Mar  6 10:04:45.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 describe service redis-master --namespace=kubectl-570'
Mar  6 10:04:46.073: INFO: stderr: ""
Mar  6 10:04:46.073: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-570\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.96.152.8\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.4.10:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  6 10:04:46.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 describe node wisecloud-master01'
Mar  6 10:04:46.266: INFO: stderr: ""
Mar  6 10:04:46.266: INFO: stdout: "Name:               wisecloud-master01\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wisecloud-master01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"52:56:50:5f:f8:8b\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.20.8.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 06 Mar 2020 07:20:04 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 06 Mar 2020 10:04:40 +0000   Fri, 06 Mar 2020 07:20:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 06 Mar 2020 10:04:40 +0000   Fri, 06 Mar 2020 07:20:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 06 Mar 2020 10:04:40 +0000   Fri, 06 Mar 2020 07:20:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 06 Mar 2020 10:04:40 +0000   Fri, 06 Mar 2020 07:21:00 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.20.8.2\n  Hostname:    wisecloud-master01\nCapacity:\n cpu:                8\n ephemeral-storage:  51175Mi\n hugepages-2Mi:      0\n memory:             3878044Ki\n pods:               110\nAllocatable:\n cpu:                7\n ephemeral-storage:  46147305393\n hugepages-2Mi:      0\n memory:             2342044Ki\n pods:               110\nSystem Info:\n Machine ID:                 c1e2a7e37ea041feb133fd939fc56629\n System UUID:                564D17FA-35A3-5FA5-545D-CA0C20E0EA19\n Boot ID:                    911f01c5-1207-4bff-bfd7-9c6c3ec04f18\n Kernel Version:             3.10.0-693.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.15.5\n Kube-Proxy Version:         v1.15.5\nPodCIDR:                     10.244.1.0/24\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                kube-apiserver-wisecloud-master01                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                kube-controller-manager-wisecloud-master01                 200m (2%)     0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                kube-flannel-ds-amd64-s5kl8                                100m (1%)     100m (1%)   50Mi (2%)        50Mi (2%)      163m\n  kube-system                kube-proxy-x24mq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                kube-scheduler-wisecloud-master01                          100m (1%)     0 (0%)      0 (0%)           0 (0%)         163m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-cc921f0ac7284fb7-sslpg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         104m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                650m (9%)  100m (1%)\n  memory             50Mi (2%)  50Mi (2%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:              <none>\n"
Mar  6 10:04:46.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201274422 describe namespace kubectl-570'
Mar  6 10:04:46.443: INFO: stderr: ""
Mar  6 10:04:46.443: INFO: stdout: "Name:         kubectl-570\nLabels:       e2e-framework=kubectl\n              e2e-run=98e25c8e-6913-4318-a1ba-cbcca8066eb6\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:04:46.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-570" for this suite.
Mar  6 10:05:10.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:05:10.584: INFO: namespace kubectl-570 deletion completed in 24.135862189s

• [SLOW TEST:29.579 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:05:10.584: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6687/configmap-test-4fb6240d-abee-46a4-ad9d-29cbbf4357c7
STEP: Creating a pod to test consume configMaps
Mar  6 10:05:10.925: INFO: Waiting up to 5m0s for pod "pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8" in namespace "configmap-6687" to be "success or failure"
Mar  6 10:05:10.941: INFO: Pod "pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023394ms
Mar  6 10:05:12.946: INFO: Pod "pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021382831s
Mar  6 10:05:14.952: INFO: Pod "pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027281923s
STEP: Saw pod success
Mar  6 10:05:14.952: INFO: Pod "pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8" satisfied condition "success or failure"
Mar  6 10:05:14.956: INFO: Trying to get logs from node wisecloud-worker02 pod pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8 container env-test: <nil>
STEP: delete the pod
Mar  6 10:05:15.056: INFO: Waiting for pod pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8 to disappear
Mar  6 10:05:15.061: INFO: Pod pod-configmaps-e900d7de-f4dd-497a-92bd-5c81e010b5a8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:05:15.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6687" for this suite.
Mar  6 10:05:21.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:05:21.224: INFO: namespace configmap-6687 deletion completed in 6.15714367s

• [SLOW TEST:10.640 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:05:21.224: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Mar  6 10:05:21.582: INFO: Waiting up to 5m0s for pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131" in namespace "var-expansion-1172" to be "success or failure"
Mar  6 10:05:21.586: INFO: Pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131": Phase="Pending", Reason="", readiness=false. Elapsed: 3.676469ms
Mar  6 10:05:23.599: INFO: Pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016830637s
Mar  6 10:05:25.605: INFO: Pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022518058s
Mar  6 10:05:27.609: INFO: Pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026782137s
STEP: Saw pod success
Mar  6 10:05:27.609: INFO: Pod "var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131" satisfied condition "success or failure"
Mar  6 10:05:27.612: INFO: Trying to get logs from node wisecloud-worker02 pod var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131 container dapi-container: <nil>
STEP: delete the pod
Mar  6 10:05:27.682: INFO: Waiting for pod var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131 to disappear
Mar  6 10:05:27.686: INFO: Pod var-expansion-e29496d4-0231-4a0f-a1ca-1605f8c17131 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:05:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1172" for this suite.
Mar  6 10:05:33.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:05:33.826: INFO: namespace var-expansion-1172 deletion completed in 6.133801499s

• [SLOW TEST:12.602 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:05:33.827: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  6 10:05:34.129: INFO: Waiting up to 5m0s for pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76" in namespace "downward-api-3951" to be "success or failure"
Mar  6 10:05:34.134: INFO: Pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537455ms
Mar  6 10:05:36.141: INFO: Pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011310476s
Mar  6 10:05:38.146: INFO: Pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016625445s
Mar  6 10:05:40.151: INFO: Pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021903035s
STEP: Saw pod success
Mar  6 10:05:40.151: INFO: Pod "downward-api-88037946-f88a-4d39-8921-8c45c723ae76" satisfied condition "success or failure"
Mar  6 10:05:40.155: INFO: Trying to get logs from node wisecloud-worker02 pod downward-api-88037946-f88a-4d39-8921-8c45c723ae76 container dapi-container: <nil>
STEP: delete the pod
Mar  6 10:05:40.221: INFO: Waiting for pod downward-api-88037946-f88a-4d39-8921-8c45c723ae76 to disappear
Mar  6 10:05:40.225: INFO: Pod downward-api-88037946-f88a-4d39-8921-8c45c723ae76 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:05:40.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3951" for this suite.
Mar  6 10:05:46.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:05:46.378: INFO: namespace downward-api-3951 deletion completed in 6.145456827s

• [SLOW TEST:12.552 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:05:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  6 10:05:46.713: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  6 10:05:46.747: INFO: Number of nodes with available pods: 0
Mar  6 10:05:46.747: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  6 10:05:46.847: INFO: Number of nodes with available pods: 0
Mar  6 10:05:46.847: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:47.853: INFO: Number of nodes with available pods: 0
Mar  6 10:05:47.853: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:48.852: INFO: Number of nodes with available pods: 0
Mar  6 10:05:48.852: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:49.852: INFO: Number of nodes with available pods: 1
Mar  6 10:05:49.852: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  6 10:05:49.919: INFO: Number of nodes with available pods: 1
Mar  6 10:05:49.919: INFO: Number of running nodes: 0, number of available pods: 1
Mar  6 10:05:50.934: INFO: Number of nodes with available pods: 0
Mar  6 10:05:50.934: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  6 10:05:50.961: INFO: Number of nodes with available pods: 0
Mar  6 10:05:50.961: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:51.966: INFO: Number of nodes with available pods: 0
Mar  6 10:05:51.966: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:53.010: INFO: Number of nodes with available pods: 0
Mar  6 10:05:53.010: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:53.965: INFO: Number of nodes with available pods: 0
Mar  6 10:05:53.965: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:54.967: INFO: Number of nodes with available pods: 0
Mar  6 10:05:54.967: INFO: Node wisecloud-worker01 is running more than one daemon pod
Mar  6 10:05:55.966: INFO: Number of nodes with available pods: 1
Mar  6 10:05:55.966: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6817, will wait for the garbage collector to delete the pods
Mar  6 10:05:56.073: INFO: Deleting DaemonSet.extensions daemon-set took: 44.940384ms
Mar  6 10:05:56.373: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.290164ms
Mar  6 10:06:08.505: INFO: Number of nodes with available pods: 0
Mar  6 10:06:08.505: INFO: Number of running nodes: 0, number of available pods: 0
Mar  6 10:06:08.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6817/daemonsets","resourceVersion":"32171"},"items":null}

Mar  6 10:06:08.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6817/pods","resourceVersion":"32171"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:06:08.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6817" for this suite.
Mar  6 10:06:14.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:06:14.723: INFO: namespace daemonsets-6817 deletion completed in 6.142257924s

• [SLOW TEST:28.344 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:06:14.724: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 10:06:15.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384" in namespace "projected-1879" to be "success or failure"
Mar  6 10:06:15.039: INFO: Pod "downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097865ms
Mar  6 10:06:17.044: INFO: Pod "downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530332s
Mar  6 10:06:19.076: INFO: Pod "downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040609321s
STEP: Saw pod success
Mar  6 10:06:19.076: INFO: Pod "downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384" satisfied condition "success or failure"
Mar  6 10:06:19.079: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384 container client-container: <nil>
STEP: delete the pod
Mar  6 10:06:19.149: INFO: Waiting for pod downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384 to disappear
Mar  6 10:06:19.153: INFO: Pod downwardapi-volume-dc774f67-2910-455f-a17f-d17009f53384 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:06:19.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1879" for this suite.
Mar  6 10:06:27.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:06:27.431: INFO: namespace projected-1879 deletion completed in 8.270835169s

• [SLOW TEST:12.708 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  6 10:06:27.431: INFO: >>> kubeConfig: /tmp/kubeconfig-201274422
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  6 10:06:27.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d" in namespace "downward-api-7511" to be "success or failure"
Mar  6 10:06:27.756: INFO: Pod "downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.548931ms
Mar  6 10:06:29.763: INFO: Pod "downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030161471s
Mar  6 10:06:31.767: INFO: Pod "downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034613783s
STEP: Saw pod success
Mar  6 10:06:31.767: INFO: Pod "downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d" satisfied condition "success or failure"
Mar  6 10:06:31.772: INFO: Trying to get logs from node wisecloud-worker02 pod downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d container client-container: <nil>
STEP: delete the pod
Mar  6 10:06:31.854: INFO: Waiting for pod downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d to disappear
Mar  6 10:06:31.858: INFO: Pod downwardapi-volume-bd41c1f8-50bd-4d77-ba09-94724f49147d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  6 10:06:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7511" for this suite.
Mar  6 10:06:37.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  6 10:06:38.035: INFO: namespace downward-api-7511 deletion completed in 6.161422285s

• [SLOW TEST:10.604 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSMar  6 10:06:38.035: INFO: Running AfterSuite actions on all nodes
Mar  6 10:06:38.035: INFO: Running AfterSuite actions on node 1
Mar  6 10:06:38.035: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6328.330 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h45m31.684089135s
Test Suite Passed
