I0722 20:29:14.777329      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-425149679
I0722 20:29:14.777547      15 e2e.go:241] Starting e2e run "b790c7d7-b8c7-4caa-9229-95acf423d555" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1563827353 - Will randomize all specs
Will run 215 of 4413 specs

Jul 22 20:29:15.035: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:29:15.037: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 22 20:29:15.064: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 22 20:29:15.098: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 22 20:29:15.098: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jul 22 20:29:15.098: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 22 20:29:15.106: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
Jul 22 20:29:15.106: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jul 22 20:29:15.106: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 22 20:29:15.106: INFO: e2e test version: v1.15.1
Jul 22 20:29:15.108: INFO: kube-apiserver version: v1.15.1
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:29:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
Jul 22 20:29:15.146: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jul 22 20:29:15.155: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 20:29:15.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9057'
Jul 22 20:29:15.923: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 20:29:15.923: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 22 20:29:15.949: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dnffx]
Jul 22 20:29:15.949: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dnffx" in namespace "kubectl-9057" to be "running and ready"
Jul 22 20:29:15.952: INFO: Pod "e2e-test-nginx-rc-dnffx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.221984ms
Jul 22 20:29:17.955: INFO: Pod "e2e-test-nginx-rc-dnffx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006076196s
Jul 22 20:29:19.958: INFO: Pod "e2e-test-nginx-rc-dnffx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009254405s
Jul 22 20:29:21.962: INFO: Pod "e2e-test-nginx-rc-dnffx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013453207s
Jul 22 20:29:23.964: INFO: Pod "e2e-test-nginx-rc-dnffx": Phase="Running", Reason="", readiness=true. Elapsed: 8.015662819s
Jul 22 20:29:23.964: INFO: Pod "e2e-test-nginx-rc-dnffx" satisfied condition "running and ready"
Jul 22 20:29:23.964: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dnffx]
Jul 22 20:29:23.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 logs rc/e2e-test-nginx-rc --namespace=kubectl-9057'
Jul 22 20:29:24.141: INFO: stderr: ""
Jul 22 20:29:24.141: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 22 20:29:24.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete rc e2e-test-nginx-rc --namespace=kubectl-9057'
Jul 22 20:29:24.238: INFO: stderr: ""
Jul 22 20:29:24.238: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:29:24.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9057" for this suite.
Jul 22 20:29:30.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:29:30.320: INFO: namespace kubectl-9057 deletion completed in 6.079575153s

• [SLOW TEST:15.212 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:29:30.321: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:29:50.478: INFO: Container started at 2019-07-22 20:29:33 +0000 UTC, pod became ready at 2019-07-22 20:29:50 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:29:50.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7621" for this suite.
Jul 22 20:30:12.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:30:12.572: INFO: namespace container-probe-7621 deletion completed in 22.090720598s

• [SLOW TEST:42.251 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:30:12.572: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:30:12.729: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020" in namespace "projected-2718" to be "success or failure"
Jul 22 20:30:12.732: INFO: Pod "downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967885ms
Jul 22 20:30:14.735: INFO: Pod "downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006062857s
Jul 22 20:30:16.740: INFO: Pod "downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010636121s
STEP: Saw pod success
Jul 22 20:30:16.740: INFO: Pod "downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020" satisfied condition "success or failure"
Jul 22 20:30:16.743: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020 container client-container: <nil>
STEP: delete the pod
Jul 22 20:30:16.814: INFO: Waiting for pod downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020 to disappear
Jul 22 20:30:16.816: INFO: Pod downwardapi-volume-3046e4e2-6b5b-477d-bd02-b644a7f3b020 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:30:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2718" for this suite.
Jul 22 20:30:22.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:30:22.910: INFO: namespace projected-2718 deletion completed in 6.090496992s

• [SLOW TEST:10.338 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:30:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6015
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 20:30:23.076: INFO: Waiting up to 5m0s for pod "pod-326f0d57-2c5d-48f8-94e4-48a8e305f711" in namespace "emptydir-6015" to be "success or failure"
Jul 22 20:30:23.079: INFO: Pod "pod-326f0d57-2c5d-48f8-94e4-48a8e305f711": Phase="Pending", Reason="", readiness=false. Elapsed: 3.094484ms
Jul 22 20:30:25.082: INFO: Pod "pod-326f0d57-2c5d-48f8-94e4-48a8e305f711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006082151s
Jul 22 20:30:27.085: INFO: Pod "pod-326f0d57-2c5d-48f8-94e4-48a8e305f711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008925517s
STEP: Saw pod success
Jul 22 20:30:27.086: INFO: Pod "pod-326f0d57-2c5d-48f8-94e4-48a8e305f711" satisfied condition "success or failure"
Jul 22 20:30:27.089: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-326f0d57-2c5d-48f8-94e4-48a8e305f711 container test-container: <nil>
STEP: delete the pod
Jul 22 20:30:27.107: INFO: Waiting for pod pod-326f0d57-2c5d-48f8-94e4-48a8e305f711 to disappear
Jul 22 20:30:27.109: INFO: Pod pod-326f0d57-2c5d-48f8-94e4-48a8e305f711 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:30:27.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6015" for this suite.
Jul 22 20:30:33.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:30:33.191: INFO: namespace emptydir-6015 deletion completed in 6.07915803s

• [SLOW TEST:10.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:30:33.194: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-5f08da12-a251-427c-b856-9d4b6f3e7f48
STEP: Creating a pod to test consume configMaps
Jul 22 20:30:33.363: INFO: Waiting up to 5m0s for pod "pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4" in namespace "configmap-7417" to be "success or failure"
Jul 22 20:30:33.368: INFO: Pod "pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.183479ms
Jul 22 20:30:35.382: INFO: Pod "pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018538781s
Jul 22 20:30:37.385: INFO: Pod "pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021813438s
STEP: Saw pod success
Jul 22 20:30:37.385: INFO: Pod "pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4" satisfied condition "success or failure"
Jul 22 20:30:37.387: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:30:37.406: INFO: Waiting for pod pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4 to disappear
Jul 22 20:30:37.408: INFO: Pod pod-configmaps-1146da13-ed43-47d3-adae-660d0a5943b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:30:37.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7417" for this suite.
Jul 22 20:30:43.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:30:43.501: INFO: namespace configmap-7417 deletion completed in 6.089764257s

• [SLOW TEST:10.307 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:30:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 22 20:30:53.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:53.678: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:53.813: INFO: Exec stderr: ""
Jul 22 20:30:53.813: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:53.813: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:53.945: INFO: Exec stderr: ""
Jul 22 20:30:53.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:53.945: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.072: INFO: Exec stderr: ""
Jul 22 20:30:54.072: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.279: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 22 20:30:54.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.280: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.414: INFO: Exec stderr: ""
Jul 22 20:30:54.414: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.414: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.539: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 22 20:30:54.539: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.539: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.678: INFO: Exec stderr: ""
Jul 22 20:30:54.678: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.678: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.812: INFO: Exec stderr: ""
Jul 22 20:30:54.812: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.813: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:54.951: INFO: Exec stderr: ""
Jul 22 20:30:54.951: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:30:54.951: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:30:55.095: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:30:55.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-531" for this suite.
Jul 22 20:31:33.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:31:33.181: INFO: namespace e2e-kubelet-etc-hosts-531 deletion completed in 38.082676269s

• [SLOW TEST:49.679 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:31:33.183: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:31:33.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd" in namespace "downward-api-7099" to be "success or failure"
Jul 22 20:31:33.343: INFO: Pod "downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107548ms
Jul 22 20:31:35.346: INFO: Pod "downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013608171s
Jul 22 20:31:37.349: INFO: Pod "downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016585396s
STEP: Saw pod success
Jul 22 20:31:37.349: INFO: Pod "downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd" satisfied condition "success or failure"
Jul 22 20:31:37.351: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd container client-container: <nil>
STEP: delete the pod
Jul 22 20:31:37.371: INFO: Waiting for pod downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd to disappear
Jul 22 20:31:37.373: INFO: Pod downwardapi-volume-dd22ee5a-3777-45fe-9b7f-a512ac3590bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:31:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7099" for this suite.
Jul 22 20:31:43.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:31:43.470: INFO: namespace downward-api-7099 deletion completed in 6.093949531s

• [SLOW TEST:10.287 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:31:43.470: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 22 20:31:43.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 api-versions'
Jul 22 20:31:43.705: INFO: stderr: ""
Jul 22 20:31:43.705: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:31:43.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6735" for this suite.
Jul 22 20:31:49.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:31:49.800: INFO: namespace kubectl-6735 deletion completed in 6.092607627s

• [SLOW TEST:6.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:31:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-2cph
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:31:49.960: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2cph" in namespace "subpath-6570" to be "success or failure"
Jul 22 20:31:49.962: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Pending", Reason="", readiness=false. Elapsed: 1.863791ms
Jul 22 20:31:51.965: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004921907s
Jul 22 20:31:53.967: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 4.007553725s
Jul 22 20:31:55.971: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 6.010787238s
Jul 22 20:31:57.974: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 8.013922951s
Jul 22 20:31:59.976: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 10.016563765s
Jul 22 20:32:01.979: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 12.019502477s
Jul 22 20:32:03.982: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 14.022283488s
Jul 22 20:32:05.985: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 16.025124898s
Jul 22 20:32:07.988: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 18.028475905s
Jul 22 20:32:09.992: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 20.03183421s
Jul 22 20:32:11.995: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Running", Reason="", readiness=true. Elapsed: 22.035108015s
Jul 22 20:32:13.998: INFO: Pod "pod-subpath-test-downwardapi-2cph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038222719s
STEP: Saw pod success
Jul 22 20:32:13.998: INFO: Pod "pod-subpath-test-downwardapi-2cph" satisfied condition "success or failure"
Jul 22 20:32:14.000: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-subpath-test-downwardapi-2cph container test-container-subpath-downwardapi-2cph: <nil>
STEP: delete the pod
Jul 22 20:32:14.013: INFO: Waiting for pod pod-subpath-test-downwardapi-2cph to disappear
Jul 22 20:32:14.015: INFO: Pod pod-subpath-test-downwardapi-2cph no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2cph
Jul 22 20:32:14.016: INFO: Deleting pod "pod-subpath-test-downwardapi-2cph" in namespace "subpath-6570"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:32:14.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6570" for this suite.
Jul 22 20:32:20.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:32:20.108: INFO: namespace subpath-6570 deletion completed in 6.086878109s

• [SLOW TEST:30.307 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:32:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-86e5b46f-6ff9-4463-b324-b53cfa962925 in namespace container-probe-2530
Jul 22 20:32:24.328: INFO: Started pod liveness-86e5b46f-6ff9-4463-b324-b53cfa962925 in namespace container-probe-2530
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:32:24.330: INFO: Initial restart count of pod liveness-86e5b46f-6ff9-4463-b324-b53cfa962925 is 0
Jul 22 20:32:42.358: INFO: Restart count of pod container-probe-2530/liveness-86e5b46f-6ff9-4463-b324-b53cfa962925 is now 1 (18.027795952s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:32:42.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2530" for this suite.
Jul 22 20:32:48.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:32:48.463: INFO: namespace container-probe-2530 deletion completed in 6.091941341s

• [SLOW TEST:28.355 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:32:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-1425/secret-test-70b31ac8-879c-449d-8c67-d179c8f84134
STEP: Creating a pod to test consume secrets
Jul 22 20:32:48.655: INFO: Waiting up to 5m0s for pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5" in namespace "secrets-1425" to be "success or failure"
Jul 22 20:32:48.658: INFO: Pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.147789ms
Jul 22 20:32:50.660: INFO: Pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004978177s
Jul 22 20:32:52.664: INFO: Pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008172462s
Jul 22 20:32:54.670: INFO: Pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01466723s
STEP: Saw pod success
Jul 22 20:32:54.670: INFO: Pod "pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5" satisfied condition "success or failure"
Jul 22 20:32:54.674: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5 container env-test: <nil>
STEP: delete the pod
Jul 22 20:32:54.690: INFO: Waiting for pod pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5 to disappear
Jul 22 20:32:54.692: INFO: Pod pod-configmaps-e12c8cc0-6bc2-4246-ac9b-b718528b3db5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:32:54.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1425" for this suite.
Jul 22 20:33:00.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:33:00.794: INFO: namespace secrets-1425 deletion completed in 6.099112687s

• [SLOW TEST:12.330 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:33:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2483
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 22 20:33:00.946: INFO: Waiting up to 5m0s for pod "pod-f825695f-dbd7-42a1-8885-c180f7edace7" in namespace "emptydir-2483" to be "success or failure"
Jul 22 20:33:00.948: INFO: Pod "pod-f825695f-dbd7-42a1-8885-c180f7edace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131089ms
Jul 22 20:33:02.951: INFO: Pod "pod-f825695f-dbd7-42a1-8885-c180f7edace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005874567s
Jul 22 20:33:04.954: INFO: Pod "pod-f825695f-dbd7-42a1-8885-c180f7edace7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008824148s
STEP: Saw pod success
Jul 22 20:33:04.954: INFO: Pod "pod-f825695f-dbd7-42a1-8885-c180f7edace7" satisfied condition "success or failure"
Jul 22 20:33:04.956: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-f825695f-dbd7-42a1-8885-c180f7edace7 container test-container: <nil>
STEP: delete the pod
Jul 22 20:33:04.970: INFO: Waiting for pod pod-f825695f-dbd7-42a1-8885-c180f7edace7 to disappear
Jul 22 20:33:04.972: INFO: Pod pod-f825695f-dbd7-42a1-8885-c180f7edace7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:33:04.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2483" for this suite.
Jul 22 20:33:10.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:33:11.067: INFO: namespace emptydir-2483 deletion completed in 6.092080409s

• [SLOW TEST:10.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:33:11.067: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:33:11.242: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"66f9503f-3cea-4312-8228-8e70f2837aca", Controller:(*bool)(0xc001746016), BlockOwnerDeletion:(*bool)(0xc001746017)}}
Jul 22 20:33:11.254: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fa4b0a4d-1667-4cbf-93e1-bc89546dbb9a", Controller:(*bool)(0xc0021fa7a6), BlockOwnerDeletion:(*bool)(0xc0021fa7a7)}}
Jul 22 20:33:11.284: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0c0a14c4-83e5-402a-9e9c-25d6a6d0ccda", Controller:(*bool)(0xc00174621e), BlockOwnerDeletion:(*bool)(0xc00174621f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:33:16.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7300" for this suite.
Jul 22 20:33:22.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:33:22.385: INFO: namespace gc-7300 deletion completed in 6.087216718s

• [SLOW TEST:11.318 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:33:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 20:33:22.558: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:22.621: INFO: Number of nodes with available pods: 0
Jul 22 20:33:22.621: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:33:23.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:23.630: INFO: Number of nodes with available pods: 0
Jul 22 20:33:23.630: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:33:24.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:24.629: INFO: Number of nodes with available pods: 0
Jul 22 20:33:24.629: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:33:25.624: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:25.627: INFO: Number of nodes with available pods: 1
Jul 22 20:33:25.627: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:26.624: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:26.628: INFO: Number of nodes with available pods: 1
Jul 22 20:33:26.628: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:27.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:27.629: INFO: Number of nodes with available pods: 1
Jul 22 20:33:27.629: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:28.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:28.628: INFO: Number of nodes with available pods: 1
Jul 22 20:33:28.628: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:29.626: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:29.629: INFO: Number of nodes with available pods: 1
Jul 22 20:33:29.629: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:30.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:30.627: INFO: Number of nodes with available pods: 2
Jul 22 20:33:30.627: INFO: Node k8s-linuxpool-16111918-2 is running more than one daemon pod
Jul 22 20:33:31.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:31.627: INFO: Number of nodes with available pods: 2
Jul 22 20:33:31.627: INFO: Node k8s-linuxpool-16111918-2 is running more than one daemon pod
Jul 22 20:33:32.648: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:32.650: INFO: Number of nodes with available pods: 2
Jul 22 20:33:32.651: INFO: Node k8s-linuxpool-16111918-2 is running more than one daemon pod
Jul 22 20:33:33.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:33.627: INFO: Number of nodes with available pods: 2
Jul 22 20:33:33.627: INFO: Node k8s-linuxpool-16111918-2 is running more than one daemon pod
Jul 22 20:33:34.624: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:34.627: INFO: Number of nodes with available pods: 2
Jul 22 20:33:34.627: INFO: Node k8s-linuxpool-16111918-2 is running more than one daemon pod
Jul 22 20:33:35.625: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:35.628: INFO: Number of nodes with available pods: 3
Jul 22 20:33:35.628: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 22 20:33:35.702: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:35.709: INFO: Number of nodes with available pods: 2
Jul 22 20:33:35.710: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:36.713: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:36.718: INFO: Number of nodes with available pods: 2
Jul 22 20:33:36.719: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:37.713: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:37.717: INFO: Number of nodes with available pods: 2
Jul 22 20:33:37.717: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 20:33:38.724: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 20:33:38.726: INFO: Number of nodes with available pods: 3
Jul 22 20:33:38.727: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3311, will wait for the garbage collector to delete the pods
Jul 22 20:33:38.788: INFO: Deleting DaemonSet.extensions daemon-set took: 5.87947ms
Jul 22 20:33:39.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.249951ms
Jul 22 20:33:47.991: INFO: Number of nodes with available pods: 0
Jul 22 20:33:47.991: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 20:33:47.995: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3311/daemonsets","resourceVersion":"9179"},"items":null}

Jul 22 20:33:47.997: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3311/pods","resourceVersion":"9179"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:33:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3311" for this suite.
Jul 22 20:33:54.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:33:54.098: INFO: namespace daemonsets-3311 deletion completed in 6.089338165s

• [SLOW TEST:31.712 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:33:54.101: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:33:54.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57" in namespace "projected-9640" to be "success or failure"
Jul 22 20:33:54.275: INFO: Pod "downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57": Phase="Pending", Reason="", readiness=false. Elapsed: 22.280385ms
Jul 22 20:33:56.279: INFO: Pod "downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026034339s
Jul 22 20:33:58.282: INFO: Pod "downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029007097s
STEP: Saw pod success
Jul 22 20:33:58.282: INFO: Pod "downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57" satisfied condition "success or failure"
Jul 22 20:33:58.284: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57 container client-container: <nil>
STEP: delete the pod
Jul 22 20:33:58.297: INFO: Waiting for pod downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57 to disappear
Jul 22 20:33:58.300: INFO: Pod downwardapi-volume-bf0824cf-f1d4-4a40-a922-9a8b47d8ff57 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:33:58.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9640" for this suite.
Jul 22 20:34:04.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:34:04.396: INFO: namespace projected-9640 deletion completed in 6.093479531s

• [SLOW TEST:10.295 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:34:04.397: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:34:04.550: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f" in namespace "projected-3682" to be "success or failure"
Jul 22 20:34:04.557: INFO: Pod "downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.272962ms
Jul 22 20:34:06.560: INFO: Pod "downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009761119s
Jul 22 20:34:08.563: INFO: Pod "downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013004471s
STEP: Saw pod success
Jul 22 20:34:08.563: INFO: Pod "downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f" satisfied condition "success or failure"
Jul 22 20:34:08.565: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f container client-container: <nil>
STEP: delete the pod
Jul 22 20:34:08.582: INFO: Waiting for pod downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f to disappear
Jul 22 20:34:08.584: INFO: Pod downwardapi-volume-6a64539a-2bf8-445a-982f-60bcae624a3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:34:08.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3682" for this suite.
Jul 22 20:34:14.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:34:14.678: INFO: namespace projected-3682 deletion completed in 6.091002731s

• [SLOW TEST:10.281 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:34:14.678: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 22 20:34:19.354: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4769 pod-service-account-fef15f80-9e74-4d19-a58f-2a725300ee19 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 22 20:34:19.565: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4769 pod-service-account-fef15f80-9e74-4d19-a58f-2a725300ee19 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 22 20:34:19.834: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4769 pod-service-account-fef15f80-9e74-4d19-a58f-2a725300ee19 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:34:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4769" for this suite.
Jul 22 20:34:26.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:34:26.198: INFO: namespace svcaccounts-4769 deletion completed in 6.118281777s

• [SLOW TEST:11.520 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:34:26.199: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 20:34:26.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7099'
Jul 22 20:34:26.464: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 20:34:26.464: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 22 20:34:26.477: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 22 20:34:26.482: INFO: scanned /root for discovery docs: <nil>
Jul 22 20:34:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7099'
Jul 22 20:34:42.238: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 22 20:34:42.238: INFO: stdout: "Created e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510\nScaling up e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 22 20:34:42.238: INFO: stdout: "Created e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510\nScaling up e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 22 20:34:42.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7099'
Jul 22 20:34:42.332: INFO: stderr: ""
Jul 22 20:34:42.333: INFO: stdout: "e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510-t55f8 "
Jul 22 20:34:42.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510-t55f8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7099'
Jul 22 20:34:42.414: INFO: stderr: ""
Jul 22 20:34:42.414: INFO: stdout: "true"
Jul 22 20:34:42.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510-t55f8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7099'
Jul 22 20:34:42.501: INFO: stderr: ""
Jul 22 20:34:42.501: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 22 20:34:42.501: INFO: e2e-test-nginx-rc-dd437c1f1c5cee9ec02d1a390850e510-t55f8 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 22 20:34:42.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete rc e2e-test-nginx-rc --namespace=kubectl-7099'
Jul 22 20:34:42.591: INFO: stderr: ""
Jul 22 20:34:42.591: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:34:42.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7099" for this suite.
Jul 22 20:35:04.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:35:04.682: INFO: namespace kubectl-7099 deletion completed in 22.087551908s

• [SLOW TEST:38.483 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:35:04.685: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 22 20:35:04.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 cluster-info'
Jul 22 20:35:04.931: INFO: stderr: ""
Jul 22 20:35:04.931: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtiller-deploy\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/tiller-deploy:tiller/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:35:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2970" for this suite.
Jul 22 20:35:10.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:35:11.035: INFO: namespace kubectl-2970 deletion completed in 6.100983014s

• [SLOW TEST:6.350 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:35:11.035: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 20:35:11.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5577'
Jul 22 20:35:11.269: INFO: stderr: ""
Jul 22 20:35:11.269: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 22 20:35:16.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pod e2e-test-nginx-pod --namespace=kubectl-5577 -o json'
Jul 22 20:35:16.412: INFO: stderr: ""
Jul 22 20:35:16.412: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-07-22T20:35:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5577\",\n        \"resourceVersion\": \"9536\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5577/pods/e2e-test-nginx-pod\",\n        \"uid\": \"f24769c9-ce5a-41c2-940d-15410b117e22\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-c5lpg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-linuxpool-16111918-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-c5lpg\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-c5lpg\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T20:35:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T20:35:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T20:35:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T20:35:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6006d5769f52c52c873cc7704b6da96f2404b9a82aa0677572fef7ef03636b15\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-22T20:35:13Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.24\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-22T20:35:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 22 20:35:16.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 replace -f - --namespace=kubectl-5577'
Jul 22 20:35:16.728: INFO: stderr: ""
Jul 22 20:35:16.728: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 22 20:35:16.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete pods e2e-test-nginx-pod --namespace=kubectl-5577'
Jul 22 20:35:27.890: INFO: stderr: ""
Jul 22 20:35:27.890: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:35:27.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5577" for this suite.
Jul 22 20:35:33.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:35:33.981: INFO: namespace kubectl-5577 deletion completed in 6.086879863s

• [SLOW TEST:22.946 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:35:33.981: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 20:35:34.130: INFO: Waiting up to 5m0s for pod "pod-ec087c19-08a3-4caf-85d0-ec132bcb0329" in namespace "emptydir-5983" to be "success or failure"
Jul 22 20:35:34.135: INFO: Pod "pod-ec087c19-08a3-4caf-85d0-ec132bcb0329": Phase="Pending", Reason="", readiness=false. Elapsed: 5.007174ms
Jul 22 20:35:36.138: INFO: Pod "pod-ec087c19-08a3-4caf-85d0-ec132bcb0329": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008162993s
Jul 22 20:35:38.141: INFO: Pod "pod-ec087c19-08a3-4caf-85d0-ec132bcb0329": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011028914s
STEP: Saw pod success
Jul 22 20:35:38.141: INFO: Pod "pod-ec087c19-08a3-4caf-85d0-ec132bcb0329" satisfied condition "success or failure"
Jul 22 20:35:38.144: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-ec087c19-08a3-4caf-85d0-ec132bcb0329 container test-container: <nil>
STEP: delete the pod
Jul 22 20:35:38.161: INFO: Waiting for pod pod-ec087c19-08a3-4caf-85d0-ec132bcb0329 to disappear
Jul 22 20:35:38.163: INFO: Pod pod-ec087c19-08a3-4caf-85d0-ec132bcb0329 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:35:38.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5983" for this suite.
Jul 22 20:35:44.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:35:44.252: INFO: namespace emptydir-5983 deletion completed in 6.084048167s

• [SLOW TEST:10.271 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:35:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:35:44.412: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 22 20:35:44.421: INFO: Number of nodes with available pods: 0
Jul 22 20:35:44.421: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 22 20:35:44.435: INFO: Number of nodes with available pods: 0
Jul 22 20:35:44.435: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:45.438: INFO: Number of nodes with available pods: 0
Jul 22 20:35:45.438: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:46.438: INFO: Number of nodes with available pods: 0
Jul 22 20:35:46.438: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:47.438: INFO: Number of nodes with available pods: 1
Jul 22 20:35:47.438: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 22 20:35:47.454: INFO: Number of nodes with available pods: 1
Jul 22 20:35:47.454: INFO: Number of running nodes: 0, number of available pods: 1
Jul 22 20:35:48.457: INFO: Number of nodes with available pods: 0
Jul 22 20:35:48.457: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 22 20:35:48.472: INFO: Number of nodes with available pods: 0
Jul 22 20:35:48.472: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:49.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:49.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:50.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:50.476: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:51.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:51.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:52.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:52.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:53.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:53.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:54.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:54.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:55.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:55.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:56.476: INFO: Number of nodes with available pods: 0
Jul 22 20:35:56.476: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:57.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:57.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:58.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:58.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:35:59.475: INFO: Number of nodes with available pods: 0
Jul 22 20:35:59.475: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:36:00.476: INFO: Number of nodes with available pods: 0
Jul 22 20:36:00.476: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 20:36:01.475: INFO: Number of nodes with available pods: 1
Jul 22 20:36:01.475: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1819, will wait for the garbage collector to delete the pods
Jul 22 20:36:01.536: INFO: Deleting DaemonSet.extensions daemon-set took: 5.111474ms
Jul 22 20:36:01.836: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.271243ms
Jul 22 20:36:07.939: INFO: Number of nodes with available pods: 0
Jul 22 20:36:07.939: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 20:36:07.941: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1819/daemonsets","resourceVersion":"9722"},"items":null}

Jul 22 20:36:07.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1819/pods","resourceVersion":"9722"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:36:07.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1819" for this suite.
Jul 22 20:36:13.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:36:14.061: INFO: namespace daemonsets-1819 deletion completed in 6.083463641s

• [SLOW TEST:29.808 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:36:14.062: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 20:36:14.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1'
Jul 22 20:36:14.295: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 20:36:14.295: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 22 20:36:14.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete jobs e2e-test-nginx-job --namespace=kubectl-1'
Jul 22 20:36:14.379: INFO: stderr: ""
Jul 22 20:36:14.379: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:36:14.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1" for this suite.
Jul 22 20:36:20.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:36:20.468: INFO: namespace kubectl-1 deletion completed in 6.084625428s

• [SLOW TEST:6.406 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:36:20.471: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-ce5cdfb8-1f27-400b-af55-536fe87f7fcc
STEP: Creating a pod to test consume configMaps
Jul 22 20:36:20.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7" in namespace "projected-6397" to be "success or failure"
Jul 22 20:36:20.626: INFO: Pod "pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04469ms
Jul 22 20:36:22.629: INFO: Pod "pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295194s
Jul 22 20:36:24.632: INFO: Pod "pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615597s
STEP: Saw pod success
Jul 22 20:36:24.633: INFO: Pod "pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7" satisfied condition "success or failure"
Jul 22 20:36:24.635: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:36:24.649: INFO: Waiting for pod pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7 to disappear
Jul 22 20:36:24.652: INFO: Pod pod-projected-configmaps-d566b498-f5bc-4561-ab1d-119f7cc56ed7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:36:24.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6397" for this suite.
Jul 22 20:36:30.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:36:30.761: INFO: namespace projected-6397 deletion completed in 6.106513805s

• [SLOW TEST:10.290 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:36:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 22 20:36:34.925: INFO: Pod pod-hostip-bfd05f95-e84d-4875-8880-361ae794ca89 has hostIP: 10.240.0.6
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:36:34.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2202" for this suite.
Jul 22 20:36:56.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:36:57.068: INFO: namespace pods-2202 deletion completed in 22.139984925s

• [SLOW TEST:26.303 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:36:57.068: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:37:00.243: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:37:00.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1584" for this suite.
Jul 22 20:37:06.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:37:06.345: INFO: namespace container-runtime-1584 deletion completed in 6.083594792s

• [SLOW TEST:9.277 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:37:06.348: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 22 20:37:06.500: INFO: Waiting up to 5m0s for pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c" in namespace "containers-9593" to be "success or failure"
Jul 22 20:37:06.506: INFO: Pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.260968ms
Jul 22 20:37:08.510: INFO: Pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010227255s
Jul 22 20:37:10.515: INFO: Pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015660734s
Jul 22 20:37:12.519: INFO: Pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018737324s
STEP: Saw pod success
Jul 22 20:37:12.519: INFO: Pod "client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c" satisfied condition "success or failure"
Jul 22 20:37:12.521: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c container test-container: <nil>
STEP: delete the pod
Jul 22 20:37:12.546: INFO: Waiting for pod client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c to disappear
Jul 22 20:37:12.548: INFO: Pod client-containers-512ff912-ce8b-45d5-a17d-ccf2393b011c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:37:12.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9593" for this suite.
Jul 22 20:37:18.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:37:18.642: INFO: namespace containers-9593 deletion completed in 6.091421541s

• [SLOW TEST:12.295 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:37:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 22 20:37:18.788: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 22 20:37:19.087: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 22 20:37:21.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:23.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:25.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:27.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:29.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:31.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:33.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:35.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:37.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:39.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:41.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699424639, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:37:43.952: INFO: Waited 820.372033ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:37:44.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4032" for this suite.
Jul 22 20:37:50.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:37:50.674: INFO: namespace aggregator-4032 deletion completed in 6.185572525s

• [SLOW TEST:32.031 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:37:50.675: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5286
I0722 20:37:50.846874      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5286, replica count: 1
I0722 20:37:51.897428      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:37:52.898644      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:37:53.899733      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:37:54.900107      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:37:55.010: INFO: Created: latency-svc-vgnnt
Jul 22 20:37:55.022: INFO: Got endpoints: latency-svc-vgnnt [21.765687ms]
Jul 22 20:37:55.055: INFO: Created: latency-svc-xpnqj
Jul 22 20:37:55.063: INFO: Created: latency-svc-nkq9g
Jul 22 20:37:55.064: INFO: Got endpoints: latency-svc-xpnqj [41.680583ms]
Jul 22 20:37:55.073: INFO: Got endpoints: latency-svc-nkq9g [50.525237ms]
Jul 22 20:37:55.073: INFO: Created: latency-svc-qbqst
Jul 22 20:37:55.084: INFO: Created: latency-svc-4lnb6
Jul 22 20:37:55.091: INFO: Got endpoints: latency-svc-qbqst [69.20664ms]
Jul 22 20:37:55.098: INFO: Got endpoints: latency-svc-4lnb6 [75.267608ms]
Jul 22 20:37:55.107: INFO: Created: latency-svc-lf7fg
Jul 22 20:37:55.115: INFO: Created: latency-svc-z767q
Jul 22 20:37:55.115: INFO: Created: latency-svc-ngqmf
Jul 22 20:37:55.119: INFO: Got endpoints: latency-svc-z767q [96.274999ms]
Jul 22 20:37:55.125: INFO: Got endpoints: latency-svc-lf7fg [102.453167ms]
Jul 22 20:37:55.125: INFO: Got endpoints: latency-svc-ngqmf [102.534866ms]
Jul 22 20:37:55.127: INFO: Created: latency-svc-ps9hn
Jul 22 20:37:55.136: INFO: Got endpoints: latency-svc-ps9hn [113.279511ms]
Jul 22 20:37:55.139: INFO: Created: latency-svc-vsf2z
Jul 22 20:37:55.149: INFO: Got endpoints: latency-svc-vsf2z [126.76114ms]
Jul 22 20:37:55.151: INFO: Created: latency-svc-mxkc5
Jul 22 20:37:55.155: INFO: Created: latency-svc-dmxw4
Jul 22 20:37:55.155: INFO: Got endpoints: latency-svc-dmxw4 [132.220312ms]
Jul 22 20:37:55.162: INFO: Created: latency-svc-lr68c
Jul 22 20:37:55.167: INFO: Got endpoints: latency-svc-mxkc5 [143.508253ms]
Jul 22 20:37:55.199: INFO: Created: latency-svc-chzws
Jul 22 20:37:55.199: INFO: Got endpoints: latency-svc-lr68c [176.062784ms]
Jul 22 20:37:55.206: INFO: Created: latency-svc-h7zp7
Jul 22 20:37:55.208: INFO: Got endpoints: latency-svc-chzws [185.033937ms]
Jul 22 20:37:55.223: INFO: Created: latency-svc-wgccc
Jul 22 20:37:55.228: INFO: Got endpoints: latency-svc-h7zp7 [204.978634ms]
Jul 22 20:37:55.231: INFO: Got endpoints: latency-svc-wgccc [207.796619ms]
Jul 22 20:37:55.235: INFO: Created: latency-svc-54dcv
Jul 22 20:37:55.244: INFO: Got endpoints: latency-svc-54dcv [180.788859ms]
Jul 22 20:37:55.249: INFO: Created: latency-svc-t8w7s
Jul 22 20:37:55.253: INFO: Created: latency-svc-k7zwd
Jul 22 20:37:55.257: INFO: Got endpoints: latency-svc-t8w7s [184.241642ms]
Jul 22 20:37:55.260: INFO: Got endpoints: latency-svc-k7zwd [31.377337ms]
Jul 22 20:37:55.272: INFO: Created: latency-svc-g84lr
Jul 22 20:37:55.274: INFO: Created: latency-svc-tz7kw
Jul 22 20:37:55.281: INFO: Got endpoints: latency-svc-tz7kw [189.425415ms]
Jul 22 20:37:55.284: INFO: Created: latency-svc-rtbsl
Jul 22 20:37:55.290: INFO: Got endpoints: latency-svc-g84lr [192.283299ms]
Jul 22 20:37:55.311: INFO: Created: latency-svc-nrxpr
Jul 22 20:37:55.328: INFO: Got endpoints: latency-svc-nrxpr [202.457446ms]
Jul 22 20:37:55.328: INFO: Got endpoints: latency-svc-rtbsl [209.324811ms]
Jul 22 20:37:55.334: INFO: Created: latency-svc-bt7hr
Jul 22 20:37:55.341: INFO: Created: latency-svc-ts6z4
Jul 22 20:37:55.346: INFO: Got endpoints: latency-svc-bt7hr [220.661952ms]
Jul 22 20:37:55.349: INFO: Got endpoints: latency-svc-ts6z4 [212.435094ms]
Jul 22 20:37:55.357: INFO: Created: latency-svc-dvqr6
Jul 22 20:37:55.361: INFO: Got endpoints: latency-svc-dvqr6 [211.848398ms]
Jul 22 20:37:55.369: INFO: Created: latency-svc-cgqj7
Jul 22 20:37:55.380: INFO: Created: latency-svc-4pzfl
Jul 22 20:37:55.390: INFO: Got endpoints: latency-svc-cgqj7 [234.47548ms]
Jul 22 20:37:55.412: INFO: Created: latency-svc-7695z
Jul 22 20:37:55.419: INFO: Got endpoints: latency-svc-4pzfl [251.999589ms]
Jul 22 20:37:55.425: INFO: Got endpoints: latency-svc-7695z [225.888525ms]
Jul 22 20:37:55.435: INFO: Created: latency-svc-b8lhp
Jul 22 20:37:55.445: INFO: Got endpoints: latency-svc-b8lhp [237.369065ms]
Jul 22 20:37:55.449: INFO: Created: latency-svc-cnwmb
Jul 22 20:37:55.457: INFO: Created: latency-svc-pp6p2
Jul 22 20:37:55.465: INFO: Created: latency-svc-vr9ch
Jul 22 20:37:55.474: INFO: Created: latency-svc-mf7df
Jul 22 20:37:55.480: INFO: Got endpoints: latency-svc-mf7df [222.686142ms]
Jul 22 20:37:55.480: INFO: Created: latency-svc-spf7q
Jul 22 20:37:55.480: INFO: Got endpoints: latency-svc-pp6p2 [235.519875ms]
Jul 22 20:37:55.480: INFO: Got endpoints: latency-svc-cnwmb [249.091904ms]
Jul 22 20:37:55.480: INFO: Got endpoints: latency-svc-vr9ch [220.702451ms]
Jul 22 20:37:55.490: INFO: Got endpoints: latency-svc-spf7q [208.779714ms]
Jul 22 20:37:55.499: INFO: Created: latency-svc-5v9hz
Jul 22 20:37:55.500: INFO: Got endpoints: latency-svc-5v9hz [210.177306ms]
Jul 22 20:37:55.512: INFO: Created: latency-svc-x65dn
Jul 22 20:37:55.513: INFO: Got endpoints: latency-svc-x65dn [185.130537ms]
Jul 22 20:37:55.514: INFO: Created: latency-svc-5m74x
Jul 22 20:37:55.522: INFO: Created: latency-svc-xrcl4
Jul 22 20:37:55.524: INFO: Got endpoints: latency-svc-5m74x [196.241279ms]
Jul 22 20:37:55.529: INFO: Got endpoints: latency-svc-xrcl4 [183.282046ms]
Jul 22 20:37:55.541: INFO: Created: latency-svc-5krkd
Jul 22 20:37:55.576: INFO: Created: latency-svc-qmnp9
Jul 22 20:37:55.582: INFO: Got endpoints: latency-svc-5krkd [233.190087ms]
Jul 22 20:37:55.586: INFO: Got endpoints: latency-svc-qmnp9 [224.775731ms]
Jul 22 20:37:55.589: INFO: Created: latency-svc-hgtlp
Jul 22 20:37:55.604: INFO: Created: latency-svc-nv5m9
Jul 22 20:37:55.639: INFO: Created: latency-svc-qtdn9
Jul 22 20:37:55.652: INFO: Got endpoints: latency-svc-hgtlp [261.992037ms]
Jul 22 20:37:55.659: INFO: Created: latency-svc-ks5dn
Jul 22 20:37:55.669: INFO: Created: latency-svc-8kvsb
Jul 22 20:37:55.671: INFO: Got endpoints: latency-svc-nv5m9 [252.195688ms]
Jul 22 20:37:55.689: INFO: Created: latency-svc-bb9sn
Jul 22 20:37:55.690: INFO: Created: latency-svc-6zfjk
Jul 22 20:37:55.743: INFO: Created: latency-svc-xdjcf
Jul 22 20:37:55.753: INFO: Created: latency-svc-dhjq8
Jul 22 20:37:55.759: INFO: Got endpoints: latency-svc-qtdn9 [333.421065ms]
Jul 22 20:37:55.773: INFO: Created: latency-svc-8f8w2
Jul 22 20:37:55.774: INFO: Got endpoints: latency-svc-ks5dn [328.281692ms]
Jul 22 20:37:55.788: INFO: Created: latency-svc-npppd
Jul 22 20:37:55.801: INFO: Created: latency-svc-g84ph
Jul 22 20:37:55.823: INFO: Created: latency-svc-4bvms
Jul 22 20:37:55.838: INFO: Created: latency-svc-btdjx
Jul 22 20:37:55.844: INFO: Got endpoints: latency-svc-8kvsb [364.095405ms]
Jul 22 20:37:55.850: INFO: Created: latency-svc-cd5qq
Jul 22 20:37:55.856: INFO: Created: latency-svc-9f6gx
Jul 22 20:37:55.871: INFO: Got endpoints: latency-svc-bb9sn [390.439369ms]
Jul 22 20:37:55.876: INFO: Created: latency-svc-gr4mk
Jul 22 20:37:55.883: INFO: Created: latency-svc-b7jv7
Jul 22 20:37:55.900: INFO: Created: latency-svc-zfmpk
Jul 22 20:37:55.912: INFO: Created: latency-svc-l7ggx
Jul 22 20:37:55.925: INFO: Got endpoints: latency-svc-6zfjk [444.329589ms]
Jul 22 20:37:55.934: INFO: Created: latency-svc-dn4dj
Jul 22 20:37:55.947: INFO: Created: latency-svc-rnxmd
Jul 22 20:37:55.978: INFO: Got endpoints: latency-svc-xdjcf [497.805011ms]
Jul 22 20:37:56.001: INFO: Created: latency-svc-wwbmf
Jul 22 20:37:56.024: INFO: Got endpoints: latency-svc-dhjq8 [534.089821ms]
Jul 22 20:37:56.046: INFO: Created: latency-svc-tkpm9
Jul 22 20:37:56.065: INFO: Got endpoints: latency-svc-8f8w2 [564.890061ms]
Jul 22 20:37:56.078: INFO: Created: latency-svc-prtr2
Jul 22 20:37:56.159: INFO: Got endpoints: latency-svc-npppd [645.974339ms]
Jul 22 20:37:56.168: INFO: Got endpoints: latency-svc-g84ph [643.94985ms]
Jul 22 20:37:56.174: INFO: Created: latency-svc-jgkcc
Jul 22 20:37:56.244: INFO: Got endpoints: latency-svc-4bvms [714.321984ms]
Jul 22 20:37:56.244: INFO: Created: latency-svc-24lcp
Jul 22 20:37:56.259: INFO: Created: latency-svc-47v7m
Jul 22 20:37:56.271: INFO: Got endpoints: latency-svc-btdjx [688.720716ms]
Jul 22 20:37:56.294: INFO: Created: latency-svc-4lg8b
Jul 22 20:37:56.315: INFO: Got endpoints: latency-svc-cd5qq [726.415521ms]
Jul 22 20:37:56.328: INFO: Created: latency-svc-5xr97
Jul 22 20:37:56.367: INFO: Got endpoints: latency-svc-9f6gx [715.01038ms]
Jul 22 20:37:56.390: INFO: Created: latency-svc-9xxh8
Jul 22 20:37:56.415: INFO: Got endpoints: latency-svc-gr4mk [743.70593ms]
Jul 22 20:37:56.429: INFO: Created: latency-svc-pngsj
Jul 22 20:37:56.465: INFO: Got endpoints: latency-svc-b7jv7 [706.766823ms]
Jul 22 20:37:56.475: INFO: Created: latency-svc-mrpnv
Jul 22 20:37:56.521: INFO: Got endpoints: latency-svc-zfmpk [746.982813ms]
Jul 22 20:37:56.532: INFO: Created: latency-svc-k4zjd
Jul 22 20:37:56.565: INFO: Got endpoints: latency-svc-l7ggx [721.090048ms]
Jul 22 20:37:56.575: INFO: Created: latency-svc-tlfqs
Jul 22 20:37:56.622: INFO: Got endpoints: latency-svc-dn4dj [751.104992ms]
Jul 22 20:37:56.635: INFO: Created: latency-svc-sdntq
Jul 22 20:37:56.672: INFO: Got endpoints: latency-svc-rnxmd [747.63651ms]
Jul 22 20:37:56.683: INFO: Created: latency-svc-tpvcp
Jul 22 20:37:56.738: INFO: Got endpoints: latency-svc-wwbmf [760.176145ms]
Jul 22 20:37:56.780: INFO: Created: latency-svc-w9bpk
Jul 22 20:37:56.869: INFO: Got endpoints: latency-svc-prtr2 [802.931822ms]
Jul 22 20:37:56.869: INFO: Got endpoints: latency-svc-tkpm9 [844.978804ms]
Jul 22 20:37:56.880: INFO: Created: latency-svc-b22fb
Jul 22 20:37:56.889: INFO: Created: latency-svc-hxllh
Jul 22 20:37:56.912: INFO: Got endpoints: latency-svc-jgkcc [752.374285ms]
Jul 22 20:37:56.918: INFO: Got endpoints: latency-svc-24lcp [749.778899ms]
Jul 22 20:37:56.939: INFO: Created: latency-svc-xjfn2
Jul 22 20:37:56.939: INFO: Created: latency-svc-wn8w8
Jul 22 20:37:56.970: INFO: Got endpoints: latency-svc-47v7m [725.902123ms]
Jul 22 20:37:56.980: INFO: Created: latency-svc-s5gjm
Jul 22 20:37:57.019: INFO: Got endpoints: latency-svc-4lg8b [748.007309ms]
Jul 22 20:37:57.048: INFO: Created: latency-svc-74vlc
Jul 22 20:37:57.090: INFO: Got endpoints: latency-svc-5xr97 [774.476471ms]
Jul 22 20:37:57.111: INFO: Created: latency-svc-vtbv2
Jul 22 20:37:57.117: INFO: Got endpoints: latency-svc-9xxh8 [749.930498ms]
Jul 22 20:37:57.128: INFO: Created: latency-svc-kjt8m
Jul 22 20:37:57.167: INFO: Got endpoints: latency-svc-pngsj [752.105187ms]
Jul 22 20:37:57.183: INFO: Created: latency-svc-8ddmr
Jul 22 20:37:57.216: INFO: Got endpoints: latency-svc-mrpnv [750.529195ms]
Jul 22 20:37:57.229: INFO: Created: latency-svc-n8vmd
Jul 22 20:37:57.265: INFO: Got endpoints: latency-svc-k4zjd [744.567126ms]
Jul 22 20:37:57.283: INFO: Created: latency-svc-2m6tx
Jul 22 20:37:57.316: INFO: Got endpoints: latency-svc-tlfqs [750.625195ms]
Jul 22 20:37:57.328: INFO: Created: latency-svc-6jv4p
Jul 22 20:37:57.367: INFO: Got endpoints: latency-svc-sdntq [745.036124ms]
Jul 22 20:37:57.377: INFO: Created: latency-svc-wcdnw
Jul 22 20:37:57.416: INFO: Got endpoints: latency-svc-tpvcp [743.648031ms]
Jul 22 20:37:57.428: INFO: Created: latency-svc-lsdgk
Jul 22 20:37:57.466: INFO: Got endpoints: latency-svc-w9bpk [727.558014ms]
Jul 22 20:37:57.480: INFO: Created: latency-svc-qjhw8
Jul 22 20:37:57.516: INFO: Got endpoints: latency-svc-b22fb [646.880034ms]
Jul 22 20:37:57.526: INFO: Created: latency-svc-xplbh
Jul 22 20:37:57.576: INFO: Got endpoints: latency-svc-hxllh [706.816922ms]
Jul 22 20:37:57.595: INFO: Created: latency-svc-m8x4p
Jul 22 20:37:57.623: INFO: Got endpoints: latency-svc-wn8w8 [711.663197ms]
Jul 22 20:37:57.635: INFO: Created: latency-svc-n64b6
Jul 22 20:37:57.688: INFO: Got endpoints: latency-svc-xjfn2 [769.298097ms]
Jul 22 20:37:57.701: INFO: Created: latency-svc-9fq7k
Jul 22 20:37:57.721: INFO: Got endpoints: latency-svc-s5gjm [751.323791ms]
Jul 22 20:37:57.763: INFO: Created: latency-svc-vlfzp
Jul 22 20:37:57.790: INFO: Got endpoints: latency-svc-74vlc [770.67239ms]
Jul 22 20:37:57.839: INFO: Got endpoints: latency-svc-vtbv2 [749.164502ms]
Jul 22 20:37:57.845: INFO: Created: latency-svc-8zf8d
Jul 22 20:37:57.883: INFO: Got endpoints: latency-svc-kjt8m [765.376317ms]
Jul 22 20:37:57.907: INFO: Created: latency-svc-w4gkl
Jul 22 20:37:57.907: INFO: Created: latency-svc-lnwsc
Jul 22 20:37:57.930: INFO: Got endpoints: latency-svc-8ddmr [762.762932ms]
Jul 22 20:37:57.966: INFO: Got endpoints: latency-svc-n8vmd [749.448201ms]
Jul 22 20:37:57.966: INFO: Created: latency-svc-9p7pn
Jul 22 20:37:57.976: INFO: Created: latency-svc-xw49t
Jul 22 20:37:58.035: INFO: Got endpoints: latency-svc-2m6tx [769.936494ms]
Jul 22 20:37:58.062: INFO: Created: latency-svc-bvs7z
Jul 22 20:37:58.073: INFO: Got endpoints: latency-svc-6jv4p [757.29306ms]
Jul 22 20:37:58.086: INFO: Created: latency-svc-kfkdt
Jul 22 20:37:58.116: INFO: Got endpoints: latency-svc-wcdnw [748.694904ms]
Jul 22 20:37:58.130: INFO: Created: latency-svc-8n6nj
Jul 22 20:37:58.169: INFO: Got endpoints: latency-svc-lsdgk [752.271286ms]
Jul 22 20:37:58.186: INFO: Created: latency-svc-6mbvv
Jul 22 20:37:58.215: INFO: Got endpoints: latency-svc-qjhw8 [748.748704ms]
Jul 22 20:37:58.232: INFO: Created: latency-svc-ckmrk
Jul 22 20:37:58.264: INFO: Got endpoints: latency-svc-xplbh [748.335107ms]
Jul 22 20:37:58.279: INFO: Created: latency-svc-c8782
Jul 22 20:37:58.320: INFO: Got endpoints: latency-svc-m8x4p [743.507432ms]
Jul 22 20:37:58.338: INFO: Created: latency-svc-h8vdg
Jul 22 20:37:58.366: INFO: Got endpoints: latency-svc-n64b6 [742.696636ms]
Jul 22 20:37:58.381: INFO: Created: latency-svc-k9gnf
Jul 22 20:37:58.417: INFO: Got endpoints: latency-svc-9fq7k [729.391905ms]
Jul 22 20:37:58.428: INFO: Created: latency-svc-nh6j5
Jul 22 20:37:58.464: INFO: Got endpoints: latency-svc-vlfzp [742.536837ms]
Jul 22 20:37:58.477: INFO: Created: latency-svc-wld8z
Jul 22 20:37:58.522: INFO: Got endpoints: latency-svc-8zf8d [732.663888ms]
Jul 22 20:37:58.544: INFO: Created: latency-svc-t8xkn
Jul 22 20:37:58.572: INFO: Got endpoints: latency-svc-w4gkl [732.33439ms]
Jul 22 20:37:58.589: INFO: Created: latency-svc-ghqcf
Jul 22 20:37:58.621: INFO: Got endpoints: latency-svc-lnwsc [738.250359ms]
Jul 22 20:37:58.630: INFO: Created: latency-svc-qs8x7
Jul 22 20:37:58.666: INFO: Got endpoints: latency-svc-9p7pn [736.15557ms]
Jul 22 20:37:58.678: INFO: Created: latency-svc-6fggs
Jul 22 20:37:58.717: INFO: Got endpoints: latency-svc-xw49t [751.011592ms]
Jul 22 20:37:58.732: INFO: Created: latency-svc-mn7rr
Jul 22 20:37:58.766: INFO: Got endpoints: latency-svc-bvs7z [730.007602ms]
Jul 22 20:37:58.776: INFO: Created: latency-svc-cpqxp
Jul 22 20:37:58.817: INFO: Got endpoints: latency-svc-kfkdt [743.879229ms]
Jul 22 20:37:58.830: INFO: Created: latency-svc-4rx7x
Jul 22 20:37:58.869: INFO: Got endpoints: latency-svc-8n6nj [753.085181ms]
Jul 22 20:37:58.893: INFO: Created: latency-svc-r8pzz
Jul 22 20:37:58.924: INFO: Got endpoints: latency-svc-6mbvv [755.027072ms]
Jul 22 20:37:58.936: INFO: Created: latency-svc-gl98v
Jul 22 20:37:58.968: INFO: Got endpoints: latency-svc-ckmrk [752.588585ms]
Jul 22 20:37:58.979: INFO: Created: latency-svc-xr7gw
Jul 22 20:37:59.029: INFO: Got endpoints: latency-svc-c8782 [765.341118ms]
Jul 22 20:37:59.047: INFO: Created: latency-svc-rvpk4
Jul 22 20:37:59.078: INFO: Got endpoints: latency-svc-h8vdg [750.077997ms]
Jul 22 20:37:59.088: INFO: Created: latency-svc-mhx6w
Jul 22 20:37:59.116: INFO: Got endpoints: latency-svc-k9gnf [749.694499ms]
Jul 22 20:37:59.127: INFO: Created: latency-svc-xq7b8
Jul 22 20:37:59.170: INFO: Got endpoints: latency-svc-nh6j5 [753.239981ms]
Jul 22 20:37:59.182: INFO: Created: latency-svc-8zv2g
Jul 22 20:37:59.217: INFO: Got endpoints: latency-svc-wld8z [752.614884ms]
Jul 22 20:37:59.228: INFO: Created: latency-svc-vqwzj
Jul 22 20:37:59.267: INFO: Got endpoints: latency-svc-t8xkn [744.636925ms]
Jul 22 20:37:59.278: INFO: Created: latency-svc-992dx
Jul 22 20:37:59.320: INFO: Got endpoints: latency-svc-ghqcf [744.175028ms]
Jul 22 20:37:59.334: INFO: Created: latency-svc-bxh8c
Jul 22 20:37:59.367: INFO: Got endpoints: latency-svc-qs8x7 [745.658021ms]
Jul 22 20:37:59.376: INFO: Created: latency-svc-8n8pj
Jul 22 20:37:59.416: INFO: Got endpoints: latency-svc-6fggs [750.147797ms]
Jul 22 20:37:59.431: INFO: Created: latency-svc-pm67t
Jul 22 20:37:59.466: INFO: Got endpoints: latency-svc-mn7rr [749.244401ms]
Jul 22 20:37:59.478: INFO: Created: latency-svc-mtmrg
Jul 22 20:37:59.522: INFO: Got endpoints: latency-svc-cpqxp [756.387964ms]
Jul 22 20:37:59.598: INFO: Created: latency-svc-gjrrx
Jul 22 20:37:59.598: INFO: Got endpoints: latency-svc-4rx7x [780.23134ms]
Jul 22 20:37:59.612: INFO: Created: latency-svc-wxp5w
Jul 22 20:37:59.614: INFO: Got endpoints: latency-svc-r8pzz [744.970423ms]
Jul 22 20:37:59.632: INFO: Created: latency-svc-sw97g
Jul 22 20:37:59.666: INFO: Got endpoints: latency-svc-gl98v [741.76964ms]
Jul 22 20:37:59.680: INFO: Created: latency-svc-rj7fg
Jul 22 20:37:59.715: INFO: Got endpoints: latency-svc-xr7gw [747.142613ms]
Jul 22 20:37:59.725: INFO: Created: latency-svc-7mtds
Jul 22 20:37:59.777: INFO: Got endpoints: latency-svc-rvpk4 [747.209812ms]
Jul 22 20:37:59.787: INFO: Created: latency-svc-mx8v8
Jul 22 20:37:59.822: INFO: Got endpoints: latency-svc-mhx6w [743.655831ms]
Jul 22 20:37:59.833: INFO: Created: latency-svc-b75sn
Jul 22 20:37:59.870: INFO: Got endpoints: latency-svc-xq7b8 [753.786778ms]
Jul 22 20:37:59.882: INFO: Created: latency-svc-5sxgj
Jul 22 20:37:59.917: INFO: Got endpoints: latency-svc-8zv2g [746.429717ms]
Jul 22 20:37:59.930: INFO: Created: latency-svc-k2m46
Jul 22 20:37:59.968: INFO: Got endpoints: latency-svc-vqwzj [750.739994ms]
Jul 22 20:37:59.978: INFO: Created: latency-svc-d9h2k
Jul 22 20:38:00.020: INFO: Got endpoints: latency-svc-992dx [752.869983ms]
Jul 22 20:38:00.031: INFO: Created: latency-svc-7dbx5
Jul 22 20:38:00.090: INFO: Got endpoints: latency-svc-bxh8c [770.176293ms]
Jul 22 20:38:00.101: INFO: Created: latency-svc-np2cg
Jul 22 20:38:00.115: INFO: Got endpoints: latency-svc-8n8pj [748.550405ms]
Jul 22 20:38:00.129: INFO: Created: latency-svc-xwh2f
Jul 22 20:38:00.165: INFO: Got endpoints: latency-svc-pm67t [748.371406ms]
Jul 22 20:38:00.235: INFO: Got endpoints: latency-svc-mtmrg [768.934299ms]
Jul 22 20:38:00.235: INFO: Created: latency-svc-svp7k
Jul 22 20:38:00.259: INFO: Created: latency-svc-w7n7l
Jul 22 20:38:00.269: INFO: Got endpoints: latency-svc-gjrrx [746.317317ms]
Jul 22 20:38:00.283: INFO: Created: latency-svc-pdlhg
Jul 22 20:38:00.316: INFO: Got endpoints: latency-svc-wxp5w [717.861065ms]
Jul 22 20:38:00.328: INFO: Created: latency-svc-k2vzf
Jul 22 20:38:00.366: INFO: Got endpoints: latency-svc-sw97g [751.299391ms]
Jul 22 20:38:00.379: INFO: Created: latency-svc-8lc98
Jul 22 20:38:00.415: INFO: Got endpoints: latency-svc-rj7fg [748.880203ms]
Jul 22 20:38:00.426: INFO: Created: latency-svc-jjzdz
Jul 22 20:38:00.464: INFO: Got endpoints: latency-svc-7mtds [748.830804ms]
Jul 22 20:38:00.478: INFO: Created: latency-svc-xq5cd
Jul 22 20:38:00.515: INFO: Got endpoints: latency-svc-mx8v8 [738.343358ms]
Jul 22 20:38:00.528: INFO: Created: latency-svc-5vmfg
Jul 22 20:38:00.570: INFO: Got endpoints: latency-svc-b75sn [748.355606ms]
Jul 22 20:38:00.593: INFO: Created: latency-svc-wft7b
Jul 22 20:38:00.616: INFO: Got endpoints: latency-svc-5sxgj [746.233017ms]
Jul 22 20:38:00.626: INFO: Created: latency-svc-bwg95
Jul 22 20:38:00.665: INFO: Got endpoints: latency-svc-k2m46 [748.029908ms]
Jul 22 20:38:00.685: INFO: Created: latency-svc-tlmf5
Jul 22 20:38:00.717: INFO: Got endpoints: latency-svc-d9h2k [749.347901ms]
Jul 22 20:38:00.737: INFO: Created: latency-svc-fbp4t
Jul 22 20:38:00.770: INFO: Got endpoints: latency-svc-7dbx5 [749.705699ms]
Jul 22 20:38:00.785: INFO: Created: latency-svc-tr5dk
Jul 22 20:38:00.822: INFO: Got endpoints: latency-svc-np2cg [731.537994ms]
Jul 22 20:38:00.831: INFO: Created: latency-svc-5n67w
Jul 22 20:38:00.866: INFO: Got endpoints: latency-svc-xwh2f [750.974492ms]
Jul 22 20:38:00.879: INFO: Created: latency-svc-pn4lw
Jul 22 20:38:00.914: INFO: Got endpoints: latency-svc-svp7k [748.916804ms]
Jul 22 20:38:00.925: INFO: Created: latency-svc-7nn6j
Jul 22 20:38:00.966: INFO: Got endpoints: latency-svc-w7n7l [730.765497ms]
Jul 22 20:38:00.978: INFO: Created: latency-svc-mvmfq
Jul 22 20:38:01.019: INFO: Got endpoints: latency-svc-pdlhg [750.098197ms]
Jul 22 20:38:01.028: INFO: Created: latency-svc-5dv6g
Jul 22 20:38:01.067: INFO: Got endpoints: latency-svc-k2vzf [749.711999ms]
Jul 22 20:38:01.083: INFO: Created: latency-svc-cf6tz
Jul 22 20:38:01.115: INFO: Got endpoints: latency-svc-8lc98 [749.673699ms]
Jul 22 20:38:01.135: INFO: Created: latency-svc-s8cgb
Jul 22 20:38:01.166: INFO: Got endpoints: latency-svc-jjzdz [751.760188ms]
Jul 22 20:38:01.179: INFO: Created: latency-svc-pxtx8
Jul 22 20:38:01.219: INFO: Got endpoints: latency-svc-xq5cd [754.986671ms]
Jul 22 20:38:01.236: INFO: Created: latency-svc-jlvfw
Jul 22 20:38:01.266: INFO: Got endpoints: latency-svc-5vmfg [750.860893ms]
Jul 22 20:38:01.281: INFO: Created: latency-svc-fx2fl
Jul 22 20:38:01.321: INFO: Got endpoints: latency-svc-wft7b [750.473895ms]
Jul 22 20:38:01.335: INFO: Created: latency-svc-rvg5x
Jul 22 20:38:01.367: INFO: Got endpoints: latency-svc-bwg95 [750.188496ms]
Jul 22 20:38:01.377: INFO: Created: latency-svc-p88xt
Jul 22 20:38:01.421: INFO: Got endpoints: latency-svc-tlmf5 [756.351464ms]
Jul 22 20:38:01.433: INFO: Created: latency-svc-6x4w5
Jul 22 20:38:01.472: INFO: Got endpoints: latency-svc-fbp4t [754.608573ms]
Jul 22 20:38:01.485: INFO: Created: latency-svc-gvx4f
Jul 22 20:38:01.515: INFO: Got endpoints: latency-svc-tr5dk [744.706725ms]
Jul 22 20:38:01.529: INFO: Created: latency-svc-p64h9
Jul 22 20:38:01.568: INFO: Got endpoints: latency-svc-5n67w [745.78812ms]
Jul 22 20:38:01.581: INFO: Created: latency-svc-gngj2
Jul 22 20:38:01.615: INFO: Got endpoints: latency-svc-pn4lw [748.629004ms]
Jul 22 20:38:01.626: INFO: Created: latency-svc-v7hx4
Jul 22 20:38:01.665: INFO: Got endpoints: latency-svc-7nn6j [751.372691ms]
Jul 22 20:38:01.674: INFO: Created: latency-svc-877sf
Jul 22 20:38:01.716: INFO: Got endpoints: latency-svc-mvmfq [749.650099ms]
Jul 22 20:38:01.727: INFO: Created: latency-svc-57vwp
Jul 22 20:38:01.766: INFO: Got endpoints: latency-svc-5dv6g [747.46531ms]
Jul 22 20:38:01.784: INFO: Created: latency-svc-psll2
Jul 22 20:38:01.816: INFO: Got endpoints: latency-svc-cf6tz [748.785903ms]
Jul 22 20:38:01.827: INFO: Created: latency-svc-k87kf
Jul 22 20:38:01.865: INFO: Got endpoints: latency-svc-s8cgb [749.5259ms]
Jul 22 20:38:01.891: INFO: Created: latency-svc-htlnn
Jul 22 20:38:01.918: INFO: Got endpoints: latency-svc-pxtx8 [751.301791ms]
Jul 22 20:38:01.935: INFO: Created: latency-svc-9tctl
Jul 22 20:38:01.973: INFO: Got endpoints: latency-svc-jlvfw [751.663188ms]
Jul 22 20:38:01.982: INFO: Created: latency-svc-6qnn4
Jul 22 20:38:02.017: INFO: Got endpoints: latency-svc-fx2fl [750.379196ms]
Jul 22 20:38:02.029: INFO: Created: latency-svc-gtvk6
Jul 22 20:38:02.078: INFO: Got endpoints: latency-svc-rvg5x [757.647458ms]
Jul 22 20:38:02.088: INFO: Created: latency-svc-dzfp6
Jul 22 20:38:02.117: INFO: Got endpoints: latency-svc-p88xt [750.565394ms]
Jul 22 20:38:02.131: INFO: Created: latency-svc-7s8k6
Jul 22 20:38:02.169: INFO: Got endpoints: latency-svc-6x4w5 [747.456711ms]
Jul 22 20:38:02.203: INFO: Created: latency-svc-rkxdj
Jul 22 20:38:02.216: INFO: Got endpoints: latency-svc-gvx4f [743.887229ms]
Jul 22 20:38:02.229: INFO: Created: latency-svc-5fjhr
Jul 22 20:38:02.266: INFO: Got endpoints: latency-svc-p64h9 [750.842993ms]
Jul 22 20:38:02.278: INFO: Created: latency-svc-xwbxz
Jul 22 20:38:02.320: INFO: Got endpoints: latency-svc-gngj2 [748.075007ms]
Jul 22 20:38:02.330: INFO: Created: latency-svc-5mlhp
Jul 22 20:38:02.366: INFO: Got endpoints: latency-svc-v7hx4 [750.935093ms]
Jul 22 20:38:02.382: INFO: Created: latency-svc-85cm7
Jul 22 20:38:02.417: INFO: Got endpoints: latency-svc-877sf [751.853188ms]
Jul 22 20:38:02.430: INFO: Created: latency-svc-7fx65
Jul 22 20:38:02.468: INFO: Got endpoints: latency-svc-57vwp [751.277791ms]
Jul 22 20:38:02.499: INFO: Created: latency-svc-6s826
Jul 22 20:38:02.520: INFO: Got endpoints: latency-svc-psll2 [753.39178ms]
Jul 22 20:38:02.535: INFO: Created: latency-svc-bxrkv
Jul 22 20:38:02.568: INFO: Got endpoints: latency-svc-k87kf [752.366585ms]
Jul 22 20:38:02.594: INFO: Created: latency-svc-wzb7t
Jul 22 20:38:02.622: INFO: Got endpoints: latency-svc-htlnn [756.498564ms]
Jul 22 20:38:02.632: INFO: Created: latency-svc-frp5c
Jul 22 20:38:02.668: INFO: Got endpoints: latency-svc-9tctl [749.736998ms]
Jul 22 20:38:02.680: INFO: Created: latency-svc-lwwz5
Jul 22 20:38:02.728: INFO: Got endpoints: latency-svc-6qnn4 [754.133075ms]
Jul 22 20:38:02.754: INFO: Created: latency-svc-xp6m4
Jul 22 20:38:02.766: INFO: Got endpoints: latency-svc-gtvk6 [749.146102ms]
Jul 22 20:38:02.777: INFO: Created: latency-svc-hwxfp
Jul 22 20:38:02.817: INFO: Got endpoints: latency-svc-dzfp6 [738.09226ms]
Jul 22 20:38:02.834: INFO: Created: latency-svc-9j7jl
Jul 22 20:38:02.864: INFO: Got endpoints: latency-svc-7s8k6 [746.703915ms]
Jul 22 20:38:02.968: INFO: Got endpoints: latency-svc-5fjhr [751.713088ms]
Jul 22 20:38:02.968: INFO: Got endpoints: latency-svc-rkxdj [798.774944ms]
Jul 22 20:38:03.017: INFO: Got endpoints: latency-svc-xwbxz [751.169791ms]
Jul 22 20:38:03.067: INFO: Got endpoints: latency-svc-5mlhp [746.865914ms]
Jul 22 20:38:03.116: INFO: Got endpoints: latency-svc-85cm7 [749.4103ms]
Jul 22 20:38:03.165: INFO: Got endpoints: latency-svc-7fx65 [748.049807ms]
Jul 22 20:38:03.219: INFO: Got endpoints: latency-svc-6s826 [751.308991ms]
Jul 22 20:38:03.265: INFO: Got endpoints: latency-svc-bxrkv [745.475621ms]
Jul 22 20:38:03.317: INFO: Got endpoints: latency-svc-wzb7t [733.786382ms]
Jul 22 20:38:03.366: INFO: Got endpoints: latency-svc-frp5c [744.504226ms]
Jul 22 20:38:03.416: INFO: Got endpoints: latency-svc-lwwz5 [748.595104ms]
Jul 22 20:38:03.465: INFO: Got endpoints: latency-svc-xp6m4 [737.017765ms]
Jul 22 20:38:03.593: INFO: Got endpoints: latency-svc-hwxfp [826.657099ms]
Jul 22 20:38:03.596: INFO: Got endpoints: latency-svc-9j7jl [778.850948ms]
Jul 22 20:38:03.596: INFO: Latencies: [31.377337ms 41.680583ms 50.525237ms 69.20664ms 75.267608ms 96.274999ms 102.453167ms 102.534866ms 113.279511ms 126.76114ms 132.220312ms 143.508253ms 176.062784ms 180.788859ms 183.282046ms 184.241642ms 185.033937ms 185.130537ms 189.425415ms 192.283299ms 196.241279ms 202.457446ms 204.978634ms 207.796619ms 208.779714ms 209.324811ms 210.177306ms 211.848398ms 212.435094ms 220.661952ms 220.702451ms 222.686142ms 224.775731ms 225.888525ms 233.190087ms 234.47548ms 235.519875ms 237.369065ms 249.091904ms 251.999589ms 252.195688ms 261.992037ms 328.281692ms 333.421065ms 364.095405ms 390.439369ms 444.329589ms 497.805011ms 534.089821ms 564.890061ms 643.94985ms 645.974339ms 646.880034ms 688.720716ms 706.766823ms 706.816922ms 711.663197ms 714.321984ms 715.01038ms 717.861065ms 721.090048ms 725.902123ms 726.415521ms 727.558014ms 729.391905ms 730.007602ms 730.765497ms 731.537994ms 732.33439ms 732.663888ms 733.786382ms 736.15557ms 737.017765ms 738.09226ms 738.250359ms 738.343358ms 741.76964ms 742.536837ms 742.696636ms 743.507432ms 743.648031ms 743.655831ms 743.70593ms 743.879229ms 743.887229ms 744.175028ms 744.504226ms 744.567126ms 744.636925ms 744.706725ms 744.970423ms 745.036124ms 745.475621ms 745.658021ms 745.78812ms 746.233017ms 746.317317ms 746.429717ms 746.703915ms 746.865914ms 746.982813ms 747.142613ms 747.209812ms 747.456711ms 747.46531ms 747.63651ms 748.007309ms 748.029908ms 748.049807ms 748.075007ms 748.335107ms 748.355606ms 748.371406ms 748.550405ms 748.595104ms 748.629004ms 748.694904ms 748.748704ms 748.785903ms 748.830804ms 748.880203ms 748.916804ms 749.146102ms 749.164502ms 749.244401ms 749.347901ms 749.4103ms 749.448201ms 749.5259ms 749.650099ms 749.673699ms 749.694499ms 749.705699ms 749.711999ms 749.736998ms 749.778899ms 749.930498ms 750.077997ms 750.098197ms 750.147797ms 750.188496ms 750.379196ms 750.473895ms 750.529195ms 750.565394ms 750.625195ms 750.739994ms 750.842993ms 750.860893ms 750.935093ms 750.974492ms 751.011592ms 751.104992ms 751.169791ms 751.277791ms 751.299391ms 751.301791ms 751.308991ms 751.323791ms 751.372691ms 751.663188ms 751.713088ms 751.760188ms 751.853188ms 752.105187ms 752.271286ms 752.366585ms 752.374285ms 752.588585ms 752.614884ms 752.869983ms 753.085181ms 753.239981ms 753.39178ms 753.786778ms 754.133075ms 754.608573ms 754.986671ms 755.027072ms 756.351464ms 756.387964ms 756.498564ms 757.29306ms 757.647458ms 760.176145ms 762.762932ms 765.341118ms 765.376317ms 768.934299ms 769.298097ms 769.936494ms 770.176293ms 770.67239ms 774.476471ms 778.850948ms 780.23134ms 798.774944ms 802.931822ms 826.657099ms 844.978804ms]
Jul 22 20:38:03.596: INFO: 50 %ile: 746.982813ms
Jul 22 20:38:03.596: INFO: 90 %ile: 756.387964ms
Jul 22 20:38:03.596: INFO: 99 %ile: 826.657099ms
Jul 22 20:38:03.596: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:38:03.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5286" for this suite.
Jul 22 20:38:19.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:38:19.715: INFO: namespace svc-latency-5286 deletion completed in 16.107891862s

• [SLOW TEST:29.040 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:38:19.715: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7d501906-c870-4351-acaf-1dd78cb50d5e
STEP: Creating a pod to test consume configMaps
Jul 22 20:38:19.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef" in namespace "configmap-4640" to be "success or failure"
Jul 22 20:38:19.874: INFO: Pod "pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07439ms
Jul 22 20:38:21.878: INFO: Pod "pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005797958s
Jul 22 20:38:23.881: INFO: Pod "pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00881163s
STEP: Saw pod success
Jul 22 20:38:23.881: INFO: Pod "pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef" satisfied condition "success or failure"
Jul 22 20:38:23.883: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:38:23.901: INFO: Waiting for pod pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef to disappear
Jul 22 20:38:23.902: INFO: Pod pod-configmaps-ec98b409-9cdd-486a-a5e0-133c86e807ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:38:23.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4640" for this suite.
Jul 22 20:38:29.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:38:29.993: INFO: namespace configmap-4640 deletion completed in 6.087164807s

• [SLOW TEST:10.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:38:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:38:30.143: INFO: Creating ReplicaSet my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7
Jul 22 20:38:30.151: INFO: Pod name my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7: Found 0 pods out of 1
Jul 22 20:38:35.155: INFO: Pod name my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7: Found 1 pods out of 1
Jul 22 20:38:35.155: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7" is running
Jul 22 20:38:35.157: INFO: Pod "my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7-n2qml" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 20:38:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 20:38:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 20:38:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 20:38:30 +0000 UTC Reason: Message:}])
Jul 22 20:38:35.158: INFO: Trying to dial the pod
Jul 22 20:38:40.166: INFO: Controller my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7: Got expected result from replica 1 [my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7-n2qml]: "my-hostname-basic-2155aa20-6a1e-4662-b58f-d0f8f7b3a5b7-n2qml", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:38:40.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5020" for this suite.
Jul 22 20:38:46.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:38:46.262: INFO: namespace replicaset-5020 deletion completed in 6.092586566s

• [SLOW TEST:16.270 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:38:46.266: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 20:38:56.644: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:38:56.658: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:38:58.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:38:58.662: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:00.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:00.661: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:02.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:02.662: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:04.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:04.661: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:06.659: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:06.670: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:08.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:08.662: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:10.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:10.661: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:12.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:12.662: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:39:14.658: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:39:14.662: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:39:14.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3644" for this suite.
Jul 22 20:39:36.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:39:36.787: INFO: namespace container-lifecycle-hook-3644 deletion completed in 22.115287404s

• [SLOW TEST:50.521 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:39:36.788: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7297
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 22 20:39:36.959: INFO: Found 0 stateful pods, waiting for 3
Jul 22 20:39:46.963: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:39:46.963: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:39:46.963: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:39:46.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-7297 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 20:39:47.773: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 20:39:47.773: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 20:39:47.773: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 22 20:39:57.799: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 22 20:40:07.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-7297 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 20:40:08.050: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 20:40:08.050: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 20:40:08.050: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 20:40:18.065: INFO: Waiting for StatefulSet statefulset-7297/ss2 to complete update
Jul 22 20:40:18.065: INFO: Waiting for Pod statefulset-7297/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 22 20:40:18.065: INFO: Waiting for Pod statefulset-7297/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 22 20:40:28.071: INFO: Waiting for StatefulSet statefulset-7297/ss2 to complete update
Jul 22 20:40:28.071: INFO: Waiting for Pod statefulset-7297/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 22 20:40:38.071: INFO: Waiting for StatefulSet statefulset-7297/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 22 20:40:48.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-7297 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 20:40:48.293: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 20:40:48.293: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 20:40:48.293: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 20:40:58.320: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 22 20:41:08.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-7297 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 20:41:08.578: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 20:41:08.578: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 20:41:08.578: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 20:41:28.597: INFO: Waiting for StatefulSet statefulset-7297/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 22 20:41:38.602: INFO: Deleting all statefulset in ns statefulset-7297
Jul 22 20:41:38.604: INFO: Scaling statefulset ss2 to 0
Jul 22 20:42:08.692: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:42:08.695: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:42:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7297" for this suite.
Jul 22 20:42:14.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:42:14.812: INFO: namespace statefulset-7297 deletion completed in 6.103879384s

• [SLOW TEST:158.025 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:42:14.813: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:42:15.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51" in namespace "projected-6359" to be "success or failure"
Jul 22 20:42:15.038: INFO: Pod "downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00839ms
Jul 22 20:42:17.041: INFO: Pod "downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005223015s
Jul 22 20:42:19.044: INFO: Pod "downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007708943s
STEP: Saw pod success
Jul 22 20:42:19.044: INFO: Pod "downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51" satisfied condition "success or failure"
Jul 22 20:42:19.046: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51 container client-container: <nil>
STEP: delete the pod
Jul 22 20:42:19.060: INFO: Waiting for pod downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51 to disappear
Jul 22 20:42:19.072: INFO: Pod downwardapi-volume-6e03c91d-c616-4fb6-a3ed-984a2ae61d51 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:42:19.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6359" for this suite.
Jul 22 20:42:25.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:42:25.168: INFO: namespace projected-6359 deletion completed in 6.092845537s

• [SLOW TEST:10.355 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:42:25.169: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:42:28.397: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:42:28.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9838" for this suite.
Jul 22 20:42:34.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:42:34.500: INFO: namespace container-runtime-9838 deletion completed in 6.086066068s

• [SLOW TEST:9.332 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:42:34.502: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:42:34.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6919" for this suite.
Jul 22 20:42:40.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:42:40.781: INFO: namespace kubelet-test-6919 deletion completed in 6.10047949s

• [SLOW TEST:6.279 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:42:40.783: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 22 20:42:41.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-978'
Jul 22 20:42:41.380: INFO: stderr: ""
Jul 22 20:42:41.380: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 20:42:42.383: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:42.383: INFO: Found 0 / 1
Jul 22 20:42:43.383: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:43.383: INFO: Found 0 / 1
Jul 22 20:42:44.383: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:44.383: INFO: Found 0 / 1
Jul 22 20:42:45.383: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:45.384: INFO: Found 1 / 1
Jul 22 20:42:45.384: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 22 20:42:45.386: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:45.386: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 20:42:45.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 patch pod redis-master-m2xd9 --namespace=kubectl-978 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 22 20:42:45.471: INFO: stderr: ""
Jul 22 20:42:45.471: INFO: stdout: "pod/redis-master-m2xd9 patched\n"
STEP: checking annotations
Jul 22 20:42:45.473: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:42:45.473: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:42:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-978" for this suite.
Jul 22 20:43:07.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:43:07.681: INFO: namespace kubectl-978 deletion completed in 22.204729422s

• [SLOW TEST:26.898 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:43:07.684: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-684c1589-2c69-4128-9a1f-345bd24bf01a
STEP: Creating secret with name secret-projected-all-test-volume-754ff57e-91be-40f0-815a-80c21ec6522f
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 22 20:43:07.842: INFO: Waiting up to 5m0s for pod "projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085" in namespace "projected-7666" to be "success or failure"
Jul 22 20:43:07.849: INFO: Pod "projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085": Phase="Pending", Reason="", readiness=false. Elapsed: 6.466466ms
Jul 22 20:43:09.853: INFO: Pod "projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010684978s
Jul 22 20:43:11.856: INFO: Pod "projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013760696s
STEP: Saw pod success
Jul 22 20:43:11.856: INFO: Pod "projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085" satisfied condition "success or failure"
Jul 22 20:43:11.859: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 22 20:43:11.871: INFO: Waiting for pod projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085 to disappear
Jul 22 20:43:11.873: INFO: Pod projected-volume-9cc15f0f-247e-4f3d-af32-365d1811c085 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:43:11.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7666" for this suite.
Jul 22 20:43:17.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:43:18.028: INFO: namespace projected-7666 deletion completed in 6.152406202s

• [SLOW TEST:10.344 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:43:18.031: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5673
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5673
STEP: Creating statefulset with conflicting port in namespace statefulset-5673
STEP: Waiting until pod test-pod will start running in namespace statefulset-5673
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5673
Jul 22 20:43:22.234: INFO: Observed stateful pod in namespace: statefulset-5673, name: ss-0, uid: 64a1ffb5-a880-4602-9ded-a2b581700ee5, status phase: Pending. Waiting for statefulset controller to delete.
Jul 22 20:43:27.877: INFO: Observed stateful pod in namespace: statefulset-5673, name: ss-0, uid: 64a1ffb5-a880-4602-9ded-a2b581700ee5, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 20:43:27.944: INFO: Observed stateful pod in namespace: statefulset-5673, name: ss-0, uid: 64a1ffb5-a880-4602-9ded-a2b581700ee5, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 20:43:27.944: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5673
STEP: Removing pod with conflicting port in namespace statefulset-5673
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5673 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 22 20:43:32.027: INFO: Deleting all statefulset in ns statefulset-5673
Jul 22 20:43:32.030: INFO: Scaling statefulset ss to 0
Jul 22 20:43:42.042: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:43:42.045: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:43:42.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5673" for this suite.
Jul 22 20:43:48.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:43:48.214: INFO: namespace statefulset-5673 deletion completed in 6.154647178s

• [SLOW TEST:30.183 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:43:48.215: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:43:48.374: INFO: (0) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.211273ms)
Jul 22 20:43:48.377: INFO: (1) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.144884ms)
Jul 22 20:43:48.384: INFO: (2) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 7.140462ms)
Jul 22 20:43:48.387: INFO: (3) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.009484ms)
Jul 22 20:43:48.390: INFO: (4) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.903385ms)
Jul 22 20:43:48.396: INFO: (5) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.953568ms)
Jul 22 20:43:48.399: INFO: (6) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.334183ms)
Jul 22 20:43:48.403: INFO: (7) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.161283ms)
Jul 22 20:43:48.406: INFO: (8) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.779986ms)
Jul 22 20:43:48.409: INFO: (9) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.865785ms)
Jul 22 20:43:48.412: INFO: (10) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.082984ms)
Jul 22 20:43:48.415: INFO: (11) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.396483ms)
Jul 22 20:43:48.419: INFO: (12) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.140684ms)
Jul 22 20:43:48.422: INFO: (13) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.935985ms)
Jul 22 20:43:48.425: INFO: (14) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.938685ms)
Jul 22 20:43:48.428: INFO: (15) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.307983ms)
Jul 22 20:43:48.432: INFO: (16) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.411983ms)
Jul 22 20:43:48.435: INFO: (17) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.357483ms)
Jul 22 20:43:48.439: INFO: (18) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.371682ms)
Jul 22 20:43:48.442: INFO: (19) /api/v1/nodes/k8s-linuxpool-16111918-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.620981ms)
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:43:48.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6524" for this suite.
Jul 22 20:43:54.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:43:54.545: INFO: namespace proxy-6524 deletion completed in 6.098492069s

• [SLOW TEST:6.330 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:43:54.546: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-df7dd154-e294-4969-8e36-4db4848ebfd7
STEP: Creating a pod to test consume secrets
Jul 22 20:43:54.701: INFO: Waiting up to 5m0s for pod "pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8" in namespace "secrets-2592" to be "success or failure"
Jul 22 20:43:54.710: INFO: Pod "pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.972453ms
Jul 22 20:43:56.713: INFO: Pod "pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012511262s
Jul 22 20:43:58.720: INFO: Pod "pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019452653s
STEP: Saw pod success
Jul 22 20:43:58.720: INFO: Pod "pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8" satisfied condition "success or failure"
Jul 22 20:43:58.723: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:43:58.786: INFO: Waiting for pod pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8 to disappear
Jul 22 20:43:58.790: INFO: Pod pod-secrets-3c12ba72-f386-4af8-acc5-9f89cf9b37e8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:43:58.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2592" for this suite.
Jul 22 20:44:04.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:44:04.939: INFO: namespace secrets-2592 deletion completed in 6.146870312s

• [SLOW TEST:10.394 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:44:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 22 20:44:05.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-3308'
Jul 22 20:44:05.366: INFO: stderr: ""
Jul 22 20:44:05.366: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 22 20:44:06.370: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:44:06.370: INFO: Found 0 / 1
Jul 22 20:44:07.370: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:44:07.370: INFO: Found 0 / 1
Jul 22 20:44:08.370: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:44:08.370: INFO: Found 1 / 1
Jul 22 20:44:08.370: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 20:44:08.372: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:44:08.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 22 20:44:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 logs redis-master-bk4lb redis-master --namespace=kubectl-3308'
Jul 22 20:44:08.469: INFO: stderr: ""
Jul 22 20:44:08.469: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 20:44:07.058 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 20:44:07.058 # Server started, Redis version 3.2.12\n1:M 22 Jul 20:44:07.058 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 20:44:07.058 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 22 20:44:08.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 log redis-master-bk4lb redis-master --namespace=kubectl-3308 --tail=1'
Jul 22 20:44:08.581: INFO: stderr: ""
Jul 22 20:44:08.581: INFO: stdout: "1:M 22 Jul 20:44:07.058 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 22 20:44:08.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 log redis-master-bk4lb redis-master --namespace=kubectl-3308 --limit-bytes=1'
Jul 22 20:44:08.683: INFO: stderr: ""
Jul 22 20:44:08.683: INFO: stdout: " "
STEP: exposing timestamps
Jul 22 20:44:08.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 log redis-master-bk4lb redis-master --namespace=kubectl-3308 --tail=1 --timestamps'
Jul 22 20:44:08.777: INFO: stderr: ""
Jul 22 20:44:08.777: INFO: stdout: "2019-07-22T20:44:07.059056637Z 1:M 22 Jul 20:44:07.058 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 22 20:44:11.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 log redis-master-bk4lb redis-master --namespace=kubectl-3308 --since=1s'
Jul 22 20:44:11.366: INFO: stderr: ""
Jul 22 20:44:11.367: INFO: stdout: ""
Jul 22 20:44:11.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 log redis-master-bk4lb redis-master --namespace=kubectl-3308 --since=24h'
Jul 22 20:44:11.463: INFO: stderr: ""
Jul 22 20:44:11.463: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 20:44:07.058 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 20:44:07.058 # Server started, Redis version 3.2.12\n1:M 22 Jul 20:44:07.058 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 20:44:07.058 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 22 20:44:11.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-3308'
Jul 22 20:44:11.555: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:44:11.555: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 22 20:44:11.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3308'
Jul 22 20:44:11.709: INFO: stderr: "No resources found.\n"
Jul 22 20:44:11.709: INFO: stdout: ""
Jul 22 20:44:11.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=nginx --namespace=kubectl-3308 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 20:44:11.799: INFO: stderr: ""
Jul 22 20:44:11.800: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:44:11.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3308" for this suite.
Jul 22 20:44:33.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:44:33.891: INFO: namespace kubectl-3308 deletion completed in 22.08729731s

• [SLOW TEST:28.951 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:44:33.893: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 22 20:44:38.576: INFO: Successfully updated pod "labelsupdate2410e1eb-8f21-4847-a17a-b08e1151c7f9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:44:40.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4441" for this suite.
Jul 22 20:45:02.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:45:02.687: INFO: namespace projected-4441 deletion completed in 22.095127631s

• [SLOW TEST:28.794 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:45:02.687: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 22 20:45:02.847: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3430" to be "success or failure"
Jul 22 20:45:02.853: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.82757ms
Jul 22 20:45:04.856: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009292571s
Jul 22 20:45:06.859: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012041476s
STEP: Saw pod success
Jul 22 20:45:06.859: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 22 20:45:06.861: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 22 20:45:06.875: INFO: Waiting for pod pod-host-path-test to disappear
Jul 22 20:45:06.877: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:45:06.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3430" for this suite.
Jul 22 20:45:12.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:45:12.964: INFO: namespace hostpath-3430 deletion completed in 6.084671112s

• [SLOW TEST:10.277 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:45:12.966: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 22 20:45:13.116: INFO: Waiting up to 5m0s for pod "client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4" in namespace "containers-4937" to be "success or failure"
Jul 22 20:45:13.119: INFO: Pod "client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620281ms
Jul 22 20:45:15.165: INFO: Pod "client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049834157s
Jul 22 20:45:17.169: INFO: Pod "client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053251157s
STEP: Saw pod success
Jul 22 20:45:17.169: INFO: Pod "client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4" satisfied condition "success or failure"
Jul 22 20:45:17.171: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4 container test-container: <nil>
STEP: delete the pod
Jul 22 20:45:17.186: INFO: Waiting for pod client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4 to disappear
Jul 22 20:45:17.191: INFO: Pod client-containers-2e386ad1-96c4-4fcc-bed2-0f4005ae59a4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:45:17.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4937" for this suite.
Jul 22 20:45:23.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:45:23.284: INFO: namespace containers-4937 deletion completed in 6.089694883s

• [SLOW TEST:10.318 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:45:23.284: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-aa0dc2f9-113f-421c-b5e7-732240089c42
STEP: Creating a pod to test consume configMaps
Jul 22 20:45:23.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8" in namespace "configmap-2172" to be "success or failure"
Jul 22 20:45:23.447: INFO: Pod "pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.685249ms
Jul 22 20:45:25.450: INFO: Pod "pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01271685s
Jul 22 20:45:27.453: INFO: Pod "pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015655852s
STEP: Saw pod success
Jul 22 20:45:27.453: INFO: Pod "pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8" satisfied condition "success or failure"
Jul 22 20:45:27.455: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:45:27.472: INFO: Waiting for pod pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8 to disappear
Jul 22 20:45:27.474: INFO: Pod pod-configmaps-cf9716ea-ffa0-4e51-a3f5-e5974c5c10b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:45:27.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2172" for this suite.
Jul 22 20:45:33.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:45:33.587: INFO: namespace configmap-2172 deletion completed in 6.108909779s

• [SLOW TEST:10.302 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:45:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:46:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4014" for this suite.
Jul 22 20:46:55.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:46:55.871: INFO: namespace container-probe-4014 deletion completed in 22.12613226s

• [SLOW TEST:82.284 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:46:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-ab42ccb8-d609-4765-9e5e-0e0f56e74284
STEP: Creating a pod to test consume secrets
Jul 22 20:46:56.056: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462" in namespace "projected-9279" to be "success or failure"
Jul 22 20:46:56.059: INFO: Pod "pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133788ms
Jul 22 20:46:58.071: INFO: Pod "pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015267313s
Jul 22 20:47:00.074: INFO: Pod "pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018641565s
STEP: Saw pod success
Jul 22 20:47:00.074: INFO: Pod "pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462" satisfied condition "success or failure"
Jul 22 20:47:00.076: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:47:00.098: INFO: Waiting for pod pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462 to disappear
Jul 22 20:47:00.105: INFO: Pod pod-projected-secrets-6ae0c7a2-2ac4-4436-a551-c1727666f462 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:47:00.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9279" for this suite.
Jul 22 20:47:06.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:47:06.196: INFO: namespace projected-9279 deletion completed in 6.087570596s

• [SLOW TEST:10.322 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:47:06.196: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-bcb28b01-d283-443e-85e3-1d35cda64559
STEP: Creating a pod to test consume secrets
Jul 22 20:47:06.353: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f" in namespace "projected-8675" to be "success or failure"
Jul 22 20:47:06.356: INFO: Pod "pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998688ms
Jul 22 20:47:08.359: INFO: Pod "pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005965109s
Jul 22 20:47:10.423: INFO: Pod "pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069468477s
STEP: Saw pod success
Jul 22 20:47:10.423: INFO: Pod "pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f" satisfied condition "success or failure"
Jul 22 20:47:10.425: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:47:10.449: INFO: Waiting for pod pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f to disappear
Jul 22 20:47:10.451: INFO: Pod pod-projected-secrets-2d74d09d-6262-4cf2-b551-050d36ec7c0f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:47:10.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8675" for this suite.
Jul 22 20:47:16.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:47:16.546: INFO: namespace projected-8675 deletion completed in 6.091108657s

• [SLOW TEST:10.350 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:47:16.547: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d08e70de-ffd3-4dc2-a2dd-a4c62e0415ed
STEP: Creating a pod to test consume configMaps
Jul 22 20:47:16.707: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7" in namespace "projected-6231" to be "success or failure"
Jul 22 20:47:16.717: INFO: Pod "pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.90976ms
Jul 22 20:47:18.721: INFO: Pod "pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013529437s
Jul 22 20:47:20.724: INFO: Pod "pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016681309s
STEP: Saw pod success
Jul 22 20:47:20.724: INFO: Pod "pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7" satisfied condition "success or failure"
Jul 22 20:47:20.726: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:47:20.748: INFO: Waiting for pod pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7 to disappear
Jul 22 20:47:20.751: INFO: Pod pod-projected-configmaps-05c284de-0030-4081-9328-d3a90aeb02f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:47:20.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6231" for this suite.
Jul 22 20:47:26.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:47:26.849: INFO: namespace projected-6231 deletion completed in 6.095361119s

• [SLOW TEST:10.303 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:47:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-577
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-9e0fbcf6-1f74-4473-8518-befaa1de9c10
STEP: Creating configMap with name cm-test-opt-upd-6a8c0c70-cf20-43db-84f9-be30703d10b2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9e0fbcf6-1f74-4473-8518-befaa1de9c10
STEP: Updating configmap cm-test-opt-upd-6a8c0c70-cf20-43db-84f9-be30703d10b2
STEP: Creating configMap with name cm-test-opt-create-3ac3c8e3-a554-426c-91ea-31fdfe603242
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:47:35.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-577" for this suite.
Jul 22 20:47:57.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:47:57.218: INFO: namespace configmap-577 deletion completed in 22.110802882s

• [SLOW TEST:30.369 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:47:57.224: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-c4f75e94-462b-49d5-98b8-256614214478 in namespace container-probe-7016
Jul 22 20:48:01.390: INFO: Started pod busybox-c4f75e94-462b-49d5-98b8-256614214478 in namespace container-probe-7016
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:48:01.392: INFO: Initial restart count of pod busybox-c4f75e94-462b-49d5-98b8-256614214478 is 0
Jul 22 20:48:53.478: INFO: Restart count of pod container-probe-7016/busybox-c4f75e94-462b-49d5-98b8-256614214478 is now 1 (52.085503897s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:48:53.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7016" for this suite.
Jul 22 20:48:59.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:48:59.644: INFO: namespace container-probe-7016 deletion completed in 6.156089572s

• [SLOW TEST:62.421 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:48:59.646: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 22 20:48:59.791: INFO: namespace kubectl-3290
Jul 22 20:48:59.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-3290'
Jul 22 20:49:00.054: INFO: stderr: ""
Jul 22 20:49:00.054: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 20:49:01.057: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:49:01.057: INFO: Found 0 / 1
Jul 22 20:49:02.057: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:49:02.058: INFO: Found 0 / 1
Jul 22 20:49:03.057: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:49:03.057: INFO: Found 1 / 1
Jul 22 20:49:03.057: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 20:49:03.060: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 20:49:03.060: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 20:49:03.060: INFO: wait on redis-master startup in kubectl-3290 
Jul 22 20:49:03.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 logs redis-master-mbvvs redis-master --namespace=kubectl-3290'
Jul 22 20:49:03.156: INFO: stderr: ""
Jul 22 20:49:03.156: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 20:49:01.937 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 20:49:01.937 # Server started, Redis version 3.2.12\n1:M 22 Jul 20:49:01.937 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 20:49:01.937 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 22 20:49:03.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3290'
Jul 22 20:49:03.255: INFO: stderr: ""
Jul 22 20:49:03.255: INFO: stdout: "service/rm2 exposed\n"
Jul 22 20:49:03.258: INFO: Service rm2 in namespace kubectl-3290 found.
STEP: exposing service
Jul 22 20:49:05.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3290'
Jul 22 20:49:05.372: INFO: stderr: ""
Jul 22 20:49:05.372: INFO: stdout: "service/rm3 exposed\n"
Jul 22 20:49:05.379: INFO: Service rm3 in namespace kubectl-3290 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:49:07.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3290" for this suite.
Jul 22 20:49:29.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:49:29.472: INFO: namespace kubectl-3290 deletion completed in 22.084943573s

• [SLOW TEST:29.826 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:49:29.474: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5876
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:49:29.624: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 20:49:55.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.62:8080/dial?request=hostName&protocol=udp&host=10.244.3.61&port=8081&tries=1'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:49:55.725: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:49:55.886: INFO: Waiting for endpoints: map[]
Jul 22 20:49:55.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.62:8080/dial?request=hostName&protocol=udp&host=10.244.2.8&port=8081&tries=1'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:49:55.895: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:49:56.073: INFO: Waiting for endpoints: map[]
Jul 22 20:49:56.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.62:8080/dial?request=hostName&protocol=udp&host=10.244.1.10&port=8081&tries=1'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 20:49:56.077: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 20:49:56.215: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:49:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5876" for this suite.
Jul 22 20:50:18.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:50:18.306: INFO: namespace pod-network-test-5876 deletion completed in 22.086061194s

• [SLOW TEST:48.832 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:50:18.306: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 22 20:50:18.460: INFO: Waiting up to 5m0s for pod "var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5" in namespace "var-expansion-6931" to be "success or failure"
Jul 22 20:50:18.462: INFO: Pod "var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26219ms
Jul 22 20:50:20.465: INFO: Pod "var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005613476s
Jul 22 20:50:22.469: INFO: Pod "var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009460653s
STEP: Saw pod success
Jul 22 20:50:22.469: INFO: Pod "var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5" satisfied condition "success or failure"
Jul 22 20:50:22.472: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:50:22.492: INFO: Waiting for pod var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5 to disappear
Jul 22 20:50:22.495: INFO: Pod var-expansion-cf3ec4cf-4b4d-4f47-96c5-652d804f6fe5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:50:22.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6931" for this suite.
Jul 22 20:50:28.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:50:28.614: INFO: namespace var-expansion-6931 deletion completed in 6.112305761s

• [SLOW TEST:10.307 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:50:28.614: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:50:29.001: INFO: Pod name wrapped-volume-race-f2b365da-ade5-488e-bed0-1ca23a3e5d31: Found 0 pods out of 5
Jul 22 20:50:34.009: INFO: Pod name wrapped-volume-race-f2b365da-ade5-488e-bed0-1ca23a3e5d31: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f2b365da-ade5-488e-bed0-1ca23a3e5d31 in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Jul 22 20:50:46.093: INFO: Deleting ReplicationController wrapped-volume-race-f2b365da-ade5-488e-bed0-1ca23a3e5d31 took: 5.242677ms
Jul 22 20:50:46.393: INFO: Terminating ReplicationController wrapped-volume-race-f2b365da-ade5-488e-bed0-1ca23a3e5d31 pods took: 300.336283ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:51:28.008: INFO: Pod name wrapped-volume-race-0f506991-4cf9-481c-a5f4-6b4a59bb705f: Found 0 pods out of 5
Jul 22 20:51:33.014: INFO: Pod name wrapped-volume-race-0f506991-4cf9-481c-a5f4-6b4a59bb705f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0f506991-4cf9-481c-a5f4-6b4a59bb705f in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Jul 22 20:51:47.148: INFO: Deleting ReplicationController wrapped-volume-race-0f506991-4cf9-481c-a5f4-6b4a59bb705f took: 11.18035ms
Jul 22 20:51:47.448: INFO: Terminating ReplicationController wrapped-volume-race-0f506991-4cf9-481c-a5f4-6b4a59bb705f pods took: 300.184061ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:52:28.964: INFO: Pod name wrapped-volume-race-1e02164e-64a4-48eb-bb75-2cd0cd18f7f5: Found 0 pods out of 5
Jul 22 20:52:33.971: INFO: Pod name wrapped-volume-race-1e02164e-64a4-48eb-bb75-2cd0cd18f7f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1e02164e-64a4-48eb-bb75-2cd0cd18f7f5 in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Jul 22 20:52:50.113: INFO: Deleting ReplicationController wrapped-volume-race-1e02164e-64a4-48eb-bb75-2cd0cd18f7f5 took: 65.045406ms
Jul 22 20:52:50.414: INFO: Terminating ReplicationController wrapped-volume-race-1e02164e-64a4-48eb-bb75-2cd0cd18f7f5 pods took: 300.38754ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:53:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5451" for this suite.
Jul 22 20:53:36.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:53:36.852: INFO: namespace emptydir-wrapper-5451 deletion completed in 8.08358987s

• [SLOW TEST:188.238 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:53:36.852: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 22 20:53:36.997: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:53:41.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8305" for this suite.
Jul 22 20:53:47.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:53:47.539: INFO: namespace init-container-8305 deletion completed in 6.099551168s

• [SLOW TEST:10.686 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:53:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 20:53:47.693: INFO: Creating deployment "test-recreate-deployment"
Jul 22 20:53:47.696: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 22 20:53:47.712: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 22 20:53:49.718: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 22 20:53:49.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699425627, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699425627, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699425627, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699425627, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:53:51.728: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 22 20:53:51.733: INFO: Updating deployment test-recreate-deployment
Jul 22 20:53:51.733: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 22 20:53:51.818: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-386,SelfLink:/apis/apps/v1/namespaces/deployment-386/deployments/test-recreate-deployment,UID:d743424f-7f78-40ba-b6ad-b5e8432d941a,ResourceVersion:15005,Generation:2,CreationTimestamp:2019-07-22 20:53:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-22 20:53:51 +0000 UTC 2019-07-22 20:53:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-22 20:53:51 +0000 UTC 2019-07-22 20:53:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 22 20:53:51.821: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-386,SelfLink:/apis/apps/v1/namespaces/deployment-386/replicasets/test-recreate-deployment-5c8c9cc69d,UID:e716323f-ff15-4b2b-8e03-31a1b23e140b,ResourceVersion:15004,Generation:1,CreationTimestamp:2019-07-22 20:53:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d743424f-7f78-40ba-b6ad-b5e8432d941a 0xc003326f17 0xc003326f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 20:53:51.821: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 22 20:53:51.821: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-386,SelfLink:/apis/apps/v1/namespaces/deployment-386/replicasets/test-recreate-deployment-6df85df6b9,UID:990e4438-c00b-49f6-a8b4-b4096699c3d1,ResourceVersion:14994,Generation:2,CreationTimestamp:2019-07-22 20:53:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d743424f-7f78-40ba-b6ad-b5e8432d941a 0xc0033270d7 0xc0033270d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 20:53:51.824: INFO: Pod "test-recreate-deployment-5c8c9cc69d-cjsr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-cjsr5,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-386,SelfLink:/api/v1/namespaces/deployment-386/pods/test-recreate-deployment-5c8c9cc69d-cjsr5,UID:40660f1a-ebe9-4a82-a4bc-85d6bbc25e3b,ResourceVersion:15006,Generation:0,CreationTimestamp:2019-07-22 20:53:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d e716323f-ff15-4b2b-8e03-31a1b23e140b 0xc003327f37 0xc003327f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-v2fb7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2fb7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2fb7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003327fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003327fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 20:53:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 20:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 20:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 20:53:51 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-07-22 20:53:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:53:51.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-386" for this suite.
Jul 22 20:53:57.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:53:57.965: INFO: namespace deployment-386 deletion completed in 6.137546337s

• [SLOW TEST:10.424 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:53:57.969: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:54:01.145: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:54:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7630" for this suite.
Jul 22 20:54:07.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:54:07.248: INFO: namespace container-runtime-7630 deletion completed in 6.084245331s

• [SLOW TEST:9.279 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:54:07.248: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 20:54:15.456: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:15.459: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:17.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:17.463: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:19.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:19.462: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:21.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:21.463: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:23.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:23.463: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:25.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:25.462: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:27.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:27.482: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 20:54:29.459: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 20:54:29.462: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:54:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9796" for this suite.
Jul 22 20:54:51.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:54:51.549: INFO: namespace container-lifecycle-hook-9796 deletion completed in 22.084012705s

• [SLOW TEST:44.301 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:54:51.554: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-5d204580-4008-49e2-96db-cbffe618ab6c
STEP: Creating a pod to test consume secrets
Jul 22 20:54:51.709: INFO: Waiting up to 5m0s for pod "pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3" in namespace "secrets-6311" to be "success or failure"
Jul 22 20:54:51.713: INFO: Pod "pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.526784ms
Jul 22 20:54:53.716: INFO: Pod "pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006528295s
Jul 22 20:54:55.719: INFO: Pod "pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009601103s
STEP: Saw pod success
Jul 22 20:54:55.719: INFO: Pod "pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3" satisfied condition "success or failure"
Jul 22 20:54:55.721: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:54:55.738: INFO: Waiting for pod pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3 to disappear
Jul 22 20:54:55.745: INFO: Pod pod-secrets-025ebcaa-66df-40b8-8f99-e91227d13ef3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:54:55.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6311" for this suite.
Jul 22 20:55:01.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:55:01.845: INFO: namespace secrets-6311 deletion completed in 6.096685298s

• [SLOW TEST:10.292 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:55:01.846: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 20:55:02.001: INFO: Waiting up to 5m0s for pod "pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43" in namespace "emptydir-4991" to be "success or failure"
Jul 22 20:55:02.004: INFO: Pod "pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05249ms
Jul 22 20:55:04.007: INFO: Pod "pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005745082s
Jul 22 20:55:06.011: INFO: Pod "pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009110772s
STEP: Saw pod success
Jul 22 20:55:06.011: INFO: Pod "pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43" satisfied condition "success or failure"
Jul 22 20:55:06.013: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43 container test-container: <nil>
STEP: delete the pod
Jul 22 20:55:06.029: INFO: Waiting for pod pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43 to disappear
Jul 22 20:55:06.035: INFO: Pod pod-eda7aedb-e6ea-4c68-a511-ca84d69d7e43 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:55:06.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4991" for this suite.
Jul 22 20:55:12.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:55:12.125: INFO: namespace emptydir-4991 deletion completed in 6.086360797s

• [SLOW TEST:10.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:55:12.126: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 20:55:12.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-9295'
Jul 22 20:55:13.048: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 20:55:13.048: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 22 20:55:15.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9295'
Jul 22 20:55:15.149: INFO: stderr: ""
Jul 22 20:55:15.149: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:55:15.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9295" for this suite.
Jul 22 20:55:21.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:55:21.238: INFO: namespace kubectl-9295 deletion completed in 6.084730962s

• [SLOW TEST:9.112 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:55:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-ae89615b-a621-4aec-93a5-06e2bfa2796f
STEP: Creating a pod to test consume configMaps
Jul 22 20:55:21.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496" in namespace "projected-2289" to be "success or failure"
Jul 22 20:55:21.401: INFO: Pod "pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496": Phase="Pending", Reason="", readiness=false. Elapsed: 7.949663ms
Jul 22 20:55:23.405: INFO: Pod "pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011795524s
Jul 22 20:55:25.408: INFO: Pod "pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014938686s
STEP: Saw pod success
Jul 22 20:55:25.408: INFO: Pod "pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496" satisfied condition "success or failure"
Jul 22 20:55:25.410: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:55:25.425: INFO: Waiting for pod pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496 to disappear
Jul 22 20:55:25.427: INFO: Pod pod-projected-configmaps-86fd8c7f-3d73-457e-986a-8d09018a5496 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:55:25.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2289" for this suite.
Jul 22 20:55:31.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:55:31.516: INFO: namespace projected-2289 deletion completed in 6.086890105s

• [SLOW TEST:10.278 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:55:31.517: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 22 20:55:31.663: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:55:31.669: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:55:31.671: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-0 before test
Jul 22 20:55:31.675: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7wl7x from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 20:55:31.675: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 20:55:31.676: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 20:28:29 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.676: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 22 20:55:31.676: INFO: kube-proxy-2xdzw from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.676: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:55:31.676: INFO: kube-flannel-ds-5tcnp from kube-system started at 2019-07-22 19:11:18 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.676: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 20:55:31.676: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 20:55:31.676: INFO: azure-ip-masq-agent-p9zj8 from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.676: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 20:55:31.676: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-1 before test
Jul 22 20:55:31.704: INFO: sonobuoy-e2e-job-fa452941f8be4107 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container e2e ready: true, restart count 0
Jul 22 20:55:31.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 20:55:31.704: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-4pkq8 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 20:55:31.704: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 20:55:31.704: INFO: coredns-7f68dcdbdb-6ndmh from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:55:31.704: INFO: kube-proxy-cg6wd from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:55:31.704: INFO: kube-flannel-ds-ftphf from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 20:55:31.704: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 20:55:31.704: INFO: azure-ip-masq-agent-xsrxn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.704: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 20:55:31.704: INFO: tiller-deploy-54c96cb5df-pp2fz from kube-system started at 2019-07-22 19:11:28 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.705: INFO: 	Container tiller ready: true, restart count 0
Jul 22 20:55:31.705: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-2 before test
Jul 22 20:55:31.734: INFO: kube-proxy-4b7fg from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:55:31.734: INFO: azure-ip-masq-agent-4qfk6 from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 20:55:31.734: INFO: kube-flannel-ds-p28wn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 20:55:31.734: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 20:55:31.734: INFO: metrics-server-864ffbc5c-x5xsw from kube-system started at 2019-07-22 19:11:24 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:55:31.734: INFO: kubernetes-dashboard-66dd8b8df7-jnsv8 from kube-system started at 2019-07-22 19:11:26 +0000 UTC (1 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 22 20:55:31.734: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7vkzc from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 20:55:31.734: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 20:55:31.734: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b3d620be4b35d7], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:55:32.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7903" for this suite.
Jul 22 20:55:38.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:55:38.843: INFO: namespace sched-pred-7903 deletion completed in 6.08535188s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.327 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:55:38.844: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0722 20:56:09.045435      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 20:56:09.045: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:56:09.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6689" for this suite.
Jul 22 20:56:15.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:56:15.142: INFO: namespace gc-6689 deletion completed in 6.094190383s

• [SLOW TEST:36.298 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:56:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 22 20:56:15.297: INFO: Waiting up to 5m0s for pod "downward-api-ef8013db-740c-48b1-b9ae-039334df5d10" in namespace "downward-api-3575" to be "success or failure"
Jul 22 20:56:15.301: INFO: Pod "downward-api-ef8013db-740c-48b1-b9ae-039334df5d10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16248ms
Jul 22 20:56:17.304: INFO: Pod "downward-api-ef8013db-740c-48b1-b9ae-039334df5d10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007350768s
Jul 22 20:56:19.307: INFO: Pod "downward-api-ef8013db-740c-48b1-b9ae-039334df5d10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010394053s
STEP: Saw pod success
Jul 22 20:56:19.307: INFO: Pod "downward-api-ef8013db-740c-48b1-b9ae-039334df5d10" satisfied condition "success or failure"
Jul 22 20:56:19.309: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downward-api-ef8013db-740c-48b1-b9ae-039334df5d10 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:56:19.335: INFO: Waiting for pod downward-api-ef8013db-740c-48b1-b9ae-039334df5d10 to disappear
Jul 22 20:56:19.338: INFO: Pod downward-api-ef8013db-740c-48b1-b9ae-039334df5d10 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:56:19.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3575" for this suite.
Jul 22 20:56:25.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:56:25.440: INFO: namespace downward-api-3575 deletion completed in 6.098781519s

• [SLOW TEST:10.296 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:56:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 22 20:56:25.591: INFO: Waiting up to 5m0s for pod "downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5" in namespace "downward-api-6661" to be "success or failure"
Jul 22 20:56:25.602: INFO: Pod "downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.76535ms
Jul 22 20:56:27.605: INFO: Pod "downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013700224s
Jul 22 20:56:29.608: INFO: Pod "downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016568797s
STEP: Saw pod success
Jul 22 20:56:29.608: INFO: Pod "downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5" satisfied condition "success or failure"
Jul 22 20:56:29.609: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:56:29.623: INFO: Waiting for pod downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5 to disappear
Jul 22 20:56:29.625: INFO: Pod downward-api-14e2dc58-c2cb-495e-8817-3a2cbd05a5b5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:56:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6661" for this suite.
Jul 22 20:56:35.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:56:35.724: INFO: namespace downward-api-6661 deletion completed in 6.095883791s

• [SLOW TEST:10.283 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:56:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:56:39.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3959" for this suite.
Jul 22 20:57:29.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:57:29.988: INFO: namespace kubelet-test-3959 deletion completed in 50.087641885s

• [SLOW TEST:54.262 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:57:29.989: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:57:30.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26" in namespace "downward-api-920" to be "success or failure"
Jul 22 20:57:30.144: INFO: Pod "downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320689ms
Jul 22 20:57:32.147: INFO: Pod "downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005377783s
Jul 22 20:57:34.150: INFO: Pod "downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008132576s
STEP: Saw pod success
Jul 22 20:57:34.150: INFO: Pod "downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26" satisfied condition "success or failure"
Jul 22 20:57:34.152: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26 container client-container: <nil>
STEP: delete the pod
Jul 22 20:57:34.171: INFO: Waiting for pod downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26 to disappear
Jul 22 20:57:34.174: INFO: Pod downwardapi-volume-fa4bdbe4-3706-4dde-a6d2-bf7d20f1dd26 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 20:57:34.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-920" for this suite.
Jul 22 20:57:40.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 20:57:40.268: INFO: namespace downward-api-920 deletion completed in 6.090900272s

• [SLOW TEST:10.279 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 20:57:40.268: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-298abd19-db16-4840-9998-59b5e9204ef2 in namespace container-probe-3069
Jul 22 20:57:44.460: INFO: Started pod test-webserver-298abd19-db16-4840-9998-59b5e9204ef2 in namespace container-probe-3069
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:57:44.466: INFO: Initial restart count of pod test-webserver-298abd19-db16-4840-9998-59b5e9204ef2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:01:44.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3069" for this suite.
Jul 22 21:01:50.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:01:51.073: INFO: namespace container-probe-3069 deletion completed in 6.091174175s

• [SLOW TEST:250.806 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:01:51.075: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 21:01:51.227: INFO: Waiting up to 5m0s for pod "pod-e9dd8200-5985-4db8-8136-30fa79f15a8e" in namespace "emptydir-9464" to be "success or failure"
Jul 22 21:01:51.232: INFO: Pod "pod-e9dd8200-5985-4db8-8136-30fa79f15a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700377ms
Jul 22 21:01:53.237: INFO: Pod "pod-e9dd8200-5985-4db8-8136-30fa79f15a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01018752s
Jul 22 21:01:55.241: INFO: Pod "pod-e9dd8200-5985-4db8-8136-30fa79f15a8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01396727s
STEP: Saw pod success
Jul 22 21:01:55.241: INFO: Pod "pod-e9dd8200-5985-4db8-8136-30fa79f15a8e" satisfied condition "success or failure"
Jul 22 21:01:55.243: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-e9dd8200-5985-4db8-8136-30fa79f15a8e container test-container: <nil>
STEP: delete the pod
Jul 22 21:01:55.268: INFO: Waiting for pod pod-e9dd8200-5985-4db8-8136-30fa79f15a8e to disappear
Jul 22 21:01:55.270: INFO: Pod pod-e9dd8200-5985-4db8-8136-30fa79f15a8e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:01:55.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9464" for this suite.
Jul 22 21:02:01.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:02:01.360: INFO: namespace emptydir-9464 deletion completed in 6.086302376s

• [SLOW TEST:10.286 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:02:01.363: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 21:02:01.532: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:01.536: INFO: Number of nodes with available pods: 0
Jul 22 21:02:01.536: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:02:02.541: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:02.544: INFO: Number of nodes with available pods: 0
Jul 22 21:02:02.544: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:02:03.540: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:03.542: INFO: Number of nodes with available pods: 0
Jul 22 21:02:03.542: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:02:04.540: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:04.543: INFO: Number of nodes with available pods: 3
Jul 22 21:02:04.544: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 22 21:02:04.560: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:04.563: INFO: Number of nodes with available pods: 2
Jul 22 21:02:04.563: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:05.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:05.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:05.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:06.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:06.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:06.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:07.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:07.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:07.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:08.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:08.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:08.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:09.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:09.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:09.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:10.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:10.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:10.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:11.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:11.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:11.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:12.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:12.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:12.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:13.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:13.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:13.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:14.566: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:14.569: INFO: Number of nodes with available pods: 2
Jul 22 21:02:14.569: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:15.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:15.570: INFO: Number of nodes with available pods: 2
Jul 22 21:02:15.570: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:16.566: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:16.569: INFO: Number of nodes with available pods: 2
Jul 22 21:02:16.569: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:02:17.567: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:02:17.569: INFO: Number of nodes with available pods: 3
Jul 22 21:02:17.569: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7454, will wait for the garbage collector to delete the pods
Jul 22 21:02:17.629: INFO: Deleting DaemonSet.extensions daemon-set took: 5.080675ms
Jul 22 21:02:17.929: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.216137ms
Jul 22 21:02:21.433: INFO: Number of nodes with available pods: 0
Jul 22 21:02:21.433: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:02:21.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7454/daemonsets","resourceVersion":"16321"},"items":null}

Jul 22 21:02:21.437: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7454/pods","resourceVersion":"16321"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:02:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7454" for this suite.
Jul 22 21:02:27.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:02:27.535: INFO: namespace daemonsets-7454 deletion completed in 6.085930724s

• [SLOW TEST:26.172 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:02:27.537: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 21:02:27.687: INFO: Waiting up to 5m0s for pod "pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02" in namespace "emptydir-5171" to be "success or failure"
Jul 22 21:02:27.696: INFO: Pod "pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02": Phase="Pending", Reason="", readiness=false. Elapsed: 8.976357ms
Jul 22 21:02:29.699: INFO: Pod "pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011810688s
Jul 22 21:02:31.738: INFO: Pod "pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051439538s
STEP: Saw pod success
Jul 22 21:02:31.739: INFO: Pod "pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02" satisfied condition "success or failure"
Jul 22 21:02:31.744: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02 container test-container: <nil>
STEP: delete the pod
Jul 22 21:02:31.757: INFO: Waiting for pod pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02 to disappear
Jul 22 21:02:31.760: INFO: Pod pod-cbc2e52a-c695-458c-bf24-8fc0029dfa02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:02:31.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5171" for this suite.
Jul 22 21:02:37.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:02:37.857: INFO: namespace emptydir-5171 deletion completed in 6.093894965s

• [SLOW TEST:10.321 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:02:37.860: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:02:38.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-964'
Jul 22 21:02:38.280: INFO: stderr: ""
Jul 22 21:02:38.280: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 22 21:02:38.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-964'
Jul 22 21:02:38.610: INFO: stderr: ""
Jul 22 21:02:38.610: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 21:02:39.614: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 21:02:39.614: INFO: Found 0 / 1
Jul 22 21:02:40.614: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 21:02:40.614: INFO: Found 0 / 1
Jul 22 21:02:41.614: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 21:02:41.614: INFO: Found 1 / 1
Jul 22 21:02:41.614: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 21:02:41.616: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 21:02:41.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 21:02:41.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 describe pod redis-master-trldr --namespace=kubectl-964'
Jul 22 21:02:41.856: INFO: stderr: ""
Jul 22 21:02:41.856: INFO: stdout: "Name:           redis-master-trldr\nNamespace:      kubectl-964\nPriority:       0\nNode:           k8s-linuxpool-16111918-0/10.240.0.6\nStart Time:     Mon, 22 Jul 2019 21:02:38 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             10.244.3.98\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://db206320c2a4574aa3d63f59bee716098104dfa8398ee5bab91a90f7b404ec6b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 22 Jul 2019 21:02:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nczkj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nczkj:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nczkj\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                               Message\n  ----    ------     ----  ----                               -------\n  Normal  Scheduled  3s    default-scheduler                  Successfully assigned kubectl-964/redis-master-trldr to k8s-linuxpool-16111918-0\n  Normal  Pulled     2s    kubelet, k8s-linuxpool-16111918-0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-linuxpool-16111918-0  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-linuxpool-16111918-0  Started container redis-master\n"
Jul 22 21:02:41.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 describe rc redis-master --namespace=kubectl-964'
Jul 22 21:02:41.987: INFO: stderr: ""
Jul 22 21:02:41.987: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-964\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-trldr\n"
Jul 22 21:02:41.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 describe service redis-master --namespace=kubectl-964'
Jul 22 21:02:42.121: INFO: stderr: ""
Jul 22 21:02:42.121: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-964\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.155.206\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.98:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 22 21:02:42.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 describe node k8s-linuxpool-16111918-0'
Jul 22 21:02:42.257: INFO: stderr: ""
Jul 22 21:02:42.257: INFO: stdout: "Name:               k8s-linuxpool-16111918-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.azure.com/cluster=kube15\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-linuxpool-16111918-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    storageprofile=managed\n                    storagetier=Standard_LRS\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"3e:dc:a4:8d:c5:61\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.240.0.6\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 22 Jul 2019 19:11:18 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 22 Jul 2019 21:02:03 +0000   Mon, 22 Jul 2019 19:11:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 22 Jul 2019 21:02:03 +0000   Mon, 22 Jul 2019 19:11:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 22 Jul 2019 21:02:03 +0000   Mon, 22 Jul 2019 19:11:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 22 Jul 2019 21:02:03 +0000   Mon, 22 Jul 2019 19:11:48 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.6\n  Hostname:    k8s-linuxpool-16111918-0\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              203234980Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7137164Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              187301357258\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6369164Ki\n pods:                           110\nSystem Info:\n Machine ID:                 7e74a692a21a4fcbb17f2fbc7adcf698\n System UUID:                F93B4562-5E78-FA40-A1F6-704B02B2E5E9\n Boot ID:                    666019b6-7f72-4cea-a089-c7c85578108a\n Kernel Version:             4.15.0-1022-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.6\n Kubelet Version:            v1.15.1\n Kube-Proxy Version:         v1.15.1\nPodCIDR:                     10.244.3.0/24\nProviderID:                  azure:///subscriptions/844d0ce5-4523-48a9-b0f3-5533da7f4170/resourceGroups/kube15/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-16111918-0\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7wl7x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                azure-ip-masq-agent-p9zj8                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     111m\n  kube-system                kube-flannel-ds-5tcnp                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         111m\n  kube-system                kube-proxy-2xdzw                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         111m\n  kubectl-964                redis-master-trldr                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests   Limits\n  --------                       --------   ------\n  cpu                            150m (7%)  50m (2%)\n  memory                         50Mi (0%)  250Mi (4%)\n  ephemeral-storage              0 (0%)     0 (0%)\n  attachable-volumes-azure-disk  0          0\nEvents:                          <none>\n"
Jul 22 21:02:42.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 describe namespace kubectl-964'
Jul 22 21:02:42.356: INFO: stderr: ""
Jul 22 21:02:42.356: INFO: stdout: "Name:         kubectl-964\nLabels:       e2e-framework=kubectl\n              e2e-run=b790c7d7-b8c7-4caa-9229-95acf423d555\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:02:42.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-964" for this suite.
Jul 22 21:03:04.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:03:04.445: INFO: namespace kubectl-964 deletion completed in 22.086087502s

• [SLOW TEST:26.585 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:03:04.447: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:03:08.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2307" for this suite.
Jul 22 21:03:14.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:03:14.738: INFO: namespace emptydir-wrapper-2307 deletion completed in 6.097115079s

• [SLOW TEST:10.292 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:03:14.739: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:03:20.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5228" for this suite.
Jul 22 21:03:26.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:03:26.576: INFO: namespace watch-5228 deletion completed in 6.181806044s

• [SLOW TEST:11.837 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:03:26.577: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 21:03:31.249: INFO: Successfully updated pod "pod-update-357da485-18a7-4707-a291-fa4db95d4291"
STEP: verifying the updated pod is in kubernetes
Jul 22 21:03:31.253: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:03:31.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7832" for this suite.
Jul 22 21:03:53.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:03:53.342: INFO: namespace pods-7832 deletion completed in 22.085321386s

• [SLOW TEST:26.765 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:03:53.342: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:03:53.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794" in namespace "projected-7318" to be "success or failure"
Jul 22 21:03:53.498: INFO: Pod "downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478988ms
Jul 22 21:03:55.501: INFO: Pod "downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005885365s
Jul 22 21:03:57.504: INFO: Pod "downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008884343s
STEP: Saw pod success
Jul 22 21:03:57.505: INFO: Pod "downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794" satisfied condition "success or failure"
Jul 22 21:03:57.507: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794 container client-container: <nil>
STEP: delete the pod
Jul 22 21:03:57.523: INFO: Waiting for pod downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794 to disappear
Jul 22 21:03:57.526: INFO: Pod downwardapi-volume-4154d4e3-5cb8-407f-b8d8-140e143a9794 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:03:57.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7318" for this suite.
Jul 22 21:04:03.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:04:03.631: INFO: namespace projected-7318 deletion completed in 6.102617467s

• [SLOW TEST:10.290 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:04:03.632: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-snp9
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:04:03.786: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-snp9" in namespace "subpath-5117" to be "success or failure"
Jul 22 21:04:03.789: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283089ms
Jul 22 21:04:05.792: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005060163s
Jul 22 21:04:07.794: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 4.007882136s
Jul 22 21:04:09.797: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010528809s
Jul 22 21:04:11.800: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 8.01337378s
Jul 22 21:04:13.803: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 10.01616905s
Jul 22 21:04:15.805: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 12.018812119s
Jul 22 21:04:17.808: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 14.021896085s
Jul 22 21:04:19.811: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 16.024426253s
Jul 22 21:04:21.816: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 18.029335009s
Jul 22 21:04:23.818: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 20.031727175s
Jul 22 21:04:25.821: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Running", Reason="", readiness=true. Elapsed: 22.034940637s
Jul 22 21:04:27.824: INFO: Pod "pod-subpath-test-configmap-snp9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0376521s
STEP: Saw pod success
Jul 22 21:04:27.824: INFO: Pod "pod-subpath-test-configmap-snp9" satisfied condition "success or failure"
Jul 22 21:04:27.827: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-subpath-test-configmap-snp9 container test-container-subpath-configmap-snp9: <nil>
STEP: delete the pod
Jul 22 21:04:27.843: INFO: Waiting for pod pod-subpath-test-configmap-snp9 to disappear
Jul 22 21:04:27.846: INFO: Pod pod-subpath-test-configmap-snp9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-snp9
Jul 22 21:04:27.846: INFO: Deleting pod "pod-subpath-test-configmap-snp9" in namespace "subpath-5117"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:04:27.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5117" for this suite.
Jul 22 21:04:33.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:04:33.942: INFO: namespace subpath-5117 deletion completed in 6.09016158s

• [SLOW TEST:30.310 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:04:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6801
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2543
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:04:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7164" for this suite.
Jul 22 21:04:46.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:04:46.490: INFO: namespace namespaces-7164 deletion completed in 6.087273374s
STEP: Destroying namespace "nsdeletetest-6801" for this suite.
Jul 22 21:04:46.493: INFO: Namespace nsdeletetest-6801 was already deleted
STEP: Destroying namespace "nsdeletetest-2543" for this suite.
Jul 22 21:04:52.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:04:52.596: INFO: namespace nsdeletetest-2543 deletion completed in 6.102965388s

• [SLOW TEST:18.652 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:04:52.596: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092
Jul 22 21:04:52.748: INFO: Pod name my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092: Found 0 pods out of 1
Jul 22 21:04:57.751: INFO: Pod name my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092: Found 1 pods out of 1
Jul 22 21:04:57.751: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092" are running
Jul 22 21:04:57.754: INFO: Pod "my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092-n4q92" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 21:04:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 21:04:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 21:04:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 21:04:52 +0000 UTC Reason: Message:}])
Jul 22 21:04:57.754: INFO: Trying to dial the pod
Jul 22 21:05:02.763: INFO: Controller my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092: Got expected result from replica 1 [my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092-n4q92]: "my-hostname-basic-85803801-473c-4782-8f65-2eb0e1765092-n4q92", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:05:02.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1019" for this suite.
Jul 22 21:05:08.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:05:08.913: INFO: namespace replication-controller-1019 deletion completed in 6.146574349s

• [SLOW TEST:16.317 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:05:08.913: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:05:13.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1458" for this suite.
Jul 22 21:05:19.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:05:19.171: INFO: namespace kubelet-test-1458 deletion completed in 6.097285277s

• [SLOW TEST:10.258 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:05:19.173: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 21:05:22.340: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:05:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5642" for this suite.
Jul 22 21:05:28.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:05:28.444: INFO: namespace container-runtime-5642 deletion completed in 6.087337113s

• [SLOW TEST:9.271 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:05:28.445: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:05:28.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261" in namespace "downward-api-341" to be "success or failure"
Jul 22 21:05:28.615: INFO: Pod "downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655168ms
Jul 22 21:05:30.618: INFO: Pod "downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010032597s
Jul 22 21:05:32.621: INFO: Pod "downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013071726s
STEP: Saw pod success
Jul 22 21:05:32.621: INFO: Pod "downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261" satisfied condition "success or failure"
Jul 22 21:05:32.623: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261 container client-container: <nil>
STEP: delete the pod
Jul 22 21:05:32.639: INFO: Waiting for pod downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261 to disappear
Jul 22 21:05:32.642: INFO: Pod downwardapi-volume-29377e64-7427-4688-aa58-ee9aabf6e261 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:05:32.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-341" for this suite.
Jul 22 21:05:38.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:05:38.774: INFO: namespace downward-api-341 deletion completed in 6.111696378s

• [SLOW TEST:10.329 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:05:38.774: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:05:39.004: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 22 21:05:44.007: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 21:05:44.007: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 22 21:05:46.010: INFO: Creating deployment "test-rollover-deployment"
Jul 22 21:05:46.015: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 22 21:05:48.020: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 22 21:05:48.025: INFO: Ensure that both replica sets have 1 created replica
Jul 22 21:05:48.029: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 22 21:05:48.037: INFO: Updating deployment test-rollover-deployment
Jul 22 21:05:48.037: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 22 21:05:50.182: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 22 21:05:50.188: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 22 21:05:50.201: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:05:50.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426348, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:05:52.208: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:05:52.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426350, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:05:54.209: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:05:54.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426350, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:05:56.208: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:05:56.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426350, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:05:58.208: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:05:58.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426350, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:06:00.207: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 21:06:00.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426350, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699426346, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:06:02.207: INFO: 
Jul 22 21:06:02.208: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 22 21:06:02.214: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-2212,SelfLink:/apis/apps/v1/namespaces/deployment-2212/deployments/test-rollover-deployment,UID:7112d2a0-2a76-413c-b99d-9d74a761d8f0,ResourceVersion:17204,Generation:2,CreationTimestamp:2019-07-22 21:05:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-22 21:05:46 +0000 UTC 2019-07-22 21:05:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-22 21:06:00 +0000 UTC 2019-07-22 21:05:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 21:06:02.217: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-2212,SelfLink:/apis/apps/v1/namespaces/deployment-2212/replicasets/test-rollover-deployment-854595fc44,UID:2733b2bd-a858-481d-bb84-b475ff58e7e6,ResourceVersion:17194,Generation:2,CreationTimestamp:2019-07-22 21:05:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7112d2a0-2a76-413c-b99d-9d74a761d8f0 0xc0003116d7 0xc0003116d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 21:06:02.217: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 22 21:06:02.217: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-2212,SelfLink:/apis/apps/v1/namespaces/deployment-2212/replicasets/test-rollover-controller,UID:7150168f-a123-4de2-ae13-fdc7a664d585,ResourceVersion:17203,Generation:2,CreationTimestamp:2019-07-22 21:05:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7112d2a0-2a76-413c-b99d-9d74a761d8f0 0xc0003114e7 0xc0003114e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 21:06:02.217: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-2212,SelfLink:/apis/apps/v1/namespaces/deployment-2212/replicasets/test-rollover-deployment-9b8b997cf,UID:69096407-2d23-4384-93f2-d1b3bdb212fb,ResourceVersion:17167,Generation:2,CreationTimestamp:2019-07-22 21:05:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7112d2a0-2a76-413c-b99d-9d74a761d8f0 0xc0003117e0 0xc0003117e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 21:06:02.219: INFO: Pod "test-rollover-deployment-854595fc44-7ld7l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-7ld7l,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-2212,SelfLink:/api/v1/namespaces/deployment-2212/pods/test-rollover-deployment-854595fc44-7ld7l,UID:1f9d5d68-c528-4cfb-acaf-d31a802a56d5,ResourceVersion:17176,Generation:0,CreationTimestamp:2019-07-22 21:05:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 2733b2bd-a858-481d-bb84-b475ff58e7e6 0xc00253c167 0xc00253c168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-khngr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-khngr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-khngr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00253c1d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00253c1f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:05:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:05:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:05:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:05:48 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.109,StartTime:2019-07-22 21:05:48 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-22 21:05:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f9d6004cf1e57d1025b1cc7835e9532e73e3938687b502beb8dea64266c0672f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:06:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2212" for this suite.
Jul 22 21:06:08.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:06:08.351: INFO: namespace deployment-2212 deletion completed in 6.128454755s

• [SLOW TEST:29.577 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:06:08.354: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c1817ba2-6f12-4b8d-913e-47a72f46b164
STEP: Creating a pod to test consume configMaps
Jul 22 21:06:08.511: INFO: Waiting up to 5m0s for pod "pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc" in namespace "configmap-2539" to be "success or failure"
Jul 22 21:06:08.514: INFO: Pod "pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901686ms
Jul 22 21:06:10.517: INFO: Pod "pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005933899s
Jul 22 21:06:12.524: INFO: Pod "pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012753592s
STEP: Saw pod success
Jul 22 21:06:12.524: INFO: Pod "pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc" satisfied condition "success or failure"
Jul 22 21:06:12.526: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:06:12.540: INFO: Waiting for pod pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc to disappear
Jul 22 21:06:12.543: INFO: Pod pod-configmaps-c59091c1-b6a2-4dc4-818d-dea643b56bcc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:06:12.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2539" for this suite.
Jul 22 21:06:18.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:06:18.639: INFO: namespace configmap-2539 deletion completed in 6.093922413s

• [SLOW TEST:10.286 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:06:18.640: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2042
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-d0a467e7-6f45-48fb-a448-482edd328948
STEP: Creating secret with name s-test-opt-upd-5d7ff03d-e27d-4713-8752-89157f47b660
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d0a467e7-6f45-48fb-a448-482edd328948
STEP: Updating secret s-test-opt-upd-5d7ff03d-e27d-4713-8752-89157f47b660
STEP: Creating secret with name s-test-opt-create-95c252be-6b1f-40f6-9663-622c1180189a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:06:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2042" for this suite.
Jul 22 21:06:48.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:06:48.985: INFO: namespace secrets-2042 deletion completed in 22.105388959s

• [SLOW TEST:30.345 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:06:48.986: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-cfc19774-e6dd-43d4-9074-fee2820ccff0
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:06:49.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6074" for this suite.
Jul 22 21:06:55.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:06:55.253: INFO: namespace configmap-6074 deletion completed in 6.112653875s

• [SLOW TEST:6.267 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:06:55.253: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 22 21:06:59.946: INFO: Successfully updated pod "labelsupdated3e548e5-0cfc-4737-b8e8-f538d0e07dbf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:07:01.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4170" for this suite.
Jul 22 21:07:23.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:07:24.087: INFO: namespace downward-api-4170 deletion completed in 22.11727045s

• [SLOW TEST:28.834 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:07:24.089: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 21:07:24.255: INFO: Waiting up to 5m0s for pod "pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1" in namespace "emptydir-6102" to be "success or failure"
Jul 22 21:07:24.260: INFO: Pod "pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146275ms
Jul 22 21:07:26.263: INFO: Pod "pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008232758s
Jul 22 21:07:28.266: INFO: Pod "pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011086441s
STEP: Saw pod success
Jul 22 21:07:28.266: INFO: Pod "pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1" satisfied condition "success or failure"
Jul 22 21:07:28.269: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1 container test-container: <nil>
STEP: delete the pod
Jul 22 21:07:28.282: INFO: Waiting for pod pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1 to disappear
Jul 22 21:07:28.284: INFO: Pod pod-7f236f3d-739f-4aed-8a70-d9da210cb7e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:07:28.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6102" for this suite.
Jul 22 21:07:34.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:07:34.378: INFO: namespace emptydir-6102 deletion completed in 6.09043794s

• [SLOW TEST:10.290 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:07:34.380: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-a2fe9280-d5be-4383-be9b-4cab47eaa3ae
STEP: Creating a pod to test consume configMaps
Jul 22 21:07:34.534: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243" in namespace "configmap-9582" to be "success or failure"
Jul 22 21:07:34.538: INFO: Pod "pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548383ms
Jul 22 21:07:36.541: INFO: Pod "pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00698536s
Jul 22 21:07:38.545: INFO: Pod "pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010734436s
STEP: Saw pod success
Jul 22 21:07:38.545: INFO: Pod "pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243" satisfied condition "success or failure"
Jul 22 21:07:38.548: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:07:38.567: INFO: Waiting for pod pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243 to disappear
Jul 22 21:07:38.570: INFO: Pod pod-configmaps-7c8330b0-ad01-447c-a540-efd88126b243 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:07:38.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9582" for this suite.
Jul 22 21:07:44.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:07:44.663: INFO: namespace configmap-9582 deletion completed in 6.089192636s

• [SLOW TEST:10.282 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:07:44.663: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5865
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 22 21:07:48.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec pod-sharedvolume-60aabbab-0789-48db-bf7c-4fe2aab20ca4 -c busybox-main-container --namespace=emptydir-5865 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 22 21:07:49.683: INFO: stderr: ""
Jul 22 21:07:49.683: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:07:49.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5865" for this suite.
Jul 22 21:07:55.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:07:55.807: INFO: namespace emptydir-5865 deletion completed in 6.119824571s

• [SLOW TEST:11.144 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:07:55.808: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4609
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9521
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:08:22.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6146" for this suite.
Jul 22 21:08:28.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:08:28.468: INFO: namespace namespaces-6146 deletion completed in 6.091499979s
STEP: Destroying namespace "nsdeletetest-4609" for this suite.
Jul 22 21:08:28.471: INFO: Namespace nsdeletetest-4609 was already deleted
STEP: Destroying namespace "nsdeletetest-9521" for this suite.
Jul 22 21:08:34.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:08:34.563: INFO: namespace nsdeletetest-9521 deletion completed in 6.09206767s

• [SLOW TEST:38.755 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:08:34.563: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-da1a70bc-8fbc-4b51-bf8a-eb57a409e7d2
STEP: Creating a pod to test consume configMaps
Jul 22 21:08:34.723: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0" in namespace "projected-3543" to be "success or failure"
Jul 22 21:08:34.742: INFO: Pod "pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.623607ms
Jul 22 21:08:36.745: INFO: Pod "pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021825066s
Jul 22 21:08:38.748: INFO: Pod "pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025004123s
STEP: Saw pod success
Jul 22 21:08:38.748: INFO: Pod "pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0" satisfied condition "success or failure"
Jul 22 21:08:38.750: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:08:38.765: INFO: Waiting for pod pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0 to disappear
Jul 22 21:08:38.767: INFO: Pod pod-projected-configmaps-4193211f-e7e4-4d62-893d-cad523bdacd0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:08:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3543" for this suite.
Jul 22 21:08:44.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:08:44.938: INFO: namespace projected-3543 deletion completed in 6.167667784s

• [SLOW TEST:10.375 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:08:44.938: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7986
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-60587132-8a40-4fa7-94e5-664032b180f0
STEP: Creating configMap with name cm-test-opt-upd-5723771e-18f0-40e5-925a-93cec6e8f530
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-60587132-8a40-4fa7-94e5-664032b180f0
STEP: Updating configmap cm-test-opt-upd-5723771e-18f0-40e5-925a-93cec6e8f530
STEP: Creating configMap with name cm-test-opt-create-f2cebdb2-ecb3-47ab-9f13-e73261189d04
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:08:53.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7986" for this suite.
Jul 22 21:09:15.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:09:15.358: INFO: namespace projected-7986 deletion completed in 22.13847593s

• [SLOW TEST:30.420 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:09:15.358: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-0efa899e-bb0a-4e28-bb99-0f3aaa2fb0ae
STEP: Creating a pod to test consume configMaps
Jul 22 21:09:15.526: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a" in namespace "projected-6361" to be "success or failure"
Jul 22 21:09:15.554: INFO: Pod "pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.10916ms
Jul 22 21:09:17.557: INFO: Pod "pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031401606s
Jul 22 21:09:19.560: INFO: Pod "pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034491551s
STEP: Saw pod success
Jul 22 21:09:19.560: INFO: Pod "pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a" satisfied condition "success or failure"
Jul 22 21:09:19.563: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:09:19.581: INFO: Waiting for pod pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a to disappear
Jul 22 21:09:19.583: INFO: Pod pod-projected-configmaps-dcded980-2a67-42ed-a9a2-e760bcfc664a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:09:19.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6361" for this suite.
Jul 22 21:09:25.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:09:25.674: INFO: namespace projected-6361 deletion completed in 6.089043837s

• [SLOW TEST:10.316 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:09:25.675: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:09:25.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4" in namespace "projected-2842" to be "success or failure"
Jul 22 21:09:25.838: INFO: Pod "downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.943951ms
Jul 22 21:09:27.842: INFO: Pod "downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014446387s
Jul 22 21:09:29.845: INFO: Pod "downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017583829s
STEP: Saw pod success
Jul 22 21:09:29.845: INFO: Pod "downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4" satisfied condition "success or failure"
Jul 22 21:09:29.847: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4 container client-container: <nil>
STEP: delete the pod
Jul 22 21:09:29.863: INFO: Waiting for pod downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4 to disappear
Jul 22 21:09:29.866: INFO: Pod downwardapi-volume-fde44e02-e350-4ba5-9e0e-584dc0eb6bc4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:09:29.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2842" for this suite.
Jul 22 21:09:35.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:09:35.958: INFO: namespace projected-2842 deletion completed in 6.087995433s

• [SLOW TEST:10.283 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:09:35.961: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 21:09:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-850'
Jul 22 21:09:36.281: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 21:09:36.281: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 22 21:09:38.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete deployment e2e-test-nginx-deployment --namespace=kubectl-850'
Jul 22 21:09:38.373: INFO: stderr: ""
Jul 22 21:09:38.373: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:09:38.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-850" for this suite.
Jul 22 21:10:00.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:10:00.458: INFO: namespace kubectl-850 deletion completed in 22.081329971s

• [SLOW TEST:24.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:10:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8348
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 22 21:10:00.610: INFO: Waiting up to 5m0s for pod "downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804" in namespace "downward-api-8348" to be "success or failure"
Jul 22 21:10:00.614: INFO: Pod "downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621482ms
Jul 22 21:10:02.617: INFO: Pod "downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006934315s
Jul 22 21:10:04.621: INFO: Pod "downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010146047s
STEP: Saw pod success
Jul 22 21:10:04.621: INFO: Pod "downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804" satisfied condition "success or failure"
Jul 22 21:10:04.623: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:10:04.646: INFO: Waiting for pod downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804 to disappear
Jul 22 21:10:04.648: INFO: Pod downward-api-97e2947a-8eae-4aaf-b9fd-473e13cc1804 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:10:04.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8348" for this suite.
Jul 22 21:10:10.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:10:10.763: INFO: namespace downward-api-8348 deletion completed in 6.11091899s

• [SLOW TEST:10.302 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:10:10.766: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2828
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:10:10.988: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 21:10:35.086: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2828 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:10:35.086: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:10:36.218: INFO: Found all expected endpoints: [netserver-0]
Jul 22 21:10:36.221: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.123 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2828 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:10:36.221: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:10:37.357: INFO: Found all expected endpoints: [netserver-1]
Jul 22 21:10:37.360: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2828 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:10:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:10:38.503: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:10:38.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2828" for this suite.
Jul 22 21:11:00.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:11:00.598: INFO: namespace pod-network-test-2828 deletion completed in 22.090795551s

• [SLOW TEST:49.832 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:11:00.602: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 22 21:11:00.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7339'
Jul 22 21:11:01.013: INFO: stderr: ""
Jul 22 21:11:01.013: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:11:01.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:01.105: INFO: stderr: ""
Jul 22 21:11:01.105: INFO: stdout: "update-demo-nautilus-glpmk update-demo-nautilus-jc49g "
Jul 22 21:11:01.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:01.188: INFO: stderr: ""
Jul 22 21:11:01.188: INFO: stdout: ""
Jul 22 21:11:01.188: INFO: update-demo-nautilus-glpmk is created but not running
Jul 22 21:11:06.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:06.441: INFO: stderr: ""
Jul 22 21:11:06.441: INFO: stdout: "update-demo-nautilus-glpmk update-demo-nautilus-jc49g "
Jul 22 21:11:06.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:06.559: INFO: stderr: ""
Jul 22 21:11:06.559: INFO: stdout: "true"
Jul 22 21:11:06.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:06.676: INFO: stderr: ""
Jul 22 21:11:06.676: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:11:06.676: INFO: validating pod update-demo-nautilus-glpmk
Jul 22 21:11:06.682: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:11:06.682: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:11:06.682: INFO: update-demo-nautilus-glpmk is verified up and running
Jul 22 21:11:06.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-jc49g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:06.784: INFO: stderr: ""
Jul 22 21:11:06.784: INFO: stdout: "true"
Jul 22 21:11:06.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-jc49g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:06.874: INFO: stderr: ""
Jul 22 21:11:06.874: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:11:06.874: INFO: validating pod update-demo-nautilus-jc49g
Jul 22 21:11:06.878: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:11:06.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:11:06.879: INFO: update-demo-nautilus-jc49g is verified up and running
STEP: scaling down the replication controller
Jul 22 21:11:06.880: INFO: scanned /root for discovery docs: <nil>
Jul 22 21:11:06.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7339'
Jul 22 21:11:08.010: INFO: stderr: ""
Jul 22 21:11:08.010: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:11:08.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:08.094: INFO: stderr: ""
Jul 22 21:11:08.094: INFO: stdout: "update-demo-nautilus-glpmk update-demo-nautilus-jc49g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 21:11:13.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:13.184: INFO: stderr: ""
Jul 22 21:11:13.184: INFO: stdout: "update-demo-nautilus-glpmk update-demo-nautilus-jc49g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 21:11:18.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:18.263: INFO: stderr: ""
Jul 22 21:11:18.263: INFO: stdout: "update-demo-nautilus-glpmk "
Jul 22 21:11:18.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:18.350: INFO: stderr: ""
Jul 22 21:11:18.350: INFO: stdout: "true"
Jul 22 21:11:18.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:18.429: INFO: stderr: ""
Jul 22 21:11:18.429: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:11:18.429: INFO: validating pod update-demo-nautilus-glpmk
Jul 22 21:11:18.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:11:18.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:11:18.432: INFO: update-demo-nautilus-glpmk is verified up and running
STEP: scaling up the replication controller
Jul 22 21:11:18.433: INFO: scanned /root for discovery docs: <nil>
Jul 22 21:11:18.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7339'
Jul 22 21:11:19.557: INFO: stderr: ""
Jul 22 21:11:19.557: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:11:19.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:19.642: INFO: stderr: ""
Jul 22 21:11:19.642: INFO: stdout: "update-demo-nautilus-8ntl5 update-demo-nautilus-glpmk "
Jul 22 21:11:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-8ntl5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:19.775: INFO: stderr: ""
Jul 22 21:11:19.775: INFO: stdout: ""
Jul 22 21:11:19.775: INFO: update-demo-nautilus-8ntl5 is created but not running
Jul 22 21:11:24.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7339'
Jul 22 21:11:24.866: INFO: stderr: ""
Jul 22 21:11:24.866: INFO: stdout: "update-demo-nautilus-8ntl5 update-demo-nautilus-glpmk "
Jul 22 21:11:24.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-8ntl5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:24.959: INFO: stderr: ""
Jul 22 21:11:24.959: INFO: stdout: "true"
Jul 22 21:11:24.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-8ntl5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:25.048: INFO: stderr: ""
Jul 22 21:11:25.048: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:11:25.048: INFO: validating pod update-demo-nautilus-8ntl5
Jul 22 21:11:25.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:11:25.051: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:11:25.051: INFO: update-demo-nautilus-8ntl5 is verified up and running
Jul 22 21:11:25.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:25.134: INFO: stderr: ""
Jul 22 21:11:25.134: INFO: stdout: "true"
Jul 22 21:11:25.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-glpmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7339'
Jul 22 21:11:25.214: INFO: stderr: ""
Jul 22 21:11:25.214: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:11:25.214: INFO: validating pod update-demo-nautilus-glpmk
Jul 22 21:11:25.217: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:11:25.217: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:11:25.217: INFO: update-demo-nautilus-glpmk is verified up and running
STEP: using delete to clean up resources
Jul 22 21:11:25.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7339'
Jul 22 21:11:25.312: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:25.312: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 21:11:25.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7339'
Jul 22 21:11:25.394: INFO: stderr: "No resources found.\n"
Jul 22 21:11:25.394: INFO: stdout: ""
Jul 22 21:11:25.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=update-demo --namespace=kubectl-7339 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:11:25.475: INFO: stderr: ""
Jul 22 21:11:25.475: INFO: stdout: "update-demo-nautilus-8ntl5\nupdate-demo-nautilus-glpmk\n"
Jul 22 21:11:25.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7339'
Jul 22 21:11:26.085: INFO: stderr: "No resources found.\n"
Jul 22 21:11:26.085: INFO: stdout: ""
Jul 22 21:11:26.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=update-demo --namespace=kubectl-7339 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:11:26.201: INFO: stderr: ""
Jul 22 21:11:26.201: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:11:26.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7339" for this suite.
Jul 22 21:11:48.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:11:48.292: INFO: namespace kubectl-7339 deletion completed in 22.087324745s

• [SLOW TEST:47.690 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:11:48.293: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-03639e04-64c2-49e9-9ee3-992e1d4012a5
STEP: Creating a pod to test consume secrets
Jul 22 21:11:48.448: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220" in namespace "projected-289" to be "success or failure"
Jul 22 21:11:48.461: INFO: Pod "pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220": Phase="Pending", Reason="", readiness=false. Elapsed: 12.852136ms
Jul 22 21:11:50.464: INFO: Pod "pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016306042s
Jul 22 21:11:52.467: INFO: Pod "pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019529848s
STEP: Saw pod success
Jul 22 21:11:52.467: INFO: Pod "pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220" satisfied condition "success or failure"
Jul 22 21:11:52.470: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:11:52.484: INFO: Waiting for pod pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220 to disappear
Jul 22 21:11:52.492: INFO: Pod pod-projected-secrets-2ddc0cd8-8ca0-439e-8e15-3f44bea26220 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:11:52.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-289" for this suite.
Jul 22 21:11:58.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:11:58.595: INFO: namespace projected-289 deletion completed in 6.100202064s

• [SLOW TEST:10.302 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:11:58.595: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 21:11:58.746: INFO: Waiting up to 5m0s for pod "pod-431f271c-5541-4932-92fb-7788b543d73d" in namespace "emptydir-8558" to be "success or failure"
Jul 22 21:11:58.752: INFO: Pod "pod-431f271c-5541-4932-92fb-7788b543d73d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449973ms
Jul 22 21:12:00.755: INFO: Pod "pod-431f271c-5541-4932-92fb-7788b543d73d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008588278s
Jul 22 21:12:02.758: INFO: Pod "pod-431f271c-5541-4932-92fb-7788b543d73d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011159385s
STEP: Saw pod success
Jul 22 21:12:02.758: INFO: Pod "pod-431f271c-5541-4932-92fb-7788b543d73d" satisfied condition "success or failure"
Jul 22 21:12:02.760: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-431f271c-5541-4932-92fb-7788b543d73d container test-container: <nil>
STEP: delete the pod
Jul 22 21:12:02.777: INFO: Waiting for pod pod-431f271c-5541-4932-92fb-7788b543d73d to disappear
Jul 22 21:12:02.779: INFO: Pod pod-431f271c-5541-4932-92fb-7788b543d73d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:12:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8558" for this suite.
Jul 22 21:12:08.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:12:08.875: INFO: namespace emptydir-8558 deletion completed in 6.093083293s

• [SLOW TEST:10.280 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:12:08.875: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 22 21:12:09.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-5568'
Jul 22 21:12:09.280: INFO: stderr: ""
Jul 22 21:12:09.280: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:12:09.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5568'
Jul 22 21:12:09.372: INFO: stderr: ""
Jul 22 21:12:09.372: INFO: stdout: "update-demo-nautilus-fn9qj update-demo-nautilus-tg5ch "
Jul 22 21:12:09.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-fn9qj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5568'
Jul 22 21:12:09.459: INFO: stderr: ""
Jul 22 21:12:09.459: INFO: stdout: ""
Jul 22 21:12:09.459: INFO: update-demo-nautilus-fn9qj is created but not running
Jul 22 21:12:14.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5568'
Jul 22 21:12:14.551: INFO: stderr: ""
Jul 22 21:12:14.551: INFO: stdout: "update-demo-nautilus-fn9qj update-demo-nautilus-tg5ch "
Jul 22 21:12:14.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-fn9qj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5568'
Jul 22 21:12:14.629: INFO: stderr: ""
Jul 22 21:12:14.629: INFO: stdout: "true"
Jul 22 21:12:14.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-fn9qj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5568'
Jul 22 21:12:14.717: INFO: stderr: ""
Jul 22 21:12:14.717: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:12:14.717: INFO: validating pod update-demo-nautilus-fn9qj
Jul 22 21:12:14.726: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:12:14.726: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:12:14.726: INFO: update-demo-nautilus-fn9qj is verified up and running
Jul 22 21:12:14.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-tg5ch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5568'
Jul 22 21:12:14.807: INFO: stderr: ""
Jul 22 21:12:14.807: INFO: stdout: "true"
Jul 22 21:12:14.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-tg5ch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5568'
Jul 22 21:12:14.890: INFO: stderr: ""
Jul 22 21:12:14.890: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:12:14.890: INFO: validating pod update-demo-nautilus-tg5ch
Jul 22 21:12:14.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:12:14.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:12:14.894: INFO: update-demo-nautilus-tg5ch is verified up and running
STEP: using delete to clean up resources
Jul 22 21:12:14.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-5568'
Jul 22 21:12:14.973: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:12:14.973: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 21:12:14.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5568'
Jul 22 21:12:15.066: INFO: stderr: "No resources found.\n"
Jul 22 21:12:15.066: INFO: stdout: ""
Jul 22 21:12:15.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=update-demo --namespace=kubectl-5568 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:12:15.153: INFO: stderr: ""
Jul 22 21:12:15.153: INFO: stdout: "update-demo-nautilus-fn9qj\nupdate-demo-nautilus-tg5ch\n"
Jul 22 21:12:15.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5568'
Jul 22 21:12:15.738: INFO: stderr: "No resources found.\n"
Jul 22 21:12:15.738: INFO: stdout: ""
Jul 22 21:12:15.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=update-demo --namespace=kubectl-5568 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:12:15.821: INFO: stderr: ""
Jul 22 21:12:15.821: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:12:15.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5568" for this suite.
Jul 22 21:12:37.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:12:37.918: INFO: namespace kubectl-5568 deletion completed in 22.093397198s

• [SLOW TEST:29.043 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:12:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:12:38.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1" in namespace "downward-api-872" to be "success or failure"
Jul 22 21:12:38.074: INFO: Pod "downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381578ms
Jul 22 21:12:40.077: INFO: Pod "downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007475275s
Jul 22 21:12:42.080: INFO: Pod "downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010620972s
STEP: Saw pod success
Jul 22 21:12:42.081: INFO: Pod "downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1" satisfied condition "success or failure"
Jul 22 21:12:42.083: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1 container client-container: <nil>
STEP: delete the pod
Jul 22 21:12:42.100: INFO: Waiting for pod downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1 to disappear
Jul 22 21:12:42.102: INFO: Pod downwardapi-volume-48f1fb8f-ed69-4177-802c-51bb08d029f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:12:42.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-872" for this suite.
Jul 22 21:12:48.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:12:48.207: INFO: namespace downward-api-872 deletion completed in 6.101696126s

• [SLOW TEST:10.289 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:12:48.207: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 22 21:12:48.353: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 21:12:48.359: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 21:12:48.361: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-0 before test
Jul 22 21:12:48.365: INFO: azure-ip-masq-agent-p9zj8 from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.365: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:12:48.365: INFO: kube-proxy-2xdzw from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.365: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:12:48.365: INFO: kube-flannel-ds-5tcnp from kube-system started at 2019-07-22 19:11:18 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.365: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:12:48.365: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:12:48.365: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 20:28:29 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.365: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 22 21:12:48.365: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7wl7x from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:12:48.365: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:12:48.365: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-1 before test
Jul 22 21:12:48.373: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-4pkq8 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:12:48.373: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:12:48.373: INFO: coredns-7f68dcdbdb-6ndmh from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:12:48.373: INFO: kube-proxy-cg6wd from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:12:48.373: INFO: kube-flannel-ds-ftphf from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:12:48.373: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:12:48.373: INFO: azure-ip-masq-agent-xsrxn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:12:48.373: INFO: tiller-deploy-54c96cb5df-pp2fz from kube-system started at 2019-07-22 19:11:28 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container tiller ready: true, restart count 0
Jul 22 21:12:48.373: INFO: sonobuoy-e2e-job-fa452941f8be4107 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.373: INFO: 	Container e2e ready: true, restart count 0
Jul 22 21:12:48.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:12:48.373: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-2 before test
Jul 22 21:12:48.381: INFO: kubernetes-dashboard-66dd8b8df7-jnsv8 from kube-system started at 2019-07-22 19:11:26 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 22 21:12:48.381: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7vkzc from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:12:48.381: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:12:48.381: INFO: kube-proxy-4b7fg from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:12:48.381: INFO: azure-ip-masq-agent-4qfk6 from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:12:48.381: INFO: kube-flannel-ds-p28wn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:12:48.381: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:12:48.381: INFO: metrics-server-864ffbc5c-x5xsw from kube-system started at 2019-07-22 19:11:24 +0000 UTC (1 container statuses recorded)
Jul 22 21:12:48.381: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-913db9c3-8dfe-42c4-ab71-1ff14ebc62f5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-913db9c3-8dfe-42c4-ab71-1ff14ebc62f5 off the node k8s-linuxpool-16111918-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-913db9c3-8dfe-42c4-ab71-1ff14ebc62f5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:12:56.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9295" for this suite.
Jul 22 21:13:08.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:08.561: INFO: namespace sched-pred-9295 deletion completed in 12.0897584s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:20.354 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:08.562: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4348
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:13:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:13:09.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4348" for this suite.
Jul 22 21:13:15.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:15.906: INFO: namespace custom-resource-definition-4348 deletion completed in 6.083854799s

• [SLOW TEST:7.344 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:15.907: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 22 21:13:16.055: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 21:13:16.062: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 21:13:16.064: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-0 before test
Jul 22 21:13:16.068: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 20:28:29 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.068: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 22 21:13:16.068: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7wl7x from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.068: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:13:16.068: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:13:16.068: INFO: azure-ip-masq-agent-p9zj8 from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.068: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:13:16.068: INFO: kube-proxy-2xdzw from kube-system started at 2019-07-22 19:11:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.068: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:13:16.068: INFO: kube-flannel-ds-5tcnp from kube-system started at 2019-07-22 19:11:18 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.068: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:13:16.069: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:13:16.069: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-1 before test
Jul 22 21:13:16.075: INFO: sonobuoy-e2e-job-fa452941f8be4107 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container e2e ready: true, restart count 0
Jul 22 21:13:16.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:13:16.075: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-4pkq8 from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:13:16.075: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:13:16.075: INFO: coredns-7f68dcdbdb-6ndmh from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:13:16.075: INFO: kube-proxy-cg6wd from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:13:16.075: INFO: kube-flannel-ds-ftphf from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:13:16.075: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:13:16.075: INFO: azure-ip-masq-agent-xsrxn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:13:16.075: INFO: tiller-deploy-54c96cb5df-pp2fz from kube-system started at 2019-07-22 19:11:28 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.075: INFO: 	Container tiller ready: true, restart count 0
Jul 22 21:13:16.075: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-16111918-2 before test
Jul 22 21:13:16.081: INFO: sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7vkzc from heptio-sonobuoy started at 2019-07-22 20:28:39 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 21:13:16.081: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 21:13:16.081: INFO: kube-proxy-4b7fg from kube-system started at 2019-07-22 19:10:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.081: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:13:16.081: INFO: azure-ip-masq-agent-4qfk6 from kube-system started at 2019-07-22 19:11:01 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.081: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jul 22 21:13:16.081: INFO: kube-flannel-ds-p28wn from kube-system started at 2019-07-22 19:11:01 +0000 UTC (2 container statuses recorded)
Jul 22 21:13:16.081: INFO: 	Container install-cni ready: true, restart count 0
Jul 22 21:13:16.081: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 22 21:13:16.081: INFO: metrics-server-864ffbc5c-x5xsw from kube-system started at 2019-07-22 19:11:24 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.082: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 21:13:16.082: INFO: kubernetes-dashboard-66dd8b8df7-jnsv8 from kube-system started at 2019-07-22 19:11:26 +0000 UTC (1 container statuses recorded)
Jul 22 21:13:16.082: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node k8s-linuxpool-16111918-0
STEP: verifying the node has the label node k8s-linuxpool-16111918-1
STEP: verifying the node has the label node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-16111918-0
Jul 22 21:13:16.125: INFO: Pod sonobuoy-e2e-job-fa452941f8be4107 requesting resource cpu=0m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod sonobuoy-systemd-logs-daemon-set-140ddde980b84794-4pkq8 requesting resource cpu=0m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7vkzc requesting resource cpu=0m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod sonobuoy-systemd-logs-daemon-set-140ddde980b84794-7wl7x requesting resource cpu=0m on Node k8s-linuxpool-16111918-0
Jul 22 21:13:16.125: INFO: Pod azure-ip-masq-agent-4qfk6 requesting resource cpu=50m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod azure-ip-masq-agent-p9zj8 requesting resource cpu=50m on Node k8s-linuxpool-16111918-0
Jul 22 21:13:16.125: INFO: Pod azure-ip-masq-agent-xsrxn requesting resource cpu=50m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod coredns-7f68dcdbdb-6ndmh requesting resource cpu=100m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod kube-flannel-ds-5tcnp requesting resource cpu=0m on Node k8s-linuxpool-16111918-0
Jul 22 21:13:16.125: INFO: Pod kube-flannel-ds-ftphf requesting resource cpu=0m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod kube-flannel-ds-p28wn requesting resource cpu=0m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod kube-proxy-2xdzw requesting resource cpu=100m on Node k8s-linuxpool-16111918-0
Jul 22 21:13:16.125: INFO: Pod kube-proxy-4b7fg requesting resource cpu=100m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod kube-proxy-cg6wd requesting resource cpu=100m on Node k8s-linuxpool-16111918-1
Jul 22 21:13:16.125: INFO: Pod kubernetes-dashboard-66dd8b8df7-jnsv8 requesting resource cpu=300m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod metrics-server-864ffbc5c-x5xsw requesting resource cpu=0m on Node k8s-linuxpool-16111918-2
Jul 22 21:13:16.125: INFO: Pod tiller-deploy-54c96cb5df-pp2fz requesting resource cpu=50m on Node k8s-linuxpool-16111918-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259.15b3d718909d17c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1782/filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259 to k8s-linuxpool-16111918-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259.15b3d718e4124a5c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259.15b3d71901eef3f0], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259.15b3d71916d7d9eb], Reason = [Created], Message = [Created container filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259.15b3d71925939c7c], Reason = [Started], Message = [Started container filler-pod-60b4ae20-519d-4c51-9f07-8bfa7db85259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade.15b3d7189102c106], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1782/filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade to k8s-linuxpool-16111918-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade.15b3d718e17888ab], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade.15b3d718fe7c496d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade.15b3d7191576d386], Reason = [Created], Message = [Created container filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade.15b3d71921654519], Reason = [Started], Message = [Started container filler-pod-b8da64c4-dcee-4366-af1a-6010ad881ade]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6.15b3d71891e5257e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1782/filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6 to k8s-linuxpool-16111918-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6.15b3d718e5e00ac6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6.15b3d718fb7c3702], Reason = [Created], Message = [Created container filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6.15b3d719085df94b], Reason = [Started], Message = [Started container filler-pod-bd63d374-6169-4fbe-ad72-fe1f29823cb6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b3d71982557b9f], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-linuxpool-16111918-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-16111918-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-16111918-0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:13:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1782" for this suite.
Jul 22 21:13:27.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:27.352: INFO: namespace sched-pred-1782 deletion completed in 6.102812598s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.445 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:27.355: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b8072cd4-1356-4c7d-80c8-436b1fb91608
STEP: Creating a pod to test consume secrets
Jul 22 21:13:27.512: INFO: Waiting up to 5m0s for pod "pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1" in namespace "secrets-3678" to be "success or failure"
Jul 22 21:13:27.515: INFO: Pod "pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855686ms
Jul 22 21:13:29.518: INFO: Pod "pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006065873s
Jul 22 21:13:31.521: INFO: Pod "pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008961861s
STEP: Saw pod success
Jul 22 21:13:31.521: INFO: Pod "pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1" satisfied condition "success or failure"
Jul 22 21:13:31.523: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:13:31.540: INFO: Waiting for pod pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1 to disappear
Jul 22 21:13:31.542: INFO: Pod pod-secrets-4df686ea-5925-44f6-813e-3af4a72e69b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:13:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3678" for this suite.
Jul 22 21:13:37.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:37.630: INFO: namespace secrets-3678 deletion completed in 6.084347585s

• [SLOW TEST:10.276 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:37.631: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-93cec047-ef81-415f-a304-a0c171157420
STEP: Creating a pod to test consume secrets
Jul 22 21:13:37.789: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4" in namespace "projected-3763" to be "success or failure"
Jul 22 21:13:37.793: INFO: Pod "pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684482ms
Jul 22 21:13:39.796: INFO: Pod "pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007068366s
Jul 22 21:13:41.800: INFO: Pod "pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010818749s
STEP: Saw pod success
Jul 22 21:13:41.800: INFO: Pod "pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4" satisfied condition "success or failure"
Jul 22 21:13:41.802: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:13:41.815: INFO: Waiting for pod pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4 to disappear
Jul 22 21:13:41.818: INFO: Pod pod-projected-secrets-2713ed30-a846-4e8c-b9c6-ab944bc2a8f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:13:41.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3763" for this suite.
Jul 22 21:13:47.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:47.905: INFO: namespace projected-3763 deletion completed in 6.083380284s

• [SLOW TEST:10.274 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:47.906: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3756
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 21:13:48.063: INFO: Waiting up to 5m0s for pod "pod-3807afc9-7d8e-47a7-938f-ab47461c367a" in namespace "emptydir-3756" to be "success or failure"
Jul 22 21:13:48.067: INFO: Pod "pod-3807afc9-7d8e-47a7-938f-ab47461c367a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204084ms
Jul 22 21:13:50.070: INFO: Pod "pod-3807afc9-7d8e-47a7-938f-ab47461c367a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006072569s
Jul 22 21:13:52.073: INFO: Pod "pod-3807afc9-7d8e-47a7-938f-ab47461c367a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009465251s
STEP: Saw pod success
Jul 22 21:13:52.073: INFO: Pod "pod-3807afc9-7d8e-47a7-938f-ab47461c367a" satisfied condition "success or failure"
Jul 22 21:13:52.075: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-3807afc9-7d8e-47a7-938f-ab47461c367a container test-container: <nil>
STEP: delete the pod
Jul 22 21:13:52.090: INFO: Waiting for pod pod-3807afc9-7d8e-47a7-938f-ab47461c367a to disappear
Jul 22 21:13:52.095: INFO: Pod pod-3807afc9-7d8e-47a7-938f-ab47461c367a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:13:52.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3756" for this suite.
Jul 22 21:13:58.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:13:58.202: INFO: namespace emptydir-3756 deletion completed in 6.103427279s

• [SLOW TEST:10.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:13:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 in namespace container-probe-9607
Jul 22 21:14:02.361: INFO: Started pod liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 in namespace container-probe-9607
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:14:02.362: INFO: Initial restart count of pod liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is 0
Jul 22 21:14:20.390: INFO: Restart count of pod container-probe-9607/liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is now 1 (18.027728824s elapsed)
Jul 22 21:14:40.418: INFO: Restart count of pod container-probe-9607/liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is now 2 (38.055517412s elapsed)
Jul 22 21:15:00.454: INFO: Restart count of pod container-probe-9607/liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is now 3 (58.091524028s elapsed)
Jul 22 21:15:20.528: INFO: Restart count of pod container-probe-9607/liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is now 4 (1m18.165283825s elapsed)
Jul 22 21:16:18.819: INFO: Restart count of pod container-probe-9607/liveness-c850cc23-23e5-4d61-84b5-efff197e84a4 is now 5 (2m16.456411018s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:16:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9607" for this suite.
Jul 22 21:16:24.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:16:24.937: INFO: namespace container-probe-9607 deletion completed in 6.1059031s

• [SLOW TEST:146.734 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:16:24.937: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:16:25.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 version'
Jul 22 21:16:25.170: INFO: stderr: ""
Jul 22 21:16:25.170: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"4485c6f18cee9a5d3c3b4e523bd27972b1b53892\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T09:18:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"6fb6917c37d5c1de367fce388c9bd0e4a36e6e19\", GitTreeState:\"clean\", BuildDate:\"2019-07-19T02:04:24Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:16:25.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-610" for this suite.
Jul 22 21:16:31.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:16:31.258: INFO: namespace kubectl-610 deletion completed in 6.085269301s

• [SLOW TEST:6.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:16:31.259: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 21:16:31.407: INFO: Waiting up to 5m0s for pod "pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d" in namespace "emptydir-1856" to be "success or failure"
Jul 22 21:16:31.409: INFO: Pod "pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.509187ms
Jul 22 21:16:33.413: INFO: Pod "pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005787946s
Jul 22 21:16:35.416: INFO: Pod "pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008903206s
STEP: Saw pod success
Jul 22 21:16:35.416: INFO: Pod "pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d" satisfied condition "success or failure"
Jul 22 21:16:35.418: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d container test-container: <nil>
STEP: delete the pod
Jul 22 21:16:35.449: INFO: Waiting for pod pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d to disappear
Jul 22 21:16:35.451: INFO: Pod pod-9a771ad9-9e73-4ca8-b73d-755b03feee1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:16:35.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1856" for this suite.
Jul 22 21:16:41.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:16:41.546: INFO: namespace emptydir-1856 deletion completed in 6.092008062s

• [SLOW TEST:10.288 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:16:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-3a513a10-793c-4aa9-b19d-ae87b8bd0e92
STEP: Creating a pod to test consume secrets
Jul 22 21:16:41.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb" in namespace "projected-1196" to be "success or failure"
Jul 22 21:16:41.770: INFO: Pod "pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.728871ms
Jul 22 21:16:43.774: INFO: Pod "pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009379027s
Jul 22 21:16:45.777: INFO: Pod "pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012794884s
STEP: Saw pod success
Jul 22 21:16:45.777: INFO: Pod "pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb" satisfied condition "success or failure"
Jul 22 21:16:45.779: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:16:45.795: INFO: Waiting for pod pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb to disappear
Jul 22 21:16:45.797: INFO: Pod pod-projected-secrets-a4c94885-e6c4-4f6c-b65c-4585d17dfadb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:16:45.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1196" for this suite.
Jul 22 21:16:51.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:16:51.888: INFO: namespace projected-1196 deletion completed in 6.087384682s

• [SLOW TEST:10.340 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:16:51.892: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 22 21:16:57.140: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:16:58.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2284" for this suite.
Jul 22 21:17:20.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:17:20.312: INFO: namespace replicaset-2284 deletion completed in 22.151881519s

• [SLOW TEST:28.420 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:17:20.312: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:17:42.479: INFO: DNS probes using dns-test-643f1435-f6d1-4178-9f68-ff0d7c4cae1e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:17:46.515: INFO: DNS probes using dns-test-b46df4ed-4dfe-45d5-a92d-ea54376a1816 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2475.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2475.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:17:50.582: INFO: DNS probes using dns-test-f425ea38-783e-41f5-8e0d-b4119d9f0608 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:17:50.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2475" for this suite.
Jul 22 21:17:56.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:17:56.765: INFO: namespace dns-2475 deletion completed in 6.148196354s

• [SLOW TEST:36.453 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:17:56.765: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8859, will wait for the garbage collector to delete the pods
Jul 22 21:18:01.029: INFO: Deleting Job.batch foo took: 4.709776ms
Jul 22 21:18:01.329: INFO: Terminating Job.batch foo pods took: 300.162494ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:18:37.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8859" for this suite.
Jul 22 21:18:43.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:18:44.014: INFO: namespace job-8859 deletion completed in 6.080030582s

• [SLOW TEST:47.249 seconds]
[sig-apps] Job
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:18:44.015: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0722 21:18:45.200129      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 21:18:45.200: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:18:45.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7790" for this suite.
Jul 22 21:18:51.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:18:51.293: INFO: namespace gc-7790 deletion completed in 6.08992073s

• [SLOW TEST:7.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:18:51.300: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8579
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-38f39f4b-7f3e-48fa-90c7-25d5581ee080
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-38f39f4b-7f3e-48fa-90c7-25d5581ee080
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:20:25.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8579" for this suite.
Jul 22 21:20:47.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:20:47.978: INFO: namespace configmap-8579 deletion completed in 22.096646676s

• [SLOW TEST:116.678 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:20:47.981: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 22 21:20:48.131: INFO: PodSpec: initContainers in spec.initContainers
Jul 22 21:21:30.036: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-119289bb-d874-4fca-8ffe-d5f7846237d7", GenerateName:"", Namespace:"init-container-9290", SelfLink:"/api/v1/namespaces/init-container-9290/pods/pod-init-119289bb-d874-4fca-8ffe-d5f7846237d7", UID:"4c59c3e2-6b74-4989-9f2b-25259b57c504", ResourceVersion:"20089", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63699427248, loc:(*time.Location)(0x80c0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"131844043"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f9zzq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002bd8340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f9zzq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f9zzq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f9zzq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002db6488), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-16111918-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0027c23c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002db6500)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002db6520)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002db6528), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002db652c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699427248, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699427248, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699427248, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699427248, loc:(*time.Location)(0x80c0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.6", PodIP:"10.244.3.149", StartTime:(*v1.Time)(0xc001d30a20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ce23f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ce2460)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f47bee7395bb67a6f490eb93785bb54474ed0b5d6ab112d1f9d64f312ad65aaa"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d30ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d30ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:21:30.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9290" for this suite.
Jul 22 21:21:52.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:21:52.141: INFO: namespace init-container-9290 deletion completed in 22.101752499s

• [SLOW TEST:64.160 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:21:52.141: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-cnfw
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:21:52.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cnfw" in namespace "subpath-1473" to be "success or failure"
Jul 22 21:21:52.306: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533187ms
Jul 22 21:21:54.309: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006193114s
Jul 22 21:21:56.312: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 4.009226745s
Jul 22 21:21:58.315: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 6.012445074s
Jul 22 21:22:00.319: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 8.0161892s
Jul 22 21:22:02.322: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 10.018894332s
Jul 22 21:22:04.325: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 12.021826362s
Jul 22 21:22:06.328: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 14.024560593s
Jul 22 21:22:08.330: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 16.027142324s
Jul 22 21:22:10.333: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 18.029598857s
Jul 22 21:22:12.383: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 20.079976748s
Jul 22 21:22:14.386: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Running", Reason="", readiness=true. Elapsed: 22.083002877s
Jul 22 21:22:16.389: INFO: Pod "pod-subpath-test-configmap-cnfw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086282204s
STEP: Saw pod success
Jul 22 21:22:16.389: INFO: Pod "pod-subpath-test-configmap-cnfw" satisfied condition "success or failure"
Jul 22 21:22:16.392: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-subpath-test-configmap-cnfw container test-container-subpath-configmap-cnfw: <nil>
STEP: delete the pod
Jul 22 21:22:16.407: INFO: Waiting for pod pod-subpath-test-configmap-cnfw to disappear
Jul 22 21:22:16.409: INFO: Pod pod-subpath-test-configmap-cnfw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cnfw
Jul 22 21:22:16.410: INFO: Deleting pod "pod-subpath-test-configmap-cnfw" in namespace "subpath-1473"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:22:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1473" for this suite.
Jul 22 21:22:22.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:22:22.513: INFO: namespace subpath-1473 deletion completed in 6.098280437s

• [SLOW TEST:30.372 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:22:22.514: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8533
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a583e30c-e82f-4256-8a5b-ae5a90503ed7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a583e30c-e82f-4256-8a5b-ae5a90503ed7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:22:28.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8533" for this suite.
Jul 22 21:22:50.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:22:50.799: INFO: namespace projected-8533 deletion completed in 22.092761502s

• [SLOW TEST:28.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:22:50.800: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 22 21:22:50.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20297,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 21:22:50.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20297,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 22 21:23:00.962: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20314,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 22 21:23:00.962: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20314,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 22 21:23:10.968: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20330,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 21:23:10.968: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20330,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 22 21:23:20.980: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20346,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 21:23:20.980: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-a,UID:8c76c91a-8602-4623-8236-01429ae63cc6,ResourceVersion:20346,Generation:0,CreationTimestamp:2019-07-22 21:22:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 22 21:23:30.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-b,UID:35a17ee2-d468-48a0-840d-68b3605643c5,ResourceVersion:20363,Generation:0,CreationTimestamp:2019-07-22 21:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 21:23:30.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-b,UID:35a17ee2-d468-48a0-840d-68b3605643c5,ResourceVersion:20363,Generation:0,CreationTimestamp:2019-07-22 21:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 22 21:23:40.990: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-b,UID:35a17ee2-d468-48a0-840d-68b3605643c5,ResourceVersion:20380,Generation:0,CreationTimestamp:2019-07-22 21:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 21:23:40.990: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6496,SelfLink:/api/v1/namespaces/watch-6496/configmaps/e2e-watch-test-configmap-b,UID:35a17ee2-d468-48a0-840d-68b3605643c5,ResourceVersion:20380,Generation:0,CreationTimestamp:2019-07-22 21:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:23:50.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6496" for this suite.
Jul 22 21:23:57.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:23:57.112: INFO: namespace watch-6496 deletion completed in 6.116894927s

• [SLOW TEST:66.312 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:23:57.113: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 22 21:23:57.362: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4007,SelfLink:/api/v1/namespaces/watch-4007/configmaps/e2e-watch-test-watch-closed,UID:531e57d8-a774-4a86-8763-823b25d83f46,ResourceVersion:20416,Generation:0,CreationTimestamp:2019-07-22 21:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 21:23:57.362: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4007,SelfLink:/api/v1/namespaces/watch-4007/configmaps/e2e-watch-test-watch-closed,UID:531e57d8-a774-4a86-8763-823b25d83f46,ResourceVersion:20417,Generation:0,CreationTimestamp:2019-07-22 21:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 22 21:23:57.378: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4007,SelfLink:/api/v1/namespaces/watch-4007/configmaps/e2e-watch-test-watch-closed,UID:531e57d8-a774-4a86-8763-823b25d83f46,ResourceVersion:20418,Generation:0,CreationTimestamp:2019-07-22 21:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 21:23:57.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4007,SelfLink:/api/v1/namespaces/watch-4007/configmaps/e2e-watch-test-watch-closed,UID:531e57d8-a774-4a86-8763-823b25d83f46,ResourceVersion:20419,Generation:0,CreationTimestamp:2019-07-22 21:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:23:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4007" for this suite.
Jul 22 21:24:03.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:24:03.471: INFO: namespace watch-4007 deletion completed in 6.089978861s

• [SLOW TEST:6.358 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:24:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-567.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-567.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-567.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-567.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-567.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-567.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:24:07.659: INFO: DNS probes using dns-567/dns-test-0191b4eb-7820-40b2-b063-7728dfe712ff succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:24:07.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-567" for this suite.
Jul 22 21:24:13.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:24:13.754: INFO: namespace dns-567 deletion completed in 6.083442593s

• [SLOW TEST:10.282 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:24:13.758: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:24:17.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-523" for this suite.
Jul 22 21:24:57.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:24:58.052: INFO: namespace kubelet-test-523 deletion completed in 40.113651252s

• [SLOW TEST:44.294 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:24:58.053: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-56vgq in namespace proxy-9325
I0722 21:24:58.222064      15 runners.go:180] Created replication controller with name: proxy-service-56vgq, namespace: proxy-9325, replica count: 1
I0722 21:24:59.272829      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:25:00.273072      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:25:01.274046      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:25:02.322194      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:25:03.322477      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:25:04.322720      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:25:05.322964      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:25:06.323178      15 runners.go:180] proxy-service-56vgq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:25:06.325: INFO: setup took 8.126845102s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 22 21:25:06.337: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.439642ms)
Jul 22 21:25:06.337: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 11.511942ms)
Jul 22 21:25:06.338: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 12.164239ms)
Jul 22 21:25:06.338: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 12.521837ms)
Jul 22 21:25:06.341: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 15.436222ms)
Jul 22 21:25:06.341: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 15.485722ms)
Jul 22 21:25:06.346: INFO: (0) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 19.999699ms)
Jul 22 21:25:06.346: INFO: (0) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 20.418797ms)
Jul 22 21:25:06.346: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 20.461097ms)
Jul 22 21:25:06.347: INFO: (0) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 21.142194ms)
Jul 22 21:25:06.347: INFO: (0) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 21.187994ms)
Jul 22 21:25:06.347: INFO: (0) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 21.180194ms)
Jul 22 21:25:06.348: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 22.219488ms)
Jul 22 21:25:06.348: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 22.550487ms)
Jul 22 21:25:06.349: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 23.044484ms)
Jul 22 21:25:06.352: INFO: (0) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 26.402167ms)
Jul 22 21:25:06.358: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 5.92297ms)
Jul 22 21:25:06.359: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 6.598367ms)
Jul 22 21:25:06.369: INFO: (1) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 15.78072ms)
Jul 22 21:25:06.369: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 16.174319ms)
Jul 22 21:25:06.369: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 16.368318ms)
Jul 22 21:25:06.370: INFO: (1) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 17.119914ms)
Jul 22 21:25:06.370: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 17.002814ms)
Jul 22 21:25:06.370: INFO: (1) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 17.812611ms)
Jul 22 21:25:06.370: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 17.275613ms)
Jul 22 21:25:06.370: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 17.299513ms)
Jul 22 21:25:06.371: INFO: (1) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 18.149508ms)
Jul 22 21:25:06.371: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 18.032209ms)
Jul 22 21:25:06.371: INFO: (1) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 18.031109ms)
Jul 22 21:25:06.371: INFO: (1) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 18.506007ms)
Jul 22 21:25:06.371: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 18.241809ms)
Jul 22 21:25:06.372: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 18.701806ms)
Jul 22 21:25:06.382: INFO: (2) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 9.365953ms)
Jul 22 21:25:06.383: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 10.228049ms)
Jul 22 21:25:06.387: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 14.220828ms)
Jul 22 21:25:06.387: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 15.095624ms)
Jul 22 21:25:06.388: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 15.069825ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 16.449717ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 16.343017ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 17.307713ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 16.998415ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 16.969114ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 16.956015ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 16.907615ms)
Jul 22 21:25:06.389: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 17.544712ms)
Jul 22 21:25:06.390: INFO: (2) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 17.190113ms)
Jul 22 21:25:06.390: INFO: (2) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 18.00971ms)
Jul 22 21:25:06.392: INFO: (2) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 19.076404ms)
Jul 22 21:25:06.403: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.129844ms)
Jul 22 21:25:06.404: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 12.280738ms)
Jul 22 21:25:06.404: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 11.646241ms)
Jul 22 21:25:06.404: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 11.719141ms)
Jul 22 21:25:06.404: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 12.229338ms)
Jul 22 21:25:06.404: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.85574ms)
Jul 22 21:25:06.406: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 13.356733ms)
Jul 22 21:25:06.407: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 14.372628ms)
Jul 22 21:25:06.407: INFO: (3) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 14.607127ms)
Jul 22 21:25:06.407: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 14.738426ms)
Jul 22 21:25:06.408: INFO: (3) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 15.047525ms)
Jul 22 21:25:06.408: INFO: (3) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 15.459422ms)
Jul 22 21:25:06.409: INFO: (3) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 15.704021ms)
Jul 22 21:25:06.409: INFO: (3) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 15.778021ms)
Jul 22 21:25:06.409: INFO: (3) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 16.386018ms)
Jul 22 21:25:06.409: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 16.447317ms)
Jul 22 21:25:06.418: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 8.818355ms)
Jul 22 21:25:06.419: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 10.212749ms)
Jul 22 21:25:06.420: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.372648ms)
Jul 22 21:25:06.420: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 10.532447ms)
Jul 22 21:25:06.420: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 10.549047ms)
Jul 22 21:25:06.420: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 10.762846ms)
Jul 22 21:25:06.420: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 11.034844ms)
Jul 22 21:25:06.421: INFO: (4) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 11.266743ms)
Jul 22 21:25:06.421: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.552441ms)
Jul 22 21:25:06.421: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 11.89954ms)
Jul 22 21:25:06.422: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 12.194939ms)
Jul 22 21:25:06.422: INFO: (4) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 12.798535ms)
Jul 22 21:25:06.422: INFO: (4) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 12.770536ms)
Jul 22 21:25:06.422: INFO: (4) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 13.157334ms)
Jul 22 21:25:06.423: INFO: (4) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 13.575931ms)
Jul 22 21:25:06.423: INFO: (4) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 13.84623ms)
Jul 22 21:25:06.426: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 2.676187ms)
Jul 22 21:25:06.426: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 3.052784ms)
Jul 22 21:25:06.430: INFO: (5) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 7.110565ms)
Jul 22 21:25:06.435: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 11.444442ms)
Jul 22 21:25:06.436: INFO: (5) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 12.377238ms)
Jul 22 21:25:06.436: INFO: (5) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.585237ms)
Jul 22 21:25:06.436: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 12.431837ms)
Jul 22 21:25:06.436: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 12.664436ms)
Jul 22 21:25:06.436: INFO: (5) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 12.384338ms)
Jul 22 21:25:06.437: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 13.720531ms)
Jul 22 21:25:06.438: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 14.391928ms)
Jul 22 21:25:06.439: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 15.663421ms)
Jul 22 21:25:06.439: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 15.81572ms)
Jul 22 21:25:06.440: INFO: (5) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 16.305718ms)
Jul 22 21:25:06.440: INFO: (5) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 16.657616ms)
Jul 22 21:25:06.440: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 16.636916ms)
Jul 22 21:25:06.444: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 4.065279ms)
Jul 22 21:25:06.445: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 4.525677ms)
Jul 22 21:25:06.449: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 7.91806ms)
Jul 22 21:25:06.449: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 8.730156ms)
Jul 22 21:25:06.450: INFO: (6) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 9.348853ms)
Jul 22 21:25:06.450: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 9.205154ms)
Jul 22 21:25:06.450: INFO: (6) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 9.816951ms)
Jul 22 21:25:06.451: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 9.82875ms)
Jul 22 21:25:06.451: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 10.817446ms)
Jul 22 21:25:06.451: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 10.418848ms)
Jul 22 21:25:06.451: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 10.424948ms)
Jul 22 21:25:06.452: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.256748ms)
Jul 22 21:25:06.454: INFO: (6) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 12.909935ms)
Jul 22 21:25:06.455: INFO: (6) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 13.953429ms)
Jul 22 21:25:06.456: INFO: (6) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 14.627826ms)
Jul 22 21:25:06.456: INFO: (6) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 14.422528ms)
Jul 22 21:25:06.461: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 5.229173ms)
Jul 22 21:25:06.462: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 6.121669ms)
Jul 22 21:25:06.463: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 7.434763ms)
Jul 22 21:25:06.464: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 7.461962ms)
Jul 22 21:25:06.464: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 7.635662ms)
Jul 22 21:25:06.464: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 7.741161ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 12.585537ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 13.019735ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 13.058434ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 13.252533ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 13.025135ms)
Jul 22 21:25:06.469: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 13.102034ms)
Jul 22 21:25:06.470: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 13.281933ms)
Jul 22 21:25:06.470: INFO: (7) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 13.340533ms)
Jul 22 21:25:06.470: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 13.606732ms)
Jul 22 21:25:06.470: INFO: (7) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 13.694631ms)
Jul 22 21:25:06.480: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 10.204849ms)
Jul 22 21:25:06.480: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 9.690451ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 10.230149ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.747046ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 10.878345ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 10.093349ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 10.109349ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 10.379748ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 10.594947ms)
Jul 22 21:25:06.481: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.304948ms)
Jul 22 21:25:06.482: INFO: (8) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 11.642141ms)
Jul 22 21:25:06.482: INFO: (8) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 11.609642ms)
Jul 22 21:25:06.483: INFO: (8) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 11.724641ms)
Jul 22 21:25:06.483: INFO: (8) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 12.175039ms)
Jul 22 21:25:06.483: INFO: (8) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 13.344033ms)
Jul 22 21:25:06.484: INFO: (8) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 13.396932ms)
Jul 22 21:25:06.491: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 6.543967ms)
Jul 22 21:25:06.492: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 7.777161ms)
Jul 22 21:25:06.492: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 7.203664ms)
Jul 22 21:25:06.494: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 9.234554ms)
Jul 22 21:25:06.494: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 9.153654ms)
Jul 22 21:25:06.494: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 8.870456ms)
Jul 22 21:25:06.494: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 8.704457ms)
Jul 22 21:25:06.495: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 9.743551ms)
Jul 22 21:25:06.495: INFO: (9) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 10.956845ms)
Jul 22 21:25:06.495: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 10.10705ms)
Jul 22 21:25:06.496: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.140544ms)
Jul 22 21:25:06.497: INFO: (9) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 12.797136ms)
Jul 22 21:25:06.498: INFO: (9) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.558937ms)
Jul 22 21:25:06.498: INFO: (9) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 13.573332ms)
Jul 22 21:25:06.498: INFO: (9) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 13.278333ms)
Jul 22 21:25:06.499: INFO: (9) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 14.048829ms)
Jul 22 21:25:06.503: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 4.379378ms)
Jul 22 21:25:06.504: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 4.881175ms)
Jul 22 21:25:06.504: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 4.873575ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 9.217953ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 9.710251ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 9.807051ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 9.489653ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 9.119855ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 9.478952ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 9.086654ms)
Jul 22 21:25:06.509: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.123449ms)
Jul 22 21:25:06.510: INFO: (10) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 10.120149ms)
Jul 22 21:25:06.510: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 10.875345ms)
Jul 22 21:25:06.510: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 10.685447ms)
Jul 22 21:25:06.512: INFO: (10) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 12.379737ms)
Jul 22 21:25:06.512: INFO: (10) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 12.772435ms)
Jul 22 21:25:06.520: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 7.750561ms)
Jul 22 21:25:06.520: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 7.90816ms)
Jul 22 21:25:06.521: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 8.073459ms)
Jul 22 21:25:06.521: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 8.522857ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 9.127054ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 9.535952ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 9.509052ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 9.675852ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 9.694451ms)
Jul 22 21:25:06.522: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 9.581752ms)
Jul 22 21:25:06.523: INFO: (11) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 10.566146ms)
Jul 22 21:25:06.524: INFO: (11) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 11.238244ms)
Jul 22 21:25:06.526: INFO: (11) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 13.055734ms)
Jul 22 21:25:06.526: INFO: (11) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 13.382633ms)
Jul 22 21:25:06.527: INFO: (11) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 14.273629ms)
Jul 22 21:25:06.527: INFO: (11) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 14.362527ms)
Jul 22 21:25:06.532: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 4.957475ms)
Jul 22 21:25:06.532: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 4.973075ms)
Jul 22 21:25:06.535: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 7.724161ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 8.630657ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 8.468957ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 9.056155ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 8.802955ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 9.098954ms)
Jul 22 21:25:06.536: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 9.069354ms)
Jul 22 21:25:06.537: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 9.377953ms)
Jul 22 21:25:06.537: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 9.445353ms)
Jul 22 21:25:06.537: INFO: (12) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 9.821751ms)
Jul 22 21:25:06.538: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.05105ms)
Jul 22 21:25:06.540: INFO: (12) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.454537ms)
Jul 22 21:25:06.541: INFO: (12) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 12.742236ms)
Jul 22 21:25:06.541: INFO: (12) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 13.506432ms)
Jul 22 21:25:06.545: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 3.680681ms)
Jul 22 21:25:06.548: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 6.999565ms)
Jul 22 21:25:06.551: INFO: (13) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 8.587757ms)
Jul 22 21:25:06.552: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 10.06385ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 10.352148ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 11.85494ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 10.918145ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.596442ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 11.176143ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 11.725741ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 11.340043ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 12.146739ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 11.383943ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 10.753046ms)
Jul 22 21:25:06.553: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 11.004745ms)
Jul 22 21:25:06.554: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 10.732346ms)
Jul 22 21:25:06.565: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 10.199148ms)
Jul 22 21:25:06.566: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.93234ms)
Jul 22 21:25:06.566: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 12.331438ms)
Jul 22 21:25:06.566: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 12.269338ms)
Jul 22 21:25:06.566: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 12.539837ms)
Jul 22 21:25:06.567: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 12.712536ms)
Jul 22 21:25:06.567: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 13.332632ms)
Jul 22 21:25:06.568: INFO: (14) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 13.751931ms)
Jul 22 21:25:06.569: INFO: (14) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 14.350327ms)
Jul 22 21:25:06.569: INFO: (14) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 15.607221ms)
Jul 22 21:25:06.570: INFO: (14) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 15.486022ms)
Jul 22 21:25:06.571: INFO: (14) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 16.865616ms)
Jul 22 21:25:06.571: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 16.344918ms)
Jul 22 21:25:06.571: INFO: (14) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 16.036819ms)
Jul 22 21:25:06.571: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 16.361617ms)
Jul 22 21:25:06.572: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 17.314013ms)
Jul 22 21:25:06.576: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 4.321878ms)
Jul 22 21:25:06.577: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 5.557472ms)
Jul 22 21:25:06.578: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 6.12217ms)
Jul 22 21:25:06.579: INFO: (15) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 6.680766ms)
Jul 22 21:25:06.579: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 6.915165ms)
Jul 22 21:25:06.581: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 8.762955ms)
Jul 22 21:25:06.582: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 10.504048ms)
Jul 22 21:25:06.583: INFO: (15) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 10.258648ms)
Jul 22 21:25:06.584: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 11.433842ms)
Jul 22 21:25:06.584: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.789541ms)
Jul 22 21:25:06.585: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.731441ms)
Jul 22 21:25:06.585: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 12.403138ms)
Jul 22 21:25:06.585: INFO: (15) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.397037ms)
Jul 22 21:25:06.585: INFO: (15) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 12.629237ms)
Jul 22 21:25:06.586: INFO: (15) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 12.830836ms)
Jul 22 21:25:06.586: INFO: (15) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 13.727631ms)
Jul 22 21:25:06.596: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 9.389853ms)
Jul 22 21:25:06.598: INFO: (16) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 11.655841ms)
Jul 22 21:25:06.598: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 11.168344ms)
Jul 22 21:25:06.599: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 11.458743ms)
Jul 22 21:25:06.599: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 11.454642ms)
Jul 22 21:25:06.600: INFO: (16) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.801235ms)
Jul 22 21:25:06.600: INFO: (16) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 13.622832ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 13.382132ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 13.039735ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 13.160733ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 13.378032ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 13.722131ms)
Jul 22 21:25:06.601: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 13.99253ms)
Jul 22 21:25:06.602: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 13.601531ms)
Jul 22 21:25:06.602: INFO: (16) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 14.192729ms)
Jul 22 21:25:06.602: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 14.590627ms)
Jul 22 21:25:06.608: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 6.11447ms)
Jul 22 21:25:06.609: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 7.352963ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 7.246764ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 7.385163ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 7.519662ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 7.519862ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 7.315063ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 7.390863ms)
Jul 22 21:25:06.610: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 7.80736ms)
Jul 22 21:25:06.612: INFO: (17) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 10.098149ms)
Jul 22 21:25:06.613: INFO: (17) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 10.494447ms)
Jul 22 21:25:06.613: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.142744ms)
Jul 22 21:25:06.614: INFO: (17) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 11.145444ms)
Jul 22 21:25:06.614: INFO: (17) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 11.84114ms)
Jul 22 21:25:06.614: INFO: (17) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 11.98814ms)
Jul 22 21:25:06.615: INFO: (17) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 12.431838ms)
Jul 22 21:25:06.621: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 6.448768ms)
Jul 22 21:25:06.627: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 11.243043ms)
Jul 22 21:25:06.627: INFO: (18) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 11.575442ms)
Jul 22 21:25:06.628: INFO: (18) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 12.337338ms)
Jul 22 21:25:06.628: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 13.002834ms)
Jul 22 21:25:06.629: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 13.297833ms)
Jul 22 21:25:06.630: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 14.665826ms)
Jul 22 21:25:06.630: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 14.805926ms)
Jul 22 21:25:06.630: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 14.383728ms)
Jul 22 21:25:06.630: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 14.194829ms)
Jul 22 21:25:06.630: INFO: (18) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 14.459228ms)
Jul 22 21:25:06.631: INFO: (18) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 15.762921ms)
Jul 22 21:25:06.631: INFO: (18) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 14.992924ms)
Jul 22 21:25:06.631: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 15.404722ms)
Jul 22 21:25:06.631: INFO: (18) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 15.241423ms)
Jul 22 21:25:06.631: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 15.550922ms)
Jul 22 21:25:06.639: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 8.079159ms)
Jul 22 21:25:06.639: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:462/proxy/: tls qux (200; 8.07946ms)
Jul 22 21:25:06.646: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">test<... (200; 14.781325ms)
Jul 22 21:25:06.647: INFO: (19) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname2/proxy/: tls qux (200; 15.144523ms)
Jul 22 21:25:06.647: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g/proxy/rewriteme">test</a> (200; 15.037024ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:1080/proxy/rewriteme">... (200; 16.091219ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname2/proxy/: bar (200; 16.398617ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname1/proxy/: foo (200; 16.714216ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/services/http:proxy-service-56vgq:portname1/proxy/: foo (200; 16.773916ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 16.824215ms)
Jul 22 21:25:06.648: INFO: (19) /api/v1/namespaces/proxy-9325/services/https:proxy-service-56vgq:tlsportname1/proxy/: tls baz (200; 16.855616ms)
Jul 22 21:25:06.649: INFO: (19) /api/v1/namespaces/proxy-9325/services/proxy-service-56vgq:portname2/proxy/: bar (200; 17.153914ms)
Jul 22 21:25:06.649: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-56vgq-nnq6g:162/proxy/: bar (200; 17.047314ms)
Jul 22 21:25:06.649: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:443/proxy/tlsrewritem... (200; 17.623911ms)
Jul 22 21:25:06.649: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-56vgq-nnq6g:160/proxy/: foo (200; 17.391912ms)
Jul 22 21:25:06.649: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-56vgq-nnq6g:460/proxy/: tls baz (200; 17.510112ms)
STEP: deleting ReplicationController proxy-service-56vgq in namespace proxy-9325, will wait for the garbage collector to delete the pods
Jul 22 21:25:06.708: INFO: Deleting ReplicationController proxy-service-56vgq took: 5.717071ms
Jul 22 21:25:07.008: INFO: Terminating ReplicationController proxy-service-56vgq pods took: 300.15439ms
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:25:17.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9325" for this suite.
Jul 22 21:25:23.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:25:23.997: INFO: namespace proxy-9325 deletion completed in 6.082605487s

• [SLOW TEST:25.945 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:25:23.998: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:25:50.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3680" for this suite.
Jul 22 21:25:56.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:25:56.475: INFO: namespace container-runtime-3680 deletion completed in 6.093668927s

• [SLOW TEST:32.478 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:25:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 22 21:25:56.631: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:26:07.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2917" for this suite.
Jul 22 21:26:13.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:26:13.974: INFO: namespace pods-2917 deletion completed in 6.085910264s

• [SLOW TEST:17.497 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:26:13.977: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:26:14.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475" in namespace "projected-8468" to be "success or failure"
Jul 22 21:26:14.131: INFO: Pod "downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607887ms
Jul 22 21:26:16.134: INFO: Pod "downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005868202s
Jul 22 21:26:18.138: INFO: Pod "downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009427016s
STEP: Saw pod success
Jul 22 21:26:18.138: INFO: Pod "downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475" satisfied condition "success or failure"
Jul 22 21:26:18.140: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475 container client-container: <nil>
STEP: delete the pod
Jul 22 21:26:18.158: INFO: Waiting for pod downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475 to disappear
Jul 22 21:26:18.160: INFO: Pod downwardapi-volume-c130a6c0-9e5a-4f45-8726-519d1e657475 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:26:18.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8468" for this suite.
Jul 22 21:26:24.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:26:24.250: INFO: namespace projected-8468 deletion completed in 6.086953558s

• [SLOW TEST:10.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:26:24.252: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1509.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1509.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1509.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1509.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1509.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1509.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1509.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1509.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1509.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1509.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:26:28.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.443: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.446: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.469: INFO: Unable to read jessie_udp@dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.471: INFO: Unable to read jessie_tcp@dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.474: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.476: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local from pod dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c: the server could not find the requested resource (get pods dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c)
Jul 22 21:26:28.494: INFO: Lookups using dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c failed for: [wheezy_udp@dns-test-service.dns-1509.svc.cluster.local wheezy_tcp@dns-test-service.dns-1509.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local jessie_udp@dns-test-service.dns-1509.svc.cluster.local jessie_tcp@dns-test-service.dns-1509.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1509.svc.cluster.local]

Jul 22 21:26:33.549: INFO: DNS probes using dns-1509/dns-test-54cb0ec5-2a5b-4f25-a9ee-d5b003a8ef6c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:26:33.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1509" for this suite.
Jul 22 21:26:39.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:26:39.718: INFO: namespace dns-1509 deletion completed in 6.099748591s

• [SLOW TEST:15.466 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:26:39.718: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 22 21:26:39.869: INFO: Waiting up to 5m0s for pod "downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92" in namespace "downward-api-7217" to be "success or failure"
Jul 22 21:26:39.877: INFO: Pod "downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.446757ms
Jul 22 21:26:41.881: INFO: Pod "downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012622667s
Jul 22 21:26:43.884: INFO: Pod "downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015658583s
STEP: Saw pod success
Jul 22 21:26:43.885: INFO: Pod "downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92" satisfied condition "success or failure"
Jul 22 21:26:43.887: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:26:43.905: INFO: Waiting for pod downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92 to disappear
Jul 22 21:26:43.908: INFO: Pod downward-api-2c2a4b27-db60-4d3b-bee7-20b225399c92 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:26:43.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7217" for this suite.
Jul 22 21:26:49.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:26:50.002: INFO: namespace downward-api-7217 deletion completed in 6.090754035s

• [SLOW TEST:10.284 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:26:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 21:26:58.189: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:26:58.193: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:00.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:00.337: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:02.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:02.196: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:04.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:04.197: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:06.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:06.196: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:08.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:08.197: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:10.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:10.196: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:12.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:12.197: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:14.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:14.197: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:16.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:16.196: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:27:18.193: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:27:18.196: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:27:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-491" for this suite.
Jul 22 21:27:40.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:27:40.286: INFO: namespace container-lifecycle-hook-491 deletion completed in 22.086776183s

• [SLOW TEST:50.284 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:27:40.289: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-ndtg
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:27:40.442: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ndtg" in namespace "subpath-4109" to be "success or failure"
Jul 22 21:27:40.448: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.83487ms
Jul 22 21:27:42.451: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008987283s
Jul 22 21:27:44.454: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 4.011889097s
Jul 22 21:27:46.458: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 6.015197909s
Jul 22 21:27:48.461: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 8.018124823s
Jul 22 21:27:50.464: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 10.021225135s
Jul 22 21:27:52.467: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 12.024452448s
Jul 22 21:27:54.470: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 14.027364361s
Jul 22 21:27:56.473: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 16.030619973s
Jul 22 21:27:58.476: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 18.033567286s
Jul 22 21:28:00.479: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 20.036985297s
Jul 22 21:28:02.482: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Running", Reason="", readiness=true. Elapsed: 22.039823111s
Jul 22 21:28:04.485: INFO: Pod "pod-subpath-test-projected-ndtg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042887323s
STEP: Saw pod success
Jul 22 21:28:04.485: INFO: Pod "pod-subpath-test-projected-ndtg" satisfied condition "success or failure"
Jul 22 21:28:04.488: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-subpath-test-projected-ndtg container test-container-subpath-projected-ndtg: <nil>
STEP: delete the pod
Jul 22 21:28:04.505: INFO: Waiting for pod pod-subpath-test-projected-ndtg to disappear
Jul 22 21:28:04.507: INFO: Pod pod-subpath-test-projected-ndtg no longer exists
STEP: Deleting pod pod-subpath-test-projected-ndtg
Jul 22 21:28:04.508: INFO: Deleting pod "pod-subpath-test-projected-ndtg" in namespace "subpath-4109"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:28:04.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4109" for this suite.
Jul 22 21:28:10.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:28:10.608: INFO: namespace subpath-4109 deletion completed in 6.094893905s

• [SLOW TEST:30.320 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:28:10.609: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:28:10.766: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114" in namespace "projected-3209" to be "success or failure"
Jul 22 21:28:10.769: INFO: Pod "downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700586ms
Jul 22 21:28:12.772: INFO: Pod "downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005705099s
Jul 22 21:28:14.775: INFO: Pod "downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009401308s
STEP: Saw pod success
Jul 22 21:28:14.776: INFO: Pod "downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114" satisfied condition "success or failure"
Jul 22 21:28:14.778: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114 container client-container: <nil>
STEP: delete the pod
Jul 22 21:28:14.794: INFO: Waiting for pod downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114 to disappear
Jul 22 21:28:14.797: INFO: Pod downwardapi-volume-c1176bb2-77cd-48b3-9cb4-14d4d7ddc114 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:28:14.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3209" for this suite.
Jul 22 21:28:20.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:28:20.911: INFO: namespace projected-3209 deletion completed in 6.110932223s

• [SLOW TEST:10.302 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:28:20.913: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 21:28:21.082: INFO: Waiting up to 5m0s for pod "pod-a7e4ac5f-43ed-434c-9c62-2301379c3921" in namespace "emptydir-6063" to be "success or failure"
Jul 22 21:28:21.089: INFO: Pod "pod-a7e4ac5f-43ed-434c-9c62-2301379c3921": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058469ms
Jul 22 21:28:23.092: INFO: Pod "pod-a7e4ac5f-43ed-434c-9c62-2301379c3921": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00945658s
Jul 22 21:28:25.095: INFO: Pod "pod-a7e4ac5f-43ed-434c-9c62-2301379c3921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01277419s
STEP: Saw pod success
Jul 22 21:28:25.095: INFO: Pod "pod-a7e4ac5f-43ed-434c-9c62-2301379c3921" satisfied condition "success or failure"
Jul 22 21:28:25.098: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-a7e4ac5f-43ed-434c-9c62-2301379c3921 container test-container: <nil>
STEP: delete the pod
Jul 22 21:28:25.116: INFO: Waiting for pod pod-a7e4ac5f-43ed-434c-9c62-2301379c3921 to disappear
Jul 22 21:28:25.119: INFO: Pod pod-a7e4ac5f-43ed-434c-9c62-2301379c3921 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:28:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6063" for this suite.
Jul 22 21:28:31.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:28:31.211: INFO: namespace emptydir-6063 deletion completed in 6.089312932s

• [SLOW TEST:10.298 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:28:31.212: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1994
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:28:31.355: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 21:28:53.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.168:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1994 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:28:53.425: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:28:53.569: INFO: Found all expected endpoints: [netserver-0]
Jul 22 21:28:53.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1994 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:28:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:28:53.707: INFO: Found all expected endpoints: [netserver-1]
Jul 22 21:28:53.709: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.19:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1994 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:28:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:28:53.846: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:28:53.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1994" for this suite.
Jul 22 21:29:15.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:29:15.949: INFO: namespace pod-network-test-1994 deletion completed in 22.099661285s

• [SLOW TEST:44.738 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:29:15.953: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-6f992570-89e6-40ca-ad44-98462efb0693
STEP: Creating a pod to test consume secrets
Jul 22 21:29:16.136: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285" in namespace "projected-6429" to be "success or failure"
Jul 22 21:29:16.139: INFO: Pod "pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622486ms
Jul 22 21:29:18.142: INFO: Pod "pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005599797s
Jul 22 21:29:20.145: INFO: Pod "pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008723907s
STEP: Saw pod success
Jul 22 21:29:20.145: INFO: Pod "pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285" satisfied condition "success or failure"
Jul 22 21:29:20.147: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:29:20.259: INFO: Waiting for pod pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285 to disappear
Jul 22 21:29:20.261: INFO: Pod pod-projected-secrets-ac40a4d3-2ecc-4f09-9f21-afa096208285 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:29:20.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6429" for this suite.
Jul 22 21:29:26.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:29:26.356: INFO: namespace projected-6429 deletion completed in 6.092097512s

• [SLOW TEST:10.404 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:29:26.357: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:29:26.516: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 22 21:29:28.541: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:29:28.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4566" for this suite.
Jul 22 21:29:34.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:29:34.644: INFO: namespace replication-controller-4566 deletion completed in 6.09641919s

• [SLOW TEST:8.288 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:29:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 21:29:34.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-410'
Jul 22 21:29:35.267: INFO: stderr: ""
Jul 22 21:29:35.267: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 22 21:29:35.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete pods e2e-test-nginx-pod --namespace=kubectl-410'
Jul 22 21:29:37.884: INFO: stderr: ""
Jul 22 21:29:37.884: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:29:37.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-410" for this suite.
Jul 22 21:29:43.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:29:44.013: INFO: namespace kubectl-410 deletion completed in 6.125262044s

• [SLOW TEST:9.368 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:29:44.015: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 22 21:29:44.193: INFO: Waiting up to 5m0s for pod "downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b" in namespace "downward-api-6422" to be "success or failure"
Jul 22 21:29:44.196: INFO: Pod "downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.716686ms
Jul 22 21:29:46.199: INFO: Pod "downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005679396s
Jul 22 21:29:48.201: INFO: Pod "downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008534707s
STEP: Saw pod success
Jul 22 21:29:48.202: INFO: Pod "downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b" satisfied condition "success or failure"
Jul 22 21:29:48.204: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:29:48.231: INFO: Waiting for pod downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b to disappear
Jul 22 21:29:48.235: INFO: Pod downward-api-9fcc84e8-d9ad-47ea-8cf3-5927aa44632b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:29:48.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6422" for this suite.
Jul 22 21:29:54.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:29:54.334: INFO: namespace downward-api-6422 deletion completed in 6.095095596s

• [SLOW TEST:10.319 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:29:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:29:54.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-440" for this suite.
Jul 22 21:30:00.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:30:00.563: INFO: namespace services-440 deletion completed in 6.078876876s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.227 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:30:00.564: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7795
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:30:00.705: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 21:30:24.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.174:8080/dial?request=hostName&protocol=http&host=10.244.2.13&port=8080&tries=1'] Namespace:pod-network-test-7795 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:30:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:30:25.006: INFO: Waiting for endpoints: map[]
Jul 22 21:30:25.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.174:8080/dial?request=hostName&protocol=http&host=10.244.3.173&port=8080&tries=1'] Namespace:pod-network-test-7795 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:30:25.009: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:30:25.145: INFO: Waiting for endpoints: map[]
Jul 22 21:30:25.148: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.174:8080/dial?request=hostName&protocol=http&host=10.244.1.21&port=8080&tries=1'] Namespace:pod-network-test-7795 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 21:30:25.148: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
Jul 22 21:30:25.287: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:30:25.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7795" for this suite.
Jul 22 21:30:47.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:30:47.375: INFO: namespace pod-network-test-7795 deletion completed in 22.084720433s

• [SLOW TEST:46.812 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:30:47.376: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 22 21:30:48.039: INFO: created pod pod-service-account-defaultsa
Jul 22 21:30:48.039: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 22 21:30:48.050: INFO: created pod pod-service-account-mountsa
Jul 22 21:30:48.050: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 22 21:30:48.059: INFO: created pod pod-service-account-nomountsa
Jul 22 21:30:48.059: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 22 21:30:48.065: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 22 21:30:48.065: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 22 21:30:48.085: INFO: created pod pod-service-account-mountsa-mountspec
Jul 22 21:30:48.085: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 22 21:30:48.095: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 22 21:30:48.095: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 22 21:30:48.103: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 22 21:30:48.103: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 22 21:30:48.111: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 22 21:30:48.111: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 22 21:30:48.120: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 22 21:30:48.121: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:30:48.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6378" for this suite.
Jul 22 21:31:00.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:31:00.226: INFO: namespace svcaccounts-6378 deletion completed in 12.090620583s

• [SLOW TEST:12.850 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:31:00.228: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:31:00.377: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 22 21:31:05.381: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 21:31:05.381: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 22 21:31:09.403: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-4890,SelfLink:/apis/apps/v1/namespaces/deployment-4890/deployments/test-cleanup-deployment,UID:c638e182-ecd1-4d09-a72f-2b40ead87b70,ResourceVersion:22049,Generation:1,CreationTimestamp:2019-07-22 21:31:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-22 21:31:05 +0000 UTC 2019-07-22 21:31:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-22 21:31:08 +0000 UTC 2019-07-22 21:31:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 21:31:09.406: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-4890,SelfLink:/apis/apps/v1/namespaces/deployment-4890/replicasets/test-cleanup-deployment-55bbcbc84c,UID:8dfdb810-f439-4421-abf6-9fe1a782d864,ResourceVersion:22038,Generation:1,CreationTimestamp:2019-07-22 21:31:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c638e182-ecd1-4d09-a72f-2b40ead87b70 0xc00318b857 0xc00318b858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 21:31:09.410: INFO: Pod "test-cleanup-deployment-55bbcbc84c-hw58r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-hw58r,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-4890,SelfLink:/api/v1/namespaces/deployment-4890/pods/test-cleanup-deployment-55bbcbc84c-hw58r,UID:7e443dd8-ffe7-4337-82bc-550a8495647e,ResourceVersion:22037,Generation:0,CreationTimestamp:2019-07-22 21:31:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 8dfdb810-f439-4421-abf6-9fe1a782d864 0xc00318bfa7 0xc00318bfa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rh7nk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rh7nk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rh7nk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002764010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002764030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:31:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:31:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:31:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:31:05 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.181,StartTime:2019-07-22 21:31:05 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-22 21:31:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ad0a2a4bf566d1af7fd40862bdca6d19b570fa2f3415403281359be48a59926f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:31:09.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4890" for this suite.
Jul 22 21:31:15.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:31:15.498: INFO: namespace deployment-4890 deletion completed in 6.085438038s

• [SLOW TEST:15.271 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:31:15.503: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-11670797-367c-4a2e-a346-004985595009
STEP: Creating a pod to test consume secrets
Jul 22 21:31:15.656: INFO: Waiting up to 5m0s for pod "pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c" in namespace "secrets-4593" to be "success or failure"
Jul 22 21:31:15.659: INFO: Pod "pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.639087ms
Jul 22 21:31:17.662: INFO: Pod "pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006160092s
Jul 22 21:31:19.665: INFO: Pod "pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0090783s
STEP: Saw pod success
Jul 22 21:31:19.665: INFO: Pod "pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c" satisfied condition "success or failure"
Jul 22 21:31:19.667: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:31:19.684: INFO: Waiting for pod pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c to disappear
Jul 22 21:31:19.686: INFO: Pod pod-secrets-2179f4e0-72fd-4167-90e2-72749c489d0c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:31:19.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4593" for this suite.
Jul 22 21:31:25.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:31:25.781: INFO: namespace secrets-4593 deletion completed in 6.091134509s

• [SLOW TEST:10.278 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:31:25.783: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 21:31:25.936: INFO: Waiting up to 5m0s for pod "pod-c2b64656-cbca-4832-9206-c348ccba0966" in namespace "emptydir-5084" to be "success or failure"
Jul 22 21:31:25.938: INFO: Pod "pod-c2b64656-cbca-4832-9206-c348ccba0966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168889ms
Jul 22 21:31:27.941: INFO: Pod "pod-c2b64656-cbca-4832-9206-c348ccba0966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005530295s
Jul 22 21:31:29.946: INFO: Pod "pod-c2b64656-cbca-4832-9206-c348ccba0966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010458193s
STEP: Saw pod success
Jul 22 21:31:29.946: INFO: Pod "pod-c2b64656-cbca-4832-9206-c348ccba0966" satisfied condition "success or failure"
Jul 22 21:31:29.948: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-c2b64656-cbca-4832-9206-c348ccba0966 container test-container: <nil>
STEP: delete the pod
Jul 22 21:31:29.961: INFO: Waiting for pod pod-c2b64656-cbca-4832-9206-c348ccba0966 to disappear
Jul 22 21:31:29.963: INFO: Pod pod-c2b64656-cbca-4832-9206-c348ccba0966 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:31:29.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5084" for this suite.
Jul 22 21:31:36.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:31:36.112: INFO: namespace emptydir-5084 deletion completed in 6.144768438s

• [SLOW TEST:10.329 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:31:36.112: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 22 21:31:36.260: INFO: Waiting up to 5m0s for pod "var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa" in namespace "var-expansion-4072" to be "success or failure"
Jul 22 21:31:36.268: INFO: Pod "var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.831661ms
Jul 22 21:31:38.271: INFO: Pod "var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010600669s
Jul 22 21:31:40.274: INFO: Pod "var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013972074s
STEP: Saw pod success
Jul 22 21:31:40.274: INFO: Pod "var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa" satisfied condition "success or failure"
Jul 22 21:31:40.277: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:31:40.293: INFO: Waiting for pod var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa to disappear
Jul 22 21:31:40.296: INFO: Pod var-expansion-92bbab49-34fa-493e-8aa2-11f13b75c7fa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:31:40.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4072" for this suite.
Jul 22 21:31:46.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:31:46.387: INFO: namespace var-expansion-4072 deletion completed in 6.087977024s

• [SLOW TEST:10.274 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:31:46.388: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 21:31:54.617: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:31:54.619: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:31:56.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:31:56.622: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:31:58.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:31:58.623: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:32:00.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:32:00.623: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:32:02.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:32:02.623: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:32:04.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:32:04.623: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:32:06.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:32:06.623: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:32:08.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:32:08.622: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:32:08.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9303" for this suite.
Jul 22 21:32:30.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:32:30.743: INFO: namespace container-lifecycle-hook-9303 deletion completed in 22.101748822s

• [SLOW TEST:44.355 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:32:30.743: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:32:35.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4453" for this suite.
Jul 22 21:32:57.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:32:58.034: INFO: namespace replication-controller-4453 deletion completed in 22.120678322s

• [SLOW TEST:27.291 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:32:58.037: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e9e831be-76e1-498b-ae0b-3c084b5c6c7b
STEP: Creating a pod to test consume secrets
Jul 22 21:32:58.196: INFO: Waiting up to 5m0s for pod "pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291" in namespace "secrets-2480" to be "success or failure"
Jul 22 21:32:58.198: INFO: Pod "pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308889ms
Jul 22 21:33:00.202: INFO: Pod "pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040791s
Jul 22 21:33:02.210: INFO: Pod "pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01424147s
STEP: Saw pod success
Jul 22 21:33:02.210: INFO: Pod "pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291" satisfied condition "success or failure"
Jul 22 21:33:02.214: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291 container secret-env-test: <nil>
STEP: delete the pod
Jul 22 21:33:02.229: INFO: Waiting for pod pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291 to disappear
Jul 22 21:33:02.231: INFO: Pod pod-secrets-81581a34-5a5b-4d49-a5bb-62d324d01291 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:33:02.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2480" for this suite.
Jul 22 21:33:08.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:33:08.323: INFO: namespace secrets-2480 deletion completed in 6.089331011s

• [SLOW TEST:10.286 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:33:08.323: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:33:08.471: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:33:12.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5742" for this suite.
Jul 22 21:33:50.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:33:50.606: INFO: namespace pods-5742 deletion completed in 38.081590771s

• [SLOW TEST:42.283 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:33:50.606: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1269
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1269
STEP: Deleting pre-stop pod
Jul 22 21:34:05.793: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:34:05.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1269" for this suite.
Jul 22 21:34:43.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:34:43.884: INFO: namespace prestop-1269 deletion completed in 38.084523839s

• [SLOW TEST:53.279 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:34:43.885: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-1786
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1786 to expose endpoints map[]
Jul 22 21:34:44.046: INFO: successfully validated that service endpoint-test2 in namespace services-1786 exposes endpoints map[] (8.472057ms elapsed)
STEP: Creating pod pod1 in namespace services-1786
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1786 to expose endpoints map[pod1:[80]]
Jul 22 21:34:48.093: INFO: successfully validated that service endpoint-test2 in namespace services-1786 exposes endpoints map[pod1:[80]] (4.033115871s elapsed)
STEP: Creating pod pod2 in namespace services-1786
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1786 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 22 21:34:51.133: INFO: successfully validated that service endpoint-test2 in namespace services-1786 exposes endpoints map[pod1:[80] pod2:[80]] (3.035971197s elapsed)
STEP: Deleting pod pod1 in namespace services-1786
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1786 to expose endpoints map[pod2:[80]]
Jul 22 21:34:52.154: INFO: successfully validated that service endpoint-test2 in namespace services-1786 exposes endpoints map[pod2:[80]] (1.017426972s elapsed)
STEP: Deleting pod pod2 in namespace services-1786
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1786 to expose endpoints map[]
Jul 22 21:34:52.172: INFO: successfully validated that service endpoint-test2 in namespace services-1786 exposes endpoints map[] (13.007434ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:34:52.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1786" for this suite.
Jul 22 21:35:14.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:35:14.323: INFO: namespace services-1786 deletion completed in 22.109682552s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.438 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:35:14.327: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 22 21:35:19.012: INFO: Successfully updated pod "annotationupdatef1e21ae2-46a6-4a15-935b-dfcd910d6dd8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:35:21.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6257" for this suite.
Jul 22 21:35:43.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:35:43.119: INFO: namespace downward-api-6257 deletion completed in 22.086663964s

• [SLOW TEST:28.792 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:35:43.120: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:35:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:35:47.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5700" for this suite.
Jul 22 21:36:33.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:36:33.610: INFO: namespace pods-5700 deletion completed in 46.096936719s

• [SLOW TEST:50.490 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:36:33.612: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:36:33.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273" in namespace "downward-api-9008" to be "success or failure"
Jul 22 21:36:33.768: INFO: Pod "downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197379ms
Jul 22 21:36:35.771: INFO: Pod "downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00750848s
Jul 22 21:36:37.775: INFO: Pod "downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01085958s
STEP: Saw pod success
Jul 22 21:36:37.775: INFO: Pod "downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273" satisfied condition "success or failure"
Jul 22 21:36:37.777: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273 container client-container: <nil>
STEP: delete the pod
Jul 22 21:36:37.795: INFO: Waiting for pod downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273 to disappear
Jul 22 21:36:37.797: INFO: Pod downwardapi-volume-6653ca2e-bfe2-4441-ad37-2ad79fe1f273 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:36:37.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9008" for this suite.
Jul 22 21:36:43.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:36:43.887: INFO: namespace downward-api-9008 deletion completed in 6.085035123s

• [SLOW TEST:10.275 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:36:43.894: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:36:44.049: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 21:36:44.068: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:44.075: INFO: Number of nodes with available pods: 0
Jul 22 21:36:44.075: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:36:45.079: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:45.082: INFO: Number of nodes with available pods: 0
Jul 22 21:36:45.082: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:36:46.078: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:46.080: INFO: Number of nodes with available pods: 0
Jul 22 21:36:46.080: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:36:47.079: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:47.082: INFO: Number of nodes with available pods: 3
Jul 22 21:36:47.082: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 22 21:36:47.116: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:47.116: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:47.116: INFO: Wrong image for pod: daemon-set-z7zqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:47.121: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:48.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:48.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:48.125: INFO: Wrong image for pod: daemon-set-z7zqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:48.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:49.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:49.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:49.125: INFO: Wrong image for pod: daemon-set-z7zqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:49.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:50.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:50.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:50.125: INFO: Wrong image for pod: daemon-set-z7zqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:50.125: INFO: Pod daemon-set-z7zqr is not available
Jul 22 21:36:50.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:51.124: INFO: Pod daemon-set-bhrhn is not available
Jul 22 21:36:51.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:51.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:51.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:52.125: INFO: Pod daemon-set-bhrhn is not available
Jul 22 21:36:52.126: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:52.126: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:52.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:53.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:53.126: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:53.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:54.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:54.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:54.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:54.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:55.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:55.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:55.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:55.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:56.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:56.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:56.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:56.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:57.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:57.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:57.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:57.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:58.126: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:58.126: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:58.126: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:58.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:36:59.142: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:59.142: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:36:59.142: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:36:59.145: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:00.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:00.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:00.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:37:00.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:01.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:01.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:01.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:37:01.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:02.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:02.125: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:02.125: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:37:02.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:03.128: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:03.128: INFO: Wrong image for pod: daemon-set-tbzjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:03.128: INFO: Pod daemon-set-tbzjm is not available
Jul 22 21:37:03.134: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:04.125: INFO: Pod daemon-set-cswsn is not available
Jul 22 21:37:04.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:04.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:05.124: INFO: Pod daemon-set-cswsn is not available
Jul 22 21:37:05.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:05.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:06.125: INFO: Pod daemon-set-cswsn is not available
Jul 22 21:37:06.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:06.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:07.125: INFO: Pod daemon-set-cswsn is not available
Jul 22 21:37:07.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:07.130: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:08.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:08.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:09.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:09.125: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:09.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:10.133: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:10.134: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:10.136: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:11.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:11.126: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:11.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:12.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:12.125: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:12.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:13.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:13.125: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:13.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:14.125: INFO: Wrong image for pod: daemon-set-h8vsw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 21:37:14.125: INFO: Pod daemon-set-h8vsw is not available
Jul 22 21:37:14.128: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:15.125: INFO: Pod daemon-set-c6xxn is not available
Jul 22 21:37:15.129: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 22 21:37:15.133: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:15.135: INFO: Number of nodes with available pods: 2
Jul 22 21:37:15.136: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:37:16.140: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:16.143: INFO: Number of nodes with available pods: 2
Jul 22 21:37:16.143: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:37:17.139: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:17.144: INFO: Number of nodes with available pods: 2
Jul 22 21:37:17.144: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:37:18.140: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:18.143: INFO: Number of nodes with available pods: 2
Jul 22 21:37:18.143: INFO: Node k8s-linuxpool-16111918-1 is running more than one daemon pod
Jul 22 21:37:19.139: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:37:19.142: INFO: Number of nodes with available pods: 3
Jul 22 21:37:19.142: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6700, will wait for the garbage collector to delete the pods
Jul 22 21:37:19.211: INFO: Deleting DaemonSet.extensions daemon-set took: 6.03967ms
Jul 22 21:37:19.511: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.329486ms
Jul 22 21:37:27.914: INFO: Number of nodes with available pods: 0
Jul 22 21:37:27.914: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:37:27.915: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6700/daemonsets","resourceVersion":"23175"},"items":null}

Jul 22 21:37:27.918: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6700/pods","resourceVersion":"23175"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:37:27.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6700" for this suite.
Jul 22 21:37:33.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:37:34.011: INFO: namespace daemonsets-6700 deletion completed in 6.081589439s

• [SLOW TEST:50.117 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:37:34.011: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 22 21:37:38.181: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-425149679 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 22 21:37:48.264: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:37:48.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9794" for this suite.
Jul 22 21:37:54.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:37:54.361: INFO: namespace pods-9794 deletion completed in 6.093055181s

• [SLOW TEST:20.351 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:37:54.364: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5474/configmap-test-566bb1a0-6e15-42bd-930b-46f8c09eb9c9
STEP: Creating a pod to test consume configMaps
Jul 22 21:37:54.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f" in namespace "configmap-5474" to be "success or failure"
Jul 22 21:37:54.527: INFO: Pod "pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210878ms
Jul 22 21:37:56.533: INFO: Pod "pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010737462s
Jul 22 21:37:58.537: INFO: Pod "pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01439586s
STEP: Saw pod success
Jul 22 21:37:58.537: INFO: Pod "pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f" satisfied condition "success or failure"
Jul 22 21:37:58.539: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f container env-test: <nil>
STEP: delete the pod
Jul 22 21:37:58.564: INFO: Waiting for pod pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f to disappear
Jul 22 21:37:58.566: INFO: Pod pod-configmaps-e681c5a9-82a7-46df-814b-0405ed65bf1f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:37:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5474" for this suite.
Jul 22 21:38:04.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:38:04.663: INFO: namespace configmap-5474 deletion completed in 6.093516178s

• [SLOW TEST:10.300 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:38:04.666: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5954/configmap-test-fcb296d2-207f-4375-9e57-8c2533a1c57e
STEP: Creating a pod to test consume configMaps
Jul 22 21:38:04.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6" in namespace "configmap-5954" to be "success or failure"
Jul 22 21:38:04.830: INFO: Pod "pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.738166ms
Jul 22 21:38:06.833: INFO: Pod "pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010386764s
Jul 22 21:38:08.836: INFO: Pod "pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013334866s
STEP: Saw pod success
Jul 22 21:38:08.837: INFO: Pod "pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6" satisfied condition "success or failure"
Jul 22 21:38:08.839: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6 container env-test: <nil>
STEP: delete the pod
Jul 22 21:38:08.852: INFO: Waiting for pod pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6 to disappear
Jul 22 21:38:08.855: INFO: Pod pod-configmaps-79ac6569-2502-45c0-9d8b-3c9761f189c6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:38:08.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5954" for this suite.
Jul 22 21:38:14.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:38:14.945: INFO: namespace configmap-5954 deletion completed in 6.08721701s

• [SLOW TEST:10.280 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:38:14.948: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-725
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-8c1673b2-bb24-4f8b-b574-9f868389c93d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:38:19.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-725" for this suite.
Jul 22 21:38:41.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:38:41.231: INFO: namespace configmap-725 deletion completed in 22.099869175s

• [SLOW TEST:26.283 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:38:41.232: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:38:41.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36" in namespace "projected-9016" to be "success or failure"
Jul 22 21:38:41.382: INFO: Pod "downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.766686ms
Jul 22 21:38:43.385: INFO: Pod "downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005889187s
Jul 22 21:38:45.389: INFO: Pod "downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009337585s
STEP: Saw pod success
Jul 22 21:38:45.389: INFO: Pod "downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36" satisfied condition "success or failure"
Jul 22 21:38:45.391: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36 container client-container: <nil>
STEP: delete the pod
Jul 22 21:38:45.409: INFO: Waiting for pod downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36 to disappear
Jul 22 21:38:45.413: INFO: Pod downwardapi-volume-66905fbb-20e2-4646-ac19-e6cfc0f9da36 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:38:45.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9016" for this suite.
Jul 22 21:38:51.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:38:51.511: INFO: namespace projected-9016 deletion completed in 6.095629266s

• [SLOW TEST:10.279 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:38:51.514: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-da2bd198-cfcf-45f0-950a-e02f59db8afe
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:38:51.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9270" for this suite.
Jul 22 21:38:57.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:38:57.762: INFO: namespace secrets-9270 deletion completed in 6.096386162s

• [SLOW TEST:6.249 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:38:57.768: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 21:38:57.921: INFO: Waiting up to 5m0s for pod "pod-8ede8a9d-f931-46dd-83d1-d84cf5340792" in namespace "emptydir-7124" to be "success or failure"
Jul 22 21:38:57.923: INFO: Pod "pod-8ede8a9d-f931-46dd-83d1-d84cf5340792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261689ms
Jul 22 21:38:59.926: INFO: Pod "pod-8ede8a9d-f931-46dd-83d1-d84cf5340792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005412489s
Jul 22 21:39:01.929: INFO: Pod "pod-8ede8a9d-f931-46dd-83d1-d84cf5340792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008551589s
STEP: Saw pod success
Jul 22 21:39:01.929: INFO: Pod "pod-8ede8a9d-f931-46dd-83d1-d84cf5340792" satisfied condition "success or failure"
Jul 22 21:39:01.932: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-8ede8a9d-f931-46dd-83d1-d84cf5340792 container test-container: <nil>
STEP: delete the pod
Jul 22 21:39:01.949: INFO: Waiting for pod pod-8ede8a9d-f931-46dd-83d1-d84cf5340792 to disappear
Jul 22 21:39:01.951: INFO: Pod pod-8ede8a9d-f931-46dd-83d1-d84cf5340792 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:39:01.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7124" for this suite.
Jul 22 21:39:07.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:39:08.044: INFO: namespace emptydir-7124 deletion completed in 6.090090894s

• [SLOW TEST:10.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:39:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-811fb42f-6e87-4e96-ac91-2c5023c23e05
STEP: Creating a pod to test consume secrets
Jul 22 21:39:08.205: INFO: Waiting up to 5m0s for pod "pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159" in namespace "secrets-286" to be "success or failure"
Jul 22 21:39:08.213: INFO: Pod "pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159": Phase="Pending", Reason="", readiness=false. Elapsed: 8.539157ms
Jul 22 21:39:10.217: INFO: Pod "pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011971655s
Jul 22 21:39:12.220: INFO: Pod "pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014943556s
STEP: Saw pod success
Jul 22 21:39:12.220: INFO: Pod "pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159" satisfied condition "success or failure"
Jul 22 21:39:12.223: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:39:12.239: INFO: Waiting for pod pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159 to disappear
Jul 22 21:39:12.241: INFO: Pod pod-secrets-56f7732e-b1f6-40fe-900c-8ecf88632159 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:39:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-286" for this suite.
Jul 22 21:39:18.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:39:18.340: INFO: namespace secrets-286 deletion completed in 6.095771464s

• [SLOW TEST:10.291 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:39:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8792
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 22 21:39:18.491: INFO: Waiting up to 5m0s for pod "pod-073f98c1-d414-4601-a7f0-7c161d78cce5" in namespace "emptydir-8792" to be "success or failure"
Jul 22 21:39:18.495: INFO: Pod "pod-073f98c1-d414-4601-a7f0-7c161d78cce5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691377ms
Jul 22 21:39:20.499: INFO: Pod "pod-073f98c1-d414-4601-a7f0-7c161d78cce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007789877s
Jul 22 21:39:22.505: INFO: Pod "pod-073f98c1-d414-4601-a7f0-7c161d78cce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014120061s
STEP: Saw pod success
Jul 22 21:39:22.510: INFO: Pod "pod-073f98c1-d414-4601-a7f0-7c161d78cce5" satisfied condition "success or failure"
Jul 22 21:39:22.521: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-073f98c1-d414-4601-a7f0-7c161d78cce5 container test-container: <nil>
STEP: delete the pod
Jul 22 21:39:22.536: INFO: Waiting for pod pod-073f98c1-d414-4601-a7f0-7c161d78cce5 to disappear
Jul 22 21:39:22.538: INFO: Pod pod-073f98c1-d414-4601-a7f0-7c161d78cce5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:39:22.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8792" for this suite.
Jul 22 21:39:28.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:39:28.619: INFO: namespace emptydir-8792 deletion completed in 6.078233853s

• [SLOW TEST:10.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:39:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 22 21:40:08.803: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 21:40:08.803257      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:40:08.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-877" for this suite.
Jul 22 21:40:14.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:40:14.916: INFO: namespace gc-877 deletion completed in 6.107023506s

• [SLOW TEST:46.296 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:40:14.916: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:40:15.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3624" for this suite.
Jul 22 21:40:37.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:40:37.192: INFO: namespace pods-3624 deletion completed in 22.111738304s

• [SLOW TEST:22.277 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:40:37.193: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 22 21:40:37.350: INFO: Waiting up to 5m0s for pod "var-expansion-961fe523-4857-4e9a-87df-be84918af5c6" in namespace "var-expansion-2452" to be "success or failure"
Jul 22 21:40:37.354: INFO: Pod "var-expansion-961fe523-4857-4e9a-87df-be84918af5c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926281ms
Jul 22 21:40:39.357: INFO: Pod "var-expansion-961fe523-4857-4e9a-87df-be84918af5c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006837581s
Jul 22 21:40:41.360: INFO: Pod "var-expansion-961fe523-4857-4e9a-87df-be84918af5c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009306984s
STEP: Saw pod success
Jul 22 21:40:41.360: INFO: Pod "var-expansion-961fe523-4857-4e9a-87df-be84918af5c6" satisfied condition "success or failure"
Jul 22 21:40:41.362: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod var-expansion-961fe523-4857-4e9a-87df-be84918af5c6 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:40:41.381: INFO: Waiting for pod var-expansion-961fe523-4857-4e9a-87df-be84918af5c6 to disappear
Jul 22 21:40:41.383: INFO: Pod var-expansion-961fe523-4857-4e9a-87df-be84918af5c6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:40:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2452" for this suite.
Jul 22 21:40:47.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:40:47.534: INFO: namespace var-expansion-2452 deletion completed in 6.147411902s

• [SLOW TEST:10.341 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:40:47.534: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:40:47.700: INFO: Create a RollingUpdate DaemonSet
Jul 22 21:40:47.704: INFO: Check that daemon pods launch on every node of the cluster
Jul 22 21:40:47.709: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:40:47.713: INFO: Number of nodes with available pods: 0
Jul 22 21:40:47.713: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:40:48.721: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:40:48.729: INFO: Number of nodes with available pods: 0
Jul 22 21:40:48.729: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:40:49.717: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:40:49.720: INFO: Number of nodes with available pods: 0
Jul 22 21:40:49.720: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:40:50.719: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:40:50.722: INFO: Number of nodes with available pods: 1
Jul 22 21:40:50.722: INFO: Node k8s-linuxpool-16111918-0 is running more than one daemon pod
Jul 22 21:40:51.717: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:40:51.720: INFO: Number of nodes with available pods: 3
Jul 22 21:40:51.720: INFO: Number of running nodes: 3, number of available pods: 3
Jul 22 21:40:51.720: INFO: Update the DaemonSet to trigger a rollout
Jul 22 21:40:51.726: INFO: Updating DaemonSet daemon-set
Jul 22 21:41:03.791: INFO: Roll back the DaemonSet before rollout is complete
Jul 22 21:41:03.796: INFO: Updating DaemonSet daemon-set
Jul 22 21:41:03.796: INFO: Make sure DaemonSet rollback is complete
Jul 22 21:41:03.799: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:03.799: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:03.808: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:04.811: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:04.811: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:04.814: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:05.812: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:05.812: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:05.815: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:06.812: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:06.812: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:06.815: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:07.811: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:07.811: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:07.815: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:08.875: INFO: Wrong image for pod: daemon-set-42lzf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 21:41:08.875: INFO: Pod daemon-set-42lzf is not available
Jul 22 21:41:08.878: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 22 21:41:09.812: INFO: Pod daemon-set-9w6vm is not available
Jul 22 21:41:09.815: INFO: DaemonSet pods can't tolerate node k8s-master-16111918-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7202, will wait for the garbage collector to delete the pods
Jul 22 21:41:09.878: INFO: Deleting DaemonSet.extensions daemon-set took: 4.819876ms
Jul 22 21:41:10.178: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.678583ms
Jul 22 21:42:29.785: INFO: Number of nodes with available pods: 0
Jul 22 21:42:29.785: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:42:29.787: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7202/daemonsets","resourceVersion":"24267"},"items":null}

Jul 22 21:42:29.790: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7202/pods","resourceVersion":"24267"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:42:29.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7202" for this suite.
Jul 22 21:42:35.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:42:35.889: INFO: namespace daemonsets-7202 deletion completed in 6.086095409s

• [SLOW TEST:108.354 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:42:35.891: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:42:40.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9011" for this suite.
Jul 22 21:43:30.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:43:30.206: INFO: namespace kubelet-test-9011 deletion completed in 50.089031106s

• [SLOW TEST:54.315 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:43:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 22 21:43:36.373: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:43:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0722 21:43:36.373101      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5999" for this suite.
Jul 22 21:43:42.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:43:42.476: INFO: namespace gc-5999 deletion completed in 6.09965424s

• [SLOW TEST:12.269 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:43:42.477: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-22f13a1c-7d4e-4613-af16-52b6b5554b77
STEP: Creating a pod to test consume secrets
Jul 22 21:43:42.627: INFO: Waiting up to 5m0s for pod "pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d" in namespace "secrets-6785" to be "success or failure"
Jul 22 21:43:42.632: INFO: Pod "pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842075ms
Jul 22 21:43:44.636: INFO: Pod "pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008497871s
Jul 22 21:43:46.639: INFO: Pod "pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01151977s
STEP: Saw pod success
Jul 22 21:43:46.639: INFO: Pod "pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d" satisfied condition "success or failure"
Jul 22 21:43:46.641: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:43:46.661: INFO: Waiting for pod pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d to disappear
Jul 22 21:43:46.663: INFO: Pod pod-secrets-3ab7c216-a58c-46c6-af82-fcda3199c44d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:43:46.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6785" for this suite.
Jul 22 21:43:52.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:43:52.757: INFO: namespace secrets-6785 deletion completed in 6.091062783s

• [SLOW TEST:10.280 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:43:52.762: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 22 21:43:52.922: INFO: Waiting up to 5m0s for pod "client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b" in namespace "containers-6848" to be "success or failure"
Jul 22 21:43:52.925: INFO: Pod "client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.032984ms
Jul 22 21:43:54.928: INFO: Pod "client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006084783s
Jul 22 21:43:56.931: INFO: Pod "client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008867883s
STEP: Saw pod success
Jul 22 21:43:56.931: INFO: Pod "client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b" satisfied condition "success or failure"
Jul 22 21:43:56.933: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b container test-container: <nil>
STEP: delete the pod
Jul 22 21:43:56.953: INFO: Waiting for pod client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b to disappear
Jul 22 21:43:56.958: INFO: Pod client-containers-88ed74ea-74fe-415d-872e-f5217c6d1c2b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:43:56.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6848" for this suite.
Jul 22 21:44:02.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:44:03.044: INFO: namespace containers-6848 deletion completed in 6.082758924s

• [SLOW TEST:10.282 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:44:03.046: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9239
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-9239
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9239
Jul 22 21:44:03.207: INFO: Found 0 stateful pods, waiting for 1
Jul 22 21:44:13.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 22 21:44:13.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 21:44:13.841: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 21:44:13.841: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 21:44:13.841: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 21:44:13.844: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 21:44:23.848: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:44:23.848: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:44:23.867: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jul 22 21:44:23.867: INFO: ss-0  k8s-linuxpool-16111918-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  }]
Jul 22 21:44:23.867: INFO: ss-1                            Pending         []
Jul 22 21:44:23.867: INFO: 
Jul 22 21:44:23.867: INFO: StatefulSet ss has not reached scale 3, at 2
Jul 22 21:44:24.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989672495s
Jul 22 21:44:25.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986218555s
Jul 22 21:44:26.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982876215s
Jul 22 21:44:27.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979723574s
Jul 22 21:44:28.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975768837s
Jul 22 21:44:29.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964524537s
Jul 22 21:44:30.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960783199s
Jul 22 21:44:31.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.922254837s
Jul 22 21:44:32.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 918.630398ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9239
Jul 22 21:44:33.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 21:44:34.255: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 21:44:34.255: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 21:44:34.255: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 21:44:34.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 21:44:34.561: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 21:44:34.561: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 21:44:34.561: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 21:44:34.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 21:44:34.815: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 21:44:34.815: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 21:44:34.815: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 21:44:34.818: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 22 21:44:44.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:44:44.822: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:44:44.822: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 22 21:44:44.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 21:44:45.052: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 21:44:45.052: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 21:44:45.052: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 21:44:45.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 21:44:45.286: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 21:44:45.286: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 21:44:45.286: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 21:44:45.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9239 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 21:44:45.544: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 21:44:45.544: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 21:44:45.544: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 21:44:45.544: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:44:45.547: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 21:44:55.554: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:44:55.554: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:44:55.554: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:44:55.564: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jul 22 21:44:55.564: INFO: ss-0  k8s-linuxpool-16111918-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  }]
Jul 22 21:44:55.564: INFO: ss-1  k8s-linuxpool-16111918-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:55.564: INFO: ss-2  k8s-linuxpool-16111918-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:55.564: INFO: 
Jul 22 21:44:55.564: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:44:56.567: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jul 22 21:44:56.567: INFO: ss-0  k8s-linuxpool-16111918-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  }]
Jul 22 21:44:56.567: INFO: ss-1  k8s-linuxpool-16111918-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:56.567: INFO: ss-2  k8s-linuxpool-16111918-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:56.567: INFO: 
Jul 22 21:44:56.567: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:44:57.571: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jul 22 21:44:57.571: INFO: ss-0  k8s-linuxpool-16111918-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:03 +0000 UTC  }]
Jul 22 21:44:57.571: INFO: ss-1  k8s-linuxpool-16111918-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:57.571: INFO: ss-2  k8s-linuxpool-16111918-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:57.571: INFO: 
Jul 22 21:44:57.571: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:44:58.589: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jul 22 21:44:58.589: INFO: ss-1  k8s-linuxpool-16111918-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:44:23 +0000 UTC  }]
Jul 22 21:44:58.589: INFO: 
Jul 22 21:44:58.589: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:44:59.592: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969481327s
Jul 22 21:45:00.595: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.966429986s
Jul 22 21:45:01.598: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.963087746s
Jul 22 21:45:02.600: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.9608136s
Jul 22 21:45:03.603: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.958110857s
Jul 22 21:45:04.610: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.453514ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9239
Jul 22 21:45:05.613: INFO: Scaling statefulset ss to 0
Jul 22 21:45:05.620: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 22 21:45:05.622: INFO: Deleting all statefulset in ns statefulset-9239
Jul 22 21:45:05.624: INFO: Scaling statefulset ss to 0
Jul 22 21:45:05.630: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:45:05.632: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:45:05.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9239" for this suite.
Jul 22 21:45:11.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:45:11.772: INFO: namespace statefulset-9239 deletion completed in 6.12513131s

• [SLOW TEST:68.726 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:45:11.773: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 22 21:45:11.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25036,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 21:45:11.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25037,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 22 21:45:11.955: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25038,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 22 21:45:21.975: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25055,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 21:45:21.975: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25056,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 22 21:45:21.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9427,SelfLink:/api/v1/namespaces/watch-9427/configmaps/e2e-watch-test-label-changed,UID:15df736e-e84d-4ff9-b205-8f8040a09982,ResourceVersion:25057,Generation:0,CreationTimestamp:2019-07-22 21:45:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:45:21.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9427" for this suite.
Jul 22 21:45:27.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:45:28.079: INFO: namespace watch-9427 deletion completed in 6.099879437s

• [SLOW TEST:16.306 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:45:28.079: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 22 21:45:32.764: INFO: Successfully updated pod "annotationupdate94cbad21-7672-4998-98c5-9e74d9ba7b0b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:45:34.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6774" for this suite.
Jul 22 21:45:56.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:45:56.891: INFO: namespace projected-6774 deletion completed in 22.09872545s

• [SLOW TEST:28.812 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:45:56.893: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 22 21:45:57.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-2986'
Jul 22 21:45:57.404: INFO: stderr: ""
Jul 22 21:45:57.404: INFO: stdout: "pod/pause created\n"
Jul 22 21:45:57.404: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 22 21:45:57.404: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2986" to be "running and ready"
Jul 22 21:45:57.408: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99278ms
Jul 22 21:45:59.412: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007808374s
Jul 22 21:46:01.415: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.010802372s
Jul 22 21:46:01.415: INFO: Pod "pause" satisfied condition "running and ready"
Jul 22 21:46:01.415: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 22 21:46:01.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 label pods pause testing-label=testing-label-value --namespace=kubectl-2986'
Jul 22 21:46:01.498: INFO: stderr: ""
Jul 22 21:46:01.498: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 22 21:46:01.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pod pause -L testing-label --namespace=kubectl-2986'
Jul 22 21:46:01.576: INFO: stderr: ""
Jul 22 21:46:01.576: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 22 21:46:01.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 label pods pause testing-label- --namespace=kubectl-2986'
Jul 22 21:46:01.663: INFO: stderr: ""
Jul 22 21:46:01.663: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 22 21:46:01.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pod pause -L testing-label --namespace=kubectl-2986'
Jul 22 21:46:01.744: INFO: stderr: ""
Jul 22 21:46:01.744: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 22 21:46:01.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-2986'
Jul 22 21:46:01.835: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:46:01.835: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 22 21:46:01.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get rc,svc -l name=pause --no-headers --namespace=kubectl-2986'
Jul 22 21:46:01.927: INFO: stderr: "No resources found.\n"
Jul 22 21:46:01.927: INFO: stdout: ""
Jul 22 21:46:01.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -l name=pause --namespace=kubectl-2986 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:46:02.004: INFO: stderr: ""
Jul 22 21:46:02.004: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:46:02.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2986" for this suite.
Jul 22 21:46:08.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:46:08.116: INFO: namespace kubectl-2986 deletion completed in 6.108991691s

• [SLOW TEST:11.224 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:46:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:46:08.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4" in namespace "downward-api-9430" to be "success or failure"
Jul 22 21:46:08.277: INFO: Pod "downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842076ms
Jul 22 21:46:10.281: INFO: Pod "downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008213372s
Jul 22 21:46:12.284: INFO: Pod "downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011487169s
STEP: Saw pod success
Jul 22 21:46:12.284: INFO: Pod "downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4" satisfied condition "success or failure"
Jul 22 21:46:12.286: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4 container client-container: <nil>
STEP: delete the pod
Jul 22 21:46:12.303: INFO: Waiting for pod downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4 to disappear
Jul 22 21:46:12.305: INFO: Pod downwardapi-volume-1c301fe6-b099-415a-9f38-70a8318740d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:46:12.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9430" for this suite.
Jul 22 21:46:18.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:46:18.408: INFO: namespace downward-api-9430 deletion completed in 6.099336638s

• [SLOW TEST:10.292 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:46:18.408: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-87c74b0b-36b0-4a4c-83cf-6b0f23145353
STEP: Creating a pod to test consume configMaps
Jul 22 21:46:18.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13" in namespace "configmap-3002" to be "success or failure"
Jul 22 21:46:18.569: INFO: Pod "pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03777ms
Jul 22 21:46:20.572: INFO: Pod "pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008959468s
Jul 22 21:46:22.575: INFO: Pod "pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011786967s
STEP: Saw pod success
Jul 22 21:46:22.575: INFO: Pod "pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13" satisfied condition "success or failure"
Jul 22 21:46:22.578: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:46:22.591: INFO: Waiting for pod pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13 to disappear
Jul 22 21:46:22.595: INFO: Pod pod-configmaps-cc77d39c-827b-4733-9f62-00d4739bce13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:46:22.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3002" for this suite.
Jul 22 21:46:28.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:46:28.686: INFO: namespace configmap-3002 deletion completed in 6.087381899s

• [SLOW TEST:10.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:46:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-8w62
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:46:28.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8w62" in namespace "subpath-1355" to be "success or failure"
Jul 22 21:46:28.862: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Pending", Reason="", readiness=false. Elapsed: 7.209364ms
Jul 22 21:46:30.867: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011365456s
Jul 22 21:46:32.870: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 4.014352855s
Jul 22 21:46:34.873: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 6.017579252s
Jul 22 21:46:36.877: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 8.022111742s
Jul 22 21:46:38.880: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 10.02511064s
Jul 22 21:46:40.883: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 12.028115838s
Jul 22 21:46:42.887: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 14.031432734s
Jul 22 21:46:44.929: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 16.073349436s
Jul 22 21:46:46.933: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 18.078124725s
Jul 22 21:46:48.940: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 20.084457207s
Jul 22 21:46:50.981: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Running", Reason="", readiness=true. Elapsed: 22.125320214s
Jul 22 21:46:52.983: INFO: Pod "pod-subpath-test-secret-8w62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.127966614s
STEP: Saw pod success
Jul 22 21:46:52.983: INFO: Pod "pod-subpath-test-secret-8w62" satisfied condition "success or failure"
Jul 22 21:46:52.985: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-subpath-test-secret-8w62 container test-container-subpath-secret-8w62: <nil>
STEP: delete the pod
Jul 22 21:46:53.004: INFO: Waiting for pod pod-subpath-test-secret-8w62 to disappear
Jul 22 21:46:53.007: INFO: Pod pod-subpath-test-secret-8w62 no longer exists
STEP: Deleting pod pod-subpath-test-secret-8w62
Jul 22 21:46:53.007: INFO: Deleting pod "pod-subpath-test-secret-8w62" in namespace "subpath-1355"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:46:53.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1355" for this suite.
Jul 22 21:46:59.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:46:59.107: INFO: namespace subpath-1355 deletion completed in 6.094110064s

• [SLOW TEST:30.420 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:46:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 22 21:46:59.250: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-425149679 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:46:59.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2441" for this suite.
Jul 22 21:47:05.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:47:05.408: INFO: namespace kubectl-2441 deletion completed in 6.082847022s

• [SLOW TEST:6.301 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:47:05.412: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 21:47:10.080: INFO: Successfully updated pod "pod-update-activedeadlineseconds-43c7a96a-e591-44b3-bd64-4357afce95b3"
Jul 22 21:47:10.080: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-43c7a96a-e591-44b3-bd64-4357afce95b3" in namespace "pods-3015" to be "terminated due to deadline exceeded"
Jul 22 21:47:10.085: INFO: Pod "pod-update-activedeadlineseconds-43c7a96a-e591-44b3-bd64-4357afce95b3": Phase="Running", Reason="", readiness=true. Elapsed: 5.373073ms
Jul 22 21:47:12.089: INFO: Pod "pod-update-activedeadlineseconds-43c7a96a-e591-44b3-bd64-4357afce95b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.008920468s
Jul 22 21:47:12.089: INFO: Pod "pod-update-activedeadlineseconds-43c7a96a-e591-44b3-bd64-4357afce95b3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:47:12.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3015" for this suite.
Jul 22 21:47:18.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:47:18.178: INFO: namespace pods-3015 deletion completed in 6.085702107s

• [SLOW TEST:12.766 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:47:18.179: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0722 21:47:28.345306      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 21:47:28.345: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:47:28.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8" for this suite.
Jul 22 21:47:34.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:47:34.495: INFO: namespace gc-8 deletion completed in 6.147145697s

• [SLOW TEST:16.316 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:47:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 22 21:47:34.637: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:47:39.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8099" for this suite.
Jul 22 21:48:01.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:01.339: INFO: namespace init-container-8099 deletion completed in 22.086985004s

• [SLOW TEST:26.844 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:01.341: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:48:01.486: INFO: Creating deployment "nginx-deployment"
Jul 22 21:48:01.490: INFO: Waiting for observed generation 1
Jul 22 21:48:03.500: INFO: Waiting for all required pods to come up
Jul 22 21:48:03.504: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 22 21:48:07.520: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 22 21:48:07.524: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 22 21:48:07.529: INFO: Updating deployment nginx-deployment
Jul 22 21:48:07.529: INFO: Waiting for observed generation 2
Jul 22 21:48:09.537: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 22 21:48:09.539: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 22 21:48:09.540: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 22 21:48:09.547: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 22 21:48:09.547: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 22 21:48:09.549: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 22 21:48:09.554: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 22 21:48:09.554: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 22 21:48:09.559: INFO: Updating deployment nginx-deployment
Jul 22 21:48:09.559: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 22 21:48:09.570: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 22 21:48:09.589: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 22 21:48:09.638: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7523,SelfLink:/apis/apps/v1/namespaces/deployment-7523/deployments/nginx-deployment,UID:4a9b974c-3568-4255-842b-487e3500379f,ResourceVersion:25800,Generation:3,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-07-22 21:48:07 +0000 UTC 2019-07-22 21:48:01 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-07-22 21:48:09 +0000 UTC 2019-07-22 21:48:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 22 21:48:09.658: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7523,SelfLink:/apis/apps/v1/namespaces/deployment-7523/replicasets/nginx-deployment-55fb7cb77f,UID:e18abe04-e16f-4779-b33b-7f8f8d0eadfa,ResourceVersion:25786,Generation:3,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4a9b974c-3568-4255-842b-487e3500379f 0xc0017475a7 0xc0017475a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 21:48:09.658: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 22 21:48:09.659: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7523,SelfLink:/apis/apps/v1/namespaces/deployment-7523/replicasets/nginx-deployment-7b8c6f4498,UID:edfb7668-df7c-4581-8e26-8541bd836b13,ResourceVersion:25784,Generation:3,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4a9b974c-3568-4255-842b-487e3500379f 0xc001747677 0xc001747678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 22 21:48:09.690: INFO: Pod "nginx-deployment-55fb7cb77f-2xgzg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2xgzg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-2xgzg,UID:648054de-6f46-4d1c-81fc-3071e5d3b9df,ResourceVersion:25818,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271a497 0xc00271a498}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271a710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271a730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.690: INFO: Pod "nginx-deployment-55fb7cb77f-5gh6r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5gh6r,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-5gh6r,UID:36eb9c17-9ebc-4e57-9de4-253e05852ac5,ResourceVersion:25822,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271a857 0xc00271a858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271a930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271aaf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-66xrd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-66xrd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-66xrd,UID:180c5119-08e2-4365-9a31-d4440b7df787,ResourceVersion:25776,Generation:0,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271ab57 0xc00271ab58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271add0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271adf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-07-22 21:48:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-8lvrm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8lvrm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-8lvrm,UID:659b1cde-bbed-4212-a2a7-a21c6e3c455e,ResourceVersion:25762,Generation:0,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271b0e0 0xc00271b0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271b270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271b340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-07-22 21:48:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-9ng59" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9ng59,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-9ng59,UID:9830ac7f-2f3e-4ca8-98a3-0a72a6c77ab3,ResourceVersion:25756,Generation:0,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271b5c0 0xc00271b5c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271b700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271b720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-07-22 21:48:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-b5ppp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-b5ppp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-b5ppp,UID:a43966ae-b080-49f2-aba5-370972af605b,ResourceVersion:25830,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271bb10 0xc00271bb11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271bb80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271bc00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-gpcpd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gpcpd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-gpcpd,UID:b4080f56-bbd4-46d8-a46a-4fa867db184d,ResourceVersion:25741,Generation:0,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc00271bdb7 0xc00271bdb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00271beb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00271bef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-07-22 21:48:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-j2pm2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j2pm2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-j2pm2,UID:aab18c95-76a4-4ef5-9b1d-2c1dd9506904,ResourceVersion:25826,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f6050 0xc0027f6051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f6240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f6260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-ncs4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ncs4j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-ncs4j,UID:a46b90fe-6f37-40ce-a053-463bd5582a9f,ResourceVersion:25823,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f6330 0xc0027f6331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f6460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f6490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.691: INFO: Pod "nginx-deployment-55fb7cb77f-pbhm2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pbhm2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-pbhm2,UID:09f8c3eb-d6ff-43b4-af46-dcf62bdca99f,ResourceVersion:25820,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f6610 0xc0027f6611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f67e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f6800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.692: INFO: Pod "nginx-deployment-55fb7cb77f-ssc8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ssc8k,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-ssc8k,UID:3c425a4b-b821-44d6-bca2-7aa4cc9d9a1b,ResourceVersion:25817,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f68d7 0xc0027f68d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f6a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f6a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.693: INFO: Pod "nginx-deployment-55fb7cb77f-wqmnc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wqmnc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-wqmnc,UID:7f8fc36c-3bb6-4a19-a9fc-96298f2d6418,ResourceVersion:25772,Generation:0,CreationTimestamp:2019-07-22 21:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f6bc0 0xc0027f6bc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f6cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f6ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:07 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-07-22 21:48:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.693: INFO: Pod "nginx-deployment-55fb7cb77f-wwn6p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wwn6p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-55fb7cb77f-wwn6p,UID:72f1cd17-08c8-4105-9677-fba11ae908c3,ResourceVersion:25797,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e18abe04-e16f-4779-b33b-7f8f8d0eadfa 0xc0027f6f70 0xc0027f6f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f70c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f70e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.693: INFO: Pod "nginx-deployment-7b8c6f4498-4j978" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4j978,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-4j978,UID:79f545db-1e63-4b1a-aec0-77f1bd3b6b25,ResourceVersion:25702,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc0027f7160 0xc0027f7161}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f71c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f71e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.235,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://207ee56f01e6c014a0a53791d84e394940527ad49a3a6039f0b95d66ce9b1a7b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.693: INFO: Pod "nginx-deployment-7b8c6f4498-5zx5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5zx5t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-5zx5t,UID:8c74e51c-2f0d-423e-ba57-a1b0ed04be17,ResourceVersion:25819,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc0027f7660 0xc0027f7661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f77e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f7800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-97lwc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-97lwc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-97lwc,UID:aaaa8015-1133-4bd0-b8ab-10e66a435f22,ResourceVersion:25717,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc0027f79f7 0xc0027f79f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f7bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f7be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.1.38,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6ecec7686c7ac1f67bacc96c4425957b54fb479c78b8489864cb6152fe3ba62d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-c5zxr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c5zxr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-c5zxr,UID:f585a8b5-8604-4db7-a2bd-3cdc8a80677b,ResourceVersion:25685,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc0027f7e50 0xc0027f7e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f7eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f7ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.28,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e6aa6aa74fb1af6f2f3ba9ce4d910232a5cb069b06dd7a31f9e33b9344695fa2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-f7dwc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f7dwc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-f7dwc,UID:f7bedc31-bd9a-4c4e-9812-b3dbc71b2e5e,ResourceVersion:25825,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc0027f7fa0 0xc0027f7fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-fsjq4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fsjq4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-fsjq4,UID:79861265-d831-4493-b2b6-ad6948c6f032,ResourceVersion:25814,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c087 0xc00338c088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-gk7p8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gk7p8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-gk7p8,UID:384581e8-5e21-411a-90d0-78abb7d7fa9b,ResourceVersion:25816,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c177 0xc00338c178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c1e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.694: INFO: Pod "nginx-deployment-7b8c6f4498-kldrb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kldrb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-kldrb,UID:a51be1f6-1550-4f87-9e27-a2237abac419,ResourceVersion:25808,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c280 0xc00338c281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-07-22 21:48:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-knh2s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-knh2s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-knh2s,UID:6b4b8abf-1f60-40f2-998b-7f426e69957a,ResourceVersion:25705,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c3d0 0xc00338c3d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.236,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f2766cfc3abd192a30cc55a72038fd576277e6f28a65d7becb5a41d49deef393}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-m7vgh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m7vgh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-m7vgh,UID:788643ce-63db-4a9d-b3be-3b27256b1c73,ResourceVersion:25682,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c520 0xc00338c521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.29,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://91b734b3cf02ff20e2b52a3d9a90d75bac62ba80b31dd744d5f1b0d55dea581b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-mfnz9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mfnz9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-mfnz9,UID:24baeba8-b8c3-4156-9efa-5a5173293fe8,ResourceVersion:25824,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c680 0xc00338c681}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-pzjxx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pzjxx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-pzjxx,UID:0b194c1c-ca4f-489d-9bb4-420f112affb6,ResourceVersion:25798,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c767 0xc00338c768}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-q58xs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-q58xs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-q58xs,UID:0a690637-f722-44e5-aee3-578b3c723d0e,ResourceVersion:25828,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c870 0xc00338c871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338c8d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338c8f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-07-22 21:48:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-rhpbf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rhpbf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-rhpbf,UID:67d96720-619c-4d32-8219-a8c47bf0a88f,ResourceVersion:25807,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338c9b0 0xc00338c9b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338ca10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338ca30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.695: INFO: Pod "nginx-deployment-7b8c6f4498-rtpb2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rtpb2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-rtpb2,UID:c8d06ae6-87ef-49e7-93ac-dac0e041ec89,ResourceVersion:25699,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338cab0 0xc00338cab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338cb10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338cb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.234,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://946cacef401988387ea766906fc80b445ca10653ded821bc95c51805fb086144}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.699: INFO: Pod "nginx-deployment-7b8c6f4498-s7gf4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s7gf4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-s7gf4,UID:abdc76f0-616b-462c-8b0a-1ddd26d3a1a5,ResourceVersion:25708,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338cc00 0xc00338cc01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338cc60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338cc80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.237,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2b17e8f67f777887aa827a43ddba17085b7016428175f6cd3f1babedeced5639}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.699: INFO: Pod "nginx-deployment-7b8c6f4498-t2x5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t2x5r,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-t2x5r,UID:7d3d9eaa-36ca-4c77-a993-b19d6f594d29,ResourceVersion:25813,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338cd50 0xc00338cd51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338cdb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338cdd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.701: INFO: Pod "nginx-deployment-7b8c6f4498-tp49n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tp49n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-tp49n,UID:f51879e3-073b-4373-9b86-20762021554d,ResourceVersion:25812,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338ce50 0xc00338ce51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338ceb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338ced0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.701: INFO: Pod "nginx-deployment-7b8c6f4498-x8wtc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x8wtc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-x8wtc,UID:5e2ae898-712c-4dc6-bc98-9852fb436ebf,ResourceVersion:25688,Generation:0,CreationTimestamp:2019-07-22 21:48:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338cf37 0xc00338cf38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338cfb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338cfd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:01 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.27,StartTime:2019-07-22 21:48:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 21:48:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1e14b83bb7444e19091fa2706a45d8e16647b39df4a49ac765379213b123a5e9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 21:48:09.701: INFO: Pod "nginx-deployment-7b8c6f4498-zrrmg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zrrmg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7523,SelfLink:/api/v1/namespaces/deployment-7523/pods/nginx-deployment-7b8c6f4498-zrrmg,UID:f37ae475-64ca-4157-9e76-9ac121f52a12,ResourceVersion:25815,Generation:0,CreationTimestamp:2019-07-22 21:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 edfb7668-df7c-4581-8e26-8541bd836b13 0xc00338d710 0xc00338d711}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7d9z7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7d9z7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7d9z7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338d770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338d790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 21:48:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:48:09.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7523" for this suite.
Jul 22 21:48:17.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:17.835: INFO: namespace deployment-7523 deletion completed in 8.118337255s

• [SLOW TEST:16.494 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:17.835: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:48:18.028: INFO: (0) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 17.505412ms)
Jul 22 21:48:18.057: INFO: (1) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 29.356652ms)
Jul 22 21:48:18.069: INFO: (2) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 12.399938ms)
Jul 22 21:48:18.077: INFO: (3) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 7.645561ms)
Jul 22 21:48:18.114: INFO: (4) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 37.109913ms)
Jul 22 21:48:18.119: INFO: (5) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.480377ms)
Jul 22 21:48:18.126: INFO: (6) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 7.462262ms)
Jul 22 21:48:18.131: INFO: (7) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.528077ms)
Jul 22 21:48:18.135: INFO: (8) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.137379ms)
Jul 22 21:48:18.138: INFO: (9) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.147784ms)
Jul 22 21:48:18.142: INFO: (10) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.421883ms)
Jul 22 21:48:18.145: INFO: (11) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.287083ms)
Jul 22 21:48:18.148: INFO: (12) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.183984ms)
Jul 22 21:48:18.152: INFO: (13) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.551282ms)
Jul 22 21:48:18.155: INFO: (14) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.254283ms)
Jul 22 21:48:18.159: INFO: (15) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.940781ms)
Jul 22 21:48:18.162: INFO: (16) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.320583ms)
Jul 22 21:48:18.166: INFO: (17) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.509482ms)
Jul 22 21:48:18.171: INFO: (18) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.464378ms)
Jul 22 21:48:18.174: INFO: (19) /api/v1/nodes/k8s-linuxpool-16111918-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.201584ms)
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:48:18.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5896" for this suite.
Jul 22 21:48:24.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:24.261: INFO: namespace proxy-5896 deletion completed in 6.083484617s

• [SLOW TEST:6.426 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:24.268: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 22 21:48:24.410: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-425149679 proxy --unix-socket=/tmp/kubectl-proxy-unix037766102/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:48:24.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3725" for this suite.
Jul 22 21:48:30.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:30.559: INFO: namespace kubectl-3725 deletion completed in 6.089733886s

• [SLOW TEST:6.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 22 21:48:40.785: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 21:48:40.785847      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:48:40.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4265" for this suite.
Jul 22 21:48:46.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:46.883: INFO: namespace gc-4265 deletion completed in 6.095224658s

• [SLOW TEST:16.324 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:46.884: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:48:47.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d" in namespace "downward-api-9728" to be "success or failure"
Jul 22 21:48:47.044: INFO: Pod "downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05367ms
Jul 22 21:48:49.047: INFO: Pod "downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009074667s
Jul 22 21:48:51.051: INFO: Pod "downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012388263s
STEP: Saw pod success
Jul 22 21:48:51.051: INFO: Pod "downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d" satisfied condition "success or failure"
Jul 22 21:48:51.053: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d container client-container: <nil>
STEP: delete the pod
Jul 22 21:48:51.067: INFO: Waiting for pod downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d to disappear
Jul 22 21:48:51.071: INFO: Pod downwardapi-volume-3b63e60e-2f8a-4d5e-a08a-9ab1fa441b2d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:48:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9728" for this suite.
Jul 22 21:48:57.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:48:57.193: INFO: namespace downward-api-9728 deletion completed in 6.11874014s

• [SLOW TEST:10.309 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:48:57.199: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3346
STEP: Creating secret with name secret-test-0df325e0-0509-4f19-8f51-9f07e04cf0e4
STEP: Creating a pod to test consume secrets
Jul 22 21:48:57.517: INFO: Waiting up to 5m0s for pod "pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7" in namespace "secrets-3079" to be "success or failure"
Jul 22 21:48:57.523: INFO: Pod "pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03707ms
Jul 22 21:48:59.526: INFO: Pod "pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008928268s
Jul 22 21:49:01.529: INFO: Pod "pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012233664s
STEP: Saw pod success
Jul 22 21:49:01.529: INFO: Pod "pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7" satisfied condition "success or failure"
Jul 22 21:49:01.531: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:49:01.556: INFO: Waiting for pod pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7 to disappear
Jul 22 21:49:01.558: INFO: Pod pod-secrets-5ec83602-ca3b-496b-882c-dc617ec190f7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:49:01.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3079" for this suite.
Jul 22 21:49:07.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:49:07.653: INFO: namespace secrets-3079 deletion completed in 6.092649671s
STEP: Destroying namespace "secret-namespace-3346" for this suite.
Jul 22 21:49:13.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:49:13.742: INFO: namespace secret-namespace-3346 deletion completed in 6.08886689s

• [SLOW TEST:16.543 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:49:13.742: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-657
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-657 to expose endpoints map[]
Jul 22 21:49:13.903: INFO: Get endpoints failed (8.537657ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 22 21:49:14.906: INFO: successfully validated that service multi-endpoint-test in namespace services-657 exposes endpoints map[] (1.0112813s elapsed)
STEP: Creating pod pod1 in namespace services-657
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-657 to expose endpoints map[pod1:[100]]
Jul 22 21:49:17.934: INFO: successfully validated that service multi-endpoint-test in namespace services-657 exposes endpoints map[pod1:[100]] (3.021368761s elapsed)
STEP: Creating pod pod2 in namespace services-657
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-657 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 22 21:49:20.973: INFO: successfully validated that service multi-endpoint-test in namespace services-657 exposes endpoints map[pod1:[100] pod2:[101]] (3.034191997s elapsed)
STEP: Deleting pod pod1 in namespace services-657
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-657 to expose endpoints map[pod2:[101]]
Jul 22 21:49:20.990: INFO: successfully validated that service multi-endpoint-test in namespace services-657 exposes endpoints map[pod2:[101]] (11.267043ms elapsed)
STEP: Deleting pod pod2 in namespace services-657
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-657 to expose endpoints map[]
Jul 22 21:49:22.007: INFO: successfully validated that service multi-endpoint-test in namespace services-657 exposes endpoints map[] (1.006419824s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:49:22.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-657" for this suite.
Jul 22 21:49:44.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:49:44.134: INFO: namespace services-657 deletion completed in 22.091951475s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.392 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:49:44.138: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 22 21:49:44.281: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:49:48.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7530" for this suite.
Jul 22 21:49:54.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:49:54.998: INFO: namespace init-container-7530 deletion completed in 6.080110134s

• [SLOW TEST:10.860 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:49:54.999: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4664dd56-8549-431f-a8dd-090e20bc0e3b
STEP: Creating a pod to test consume configMaps
Jul 22 21:49:55.169: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56" in namespace "projected-9145" to be "success or failure"
Jul 22 21:49:55.174: INFO: Pod "pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146774ms
Jul 22 21:49:57.177: INFO: Pod "pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008247271s
Jul 22 21:49:59.180: INFO: Pod "pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011227468s
STEP: Saw pod success
Jul 22 21:49:59.180: INFO: Pod "pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56" satisfied condition "success or failure"
Jul 22 21:49:59.183: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:49:59.201: INFO: Waiting for pod pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56 to disappear
Jul 22 21:49:59.204: INFO: Pod pod-projected-configmaps-46edbe8b-dc98-4f6c-9d73-8eaa5e35ee56 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:49:59.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9145" for this suite.
Jul 22 21:50:05.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:50:05.307: INFO: namespace projected-9145 deletion completed in 6.10064223s

• [SLOW TEST:10.309 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:50:05.308: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9e6e5afb-900d-4328-aa84-f0e60c291357
STEP: Creating a pod to test consume configMaps
Jul 22 21:50:05.466: INFO: Waiting up to 5m0s for pod "pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e" in namespace "configmap-605" to be "success or failure"
Jul 22 21:50:05.468: INFO: Pod "pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506187ms
Jul 22 21:50:07.472: INFO: Pod "pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006069782s
Jul 22 21:50:09.474: INFO: Pod "pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008581982s
STEP: Saw pod success
Jul 22 21:50:09.475: INFO: Pod "pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e" satisfied condition "success or failure"
Jul 22 21:50:09.477: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:50:09.497: INFO: Waiting for pod pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e to disappear
Jul 22 21:50:09.499: INFO: Pod pod-configmaps-068d1f8e-3464-47eb-98ab-5b53f6cd9a9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:50:09.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-605" for this suite.
Jul 22 21:50:15.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:50:15.582: INFO: namespace configmap-605 deletion completed in 6.080044534s

• [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:50:15.584: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 22 21:50:15.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-768'
Jul 22 21:50:15.971: INFO: stderr: ""
Jul 22 21:50:15.971: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:50:15.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-768'
Jul 22 21:50:16.068: INFO: stderr: ""
Jul 22 21:50:16.068: INFO: stdout: "update-demo-nautilus-mv5lc update-demo-nautilus-pjkkd "
Jul 22 21:50:16.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-mv5lc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:16.152: INFO: stderr: ""
Jul 22 21:50:16.152: INFO: stdout: ""
Jul 22 21:50:16.152: INFO: update-demo-nautilus-mv5lc is created but not running
Jul 22 21:50:21.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-768'
Jul 22 21:50:21.242: INFO: stderr: ""
Jul 22 21:50:21.242: INFO: stdout: "update-demo-nautilus-mv5lc update-demo-nautilus-pjkkd "
Jul 22 21:50:21.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-mv5lc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:21.330: INFO: stderr: ""
Jul 22 21:50:21.330: INFO: stdout: "true"
Jul 22 21:50:21.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-mv5lc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:21.414: INFO: stderr: ""
Jul 22 21:50:21.414: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:50:21.414: INFO: validating pod update-demo-nautilus-mv5lc
Jul 22 21:50:21.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:50:21.419: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:50:21.419: INFO: update-demo-nautilus-mv5lc is verified up and running
Jul 22 21:50:21.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-pjkkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:21.570: INFO: stderr: ""
Jul 22 21:50:21.570: INFO: stdout: "true"
Jul 22 21:50:21.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-nautilus-pjkkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:21.655: INFO: stderr: ""
Jul 22 21:50:21.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:50:21.656: INFO: validating pod update-demo-nautilus-pjkkd
Jul 22 21:50:21.660: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:50:21.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:50:21.660: INFO: update-demo-nautilus-pjkkd is verified up and running
STEP: rolling-update to new replication controller
Jul 22 21:50:21.662: INFO: scanned /root for discovery docs: <nil>
Jul 22 21:50:21.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-768'
Jul 22 21:50:44.174: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 22 21:50:44.174: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:50:44.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-768'
Jul 22 21:50:44.260: INFO: stderr: ""
Jul 22 21:50:44.260: INFO: stdout: "update-demo-kitten-rbxhd update-demo-kitten-xl2pw update-demo-nautilus-mv5lc "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jul 22 21:50:49.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-768'
Jul 22 21:50:49.344: INFO: stderr: ""
Jul 22 21:50:49.344: INFO: stdout: "update-demo-kitten-rbxhd update-demo-kitten-xl2pw "
Jul 22 21:50:49.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-kitten-rbxhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:49.427: INFO: stderr: ""
Jul 22 21:50:49.427: INFO: stdout: "true"
Jul 22 21:50:49.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-kitten-rbxhd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:49.521: INFO: stderr: ""
Jul 22 21:50:49.521: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 22 21:50:49.521: INFO: validating pod update-demo-kitten-rbxhd
Jul 22 21:50:49.525: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 22 21:50:49.525: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 22 21:50:49.525: INFO: update-demo-kitten-rbxhd is verified up and running
Jul 22 21:50:49.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-kitten-xl2pw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:49.609: INFO: stderr: ""
Jul 22 21:50:49.609: INFO: stdout: "true"
Jul 22 21:50:49.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 get pods update-demo-kitten-xl2pw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-768'
Jul 22 21:50:49.688: INFO: stderr: ""
Jul 22 21:50:49.689: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 22 21:50:49.689: INFO: validating pod update-demo-kitten-xl2pw
Jul 22 21:50:49.693: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 22 21:50:49.693: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 22 21:50:49.693: INFO: update-demo-kitten-xl2pw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:50:49.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-768" for this suite.
Jul 22 21:51:17.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:51:17.783: INFO: namespace kubectl-768 deletion completed in 28.086892036s

• [SLOW TEST:62.199 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:51:17.783: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-bd10787b-857a-43eb-a23d-1f49112905de
STEP: Creating a pod to test consume configMaps
Jul 22 21:51:17.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd" in namespace "projected-1840" to be "success or failure"
Jul 22 21:51:17.951: INFO: Pod "pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988764ms
Jul 22 21:51:19.955: INFO: Pod "pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011364755s
Jul 22 21:51:21.959: INFO: Pod "pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014546951s
STEP: Saw pod success
Jul 22 21:51:21.959: INFO: Pod "pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd" satisfied condition "success or failure"
Jul 22 21:51:21.961: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:51:21.976: INFO: Waiting for pod pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd to disappear
Jul 22 21:51:21.978: INFO: Pod pod-projected-configmaps-4a9223a6-fa13-4657-bcda-774324cf4cfd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:51:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1840" for this suite.
Jul 22 21:51:27.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:51:28.068: INFO: namespace projected-1840 deletion completed in 6.0865903s

• [SLOW TEST:10.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:51:28.070: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:51:28.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db" in namespace "downward-api-4333" to be "success or failure"
Jul 22 21:51:28.221: INFO: Pod "downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204389ms
Jul 22 21:51:30.224: INFO: Pod "downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005604985s
Jul 22 21:51:32.227: INFO: Pod "downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008332383s
STEP: Saw pod success
Jul 22 21:51:32.227: INFO: Pod "downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db" satisfied condition "success or failure"
Jul 22 21:51:32.229: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db container client-container: <nil>
STEP: delete the pod
Jul 22 21:51:32.244: INFO: Waiting for pod downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db to disappear
Jul 22 21:51:32.247: INFO: Pod downwardapi-volume-9cce413d-f7c4-4c0d-9932-6a235c8ec9db no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:51:32.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4333" for this suite.
Jul 22 21:51:38.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:51:38.462: INFO: namespace downward-api-4333 deletion completed in 6.211366371s

• [SLOW TEST:10.391 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:51:38.462: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 22 21:51:38.625: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8635,SelfLink:/api/v1/namespaces/watch-8635/configmaps/e2e-watch-test-resource-version,UID:96db3c72-1d88-4bca-b198-91ce4a04047f,ResourceVersion:27039,Generation:0,CreationTimestamp:2019-07-22 21:51:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 21:51:38.626: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8635,SelfLink:/api/v1/namespaces/watch-8635/configmaps/e2e-watch-test-resource-version,UID:96db3c72-1d88-4bca-b198-91ce4a04047f,ResourceVersion:27040,Generation:0,CreationTimestamp:2019-07-22 21:51:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:51:38.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8635" for this suite.
Jul 22 21:51:44.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:51:44.723: INFO: namespace watch-8635 deletion completed in 6.083347916s

• [SLOW TEST:6.261 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:51:44.723: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 22 21:51:44.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 --namespace=kubectl-4007 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 22 21:51:47.428: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 22 21:51:47.428: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:51:49.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4007" for this suite.
Jul 22 21:51:59.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:51:59.525: INFO: namespace kubectl-4007 deletion completed in 10.088528615s

• [SLOW TEST:14.802 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:51:59.527: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 21:52:03.724: INFO: Waiting up to 5m0s for pod "client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde" in namespace "pods-4306" to be "success or failure"
Jul 22 21:52:03.727: INFO: Pod "client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.630282ms
Jul 22 21:52:05.730: INFO: Pod "client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006288281s
Jul 22 21:52:07.733: INFO: Pod "client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009072179s
STEP: Saw pod success
Jul 22 21:52:07.733: INFO: Pod "client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde" satisfied condition "success or failure"
Jul 22 21:52:07.735: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde container env3cont: <nil>
STEP: delete the pod
Jul 22 21:52:07.748: INFO: Waiting for pod client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde to disappear
Jul 22 21:52:07.750: INFO: Pod client-envvars-1cdbae0e-877c-4405-a2fa-27265a9cabde no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:52:07.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4306" for this suite.
Jul 22 21:52:49.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:52:49.837: INFO: namespace pods-4306 deletion completed in 42.083713535s

• [SLOW TEST:50.310 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:52:49.840: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 22 21:52:49.982: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 22 21:52:49.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:50.243: INFO: stderr: ""
Jul 22 21:52:50.243: INFO: stdout: "service/redis-slave created\n"
Jul 22 21:52:50.243: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 22 21:52:50.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:50.537: INFO: stderr: ""
Jul 22 21:52:50.537: INFO: stdout: "service/redis-master created\n"
Jul 22 21:52:50.537: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 22 21:52:50.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:50.850: INFO: stderr: ""
Jul 22 21:52:50.851: INFO: stdout: "service/frontend created\n"
Jul 22 21:52:50.851: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 22 21:52:50.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:51.156: INFO: stderr: ""
Jul 22 21:52:51.156: INFO: stdout: "deployment.apps/frontend created\n"
Jul 22 21:52:51.157: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 21:52:51.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:51.426: INFO: stderr: ""
Jul 22 21:52:51.426: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 22 21:52:51.427: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 22 21:52:51.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 create -f - --namespace=kubectl-7395'
Jul 22 21:52:52.262: INFO: stderr: ""
Jul 22 21:52:52.262: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 22 21:52:52.262: INFO: Waiting for all frontend pods to be Running.
Jul 22 21:53:27.313: INFO: Waiting for frontend to serve content.
Jul 22 21:53:27.328: INFO: Trying to add a new entry to the guestbook.
Jul 22 21:53:27.339: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 22 21:53:27.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:27.516: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:27.516: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:53:27.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:27.646: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:27.646: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:53:27.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:27.814: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:27.814: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:53:27.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:27.911: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:27.911: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:53:27.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:27.996: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:27.996: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:53:27.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 delete --grace-period=0 --force -f - --namespace=kubectl-7395'
Jul 22 21:53:28.076: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:53:28.076: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:53:28.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7395" for this suite.
Jul 22 21:54:10.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:54:10.165: INFO: namespace kubectl-7395 deletion completed in 42.085209924s

• [SLOW TEST:80.325 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:54:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 22 21:54:10.315: INFO: Waiting up to 5m0s for pod "client-containers-ca257774-3f83-471a-a252-acc8f96ebc94" in namespace "containers-9119" to be "success or failure"
Jul 22 21:54:10.319: INFO: Pod "client-containers-ca257774-3f83-471a-a252-acc8f96ebc94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514563ms
Jul 22 21:54:12.323: INFO: Pod "client-containers-ca257774-3f83-471a-a252-acc8f96ebc94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007952974s
Jul 22 21:54:14.326: INFO: Pod "client-containers-ca257774-3f83-471a-a252-acc8f96ebc94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010989408s
STEP: Saw pod success
Jul 22 21:54:14.326: INFO: Pod "client-containers-ca257774-3f83-471a-a252-acc8f96ebc94" satisfied condition "success or failure"
Jul 22 21:54:14.328: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod client-containers-ca257774-3f83-471a-a252-acc8f96ebc94 container test-container: <nil>
STEP: delete the pod
Jul 22 21:54:14.343: INFO: Waiting for pod client-containers-ca257774-3f83-471a-a252-acc8f96ebc94 to disappear
Jul 22 21:54:14.346: INFO: Pod client-containers-ca257774-3f83-471a-a252-acc8f96ebc94 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:54:14.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9119" for this suite.
Jul 22 21:54:20.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:54:20.436: INFO: namespace containers-9119 deletion completed in 6.086568403s

• [SLOW TEST:10.271 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:54:20.436: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-70f75c53-0afc-40da-b24f-da712d9d7800 in namespace container-probe-6630
Jul 22 21:54:24.611: INFO: Started pod busybox-70f75c53-0afc-40da-b24f-da712d9d7800 in namespace container-probe-6630
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:54:24.613: INFO: Initial restart count of pod busybox-70f75c53-0afc-40da-b24f-da712d9d7800 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:58:25.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6630" for this suite.
Jul 22 21:58:31.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:58:31.292: INFO: namespace container-probe-6630 deletion completed in 6.086777899s

• [SLOW TEST:250.856 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:58:31.293: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:58:31.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf" in namespace "downward-api-1875" to be "success or failure"
Jul 22 21:58:31.490: INFO: Pod "downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.822145ms
Jul 22 21:58:33.492: INFO: Pod "downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010344391s
Jul 22 21:58:35.495: INFO: Pod "downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013387846s
STEP: Saw pod success
Jul 22 21:58:35.495: INFO: Pod "downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf" satisfied condition "success or failure"
Jul 22 21:58:35.498: INFO: Trying to get logs from node k8s-linuxpool-16111918-0 pod downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf container client-container: <nil>
STEP: delete the pod
Jul 22 21:58:35.515: INFO: Waiting for pod downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf to disappear
Jul 22 21:58:35.518: INFO: Pod downwardapi-volume-12eadaf3-0819-4e7e-b86e-59ecff6c84cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:58:35.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1875" for this suite.
Jul 22 21:58:41.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:58:41.610: INFO: namespace downward-api-1875 deletion completed in 6.089171478s

• [SLOW TEST:10.318 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:58:41.612: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-103.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-103.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:58:45.798: INFO: DNS probes using dns-103/dns-test-1a9b37b9-cb72-4607-9c14-c385e2b1666f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:58:45.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-103" for this suite.
Jul 22 21:58:51.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:58:51.906: INFO: namespace dns-103 deletion completed in 6.091586051s

• [SLOW TEST:10.295 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:58:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 22 21:58:52.111: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 22 21:58:57.114: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 21:58:57.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7029" for this suite.
Jul 22 21:59:03.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 21:59:03.246: INFO: namespace replication-controller-7029 deletion completed in 6.103000078s

• [SLOW TEST:11.339 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 21:59:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1130
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 22 21:59:03.452: INFO: Found 0 stateful pods, waiting for 3
Jul 22 21:59:13.455: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:59:13.455: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:59:13.455: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 22 21:59:13.478: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 22 21:59:23.510: INFO: Updating stateful set ss2
Jul 22 21:59:23.517: INFO: Waiting for Pod statefulset-1130/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 22 21:59:33.570: INFO: Found 1 stateful pods, waiting for 3
Jul 22 21:59:43.574: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:59:43.574: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:59:43.574: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 22 21:59:43.594: INFO: Updating stateful set ss2
Jul 22 21:59:43.601: INFO: Waiting for Pod statefulset-1130/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 22 21:59:53.607: INFO: Waiting for Pod statefulset-1130/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 22 22:00:03.654: INFO: Updating stateful set ss2
Jul 22 22:00:03.664: INFO: Waiting for StatefulSet statefulset-1130/ss2 to complete update
Jul 22 22:00:03.664: INFO: Waiting for Pod statefulset-1130/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 22 22:00:13.703: INFO: Deleting all statefulset in ns statefulset-1130
Jul 22 22:00:13.706: INFO: Scaling statefulset ss2 to 0
Jul 22 22:00:43.723: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 22:00:43.726: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 22:00:43.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1130" for this suite.
Jul 22 22:00:49.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 22:00:49.828: INFO: namespace statefulset-1130 deletion completed in 6.088031514s

• [SLOW TEST:106.582 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 22:00:49.830: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5762
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-84a1b1ed-a362-4ed7-a485-11edd09c5353
STEP: Creating secret with name s-test-opt-upd-1c42bdba-37a9-46d2-9649-75f60ab8ea8f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-84a1b1ed-a362-4ed7-a485-11edd09c5353
STEP: Updating secret s-test-opt-upd-1c42bdba-37a9-46d2-9649-75f60ab8ea8f
STEP: Creating secret with name s-test-opt-create-b996e8ee-e201-4d51-8b08-36a40163d02e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 22:00:58.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5762" for this suite.
Jul 22 22:01:20.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 22:01:20.144: INFO: namespace projected-5762 deletion completed in 22.076636156s

• [SLOW TEST:30.314 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 22:01:20.147: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9538
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9538
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9538
Jul 22 22:01:20.316: INFO: Found 0 stateful pods, waiting for 1
Jul 22 22:01:30.320: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 22 22:01:30.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 22:01:30.964: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 22:01:30.964: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 22:01:30.964: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 22:01:30.967: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 22:01:40.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 22:01:40.970: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 22:01:40.982: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
Jul 22 22:01:41.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996849196s
Jul 22 22:01:42.988: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99350459s
Jul 22 22:01:44.050: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990324082s
Jul 22 22:01:45.054: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.928192958s
Jul 22 22:01:46.057: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.924449449s
Jul 22 22:01:47.061: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.921139434s
Jul 22 22:01:48.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.917909417s
Jul 22 22:01:49.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.914295201s
Jul 22 22:01:50.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 910.202285ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9538
Jul 22 22:01:51.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 22:01:51.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 22:01:51.304: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 22:01:51.304: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 22:01:51.308: INFO: Found 1 stateful pods, waiting for 3
Jul 22 22:02:01.312: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 22:02:01.312: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 22:02:01.312: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 22 22:02:01.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 22:02:01.538: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 22:02:01.538: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 22:02:01.538: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 22:02:01.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 22:02:01.837: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 22:02:01.837: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 22:02:01.837: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 22:02:01.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 22:02:02.081: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 22:02:02.081: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 22:02:02.081: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 22:02:02.081: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 22:02:02.084: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 22:02:12.090: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 22:02:12.090: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 22:02:12.090: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 22:02:12.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Jul 22 22:02:13.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996649332s
Jul 22 22:02:14.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993573759s
Jul 22 22:02:15.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.934513849s
Jul 22 22:02:16.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.930650578s
Jul 22 22:02:17.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926507806s
Jul 22 22:02:18.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.922915929s
Jul 22 22:02:19.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.919483149s
Jul 22 22:02:20.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916434664s
Jul 22 22:02:21.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.264179ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9538
Jul 22 22:02:22.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 22:02:22.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 22:02:22.406: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 22:02:22.406: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 22:02:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 22:02:22.643: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 22:02:22.643: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 22:02:22.643: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 22:02:22.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-425149679 exec --namespace=statefulset-9538 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 22:02:22.880: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 22:02:22.880: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 22:02:22.880: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 22:02:22.880: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 22 22:03:02.892: INFO: Deleting all statefulset in ns statefulset-9538
Jul 22 22:03:02.894: INFO: Scaling statefulset ss to 0
Jul 22 22:03:02.902: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 22:03:02.904: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 22:03:02.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9538" for this suite.
Jul 22 22:03:08.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 22:03:09.006: INFO: namespace statefulset-9538 deletion completed in 6.088520591s

• [SLOW TEST:108.859 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 22:03:09.007: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 22 22:03:13.175: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-35f6d23a-f1e8-4bb5-95cc-92c45df4c810,GenerateName:,Namespace:events-7541,SelfLink:/api/v1/namespaces/events-7541/pods/send-events-35f6d23a-f1e8-4bb5-95cc-92c45df4c810,UID:0aaac73b-59d2-45b8-9f0f-13f11c5d3826,ResourceVersion:29085,Generation:0,CreationTimestamp:2019-07-22 22:03:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 151917390,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7w9p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7w9p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-h7w9p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003169150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003169170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:03:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:03:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:03:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:03:09 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.28,StartTime:2019-07-22 22:03:09 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-22 22:03:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://e24634dd63f9a35c7f611a2f7cd2773cf76b30cea08c1b5f207654889feb60a4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 22 22:03:15.179: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 22 22:03:17.182: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 22:03:17.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7541" for this suite.
Jul 22 22:03:59.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 22:03:59.454: INFO: namespace events-7541 deletion completed in 42.256040578s

• [SLOW TEST:50.446 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 22 22:03:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-425149679
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 22 22:03:59.600: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 22 22:03:59.606: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 22 22:04:04.609: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 22:04:04.609: INFO: Creating deployment "test-rolling-update-deployment"
Jul 22 22:04:04.613: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 22 22:04:04.618: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 22 22:04:06.624: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 22 22:04:06.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699429844, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699429844, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699429844, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699429844, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 22:04:08.629: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 22 22:04:08.639: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5703,SelfLink:/apis/apps/v1/namespaces/deployment-5703/deployments/test-rolling-update-deployment,UID:0ec3cdc0-9557-4a3d-97bc-c114302638ba,ResourceVersion:29234,Generation:1,CreationTimestamp:2019-07-22 22:04:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-22 22:04:04 +0000 UTC 2019-07-22 22:04:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-22 22:04:07 +0000 UTC 2019-07-22 22:04:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 22:04:08.644: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5703,SelfLink:/apis/apps/v1/namespaces/deployment-5703/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:417f59bf-01bd-4bf3-bd35-d72480938ac6,ResourceVersion:29223,Generation:1,CreationTimestamp:2019-07-22 22:04:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0ec3cdc0-9557-4a3d-97bc-c114302638ba 0xc002343f07 0xc002343f08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 22:04:08.644: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 22 22:04:08.644: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5703,SelfLink:/apis/apps/v1/namespaces/deployment-5703/replicasets/test-rolling-update-controller,UID:7a1fb63a-158b-428b-956c-39a3940a3f93,ResourceVersion:29233,Generation:2,CreationTimestamp:2019-07-22 22:03:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0ec3cdc0-9557-4a3d-97bc-c114302638ba 0xc002343e37 0xc002343e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 22:04:08.647: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-77m9t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-77m9t,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5703,SelfLink:/api/v1/namespaces/deployment-5703/pods/test-rolling-update-deployment-79f6b9d75c-77m9t,UID:4d527d4e-b59f-49ec-a3f1-fcf7c8b118da,ResourceVersion:29222,Generation:0,CreationTimestamp:2019-07-22 22:04:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 417f59bf-01bd-4bf3-bd35-d72480938ac6 0xc000785297 0xc000785298}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p2tjz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2tjz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-p2tjz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16111918-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000785360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000785390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:04:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:04:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:04:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 22:04:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.30,StartTime:2019-07-22 22:04:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-22 22:04:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ad35a01bcf65610183d8cf4741b5584a000b1578e0a677843fd8fe11071c79f0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 22 22:04:08.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5703" for this suite.
Jul 22 22:04:14.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 22:04:14.757: INFO: namespace deployment-5703 deletion completed in 6.106838663s

• [SLOW TEST:15.302 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSJul 22 22:04:14.758: INFO: Running AfterSuite actions on all nodes
Jul 22 22:04:14.758: INFO: Running AfterSuite actions on node 1
Jul 22 22:04:14.758: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5699.729 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h35m1.530267448s
Test Suite Passed
