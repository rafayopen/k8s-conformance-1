I0619 20:56:45.786034      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-497557601
I0619 20:56:45.786253      16 e2e.go:241] Starting e2e run "8818dd78-99e0-4c5d-b5a2-67ebdd68c3c4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1560977804 - Will randomize all specs
Will run 215 of 4411 specs

Jun 19 20:56:45.976: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 20:56:45.978: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 19 20:56:46.002: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 19 20:56:46.028: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 19 20:56:46.028: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun 19 20:56:46.028: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 19 20:56:46.034: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-cni-networkmonitor' (0 seconds elapsed)
Jun 19 20:56:46.035: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
Jun 19 20:56:46.035: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'blobfuse-flexvol-installer' (0 seconds elapsed)
Jun 19 20:56:46.035: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'keyvault-flexvolume' (0 seconds elapsed)
Jun 19 20:56:46.035: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 19 20:56:46.035: INFO: e2e test version: v1.15.0
Jun 19 20:56:46.036: INFO: kube-apiserver version: v1.15.0
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 20:56:46.037: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
Jun 19 20:56:46.085: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 19 20:56:46.096: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6528
STEP: Creating secret with name secret-test-905258c2-3f0e-4c19-9848-73c2b9eaad17
STEP: Creating a pod to test consume secrets
Jun 19 20:56:46.362: INFO: Waiting up to 5m0s for pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6" in namespace "secrets-7961" to be "success or failure"
Jun 19 20:56:46.368: INFO: Pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.119908ms
Jun 19 20:56:48.371: INFO: Pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009052724s
Jun 19 20:56:50.374: INFO: Pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011575552s
Jun 19 20:56:52.377: INFO: Pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01452898s
STEP: Saw pod success
Jun 19 20:56:52.377: INFO: Pod "pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6" satisfied condition "success or failure"
Jun 19 20:56:52.379: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6 container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 20:56:52.416: INFO: Waiting for pod pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6 to disappear
Jun 19 20:56:52.443: INFO: Pod pod-secrets-03734ffb-2b20-48dc-be61-a51efe5f5bf6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 20:56:52.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7961" for this suite.
Jun 19 20:56:58.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 20:56:58.547: INFO: namespace secrets-7961 deletion completed in 6.098425586s
STEP: Destroying namespace "secret-namespace-6528" for this suite.
Jun 19 20:57:04.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 20:57:04.630: INFO: namespace secret-namespace-6528 deletion completed in 6.082977677s

• [SLOW TEST:18.593 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 20:57:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-6198bb1f-237a-424f-b71c-36ac548ea103
STEP: Creating a pod to test consume secrets
Jun 19 20:57:04.791: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129" in namespace "projected-9628" to be "success or failure"
Jun 19 20:57:04.797: INFO: Pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559202ms
Jun 19 20:57:06.800: INFO: Pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009094784s
Jun 19 20:57:08.803: INFO: Pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012243462s
Jun 19 20:57:10.805: INFO: Pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014857155s
STEP: Saw pod success
Jun 19 20:57:10.805: INFO: Pod "pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129" satisfied condition "success or failure"
Jun 19 20:57:10.808: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 19 20:57:10.852: INFO: Waiting for pod pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129 to disappear
Jun 19 20:57:10.862: INFO: Pod pod-projected-secrets-3f71e415-57a5-450c-89d3-d7e9497ef129 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 20:57:10.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9628" for this suite.
Jun 19 20:57:16.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 20:57:16.946: INFO: namespace projected-9628 deletion completed in 6.080373734s

• [SLOW TEST:12.316 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 20:57:16.946: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2383
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 19 20:57:17.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 19 20:57:47.188: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.69:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 20:57:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 20:57:47.317: INFO: Found all expected endpoints: [netserver-0]
Jun 19 20:57:47.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.105:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 20:57:47.319: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 20:57:47.446: INFO: Found all expected endpoints: [netserver-1]
Jun 19 20:57:47.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 20:57:47.448: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 20:57:47.565: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 20:57:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2383" for this suite.
Jun 19 20:58:09.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 20:58:09.652: INFO: namespace pod-network-test-2383 deletion completed in 22.082150951s

• [SLOW TEST:52.706 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 20:58:09.652: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 19 20:58:40.323: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0619 20:58:40.323874      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 20:58:40.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8926" for this suite.
Jun 19 20:58:46.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 20:58:46.403: INFO: namespace gc-8926 deletion completed in 6.076309479s

• [SLOW TEST:36.751 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 20:58:46.403: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3671
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-3d104f0b-878c-46c5-ad9b-1206af7afa4e
STEP: Creating configMap with name cm-test-opt-upd-cb09b804-ce65-42d9-a21c-eca716d64122
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3d104f0b-878c-46c5-ad9b-1206af7afa4e
STEP: Updating configmap cm-test-opt-upd-cb09b804-ce65-42d9-a21c-eca716d64122
STEP: Creating configMap with name cm-test-opt-create-5b0a4ca3-fade-49a6-9af6-60d1a522ebaa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:00:18.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3671" for this suite.
Jun 19 21:00:41.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:00:41.072: INFO: namespace configmap-3671 deletion completed in 22.08222192s

• [SLOW TEST:114.668 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:00:41.075: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5
Jun 19 21:00:41.246: INFO: Pod name my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5: Found 0 pods out of 1
Jun 19 21:00:46.263: INFO: Pod name my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5: Found 1 pods out of 1
Jun 19 21:00:46.263: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5" are running
Jun 19 21:00:46.271: INFO: Pod "my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5-z2vmq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 21:00:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 21:00:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 21:00:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 21:00:41 +0000 UTC Reason: Message:}])
Jun 19 21:00:46.271: INFO: Trying to dial the pod
Jun 19 21:00:51.292: INFO: Controller my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5: Got expected result from replica 1 [my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5-z2vmq]: "my-hostname-basic-1a62f26c-20ad-4c04-8df6-cc995fef1ae5-z2vmq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:00:51.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6768" for this suite.
Jun 19 21:00:57.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:00:57.403: INFO: namespace replication-controller-6768 deletion completed in 6.107263252s

• [SLOW TEST:16.328 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:00:57.403: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4194
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-48d65bdd-f567-4007-8037-ddbe055a8607
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:01:05.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4194" for this suite.
Jun 19 21:01:27.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:01:27.739: INFO: namespace configmap-4194 deletion completed in 22.08869368s

• [SLOW TEST:30.336 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:01:27.740: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 21:01:27.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8230'
Jun 19 21:01:29.607: INFO: stderr: ""
Jun 19 21:01:29.607: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 19 21:01:34.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pod e2e-test-nginx-pod --namespace=kubectl-8230 -o json'
Jun 19 21:01:34.734: INFO: stderr: ""
Jun 19 21:01:34.734: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-06-19T21:01:29Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8230\",\n        \"resourceVersion\": \"12429\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8230/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d37aea97-90e0-481e-9ab3-55288ebcf8f8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-nzk6m\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-pool1-37287165-vmss000002\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-nzk6m\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-nzk6m\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-19T21:01:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-19T21:01:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-19T21:01:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-19T21:01:29Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://74f92e06931da4d4deab1d2d452f70f1360395ac67961bcd92da9b4e507cfe82\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-19T21:01:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.96\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.104\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-19T21:01:30Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 19 21:01:34.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 replace -f - --namespace=kubectl-8230'
Jun 19 21:01:35.177: INFO: stderr: ""
Jun 19 21:01:35.177: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jun 19 21:01:35.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete pods e2e-test-nginx-pod --namespace=kubectl-8230'
Jun 19 21:01:39.969: INFO: stderr: ""
Jun 19 21:01:39.969: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:01:39.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8230" for this suite.
Jun 19 21:01:45.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:01:46.053: INFO: namespace kubectl-8230 deletion completed in 6.080343735s

• [SLOW TEST:18.314 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:01:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 19 21:01:46.223: INFO: Waiting up to 5m0s for pod "pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245" in namespace "emptydir-236" to be "success or failure"
Jun 19 21:01:46.227: INFO: Pod "pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638546ms
Jun 19 21:01:48.230: INFO: Pod "pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006484815s
Jun 19 21:01:50.243: INFO: Pod "pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020056732s
STEP: Saw pod success
Jun 19 21:01:50.243: INFO: Pod "pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245" satisfied condition "success or failure"
Jun 19 21:01:50.245: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245 container test-container: <nil>
STEP: delete the pod
Jun 19 21:01:50.279: INFO: Waiting for pod pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245 to disappear
Jun 19 21:01:50.281: INFO: Pod pod-2c0f9e96-3601-4692-9d07-fd3bb0e17245 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:01:50.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-236" for this suite.
Jun 19 21:01:56.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:01:56.374: INFO: namespace emptydir-236 deletion completed in 6.089164464s

• [SLOW TEST:10.321 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:01:56.378: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 19 21:01:56.532: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12523,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 19 21:01:56.532: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12523,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 19 21:02:06.538: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12539,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 19 21:02:06.538: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12539,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 19 21:02:16.544: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12555,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 19 21:02:16.544: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12555,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 19 21:02:26.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12571,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 19 21:02:26.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-a,UID:ca1d2233-19cf-4711-a35c-9fbb3311aeca,ResourceVersion:12571,Generation:0,CreationTimestamp:2019-06-19 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 19 21:02:36.555: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-b,UID:e06caadf-6819-4c1e-bb6c-479d060e24b0,ResourceVersion:12587,Generation:0,CreationTimestamp:2019-06-19 21:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 19 21:02:36.556: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-b,UID:e06caadf-6819-4c1e-bb6c-479d060e24b0,ResourceVersion:12587,Generation:0,CreationTimestamp:2019-06-19 21:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 19 21:02:46.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-b,UID:e06caadf-6819-4c1e-bb6c-479d060e24b0,ResourceVersion:12604,Generation:0,CreationTimestamp:2019-06-19 21:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 19 21:02:46.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7692,SelfLink:/api/v1/namespaces/watch-7692/configmaps/e2e-watch-test-configmap-b,UID:e06caadf-6819-4c1e-bb6c-479d060e24b0,ResourceVersion:12604,Generation:0,CreationTimestamp:2019-06-19 21:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:02:56.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7692" for this suite.
Jun 19 21:03:02.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:03:02.643: INFO: namespace watch-7692 deletion completed in 6.078321767s

• [SLOW TEST:66.265 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:03:02.645: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-80a4f0f5-1a05-4a70-9033-8eefcd9adf03
STEP: Creating a pod to test consume configMaps
Jun 19 21:03:02.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82" in namespace "configmap-1163" to be "success or failure"
Jun 19 21:03:02.818: INFO: Pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180354ms
Jun 19 21:03:04.821: INFO: Pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005835957s
Jun 19 21:03:06.823: INFO: Pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00800657s
Jun 19 21:03:08.826: INFO: Pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011019274s
STEP: Saw pod success
Jun 19 21:03:08.826: INFO: Pod "pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82" satisfied condition "success or failure"
Jun 19 21:03:08.828: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:03:08.867: INFO: Waiting for pod pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82 to disappear
Jun 19 21:03:08.869: INFO: Pod pod-configmaps-34ac5660-97c1-4327-b6ef-9e59902b3e82 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:03:08.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1163" for this suite.
Jun 19 21:03:14.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:03:14.966: INFO: namespace configmap-1163 deletion completed in 6.093320607s

• [SLOW TEST:12.321 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:03:14.966: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:03:15.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444" in namespace "downward-api-8571" to be "success or failure"
Jun 19 21:03:15.133: INFO: Pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444": Phase="Pending", Reason="", readiness=false. Elapsed: 9.622761ms
Jun 19 21:03:17.136: INFO: Pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012787976s
Jun 19 21:03:19.138: INFO: Pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015251404s
Jun 19 21:03:21.146: INFO: Pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022477766s
STEP: Saw pod success
Jun 19 21:03:21.146: INFO: Pod "downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444" satisfied condition "success or failure"
Jun 19 21:03:21.148: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444 container client-container: <nil>
STEP: delete the pod
Jun 19 21:03:21.177: INFO: Waiting for pod downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444 to disappear
Jun 19 21:03:21.186: INFO: Pod downwardapi-volume-1475ed6c-a9f0-4b3e-8cd4-c7997bc08444 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:03:21.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8571" for this suite.
Jun 19 21:03:27.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:03:27.275: INFO: namespace downward-api-8571 deletion completed in 6.07917187s

• [SLOW TEST:12.309 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:03:27.275: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-983
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 19 21:03:27.433: INFO: Waiting up to 5m0s for pod "pod-d5d360c9-e989-4793-9f37-22fecfdc6aee" in namespace "emptydir-983" to be "success or failure"
Jun 19 21:03:27.477: INFO: Pod "pod-d5d360c9-e989-4793-9f37-22fecfdc6aee": Phase="Pending", Reason="", readiness=false. Elapsed: 44.242458ms
Jun 19 21:03:29.480: INFO: Pod "pod-d5d360c9-e989-4793-9f37-22fecfdc6aee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046780101s
Jun 19 21:03:31.483: INFO: Pod "pod-d5d360c9-e989-4793-9f37-22fecfdc6aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049718941s
STEP: Saw pod success
Jun 19 21:03:31.483: INFO: Pod "pod-d5d360c9-e989-4793-9f37-22fecfdc6aee" satisfied condition "success or failure"
Jun 19 21:03:31.489: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-d5d360c9-e989-4793-9f37-22fecfdc6aee container test-container: <nil>
STEP: delete the pod
Jun 19 21:03:31.511: INFO: Waiting for pod pod-d5d360c9-e989-4793-9f37-22fecfdc6aee to disappear
Jun 19 21:03:31.514: INFO: Pod pod-d5d360c9-e989-4793-9f37-22fecfdc6aee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:03:31.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-983" for this suite.
Jun 19 21:03:37.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:03:37.601: INFO: namespace emptydir-983 deletion completed in 6.083170761s

• [SLOW TEST:10.326 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:03:37.602: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:03:43.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3180" for this suite.
Jun 19 21:03:49.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:03:49.480: INFO: namespace watch-3180 deletion completed in 6.173275307s

• [SLOW TEST:11.879 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:03:49.481: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jun 19 21:03:49.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 --namespace=kubectl-9395 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 19 21:03:56.403: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 19 21:03:56.403: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:03:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9395" for this suite.
Jun 19 21:04:04.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:04:04.508: INFO: namespace kubectl-9395 deletion completed in 6.093558929s

• [SLOW TEST:15.028 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:04:04.508: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:04:04.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7554" for this suite.
Jun 19 21:04:10.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:04:10.772: INFO: namespace kubelet-test-7554 deletion completed in 6.080967238s

• [SLOW TEST:6.264 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:04:10.772: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca in namespace container-probe-2552
Jun 19 21:04:16.937: INFO: Started pod liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca in namespace container-probe-2552
STEP: checking the pod's current state and verifying that restartCount is present
Jun 19 21:04:16.940: INFO: Initial restart count of pod liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is 0
Jun 19 21:04:28.963: INFO: Restart count of pod container-probe-2552/liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is now 1 (12.02328981s elapsed)
Jun 19 21:04:48.993: INFO: Restart count of pod container-probe-2552/liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is now 2 (32.053457767s elapsed)
Jun 19 21:05:09.023: INFO: Restart count of pod container-probe-2552/liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is now 3 (52.082979191s elapsed)
Jun 19 21:05:29.055: INFO: Restart count of pod container-probe-2552/liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is now 4 (1m12.115937916s elapsed)
Jun 19 21:06:29.146: INFO: Restart count of pod container-probe-2552/liveness-c0d2b212-5ffb-42bc-82e1-59aeba3426ca is now 5 (2m12.205949618s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:06:29.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2552" for this suite.
Jun 19 21:06:35.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:06:35.248: INFO: namespace container-probe-2552 deletion completed in 6.089119856s

• [SLOW TEST:144.476 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:06:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-3d135729-ee83-4709-bc8e-b8e126e7a0be in namespace container-probe-1423
Jun 19 21:06:41.413: INFO: Started pod test-webserver-3d135729-ee83-4709-bc8e-b8e126e7a0be in namespace container-probe-1423
STEP: checking the pod's current state and verifying that restartCount is present
Jun 19 21:06:41.419: INFO: Initial restart count of pod test-webserver-3d135729-ee83-4709-bc8e-b8e126e7a0be is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:10:41.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1423" for this suite.
Jun 19 21:10:47.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:10:47.914: INFO: namespace container-probe-1423 deletion completed in 6.088557505s

• [SLOW TEST:252.665 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:10:47.916: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 19 21:10:48.067: INFO: Waiting up to 5m0s for pod "pod-00763e2d-900f-4734-872f-a7d26987ce54" in namespace "emptydir-1422" to be "success or failure"
Jun 19 21:10:48.071: INFO: Pod "pod-00763e2d-900f-4734-872f-a7d26987ce54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.295553ms
Jun 19 21:10:50.074: INFO: Pod "pod-00763e2d-900f-4734-872f-a7d26987ce54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006458634s
Jun 19 21:10:52.077: INFO: Pod "pod-00763e2d-900f-4734-872f-a7d26987ce54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009256021s
Jun 19 21:10:54.080: INFO: Pod "pod-00763e2d-900f-4734-872f-a7d26987ce54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012355306s
STEP: Saw pod success
Jun 19 21:10:54.080: INFO: Pod "pod-00763e2d-900f-4734-872f-a7d26987ce54" satisfied condition "success or failure"
Jun 19 21:10:54.082: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-00763e2d-900f-4734-872f-a7d26987ce54 container test-container: <nil>
STEP: delete the pod
Jun 19 21:10:54.098: INFO: Waiting for pod pod-00763e2d-900f-4734-872f-a7d26987ce54 to disappear
Jun 19 21:10:54.101: INFO: Pod pod-00763e2d-900f-4734-872f-a7d26987ce54 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:10:54.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1422" for this suite.
Jun 19 21:11:00.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:11:00.201: INFO: namespace emptydir-1422 deletion completed in 6.092834067s

• [SLOW TEST:12.285 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:11:00.201: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jun 19 21:11:00.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-7440'
Jun 19 21:11:00.730: INFO: stderr: ""
Jun 19 21:11:00.730: INFO: stdout: "pod/pause created\n"
Jun 19 21:11:00.730: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 19 21:11:00.730: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7440" to be "running and ready"
Jun 19 21:11:00.743: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.950915ms
Jun 19 21:11:02.751: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020952335s
Jun 19 21:11:04.754: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023874828s
Jun 19 21:11:06.757: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.027068219s
Jun 19 21:11:06.757: INFO: Pod "pause" satisfied condition "running and ready"
Jun 19 21:11:06.757: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 19 21:11:06.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 label pods pause testing-label=testing-label-value --namespace=kubectl-7440'
Jun 19 21:11:06.850: INFO: stderr: ""
Jun 19 21:11:06.850: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 19 21:11:06.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pod pause -L testing-label --namespace=kubectl-7440'
Jun 19 21:11:06.939: INFO: stderr: ""
Jun 19 21:11:06.939: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 19 21:11:06.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 label pods pause testing-label- --namespace=kubectl-7440'
Jun 19 21:11:07.032: INFO: stderr: ""
Jun 19 21:11:07.032: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 19 21:11:07.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pod pause -L testing-label --namespace=kubectl-7440'
Jun 19 21:11:07.111: INFO: stderr: ""
Jun 19 21:11:07.112: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jun 19 21:11:07.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-7440'
Jun 19 21:11:07.203: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:11:07.203: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 19 21:11:07.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=pause --no-headers --namespace=kubectl-7440'
Jun 19 21:11:07.288: INFO: stderr: "No resources found.\n"
Jun 19 21:11:07.288: INFO: stdout: ""
Jun 19 21:11:07.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=pause --namespace=kubectl-7440 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 21:11:07.372: INFO: stderr: ""
Jun 19 21:11:07.372: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:11:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7440" for this suite.
Jun 19 21:11:13.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:11:13.462: INFO: namespace kubectl-7440 deletion completed in 6.084550911s

• [SLOW TEST:13.261 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:11:13.463: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-b3ed5e9d-89dd-4bd0-9e02-20e59d04d163
STEP: Creating a pod to test consume configMaps
Jun 19 21:11:13.668: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248" in namespace "projected-4364" to be "success or failure"
Jun 19 21:11:13.687: INFO: Pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248": Phase="Pending", Reason="", readiness=false. Elapsed: 19.269524ms
Jun 19 21:11:15.691: INFO: Pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022788716s
Jun 19 21:11:17.696: INFO: Pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027835388s
Jun 19 21:11:19.699: INFO: Pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030887289s
STEP: Saw pod success
Jun 19 21:11:19.699: INFO: Pod "pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248" satisfied condition "success or failure"
Jun 19 21:11:19.701: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:11:19.724: INFO: Waiting for pod pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248 to disappear
Jun 19 21:11:19.726: INFO: Pod pod-projected-configmaps-3dafb557-c0ed-48b1-9fd5-86e0b465a248 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:11:19.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4364" for this suite.
Jun 19 21:11:25.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:11:25.811: INFO: namespace projected-4364 deletion completed in 6.080994885s

• [SLOW TEST:12.348 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:11:25.815: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6lg5
STEP: Creating a pod to test atomic-volume-subpath
Jun 19 21:11:25.994: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6lg5" in namespace "subpath-777" to be "success or failure"
Jun 19 21:11:26.008: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.609406ms
Jun 19 21:11:28.011: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017064906s
Jun 19 21:11:30.014: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020195412s
Jun 19 21:11:32.017: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.022986424s
Jun 19 21:11:34.020: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.025750238s
Jun 19 21:11:36.023: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.028870748s
Jun 19 21:11:38.026: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.032022558s
Jun 19 21:11:40.029: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.03517037s
Jun 19 21:11:42.032: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.038240184s
Jun 19 21:11:44.035: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.040931204s
Jun 19 21:11:46.038: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.043891222s
Jun 19 21:11:48.041: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 22.047105638s
Jun 19 21:11:50.044: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Running", Reason="", readiness=true. Elapsed: 24.050200056s
Jun 19 21:11:52.047: INFO: Pod "pod-subpath-test-configmap-6lg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.053204976s
STEP: Saw pod success
Jun 19 21:11:52.047: INFO: Pod "pod-subpath-test-configmap-6lg5" satisfied condition "success or failure"
Jun 19 21:11:52.050: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-subpath-test-configmap-6lg5 container test-container-subpath-configmap-6lg5: <nil>
STEP: delete the pod
Jun 19 21:11:52.072: INFO: Waiting for pod pod-subpath-test-configmap-6lg5 to disappear
Jun 19 21:11:52.074: INFO: Pod pod-subpath-test-configmap-6lg5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6lg5
Jun 19 21:11:52.074: INFO: Deleting pod "pod-subpath-test-configmap-6lg5" in namespace "subpath-777"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:11:52.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-777" for this suite.
Jun 19 21:11:58.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:11:58.178: INFO: namespace subpath-777 deletion completed in 6.098523592s

• [SLOW TEST:32.363 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:11:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:11:58.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7" in namespace "projected-2243" to be "success or failure"
Jun 19 21:11:58.377: INFO: Pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.883859ms
Jun 19 21:12:00.380: INFO: Pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005559489s
Jun 19 21:12:02.383: INFO: Pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008561316s
Jun 19 21:12:04.386: INFO: Pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012043236s
STEP: Saw pod success
Jun 19 21:12:04.386: INFO: Pod "downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7" satisfied condition "success or failure"
Jun 19 21:12:04.389: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7 container client-container: <nil>
STEP: delete the pod
Jun 19 21:12:04.411: INFO: Waiting for pod downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7 to disappear
Jun 19 21:12:04.414: INFO: Pod downwardapi-volume-e7edfffb-394e-4024-94b9-fff213eaa8c7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:12:04.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2243" for this suite.
Jun 19 21:12:10.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:12:10.505: INFO: namespace projected-2243 deletion completed in 6.088485855s

• [SLOW TEST:12.327 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:12:10.506: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-66ebbca8-942e-4e00-9e40-6a27d9fd182e
STEP: Creating a pod to test consume configMaps
Jun 19 21:12:10.666: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e" in namespace "configmap-3209" to be "success or failure"
Jun 19 21:12:10.668: INFO: Pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068871ms
Jun 19 21:12:12.671: INFO: Pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005053403s
Jun 19 21:12:14.674: INFO: Pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007832239s
Jun 19 21:12:16.677: INFO: Pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010739875s
STEP: Saw pod success
Jun 19 21:12:16.677: INFO: Pod "pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e" satisfied condition "success or failure"
Jun 19 21:12:16.679: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:12:16.694: INFO: Waiting for pod pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e to disappear
Jun 19 21:12:16.702: INFO: Pod pod-configmaps-9cc6ebc7-eff5-43ea-89ff-52e224ced72e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:12:16.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3209" for this suite.
Jun 19 21:12:22.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:12:22.787: INFO: namespace configmap-3209 deletion completed in 6.081567574s

• [SLOW TEST:12.282 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:12:22.788: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 19 21:12:22.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-6927'
Jun 19 21:12:25.257: INFO: stderr: ""
Jun 19 21:12:25.257: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 19 21:12:26.261: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:26.261: INFO: Found 0 / 1
Jun 19 21:12:27.260: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:27.260: INFO: Found 0 / 1
Jun 19 21:12:28.261: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:28.261: INFO: Found 0 / 1
Jun 19 21:12:29.261: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:29.262: INFO: Found 0 / 1
Jun 19 21:12:30.260: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:30.260: INFO: Found 1 / 1
Jun 19 21:12:30.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 19 21:12:30.263: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:30.263: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 19 21:12:30.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 patch pod redis-master-jnv4r --namespace=kubectl-6927 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 19 21:12:30.347: INFO: stderr: ""
Jun 19 21:12:30.347: INFO: stdout: "pod/redis-master-jnv4r patched\n"
STEP: checking annotations
Jun 19 21:12:30.350: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:12:30.350: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:12:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6927" for this suite.
Jun 19 21:12:52.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:12:52.430: INFO: namespace kubectl-6927 deletion completed in 22.074429436s

• [SLOW TEST:29.641 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:12:52.430: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 21:12:52.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4752'
Jun 19 21:12:52.658: INFO: stderr: ""
Jun 19 21:12:52.658: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jun 19 21:12:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete pods e2e-test-nginx-pod --namespace=kubectl-4752'
Jun 19 21:12:58.498: INFO: stderr: ""
Jun 19 21:12:58.498: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:12:58.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4752" for this suite.
Jun 19 21:13:04.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:13:04.606: INFO: namespace kubectl-4752 deletion completed in 6.098932393s

• [SLOW TEST:12.176 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:13:04.606: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:13:04.767: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 19 21:13:04.784: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:04.796: INFO: Number of nodes with available pods: 0
Jun 19 21:13:04.796: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:05.801: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:05.803: INFO: Number of nodes with available pods: 0
Jun 19 21:13:05.803: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:06.801: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:06.804: INFO: Number of nodes with available pods: 0
Jun 19 21:13:06.804: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:07.800: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:07.803: INFO: Number of nodes with available pods: 0
Jun 19 21:13:07.803: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:08.800: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:08.803: INFO: Number of nodes with available pods: 0
Jun 19 21:13:08.803: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:09.802: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:09.805: INFO: Number of nodes with available pods: 2
Jun 19 21:13:09.805: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:10.802: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:10.806: INFO: Number of nodes with available pods: 2
Jun 19 21:13:10.806: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:11.800: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:11.803: INFO: Number of nodes with available pods: 2
Jun 19 21:13:11.803: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:12.800: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:12.803: INFO: Number of nodes with available pods: 2
Jun 19 21:13:12.803: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:13:13.800: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:13.803: INFO: Number of nodes with available pods: 3
Jun 19 21:13:13.803: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 19 21:13:13.863: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:13.863: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:13.863: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:13.870: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:14.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:14.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:14.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:14.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:15.873: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:15.873: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:15.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:15.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:16.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:16.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:16.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:16.874: INFO: Pod daemon-set-v8brj is not available
Jun 19 21:13:16.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:17.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:17.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:17.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:17.874: INFO: Pod daemon-set-v8brj is not available
Jun 19 21:13:17.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:18.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:18.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:18.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:18.874: INFO: Pod daemon-set-v8brj is not available
Jun 19 21:13:18.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:19.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:19.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:19.874: INFO: Wrong image for pod: daemon-set-v8brj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:19.874: INFO: Pod daemon-set-v8brj is not available
Jun 19 21:13:19.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:20.874: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:20.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:20.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:20.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:21.875: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:21.875: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:21.875: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:21.879: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:22.874: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:22.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:22.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:22.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:23.874: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:23.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:23.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:23.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:24.874: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:24.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:24.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:24.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:25.874: INFO: Pod daemon-set-2ppcb is not available
Jun 19 21:13:25.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:25.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:25.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:26.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:26.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:26.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:27.873: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:27.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:27.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:28.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:28.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:28.879: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:29.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:29.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:29.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:29.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:30.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:30.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:30.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:30.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:31.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:31.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:31.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:31.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:32.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:32.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:32.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:32.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:33.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:33.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:33.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:33.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:34.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:34.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:34.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:34.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:35.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:35.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:35.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:35.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:36.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:36.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:36.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:36.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:37.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:37.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:37.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:37.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:38.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:38.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:38.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:38.879: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:39.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:39.874: INFO: Wrong image for pod: daemon-set-rmfvj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:39.874: INFO: Pod daemon-set-rmfvj is not available
Jun 19 21:13:39.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:40.876: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:40.876: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:40.891: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:41.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:41.874: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:41.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:42.875: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:42.875: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:42.879: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:43.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:43.874: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:43.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:44.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:44.874: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:44.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:45.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:45.874: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:45.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:46.873: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:46.873: INFO: Pod daemon-set-zbjb4 is not available
Jun 19 21:13:46.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:47.873: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:47.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:48.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:48.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:49.874: INFO: Wrong image for pod: daemon-set-c6j5c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 19 21:13:49.874: INFO: Pod daemon-set-c6j5c is not available
Jun 19 21:13:49.877: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:50.875: INFO: Pod daemon-set-tcqwv is not available
Jun 19 21:13:50.878: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 19 21:13:50.881: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:50.883: INFO: Number of nodes with available pods: 2
Jun 19 21:13:50.883: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 21:13:51.887: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:51.890: INFO: Number of nodes with available pods: 2
Jun 19 21:13:51.890: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 21:13:52.888: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:52.891: INFO: Number of nodes with available pods: 2
Jun 19 21:13:52.891: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 21:13:53.888: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:53.891: INFO: Number of nodes with available pods: 2
Jun 19 21:13:53.891: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 21:13:54.888: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:54.890: INFO: Number of nodes with available pods: 2
Jun 19 21:13:54.891: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 21:13:55.888: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:13:55.891: INFO: Number of nodes with available pods: 3
Jun 19 21:13:55.891: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8479, will wait for the garbage collector to delete the pods
Jun 19 21:13:55.959: INFO: Deleting DaemonSet.extensions daemon-set took: 4.90643ms
Jun 19 21:13:56.260: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.254526ms
Jun 19 21:14:10.063: INFO: Number of nodes with available pods: 0
Jun 19 21:14:10.063: INFO: Number of running nodes: 0, number of available pods: 0
Jun 19 21:14:10.065: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8479/daemonsets","resourceVersion":"14364"},"items":null}

Jun 19 21:14:10.072: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8479/pods","resourceVersion":"14364"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:14:10.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8479" for this suite.
Jun 19 21:14:16.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:14:16.165: INFO: namespace daemonsets-8479 deletion completed in 6.079762366s

• [SLOW TEST:71.559 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:14:16.170: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:14:16.327: INFO: Creating deployment "test-recreate-deployment"
Jun 19 21:14:16.330: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 19 21:14:16.345: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 19 21:14:18.350: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 19 21:14:18.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:14:20.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696575656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:14:22.355: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 19 21:14:22.361: INFO: Updating deployment test-recreate-deployment
Jun 19 21:14:22.361: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 19 21:14:22.454: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-2665,SelfLink:/apis/apps/v1/namespaces/deployment-2665/deployments/test-recreate-deployment,UID:56dee6dd-82c4-4a7a-b7b6-57250b63f778,ResourceVersion:14475,Generation:2,CreationTimestamp:2019-06-19 21:14:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-19 21:14:22 +0000 UTC 2019-06-19 21:14:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-19 21:14:22 +0000 UTC 2019-06-19 21:14:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 19 21:14:22.456: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-2665,SelfLink:/apis/apps/v1/namespaces/deployment-2665/replicasets/test-recreate-deployment-5c8c9cc69d,UID:248bef33-6ff8-4c8d-ab3a-bf9adadfc91c,ResourceVersion:14474,Generation:1,CreationTimestamp:2019-06-19 21:14:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 56dee6dd-82c4-4a7a-b7b6-57250b63f778 0xc0038246c7 0xc0038246c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 21:14:22.456: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 19 21:14:22.457: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-2665,SelfLink:/apis/apps/v1/namespaces/deployment-2665/replicasets/test-recreate-deployment-6df85df6b9,UID:6a39d3d7-4dc9-4832-aab1-b7a431e43bf0,ResourceVersion:14463,Generation:2,CreationTimestamp:2019-06-19 21:14:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 56dee6dd-82c4-4a7a-b7b6-57250b63f778 0xc003824797 0xc003824798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 21:14:22.459: INFO: Pod "test-recreate-deployment-5c8c9cc69d-wrnhp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-wrnhp,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-2665,SelfLink:/api/v1/namespaces/deployment-2665/pods/test-recreate-deployment-5c8c9cc69d-wrnhp,UID:083644f3-de62-4c4e-961d-8be4911b5738,ResourceVersion:14469,Generation:0,CreationTimestamp:2019-06-19 21:14:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 248bef33-6ff8-4c8d-ab3a-bf9adadfc91c 0xc0038250b7 0xc0038250b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p4rlh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p4rlh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p4rlh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003825120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003825140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:14:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:14:22.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2665" for this suite.
Jun 19 21:14:28.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:14:28.558: INFO: namespace deployment-2665 deletion completed in 6.09539136s

• [SLOW TEST:12.388 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:14:28.559: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7203.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7203.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7203.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7203.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7203.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7203.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 21:14:54.761: INFO: DNS probes using dns-7203/dns-test-68ffcd17-bb8c-49db-9395-5765f5682324 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:14:54.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7203" for this suite.
Jun 19 21:15:00.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:15:00.878: INFO: namespace dns-7203 deletion completed in 6.089123588s

• [SLOW TEST:32.319 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:15:00.878: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 19 21:15:01.025: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:15:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5697" for this suite.
Jun 19 21:15:16.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:15:16.687: INFO: namespace init-container-5697 deletion completed in 6.085724388s

• [SLOW TEST:15.809 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:15:16.690: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b1b6e50b-7525-4d07-ab90-09e29c00158b
STEP: Creating a pod to test consume configMaps
Jun 19 21:15:16.851: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee" in namespace "projected-164" to be "success or failure"
Jun 19 21:15:16.858: INFO: Pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.201397ms
Jun 19 21:15:18.860: INFO: Pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009128593s
Jun 19 21:15:20.863: INFO: Pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012052507s
Jun 19 21:15:22.868: INFO: Pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016714809s
STEP: Saw pod success
Jun 19 21:15:22.868: INFO: Pod "pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee" satisfied condition "success or failure"
Jun 19 21:15:22.871: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:15:22.885: INFO: Waiting for pod pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee to disappear
Jun 19 21:15:22.893: INFO: Pod pod-projected-configmaps-dd997b95-26c6-40ed-82bd-bb6d0500baee no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:15:22.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-164" for this suite.
Jun 19 21:15:28.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:15:28.977: INFO: namespace projected-164 deletion completed in 6.080806202s

• [SLOW TEST:12.287 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:15:28.978: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:15:29.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152" in namespace "downward-api-9899" to be "success or failure"
Jun 19 21:15:29.150: INFO: Pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152": Phase="Pending", Reason="", readiness=false. Elapsed: 9.459443ms
Jun 19 21:15:31.153: INFO: Pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012870037s
Jun 19 21:15:33.157: INFO: Pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016025851s
Jun 19 21:15:35.160: INFO: Pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019576677s
STEP: Saw pod success
Jun 19 21:15:35.160: INFO: Pod "downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152" satisfied condition "success or failure"
Jun 19 21:15:35.163: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152 container client-container: <nil>
STEP: delete the pod
Jun 19 21:15:35.187: INFO: Waiting for pod downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152 to disappear
Jun 19 21:15:35.190: INFO: Pod downwardapi-volume-40aecb3d-5f5e-4e0a-bb3d-758859531152 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:15:35.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9899" for this suite.
Jun 19 21:15:41.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:15:41.274: INFO: namespace downward-api-9899 deletion completed in 6.080899917s

• [SLOW TEST:12.296 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:15:41.275: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 19 21:15:47.954: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5814 pod-service-account-ea054bf8-a078-4a27-8de4-ef801a9652b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 19 21:15:48.157: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5814 pod-service-account-ea054bf8-a078-4a27-8de4-ef801a9652b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 19 21:15:48.371: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5814 pod-service-account-ea054bf8-a078-4a27-8de4-ef801a9652b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:15:48.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5814" for this suite.
Jun 19 21:15:54.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:15:54.667: INFO: namespace svcaccounts-5814 deletion completed in 6.07982767s

• [SLOW TEST:13.392 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:15:54.667: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-cb2313e0-8e81-49f4-8984-a4bdc48a7a7c
STEP: Creating a pod to test consume secrets
Jun 19 21:15:54.823: INFO: Waiting up to 5m0s for pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c" in namespace "secrets-6117" to be "success or failure"
Jun 19 21:15:54.825: INFO: Pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822354ms
Jun 19 21:15:56.829: INFO: Pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005958165s
Jun 19 21:15:58.832: INFO: Pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009320289s
Jun 19 21:16:00.835: INFO: Pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012368433s
STEP: Saw pod success
Jun 19 21:16:00.835: INFO: Pod "pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c" satisfied condition "success or failure"
Jun 19 21:16:00.837: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:16:00.864: INFO: Waiting for pod pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c to disappear
Jun 19 21:16:00.867: INFO: Pod pod-secrets-e6250ae3-9ce7-4d12-9f2c-c31482f5503c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:16:00.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6117" for this suite.
Jun 19 21:16:06.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:16:06.949: INFO: namespace secrets-6117 deletion completed in 6.078030099s

• [SLOW TEST:12.281 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:16:06.950: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 19 21:16:07.094: INFO: namespace kubectl-3542
Jun 19 21:16:07.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-3542'
Jun 19 21:16:07.472: INFO: stderr: ""
Jun 19 21:16:07.472: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 19 21:16:08.475: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:16:08.475: INFO: Found 0 / 1
Jun 19 21:16:09.475: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:16:09.475: INFO: Found 0 / 1
Jun 19 21:16:10.475: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:16:10.475: INFO: Found 0 / 1
Jun 19 21:16:11.475: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:16:11.475: INFO: Found 1 / 1
Jun 19 21:16:11.475: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 19 21:16:11.478: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:16:11.478: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 19 21:16:11.478: INFO: wait on redis-master startup in kubectl-3542 
Jun 19 21:16:11.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 logs redis-master-brknm redis-master --namespace=kubectl-3542'
Jun 19 21:16:11.577: INFO: stderr: ""
Jun 19 21:16:11.577: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Jun 21:16:10.737 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Jun 21:16:10.737 # Server started, Redis version 3.2.12\n1:M 19 Jun 21:16:10.737 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Jun 21:16:10.737 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 19 21:16:11.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3542'
Jun 19 21:16:11.679: INFO: stderr: ""
Jun 19 21:16:11.679: INFO: stdout: "service/rm2 exposed\n"
Jun 19 21:16:11.686: INFO: Service rm2 in namespace kubectl-3542 found.
STEP: exposing service
Jun 19 21:16:13.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3542'
Jun 19 21:16:13.805: INFO: stderr: ""
Jun 19 21:16:13.805: INFO: stdout: "service/rm3 exposed\n"
Jun 19 21:16:13.807: INFO: Service rm3 in namespace kubectl-3542 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:16:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3542" for this suite.
Jun 19 21:16:37.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:16:37.895: INFO: namespace kubectl-3542 deletion completed in 22.078689775s

• [SLOW TEST:30.946 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:16:37.895: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 19 21:16:44.582: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e41caaea-bb76-46b3-b1f1-860b25551cab"
Jun 19 21:16:44.582: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e41caaea-bb76-46b3-b1f1-860b25551cab" in namespace "pods-9040" to be "terminated due to deadline exceeded"
Jun 19 21:16:44.585: INFO: Pod "pod-update-activedeadlineseconds-e41caaea-bb76-46b3-b1f1-860b25551cab": Phase="Running", Reason="", readiness=true. Elapsed: 2.882753ms
Jun 19 21:16:46.588: INFO: Pod "pod-update-activedeadlineseconds-e41caaea-bb76-46b3-b1f1-860b25551cab": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005632854s
Jun 19 21:16:46.588: INFO: Pod "pod-update-activedeadlineseconds-e41caaea-bb76-46b3-b1f1-860b25551cab" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:16:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9040" for this suite.
Jun 19 21:16:52.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:16:52.675: INFO: namespace pods-9040 deletion completed in 6.083456468s

• [SLOW TEST:14.779 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:16:52.675: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 21:16:52.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7433'
Jun 19 21:16:52.916: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 19 21:16:52.916: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jun 19 21:16:52.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete jobs e2e-test-nginx-job --namespace=kubectl-7433'
Jun 19 21:16:53.012: INFO: stderr: ""
Jun 19 21:16:53.012: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:16:53.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7433" for this suite.
Jun 19 21:16:59.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:16:59.099: INFO: namespace kubectl-7433 deletion completed in 6.081680839s

• [SLOW TEST:6.424 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:16:59.100: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jun 19 21:16:59.265: INFO: Waiting up to 5m0s for pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c" in namespace "var-expansion-5836" to be "success or failure"
Jun 19 21:16:59.268: INFO: Pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061551ms
Jun 19 21:17:01.271: INFO: Pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00626165s
Jun 19 21:17:03.274: INFO: Pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009445265s
Jun 19 21:17:05.278: INFO: Pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012766491s
STEP: Saw pod success
Jun 19 21:17:05.278: INFO: Pod "var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c" satisfied condition "success or failure"
Jun 19 21:17:05.281: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c container dapi-container: <nil>
STEP: delete the pod
Jun 19 21:17:05.302: INFO: Waiting for pod var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c to disappear
Jun 19 21:17:05.304: INFO: Pod var-expansion-96fee958-61ec-48e8-918f-8616eb1b559c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:17:05.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5836" for this suite.
Jun 19 21:17:11.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:17:11.398: INFO: namespace var-expansion-5836 deletion completed in 6.089936569s

• [SLOW TEST:12.298 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:17:11.398: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:17:17.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1879" for this suite.
Jun 19 21:18:01.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:18:01.661: INFO: namespace kubelet-test-1879 deletion completed in 44.086267358s

• [SLOW TEST:50.263 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:18:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6244 to expose endpoints map[]
Jun 19 21:18:01.829: INFO: Get endpoints failed (9.37845ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 19 21:18:02.832: INFO: successfully validated that service multi-endpoint-test in namespace services-6244 exposes endpoints map[] (1.012195488s elapsed)
STEP: Creating pod pod1 in namespace services-6244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6244 to expose endpoints map[pod1:[100]]
Jun 19 21:18:06.861: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.024509975s elapsed, will retry)
Jun 19 21:18:07.866: INFO: successfully validated that service multi-endpoint-test in namespace services-6244 exposes endpoints map[pod1:[100]] (5.0291429s elapsed)
STEP: Creating pod pod2 in namespace services-6244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6244 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 19 21:18:11.917: INFO: Unexpected endpoints: found map[22732d5a-0cc6-411f-ac16-1f65be46614d:[100]], expected map[pod1:[100] pod2:[101]] (4.046677083s elapsed, will retry)
Jun 19 21:18:13.930: INFO: successfully validated that service multi-endpoint-test in namespace services-6244 exposes endpoints map[pod1:[100] pod2:[101]] (6.060195901s elapsed)
STEP: Deleting pod pod1 in namespace services-6244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6244 to expose endpoints map[pod2:[101]]
Jun 19 21:18:13.969: INFO: successfully validated that service multi-endpoint-test in namespace services-6244 exposes endpoints map[pod2:[101]] (33.250668ms elapsed)
STEP: Deleting pod pod2 in namespace services-6244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6244 to expose endpoints map[]
Jun 19 21:18:13.978: INFO: successfully validated that service multi-endpoint-test in namespace services-6244 exposes endpoints map[] (4.603426ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:18:13.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6244" for this suite.
Jun 19 21:18:36.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:18:36.093: INFO: namespace services-6244 deletion completed in 22.085369323s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:34.432 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:18:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9230
I0619 21:18:36.298290      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9230, replica count: 1
I0619 21:18:37.348686      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 21:18:38.348847      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 21:18:39.349051      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 21:18:40.349271      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 21:18:41.349533      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 19 21:18:41.467: INFO: Created: latency-svc-mtmwc
Jun 19 21:18:41.473: INFO: Got endpoints: latency-svc-mtmwc [23.502826ms]
Jun 19 21:18:41.496: INFO: Created: latency-svc-tbtjk
Jun 19 21:18:41.499: INFO: Got endpoints: latency-svc-tbtjk [25.665592ms]
Jun 19 21:18:41.511: INFO: Created: latency-svc-9kjqs
Jun 19 21:18:41.521: INFO: Created: latency-svc-9tjvh
Jun 19 21:18:41.523: INFO: Got endpoints: latency-svc-9kjqs [47.328247ms]
Jun 19 21:18:41.536: INFO: Got endpoints: latency-svc-9tjvh [59.822049ms]
Jun 19 21:18:41.539: INFO: Created: latency-svc-nhqhb
Jun 19 21:18:41.547: INFO: Created: latency-svc-5w7q8
Jun 19 21:18:41.548: INFO: Got endpoints: latency-svc-nhqhb [71.552762ms]
Jun 19 21:18:41.556: INFO: Got endpoints: latency-svc-5w7q8 [79.154142ms]
Jun 19 21:18:41.565: INFO: Created: latency-svc-xbrdl
Jun 19 21:18:41.568: INFO: Got endpoints: latency-svc-xbrdl [91.709242ms]
Jun 19 21:18:41.580: INFO: Created: latency-svc-vqmv9
Jun 19 21:18:41.599: INFO: Created: latency-svc-thmxd
Jun 19 21:18:41.601: INFO: Got endpoints: latency-svc-vqmv9 [123.874131ms]
Jun 19 21:18:41.608: INFO: Got endpoints: latency-svc-thmxd [131.167715ms]
Jun 19 21:18:41.623: INFO: Created: latency-svc-bshp4
Jun 19 21:18:41.629: INFO: Created: latency-svc-j22bt
Jun 19 21:18:41.633: INFO: Got endpoints: latency-svc-bshp4 [156.367814ms]
Jun 19 21:18:41.639: INFO: Got endpoints: latency-svc-j22bt [161.729829ms]
Jun 19 21:18:41.649: INFO: Created: latency-svc-b665d
Jun 19 21:18:41.660: INFO: Created: latency-svc-999x5
Jun 19 21:18:41.663: INFO: Got endpoints: latency-svc-b665d [185.644848ms]
Jun 19 21:18:41.668: INFO: Got endpoints: latency-svc-999x5 [192.151746ms]
Jun 19 21:18:41.678: INFO: Created: latency-svc-svd45
Jun 19 21:18:41.709: INFO: Got endpoints: latency-svc-svd45 [232.10971ms]
Jun 19 21:18:41.717: INFO: Created: latency-svc-bwt2j
Jun 19 21:18:41.730: INFO: Created: latency-svc-gzsvx
Jun 19 21:18:41.732: INFO: Got endpoints: latency-svc-bwt2j [255.907132ms]
Jun 19 21:18:41.763: INFO: Created: latency-svc-xt4kr
Jun 19 21:18:41.766: INFO: Got endpoints: latency-svc-xt4kr [267.644145ms]
Jun 19 21:18:41.766: INFO: Got endpoints: latency-svc-gzsvx [289.551697ms]
Jun 19 21:18:41.779: INFO: Created: latency-svc-2nlqk
Jun 19 21:18:41.791: INFO: Created: latency-svc-c96b7
Jun 19 21:18:41.793: INFO: Got endpoints: latency-svc-2nlqk [270.164205ms]
Jun 19 21:18:41.801: INFO: Got endpoints: latency-svc-c96b7 [264.715092ms]
Jun 19 21:18:41.820: INFO: Created: latency-svc-sh2n2
Jun 19 21:18:41.836: INFO: Got endpoints: latency-svc-sh2n2 [287.219334ms]
Jun 19 21:18:41.839: INFO: Created: latency-svc-f89c8
Jun 19 21:18:41.856: INFO: Got endpoints: latency-svc-f89c8 [296.317889ms]
Jun 19 21:18:41.864: INFO: Created: latency-svc-skmvc
Jun 19 21:18:41.867: INFO: Got endpoints: latency-svc-skmvc [296.418887ms]
Jun 19 21:18:41.884: INFO: Created: latency-svc-v9spf
Jun 19 21:18:41.894: INFO: Got endpoints: latency-svc-v9spf [292.891844ms]
Jun 19 21:18:41.899: INFO: Created: latency-svc-wp97s
Jun 19 21:18:41.904: INFO: Got endpoints: latency-svc-wp97s [296.119093ms]
Jun 19 21:18:41.927: INFO: Created: latency-svc-r6dz4
Jun 19 21:18:41.935: INFO: Created: latency-svc-j68vp
Jun 19 21:18:41.939: INFO: Got endpoints: latency-svc-r6dz4 [305.429744ms]
Jun 19 21:18:41.946: INFO: Got endpoints: latency-svc-j68vp [306.592827ms]
Jun 19 21:18:41.954: INFO: Created: latency-svc-8scct
Jun 19 21:18:41.973: INFO: Got endpoints: latency-svc-8scct [309.972073ms]
Jun 19 21:18:41.978: INFO: Created: latency-svc-vpcbx
Jun 19 21:18:41.981: INFO: Got endpoints: latency-svc-vpcbx [312.465933ms]
Jun 19 21:18:41.996: INFO: Created: latency-svc-m2xm9
Jun 19 21:18:42.004: INFO: Got endpoints: latency-svc-m2xm9 [295.700099ms]
Jun 19 21:18:42.009: INFO: Created: latency-svc-j9b4q
Jun 19 21:18:42.019: INFO: Got endpoints: latency-svc-j9b4q [287.220234ms]
Jun 19 21:18:42.026: INFO: Created: latency-svc-8tkm8
Jun 19 21:18:42.034: INFO: Got endpoints: latency-svc-8tkm8 [265.763976ms]
Jun 19 21:18:42.050: INFO: Created: latency-svc-pst67
Jun 19 21:18:42.067: INFO: Created: latency-svc-l9fqp
Jun 19 21:18:42.068: INFO: Got endpoints: latency-svc-l9fqp [274.242141ms]
Jun 19 21:18:42.068: INFO: Got endpoints: latency-svc-pst67 [301.194412ms]
Jun 19 21:18:42.083: INFO: Created: latency-svc-cr46m
Jun 19 21:18:42.089: INFO: Got endpoints: latency-svc-cr46m [287.603528ms]
Jun 19 21:18:42.089: INFO: Created: latency-svc-szcws
Jun 19 21:18:42.103: INFO: Created: latency-svc-p4jw8
Jun 19 21:18:42.104: INFO: Got endpoints: latency-svc-szcws [268.443833ms]
Jun 19 21:18:42.110: INFO: Got endpoints: latency-svc-p4jw8 [253.466871ms]
Jun 19 21:18:42.119: INFO: Created: latency-svc-9lxsn
Jun 19 21:18:42.131: INFO: Got endpoints: latency-svc-9lxsn [263.265115ms]
Jun 19 21:18:42.133: INFO: Created: latency-svc-4782q
Jun 19 21:18:42.154: INFO: Got endpoints: latency-svc-4782q [259.82027ms]
Jun 19 21:18:42.154: INFO: Created: latency-svc-wvhpg
Jun 19 21:18:42.156: INFO: Got endpoints: latency-svc-wvhpg [252.155491ms]
Jun 19 21:18:42.166: INFO: Created: latency-svc-jbk7j
Jun 19 21:18:42.180: INFO: Got endpoints: latency-svc-jbk7j [241.290064ms]
Jun 19 21:18:42.190: INFO: Created: latency-svc-6jw7w
Jun 19 21:18:42.199: INFO: Got endpoints: latency-svc-6jw7w [250.650716ms]
Jun 19 21:18:42.209: INFO: Created: latency-svc-58vts
Jun 19 21:18:42.217: INFO: Got endpoints: latency-svc-58vts [243.984722ms]
Jun 19 21:18:42.228: INFO: Created: latency-svc-5kgfk
Jun 19 21:18:42.237: INFO: Created: latency-svc-rg5m2
Jun 19 21:18:42.238: INFO: Got endpoints: latency-svc-5kgfk [257.248611ms]
Jun 19 21:18:42.253: INFO: Got endpoints: latency-svc-rg5m2 [249.12924ms]
Jun 19 21:18:42.257: INFO: Created: latency-svc-xxms4
Jun 19 21:18:42.264: INFO: Got endpoints: latency-svc-xxms4 [244.064321ms]
Jun 19 21:18:42.273: INFO: Created: latency-svc-t7fvg
Jun 19 21:18:42.284: INFO: Got endpoints: latency-svc-t7fvg [249.708531ms]
Jun 19 21:18:42.284: INFO: Created: latency-svc-hfjkn
Jun 19 21:18:42.303: INFO: Created: latency-svc-4l5jh
Jun 19 21:18:42.316: INFO: Created: latency-svc-hbnpp
Jun 19 21:18:42.330: INFO: Got endpoints: latency-svc-hfjkn [262.000936ms]
Jun 19 21:18:42.333: INFO: Created: latency-svc-2vvdv
Jun 19 21:18:42.342: INFO: Created: latency-svc-96n8m
Jun 19 21:18:42.367: INFO: Created: latency-svc-4tnl9
Jun 19 21:18:42.376: INFO: Created: latency-svc-w77mz
Jun 19 21:18:42.379: INFO: Got endpoints: latency-svc-4l5jh [311.122755ms]
Jun 19 21:18:42.388: INFO: Created: latency-svc-ftr6p
Jun 19 21:18:42.404: INFO: Created: latency-svc-r2w2k
Jun 19 21:18:42.417: INFO: Created: latency-svc-hwx2b
Jun 19 21:18:42.420: INFO: Got endpoints: latency-svc-hbnpp [331.352734ms]
Jun 19 21:18:42.443: INFO: Created: latency-svc-9zx66
Jun 19 21:18:42.484: INFO: Created: latency-svc-r7snk
Jun 19 21:18:42.487: INFO: Got endpoints: latency-svc-2vvdv [382.866114ms]
Jun 19 21:18:42.498: INFO: Created: latency-svc-zlbq6
Jun 19 21:18:42.512: INFO: Created: latency-svc-ltc8q
Jun 19 21:18:42.523: INFO: Created: latency-svc-hds7f
Jun 19 21:18:42.525: INFO: Got endpoints: latency-svc-96n8m [414.889906ms]
Jun 19 21:18:42.549: INFO: Created: latency-svc-rhd6q
Jun 19 21:18:42.565: INFO: Created: latency-svc-lgztq
Jun 19 21:18:42.619: INFO: Created: latency-svc-r266m
Jun 19 21:18:42.619: INFO: Got endpoints: latency-svc-w77mz [465.008109ms]
Jun 19 21:18:42.622: INFO: Got endpoints: latency-svc-4tnl9 [488.416637ms]
Jun 19 21:18:42.630: INFO: Created: latency-svc-schgb
Jun 19 21:18:42.643: INFO: Created: latency-svc-z9cg9
Jun 19 21:18:42.656: INFO: Created: latency-svc-68m68
Jun 19 21:18:42.668: INFO: Got endpoints: latency-svc-ftr6p [508.266621ms]
Jun 19 21:18:42.670: INFO: Created: latency-svc-hjj69
Jun 19 21:18:42.684: INFO: Created: latency-svc-vfc5m
Jun 19 21:18:42.718: INFO: Got endpoints: latency-svc-r2w2k [536.282776ms]
Jun 19 21:18:42.761: INFO: Created: latency-svc-r5pkk
Jun 19 21:18:42.766: INFO: Got endpoints: latency-svc-hwx2b [566.988489ms]
Jun 19 21:18:42.780: INFO: Created: latency-svc-f5xkx
Jun 19 21:18:42.817: INFO: Got endpoints: latency-svc-9zx66 [599.797067ms]
Jun 19 21:18:42.832: INFO: Created: latency-svc-n96t5
Jun 19 21:18:42.869: INFO: Got endpoints: latency-svc-r7snk [629.720591ms]
Jun 19 21:18:42.885: INFO: Created: latency-svc-lqhk5
Jun 19 21:18:42.917: INFO: Got endpoints: latency-svc-zlbq6 [663.688151ms]
Jun 19 21:18:42.992: INFO: Created: latency-svc-sk482
Jun 19 21:18:42.995: INFO: Got endpoints: latency-svc-ltc8q [731.558673ms]
Jun 19 21:18:43.030: INFO: Created: latency-svc-q4gtf
Jun 19 21:18:43.033: INFO: Got endpoints: latency-svc-hds7f [749.231992ms]
Jun 19 21:18:43.049: INFO: Created: latency-svc-phn2v
Jun 19 21:18:43.070: INFO: Got endpoints: latency-svc-rhd6q [740.604628ms]
Jun 19 21:18:43.111: INFO: Created: latency-svc-fjb78
Jun 19 21:18:43.123: INFO: Got endpoints: latency-svc-lgztq [743.781378ms]
Jun 19 21:18:43.139: INFO: Created: latency-svc-fxtzb
Jun 19 21:18:43.168: INFO: Got endpoints: latency-svc-r266m [747.894913ms]
Jun 19 21:18:43.182: INFO: Created: latency-svc-fcmf2
Jun 19 21:18:43.231: INFO: Got endpoints: latency-svc-schgb [740.627329ms]
Jun 19 21:18:43.245: INFO: Created: latency-svc-9hwgd
Jun 19 21:18:43.266: INFO: Got endpoints: latency-svc-z9cg9 [738.439364ms]
Jun 19 21:18:43.279: INFO: Created: latency-svc-7qknq
Jun 19 21:18:43.316: INFO: Got endpoints: latency-svc-68m68 [694.63446ms]
Jun 19 21:18:43.350: INFO: Created: latency-svc-tnmq8
Jun 19 21:18:43.366: INFO: Got endpoints: latency-svc-hjj69 [743.984876ms]
Jun 19 21:18:43.385: INFO: Created: latency-svc-nzg2j
Jun 19 21:18:43.417: INFO: Got endpoints: latency-svc-vfc5m [746.360339ms]
Jun 19 21:18:43.431: INFO: Created: latency-svc-hc9rs
Jun 19 21:18:43.466: INFO: Got endpoints: latency-svc-r5pkk [747.790916ms]
Jun 19 21:18:43.480: INFO: Created: latency-svc-z9f5z
Jun 19 21:18:43.516: INFO: Got endpoints: latency-svc-f5xkx [750.512473ms]
Jun 19 21:18:43.533: INFO: Created: latency-svc-f6npc
Jun 19 21:18:43.575: INFO: Got endpoints: latency-svc-n96t5 [758.071353ms]
Jun 19 21:18:43.590: INFO: Created: latency-svc-ngqjj
Jun 19 21:18:43.617: INFO: Got endpoints: latency-svc-lqhk5 [748.16521ms]
Jun 19 21:18:43.639: INFO: Created: latency-svc-jr82j
Jun 19 21:18:43.667: INFO: Got endpoints: latency-svc-sk482 [749.293493ms]
Jun 19 21:18:43.702: INFO: Created: latency-svc-fpzlr
Jun 19 21:18:43.717: INFO: Got endpoints: latency-svc-q4gtf [721.539734ms]
Jun 19 21:18:43.745: INFO: Created: latency-svc-5tq44
Jun 19 21:18:43.765: INFO: Got endpoints: latency-svc-phn2v [729.712804ms]
Jun 19 21:18:43.786: INFO: Created: latency-svc-dhk2s
Jun 19 21:18:43.816: INFO: Got endpoints: latency-svc-fjb78 [745.643851ms]
Jun 19 21:18:43.831: INFO: Created: latency-svc-5zzjd
Jun 19 21:18:43.865: INFO: Got endpoints: latency-svc-fxtzb [740.417834ms]
Jun 19 21:18:43.882: INFO: Created: latency-svc-f8pxn
Jun 19 21:18:43.926: INFO: Got endpoints: latency-svc-fcmf2 [756.722475ms]
Jun 19 21:18:43.944: INFO: Created: latency-svc-4bpkh
Jun 19 21:18:43.966: INFO: Got endpoints: latency-svc-9hwgd [735.707209ms]
Jun 19 21:18:43.986: INFO: Created: latency-svc-69778
Jun 19 21:18:44.016: INFO: Got endpoints: latency-svc-7qknq [750.338676ms]
Jun 19 21:18:44.050: INFO: Created: latency-svc-jjdvh
Jun 19 21:18:44.067: INFO: Got endpoints: latency-svc-tnmq8 [747.974214ms]
Jun 19 21:18:44.085: INFO: Created: latency-svc-cjwnh
Jun 19 21:18:44.118: INFO: Got endpoints: latency-svc-nzg2j [752.055649ms]
Jun 19 21:18:44.139: INFO: Created: latency-svc-q5f8l
Jun 19 21:18:44.166: INFO: Got endpoints: latency-svc-hc9rs [749.204495ms]
Jun 19 21:18:44.228: INFO: Got endpoints: latency-svc-z9f5z [762.096089ms]
Jun 19 21:18:44.238: INFO: Created: latency-svc-7nt5j
Jun 19 21:18:44.273: INFO: Created: latency-svc-trq8l
Jun 19 21:18:44.276: INFO: Got endpoints: latency-svc-f6npc [759.830226ms]
Jun 19 21:18:44.295: INFO: Created: latency-svc-xk7r9
Jun 19 21:18:44.316: INFO: Got endpoints: latency-svc-ngqjj [740.862927ms]
Jun 19 21:18:44.333: INFO: Created: latency-svc-7mflk
Jun 19 21:18:44.368: INFO: Got endpoints: latency-svc-jr82j [750.992567ms]
Jun 19 21:18:44.398: INFO: Created: latency-svc-qz6wd
Jun 19 21:18:44.417: INFO: Got endpoints: latency-svc-fpzlr [750.453276ms]
Jun 19 21:18:44.435: INFO: Created: latency-svc-79gp7
Jun 19 21:18:44.467: INFO: Got endpoints: latency-svc-5tq44 [748.215011ms]
Jun 19 21:18:44.500: INFO: Created: latency-svc-8vqn7
Jun 19 21:18:44.521: INFO: Got endpoints: latency-svc-dhk2s [753.244131ms]
Jun 19 21:18:44.556: INFO: Created: latency-svc-kqmnm
Jun 19 21:18:44.571: INFO: Got endpoints: latency-svc-5zzjd [755.113802ms]
Jun 19 21:18:44.594: INFO: Created: latency-svc-hfpd2
Jun 19 21:18:44.616: INFO: Got endpoints: latency-svc-f8pxn [747.967816ms]
Jun 19 21:18:44.637: INFO: Created: latency-svc-nkkxt
Jun 19 21:18:44.668: INFO: Got endpoints: latency-svc-4bpkh [738.535566ms]
Jun 19 21:18:44.686: INFO: Created: latency-svc-78ls7
Jun 19 21:18:44.727: INFO: Got endpoints: latency-svc-69778 [759.553132ms]
Jun 19 21:18:44.747: INFO: Created: latency-svc-zftnd
Jun 19 21:18:44.766: INFO: Got endpoints: latency-svc-jjdvh [749.394393ms]
Jun 19 21:18:44.787: INFO: Created: latency-svc-fzc99
Jun 19 21:18:44.821: INFO: Got endpoints: latency-svc-cjwnh [751.367862ms]
Jun 19 21:18:44.839: INFO: Created: latency-svc-6j7tl
Jun 19 21:18:44.878: INFO: Got endpoints: latency-svc-q5f8l [760.131123ms]
Jun 19 21:18:44.896: INFO: Created: latency-svc-dvfq7
Jun 19 21:18:44.916: INFO: Got endpoints: latency-svc-7nt5j [750.20908ms]
Jun 19 21:18:44.958: INFO: Created: latency-svc-k6rxm
Jun 19 21:18:44.968: INFO: Got endpoints: latency-svc-trq8l [739.813446ms]
Jun 19 21:18:44.983: INFO: Created: latency-svc-4bkwc
Jun 19 21:18:45.015: INFO: Got endpoints: latency-svc-xk7r9 [739.239255ms]
Jun 19 21:18:45.031: INFO: Created: latency-svc-xlmpx
Jun 19 21:18:45.067: INFO: Got endpoints: latency-svc-7mflk [750.83967ms]
Jun 19 21:18:45.082: INFO: Created: latency-svc-9stnq
Jun 19 21:18:45.117: INFO: Got endpoints: latency-svc-qz6wd [747.283627ms]
Jun 19 21:18:45.138: INFO: Created: latency-svc-cmzlk
Jun 19 21:18:45.166: INFO: Got endpoints: latency-svc-79gp7 [745.811351ms]
Jun 19 21:18:45.204: INFO: Created: latency-svc-87x7z
Jun 19 21:18:45.216: INFO: Got endpoints: latency-svc-8vqn7 [749.563591ms]
Jun 19 21:18:45.233: INFO: Created: latency-svc-2fjpv
Jun 19 21:18:45.267: INFO: Got endpoints: latency-svc-kqmnm [745.24776ms]
Jun 19 21:18:45.282: INFO: Created: latency-svc-pn8f8
Jun 19 21:18:45.321: INFO: Got endpoints: latency-svc-hfpd2 [750.094883ms]
Jun 19 21:18:45.336: INFO: Created: latency-svc-hvwhl
Jun 19 21:18:45.366: INFO: Got endpoints: latency-svc-nkkxt [746.405942ms]
Jun 19 21:18:45.383: INFO: Created: latency-svc-zc764
Jun 19 21:18:45.419: INFO: Got endpoints: latency-svc-78ls7 [747.388026ms]
Jun 19 21:18:45.438: INFO: Created: latency-svc-5d2vt
Jun 19 21:18:45.466: INFO: Got endpoints: latency-svc-zftnd [739.199956ms]
Jun 19 21:18:45.480: INFO: Created: latency-svc-czp2w
Jun 19 21:18:45.515: INFO: Got endpoints: latency-svc-fzc99 [749.313796ms]
Jun 19 21:18:45.540: INFO: Created: latency-svc-g4sqr
Jun 19 21:18:45.568: INFO: Got endpoints: latency-svc-6j7tl [746.036448ms]
Jun 19 21:18:45.585: INFO: Created: latency-svc-z7j67
Jun 19 21:18:45.615: INFO: Got endpoints: latency-svc-dvfq7 [736.4761ms]
Jun 19 21:18:45.629: INFO: Created: latency-svc-q8zwf
Jun 19 21:18:45.667: INFO: Got endpoints: latency-svc-k6rxm [744.273476ms]
Jun 19 21:18:45.689: INFO: Created: latency-svc-47qqf
Jun 19 21:18:45.716: INFO: Got endpoints: latency-svc-4bkwc [747.494626ms]
Jun 19 21:18:45.736: INFO: Created: latency-svc-8p8bx
Jun 19 21:18:45.765: INFO: Got endpoints: latency-svc-xlmpx [747.926619ms]
Jun 19 21:18:45.783: INFO: Created: latency-svc-wgncb
Jun 19 21:18:45.816: INFO: Got endpoints: latency-svc-9stnq [747.008633ms]
Jun 19 21:18:45.837: INFO: Created: latency-svc-xblfv
Jun 19 21:18:45.870: INFO: Got endpoints: latency-svc-cmzlk [750.467879ms]
Jun 19 21:18:45.888: INFO: Created: latency-svc-t2cfk
Jun 19 21:18:45.916: INFO: Got endpoints: latency-svc-87x7z [748.562709ms]
Jun 19 21:18:45.933: INFO: Created: latency-svc-llvth
Jun 19 21:18:45.966: INFO: Got endpoints: latency-svc-2fjpv [747.299329ms]
Jun 19 21:18:45.983: INFO: Created: latency-svc-pdx9r
Jun 19 21:18:46.016: INFO: Got endpoints: latency-svc-pn8f8 [748.974002ms]
Jun 19 21:18:46.036: INFO: Created: latency-svc-m795f
Jun 19 21:18:46.067: INFO: Got endpoints: latency-svc-hvwhl [744.958466ms]
Jun 19 21:18:46.107: INFO: Created: latency-svc-6hk7w
Jun 19 21:18:46.119: INFO: Got endpoints: latency-svc-zc764 [753.387333ms]
Jun 19 21:18:46.136: INFO: Created: latency-svc-wsb48
Jun 19 21:18:46.166: INFO: Got endpoints: latency-svc-5d2vt [743.431191ms]
Jun 19 21:18:46.184: INFO: Created: latency-svc-f4csn
Jun 19 21:18:46.220: INFO: Got endpoints: latency-svc-czp2w [753.510131ms]
Jun 19 21:18:46.236: INFO: Created: latency-svc-ss4fv
Jun 19 21:18:46.266: INFO: Got endpoints: latency-svc-g4sqr [749.172799ms]
Jun 19 21:18:46.282: INFO: Created: latency-svc-9df85
Jun 19 21:18:46.316: INFO: Got endpoints: latency-svc-z7j67 [748.377812ms]
Jun 19 21:18:46.351: INFO: Created: latency-svc-5bpxr
Jun 19 21:18:46.365: INFO: Got endpoints: latency-svc-q8zwf [750.243783ms]
Jun 19 21:18:46.407: INFO: Created: latency-svc-7z2th
Jun 19 21:18:46.418: INFO: Got endpoints: latency-svc-47qqf [750.724375ms]
Jun 19 21:18:46.438: INFO: Created: latency-svc-6xctr
Jun 19 21:18:46.467: INFO: Got endpoints: latency-svc-8p8bx [749.621893ms]
Jun 19 21:18:46.494: INFO: Created: latency-svc-w5qs5
Jun 19 21:18:46.516: INFO: Got endpoints: latency-svc-wgncb [747.059634ms]
Jun 19 21:18:46.538: INFO: Created: latency-svc-794h7
Jun 19 21:18:46.569: INFO: Got endpoints: latency-svc-xblfv [745.799054ms]
Jun 19 21:18:46.585: INFO: Created: latency-svc-6f6hl
Jun 19 21:18:46.616: INFO: Got endpoints: latency-svc-t2cfk [741.483823ms]
Jun 19 21:18:46.632: INFO: Created: latency-svc-s7vch
Jun 19 21:18:46.680: INFO: Got endpoints: latency-svc-llvth [764.316761ms]
Jun 19 21:18:46.697: INFO: Created: latency-svc-xz4wc
Jun 19 21:18:46.717: INFO: Got endpoints: latency-svc-pdx9r [750.560679ms]
Jun 19 21:18:46.733: INFO: Created: latency-svc-lw797
Jun 19 21:18:46.766: INFO: Got endpoints: latency-svc-m795f [750.199285ms]
Jun 19 21:18:46.793: INFO: Created: latency-svc-g9ns2
Jun 19 21:18:46.816: INFO: Got endpoints: latency-svc-6hk7w [745.796155ms]
Jun 19 21:18:46.837: INFO: Created: latency-svc-ntwtx
Jun 19 21:18:46.873: INFO: Got endpoints: latency-svc-wsb48 [751.72516ms]
Jun 19 21:18:46.887: INFO: Created: latency-svc-prskw
Jun 19 21:18:46.917: INFO: Got endpoints: latency-svc-f4csn [747.005335ms]
Jun 19 21:18:46.933: INFO: Created: latency-svc-nb8r4
Jun 19 21:18:46.966: INFO: Got endpoints: latency-svc-ss4fv [743.54979ms]
Jun 19 21:18:46.984: INFO: Created: latency-svc-xcm7b
Jun 19 21:18:47.017: INFO: Got endpoints: latency-svc-9df85 [748.58201ms]
Jun 19 21:18:47.036: INFO: Created: latency-svc-x4rh5
Jun 19 21:18:47.066: INFO: Got endpoints: latency-svc-5bpxr [749.684593ms]
Jun 19 21:18:47.083: INFO: Created: latency-svc-bhwls
Jun 19 21:18:47.128: INFO: Got endpoints: latency-svc-7z2th [759.511437ms]
Jun 19 21:18:47.147: INFO: Created: latency-svc-xfrcj
Jun 19 21:18:47.165: INFO: Got endpoints: latency-svc-6xctr [744.83677ms]
Jun 19 21:18:47.195: INFO: Created: latency-svc-dfhx7
Jun 19 21:18:47.217: INFO: Got endpoints: latency-svc-w5qs5 [747.525228ms]
Jun 19 21:18:47.243: INFO: Created: latency-svc-lzmmj
Jun 19 21:18:47.266: INFO: Got endpoints: latency-svc-794h7 [748.885607ms]
Jun 19 21:18:47.280: INFO: Created: latency-svc-jqphw
Jun 19 21:18:47.317: INFO: Got endpoints: latency-svc-6f6hl [748.04232ms]
Jun 19 21:18:47.330: INFO: Created: latency-svc-8kwbn
Jun 19 21:18:47.366: INFO: Got endpoints: latency-svc-s7vch [747.942922ms]
Jun 19 21:18:47.387: INFO: Created: latency-svc-wmlz6
Jun 19 21:18:47.417: INFO: Got endpoints: latency-svc-xz4wc [736.7112ms]
Jun 19 21:18:47.435: INFO: Created: latency-svc-zg5dq
Jun 19 21:18:47.467: INFO: Got endpoints: latency-svc-lw797 [747.089935ms]
Jun 19 21:18:47.485: INFO: Created: latency-svc-nw2f2
Jun 19 21:18:47.517: INFO: Got endpoints: latency-svc-g9ns2 [747.524929ms]
Jun 19 21:18:47.531: INFO: Created: latency-svc-trls6
Jun 19 21:18:47.588: INFO: Got endpoints: latency-svc-ntwtx [768.715092ms]
Jun 19 21:18:47.604: INFO: Created: latency-svc-nn4f8
Jun 19 21:18:47.619: INFO: Got endpoints: latency-svc-prskw [745.198966ms]
Jun 19 21:18:47.637: INFO: Created: latency-svc-5hwzj
Jun 19 21:18:47.665: INFO: Got endpoints: latency-svc-nb8r4 [745.050969ms]
Jun 19 21:18:47.682: INFO: Created: latency-svc-vqmgp
Jun 19 21:18:47.717: INFO: Got endpoints: latency-svc-xcm7b [749.012406ms]
Jun 19 21:18:47.732: INFO: Created: latency-svc-p9lsj
Jun 19 21:18:47.767: INFO: Got endpoints: latency-svc-x4rh5 [749.891292ms]
Jun 19 21:18:47.822: INFO: Created: latency-svc-tfsjn
Jun 19 21:18:47.825: INFO: Got endpoints: latency-svc-bhwls [757.061978ms]
Jun 19 21:18:47.844: INFO: Created: latency-svc-hr5ll
Jun 19 21:18:47.866: INFO: Got endpoints: latency-svc-xfrcj [736.364206ms]
Jun 19 21:18:47.883: INFO: Created: latency-svc-lf9gt
Jun 19 21:18:47.919: INFO: Got endpoints: latency-svc-dfhx7 [743.671791ms]
Jun 19 21:18:47.945: INFO: Created: latency-svc-7j9w6
Jun 19 21:18:47.967: INFO: Got endpoints: latency-svc-lzmmj [749.605997ms]
Jun 19 21:18:47.983: INFO: Created: latency-svc-z4s4f
Jun 19 21:18:48.017: INFO: Got endpoints: latency-svc-jqphw [750.64378ms]
Jun 19 21:18:48.047: INFO: Created: latency-svc-fllsw
Jun 19 21:18:48.067: INFO: Got endpoints: latency-svc-8kwbn [750.186787ms]
Jun 19 21:18:48.083: INFO: Created: latency-svc-jbvxk
Jun 19 21:18:48.117: INFO: Got endpoints: latency-svc-wmlz6 [746.974738ms]
Jun 19 21:18:48.135: INFO: Created: latency-svc-w5bbt
Jun 19 21:18:48.167: INFO: Got endpoints: latency-svc-zg5dq [750.206487ms]
Jun 19 21:18:48.186: INFO: Created: latency-svc-5hb26
Jun 19 21:18:48.215: INFO: Got endpoints: latency-svc-nw2f2 [748.371117ms]
Jun 19 21:18:48.238: INFO: Created: latency-svc-dlskg
Jun 19 21:18:48.287: INFO: Got endpoints: latency-svc-trls6 [769.943174ms]
Jun 19 21:18:48.301: INFO: Created: latency-svc-d2c42
Jun 19 21:18:48.323: INFO: Got endpoints: latency-svc-nn4f8 [735.015129ms]
Jun 19 21:18:48.339: INFO: Created: latency-svc-fsmwg
Jun 19 21:18:48.373: INFO: Got endpoints: latency-svc-5hwzj [752.246255ms]
Jun 19 21:18:48.401: INFO: Created: latency-svc-zlkbj
Jun 19 21:18:48.417: INFO: Got endpoints: latency-svc-vqmgp [752.213856ms]
Jun 19 21:18:48.431: INFO: Created: latency-svc-h6jb7
Jun 19 21:18:48.465: INFO: Got endpoints: latency-svc-p9lsj [747.686828ms]
Jun 19 21:18:48.488: INFO: Created: latency-svc-t6dxx
Jun 19 21:18:48.517: INFO: Got endpoints: latency-svc-tfsjn [746.328849ms]
Jun 19 21:18:48.531: INFO: Created: latency-svc-sb5h8
Jun 19 21:18:48.567: INFO: Got endpoints: latency-svc-hr5ll [739.464859ms]
Jun 19 21:18:48.583: INFO: Created: latency-svc-l6j8v
Jun 19 21:18:48.625: INFO: Got endpoints: latency-svc-lf9gt [756.144394ms]
Jun 19 21:18:48.639: INFO: Created: latency-svc-7v2fw
Jun 19 21:18:48.666: INFO: Got endpoints: latency-svc-7j9w6 [746.849942ms]
Jun 19 21:18:48.686: INFO: Created: latency-svc-bsdft
Jun 19 21:18:48.717: INFO: Got endpoints: latency-svc-z4s4f [747.523532ms]
Jun 19 21:18:48.738: INFO: Created: latency-svc-qf5bs
Jun 19 21:18:48.765: INFO: Got endpoints: latency-svc-fllsw [744.964972ms]
Jun 19 21:18:48.781: INFO: Created: latency-svc-p2m9k
Jun 19 21:18:48.816: INFO: Got endpoints: latency-svc-jbvxk [747.189837ms]
Jun 19 21:18:48.850: INFO: Created: latency-svc-cjnnp
Jun 19 21:18:48.866: INFO: Got endpoints: latency-svc-w5bbt [748.755012ms]
Jun 19 21:18:48.890: INFO: Created: latency-svc-g22rv
Jun 19 21:18:48.920: INFO: Got endpoints: latency-svc-5hb26 [749.787796ms]
Jun 19 21:18:48.937: INFO: Created: latency-svc-dffvd
Jun 19 21:18:48.966: INFO: Got endpoints: latency-svc-dlskg [745.606962ms]
Jun 19 21:18:48.987: INFO: Created: latency-svc-zf69z
Jun 19 21:18:49.016: INFO: Got endpoints: latency-svc-d2c42 [728.777229ms]
Jun 19 21:18:49.033: INFO: Created: latency-svc-wvbgh
Jun 19 21:18:49.074: INFO: Got endpoints: latency-svc-fsmwg [747.918226ms]
Jun 19 21:18:49.092: INFO: Created: latency-svc-tm4gx
Jun 19 21:18:49.117: INFO: Got endpoints: latency-svc-zlkbj [737.57659ms]
Jun 19 21:18:49.137: INFO: Created: latency-svc-j8fk4
Jun 19 21:18:49.167: INFO: Got endpoints: latency-svc-h6jb7 [749.235205ms]
Jun 19 21:18:49.186: INFO: Created: latency-svc-774pc
Jun 19 21:18:49.217: INFO: Got endpoints: latency-svc-t6dxx [752.08436ms]
Jun 19 21:18:49.235: INFO: Created: latency-svc-gm2ln
Jun 19 21:18:49.267: INFO: Got endpoints: latency-svc-sb5h8 [750.060692ms]
Jun 19 21:18:49.298: INFO: Created: latency-svc-qghqk
Jun 19 21:18:49.316: INFO: Got endpoints: latency-svc-l6j8v [748.492017ms]
Jun 19 21:18:49.369: INFO: Got endpoints: latency-svc-7v2fw [743.833792ms]
Jun 19 21:18:49.418: INFO: Got endpoints: latency-svc-bsdft [752.09296ms]
Jun 19 21:18:49.471: INFO: Got endpoints: latency-svc-qf5bs [751.438171ms]
Jun 19 21:18:49.518: INFO: Got endpoints: latency-svc-p2m9k [752.407755ms]
Jun 19 21:18:49.565: INFO: Got endpoints: latency-svc-cjnnp [746.338052ms]
Jun 19 21:18:49.619: INFO: Got endpoints: latency-svc-g22rv [749.6109ms]
Jun 19 21:18:49.671: INFO: Got endpoints: latency-svc-dffvd [747.940327ms]
Jun 19 21:18:49.717: INFO: Got endpoints: latency-svc-zf69z [747.585633ms]
Jun 19 21:18:49.772: INFO: Got endpoints: latency-svc-wvbgh [754.589221ms]
Jun 19 21:18:49.817: INFO: Got endpoints: latency-svc-tm4gx [741.039737ms]
Jun 19 21:18:49.865: INFO: Got endpoints: latency-svc-j8fk4 [748.855613ms]
Jun 19 21:18:49.920: INFO: Got endpoints: latency-svc-774pc [752.77665ms]
Jun 19 21:18:49.997: INFO: Got endpoints: latency-svc-gm2ln [779.765122ms]
Jun 19 21:18:50.017: INFO: Got endpoints: latency-svc-qghqk [748.568217ms]
Jun 19 21:18:50.020: INFO: Latencies: [25.665592ms 47.328247ms 59.822049ms 71.552762ms 79.154142ms 91.709242ms 123.874131ms 131.167715ms 156.367814ms 161.729829ms 185.644848ms 192.151746ms 232.10971ms 241.290064ms 243.984722ms 244.064321ms 249.12924ms 249.708531ms 250.650716ms 252.155491ms 253.466871ms 255.907132ms 257.248611ms 259.82027ms 262.000936ms 263.265115ms 264.715092ms 265.763976ms 267.644145ms 268.443833ms 270.164205ms 274.242141ms 287.219334ms 287.220234ms 287.603528ms 289.551697ms 292.891844ms 295.700099ms 296.119093ms 296.317889ms 296.418887ms 301.194412ms 305.429744ms 306.592827ms 309.972073ms 311.122755ms 312.465933ms 331.352734ms 382.866114ms 414.889906ms 465.008109ms 488.416637ms 508.266621ms 536.282776ms 566.988489ms 599.797067ms 629.720591ms 663.688151ms 694.63446ms 721.539734ms 728.777229ms 729.712804ms 731.558673ms 735.015129ms 735.707209ms 736.364206ms 736.4761ms 736.7112ms 737.57659ms 738.439364ms 738.535566ms 739.199956ms 739.239255ms 739.464859ms 739.813446ms 740.417834ms 740.604628ms 740.627329ms 740.862927ms 741.039737ms 741.483823ms 743.431191ms 743.54979ms 743.671791ms 743.781378ms 743.833792ms 743.984876ms 744.273476ms 744.83677ms 744.958466ms 744.964972ms 745.050969ms 745.198966ms 745.24776ms 745.606962ms 745.643851ms 745.796155ms 745.799054ms 745.811351ms 746.036448ms 746.328849ms 746.338052ms 746.360339ms 746.405942ms 746.849942ms 746.974738ms 747.005335ms 747.008633ms 747.059634ms 747.089935ms 747.189837ms 747.283627ms 747.299329ms 747.388026ms 747.494626ms 747.523532ms 747.524929ms 747.525228ms 747.585633ms 747.686828ms 747.790916ms 747.894913ms 747.918226ms 747.926619ms 747.940327ms 747.942922ms 747.967816ms 747.974214ms 748.04232ms 748.16521ms 748.215011ms 748.371117ms 748.377812ms 748.492017ms 748.562709ms 748.568217ms 748.58201ms 748.755012ms 748.855613ms 748.885607ms 748.974002ms 749.012406ms 749.172799ms 749.204495ms 749.231992ms 749.235205ms 749.293493ms 749.313796ms 749.394393ms 749.563591ms 749.605997ms 749.6109ms 749.621893ms 749.684593ms 749.787796ms 749.891292ms 750.060692ms 750.094883ms 750.186787ms 750.199285ms 750.206487ms 750.20908ms 750.243783ms 750.338676ms 750.453276ms 750.467879ms 750.512473ms 750.560679ms 750.64378ms 750.724375ms 750.83967ms 750.992567ms 751.367862ms 751.438171ms 751.72516ms 752.055649ms 752.08436ms 752.09296ms 752.213856ms 752.246255ms 752.407755ms 752.77665ms 753.244131ms 753.387333ms 753.510131ms 754.589221ms 755.113802ms 756.144394ms 756.722475ms 757.061978ms 758.071353ms 759.511437ms 759.553132ms 759.830226ms 760.131123ms 762.096089ms 764.316761ms 768.715092ms 769.943174ms 779.765122ms]
Jun 19 21:18:50.020: INFO: 50 %ile: 746.328849ms
Jun 19 21:18:50.023: INFO: 90 %ile: 752.407755ms
Jun 19 21:18:50.023: INFO: 99 %ile: 769.943174ms
Jun 19 21:18:50.023: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:18:50.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9230" for this suite.
Jun 19 21:19:02.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:02.119: INFO: namespace svc-latency-9230 deletion completed in 12.091803581s

• [SLOW TEST:26.026 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ff90a78f-bfba-4005-8f0f-fe7de45d3934
STEP: Creating a pod to test consume secrets
Jun 19 21:19:02.299: INFO: Waiting up to 5m0s for pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5" in namespace "secrets-4205" to be "success or failure"
Jun 19 21:19:02.303: INFO: Pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11885ms
Jun 19 21:19:04.305: INFO: Pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040234s
Jun 19 21:19:06.308: INFO: Pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008545536s
Jun 19 21:19:08.311: INFO: Pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011300345s
STEP: Saw pod success
Jun 19 21:19:08.311: INFO: Pod "pod-secrets-923be4d3-a426-4004-8694-a491feda79b5" satisfied condition "success or failure"
Jun 19 21:19:08.313: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-secrets-923be4d3-a426-4004-8694-a491feda79b5 container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:19:08.334: INFO: Waiting for pod pod-secrets-923be4d3-a426-4004-8694-a491feda79b5 to disappear
Jun 19 21:19:08.342: INFO: Pod pod-secrets-923be4d3-a426-4004-8694-a491feda79b5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:19:08.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4205" for this suite.
Jun 19 21:19:14.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:14.428: INFO: namespace secrets-4205 deletion completed in 6.082005829s

• [SLOW TEST:12.307 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:14.429: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-bd158ae4-69a8-4ee6-b24b-05576dfbb9b2
STEP: Creating a pod to test consume configMaps
Jun 19 21:19:14.604: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687" in namespace "projected-9806" to be "success or failure"
Jun 19 21:19:14.613: INFO: Pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687": Phase="Pending", Reason="", readiness=false. Elapsed: 8.869959ms
Jun 19 21:19:16.615: INFO: Pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011474116s
Jun 19 21:19:18.618: INFO: Pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014133784s
Jun 19 21:19:20.621: INFO: Pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016957759s
STEP: Saw pod success
Jun 19 21:19:20.621: INFO: Pod "pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687" satisfied condition "success or failure"
Jun 19 21:19:20.623: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:19:20.645: INFO: Waiting for pod pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687 to disappear
Jun 19 21:19:20.648: INFO: Pod pod-projected-configmaps-462e24d3-5e0c-4b11-8ff5-41af25ec0687 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:19:20.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9806" for this suite.
Jun 19 21:19:26.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:26.746: INFO: namespace projected-9806 deletion completed in 6.094915728s

• [SLOW TEST:12.318 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:26.747: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-5580/secret-test-a0c01c5b-66df-4071-ad89-7f1faf98722c
STEP: Creating a pod to test consume secrets
Jun 19 21:19:26.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4" in namespace "secrets-5580" to be "success or failure"
Jun 19 21:19:26.913: INFO: Pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145387ms
Jun 19 21:19:28.917: INFO: Pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011275387s
Jun 19 21:19:30.920: INFO: Pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014742208s
Jun 19 21:19:32.924: INFO: Pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017983943s
STEP: Saw pod success
Jun 19 21:19:32.924: INFO: Pod "pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4" satisfied condition "success or failure"
Jun 19 21:19:32.926: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4 container env-test: <nil>
STEP: delete the pod
Jun 19 21:19:32.941: INFO: Waiting for pod pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4 to disappear
Jun 19 21:19:32.944: INFO: Pod pod-configmaps-c2716d3c-4f9b-441e-be2a-a92a203234a4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:19:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5580" for this suite.
Jun 19 21:19:38.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:39.080: INFO: namespace secrets-5580 deletion completed in 6.133890214s

• [SLOW TEST:12.333 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:39.081: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:19:39.255: INFO: (0) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.887492ms)
Jun 19 21:19:39.259: INFO: (1) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.149334ms)
Jun 19 21:19:39.264: INFO: (2) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.184634ms)
Jun 19 21:19:39.267: INFO: (3) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.118051ms)
Jun 19 21:19:39.270: INFO: (4) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.15705ms)
Jun 19 21:19:39.273: INFO: (5) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.068752ms)
Jun 19 21:19:39.277: INFO: (6) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.465245ms)
Jun 19 21:19:39.280: INFO: (7) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.978253ms)
Jun 19 21:19:39.283: INFO: (8) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.108951ms)
Jun 19 21:19:39.286: INFO: (9) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.968453ms)
Jun 19 21:19:39.289: INFO: (10) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.050652ms)
Jun 19 21:19:39.292: INFO: (11) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.311348ms)
Jun 19 21:19:39.295: INFO: (12) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.895154ms)
Jun 19 21:19:39.299: INFO: (13) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.448846ms)
Jun 19 21:19:39.302: INFO: (14) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.578644ms)
Jun 19 21:19:39.306: INFO: (15) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.518845ms)
Jun 19 21:19:39.309: INFO: (16) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.904455ms)
Jun 19 21:19:39.313: INFO: (17) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.011537ms)
Jun 19 21:19:39.316: INFO: (18) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.601843ms)
Jun 19 21:19:39.320: INFO: (19) /api/v1/nodes/k8s-pool1-37287165-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.687242ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:19:39.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2383" for this suite.
Jun 19 21:19:45.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:45.404: INFO: namespace proxy-2383 deletion completed in 6.081120346s

• [SLOW TEST:6.323 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:45.404: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5b29cc6a-0d8f-47ce-9ea4-7e77c92b156d
STEP: Creating a pod to test consume configMaps
Jun 19 21:19:45.573: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7" in namespace "projected-7478" to be "success or failure"
Jun 19 21:19:45.578: INFO: Pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342416ms
Jun 19 21:19:47.581: INFO: Pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008132335s
Jun 19 21:19:49.584: INFO: Pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011280258s
Jun 19 21:19:51.587: INFO: Pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014449092s
STEP: Saw pod success
Jun 19 21:19:51.587: INFO: Pod "pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7" satisfied condition "success or failure"
Jun 19 21:19:51.589: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:19:51.605: INFO: Waiting for pod pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7 to disappear
Jun 19 21:19:51.613: INFO: Pod pod-projected-configmaps-95bc35cd-c1da-444f-8621-5251280626f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:19:51.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7478" for this suite.
Jun 19 21:19:57.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:19:57.704: INFO: namespace projected-7478 deletion completed in 6.087457538s

• [SLOW TEST:12.300 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:19:57.705: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 19 21:19:57.864: INFO: Waiting up to 5m0s for pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5" in namespace "downward-api-3938" to be "success or failure"
Jun 19 21:19:57.866: INFO: Pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626059ms
Jun 19 21:19:59.869: INFO: Pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005667536s
Jun 19 21:20:01.872: INFO: Pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008616924s
Jun 19 21:20:03.875: INFO: Pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011624522s
STEP: Saw pod success
Jun 19 21:20:03.875: INFO: Pod "downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5" satisfied condition "success or failure"
Jun 19 21:20:03.877: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5 container dapi-container: <nil>
STEP: delete the pod
Jun 19 21:20:03.892: INFO: Waiting for pod downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5 to disappear
Jun 19 21:20:03.895: INFO: Pod downward-api-1cce1356-daea-4b82-bde2-59959f7cc0e5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:20:03.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3938" for this suite.
Jun 19 21:20:09.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:20:09.977: INFO: namespace downward-api-3938 deletion completed in 6.079227754s

• [SLOW TEST:12.272 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:20:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 19 21:20:10.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-4086'
Jun 19 21:20:10.574: INFO: stderr: ""
Jun 19 21:20:10.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:20:10.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:10.659: INFO: stderr: ""
Jun 19 21:20:10.659: INFO: stdout: "update-demo-nautilus-5skl7 update-demo-nautilus-7kzjz "
Jun 19 21:20:10.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-5skl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:10.735: INFO: stderr: ""
Jun 19 21:20:10.735: INFO: stdout: ""
Jun 19 21:20:10.735: INFO: update-demo-nautilus-5skl7 is created but not running
Jun 19 21:20:15.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:15.815: INFO: stderr: ""
Jun 19 21:20:15.815: INFO: stdout: "update-demo-nautilus-5skl7 update-demo-nautilus-7kzjz "
Jun 19 21:20:15.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-5skl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:15.892: INFO: stderr: ""
Jun 19 21:20:15.892: INFO: stdout: ""
Jun 19 21:20:15.892: INFO: update-demo-nautilus-5skl7 is created but not running
Jun 19 21:20:20.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:20.972: INFO: stderr: ""
Jun 19 21:20:20.972: INFO: stdout: "update-demo-nautilus-5skl7 update-demo-nautilus-7kzjz "
Jun 19 21:20:20.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-5skl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:21.046: INFO: stderr: ""
Jun 19 21:20:21.046: INFO: stdout: "true"
Jun 19 21:20:21.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-5skl7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:21.127: INFO: stderr: ""
Jun 19 21:20:21.127: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:21.127: INFO: validating pod update-demo-nautilus-5skl7
Jun 19 21:20:21.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:21.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:21.133: INFO: update-demo-nautilus-5skl7 is verified up and running
Jun 19 21:20:21.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:21.213: INFO: stderr: ""
Jun 19 21:20:21.213: INFO: stdout: "true"
Jun 19 21:20:21.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:21.292: INFO: stderr: ""
Jun 19 21:20:21.292: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:21.292: INFO: validating pod update-demo-nautilus-7kzjz
Jun 19 21:20:21.299: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:21.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:21.299: INFO: update-demo-nautilus-7kzjz is verified up and running
STEP: scaling down the replication controller
Jun 19 21:20:21.301: INFO: scanned /root for discovery docs: <nil>
Jun 19 21:20:21.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4086'
Jun 19 21:20:22.403: INFO: stderr: ""
Jun 19 21:20:22.403: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:20:22.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:22.488: INFO: stderr: ""
Jun 19 21:20:22.488: INFO: stdout: "update-demo-nautilus-5skl7 update-demo-nautilus-7kzjz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 19 21:20:27.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:27.569: INFO: stderr: ""
Jun 19 21:20:27.569: INFO: stdout: "update-demo-nautilus-5skl7 update-demo-nautilus-7kzjz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 19 21:20:32.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:32.645: INFO: stderr: ""
Jun 19 21:20:32.645: INFO: stdout: "update-demo-nautilus-7kzjz "
Jun 19 21:20:32.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:32.728: INFO: stderr: ""
Jun 19 21:20:32.729: INFO: stdout: "true"
Jun 19 21:20:32.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:32.804: INFO: stderr: ""
Jun 19 21:20:32.804: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:32.804: INFO: validating pod update-demo-nautilus-7kzjz
Jun 19 21:20:32.809: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:32.809: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:32.809: INFO: update-demo-nautilus-7kzjz is verified up and running
STEP: scaling up the replication controller
Jun 19 21:20:32.817: INFO: scanned /root for discovery docs: <nil>
Jun 19 21:20:32.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4086'
Jun 19 21:20:33.926: INFO: stderr: ""
Jun 19 21:20:33.926: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:20:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:34.003: INFO: stderr: ""
Jun 19 21:20:34.003: INFO: stdout: "update-demo-nautilus-7kzjz update-demo-nautilus-hq6fx "
Jun 19 21:20:34.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:34.079: INFO: stderr: ""
Jun 19 21:20:34.079: INFO: stdout: "true"
Jun 19 21:20:34.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:34.155: INFO: stderr: ""
Jun 19 21:20:34.155: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:34.155: INFO: validating pod update-demo-nautilus-7kzjz
Jun 19 21:20:34.158: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:34.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:34.158: INFO: update-demo-nautilus-7kzjz is verified up and running
Jun 19 21:20:34.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-hq6fx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:34.246: INFO: stderr: ""
Jun 19 21:20:34.246: INFO: stdout: ""
Jun 19 21:20:34.246: INFO: update-demo-nautilus-hq6fx is created but not running
Jun 19 21:20:39.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4086'
Jun 19 21:20:39.324: INFO: stderr: ""
Jun 19 21:20:39.324: INFO: stdout: "update-demo-nautilus-7kzjz update-demo-nautilus-hq6fx "
Jun 19 21:20:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:39.406: INFO: stderr: ""
Jun 19 21:20:39.406: INFO: stdout: "true"
Jun 19 21:20:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-7kzjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:39.490: INFO: stderr: ""
Jun 19 21:20:39.490: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:39.490: INFO: validating pod update-demo-nautilus-7kzjz
Jun 19 21:20:39.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:39.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:39.494: INFO: update-demo-nautilus-7kzjz is verified up and running
Jun 19 21:20:39.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-hq6fx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:39.572: INFO: stderr: ""
Jun 19 21:20:39.572: INFO: stdout: "true"
Jun 19 21:20:39.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-hq6fx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4086'
Jun 19 21:20:39.649: INFO: stderr: ""
Jun 19 21:20:39.649: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:20:39.649: INFO: validating pod update-demo-nautilus-hq6fx
Jun 19 21:20:39.655: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:20:39.655: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:20:39.655: INFO: update-demo-nautilus-hq6fx is verified up and running
STEP: using delete to clean up resources
Jun 19 21:20:39.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-4086'
Jun 19 21:20:39.744: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:20:39.744: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 19 21:20:39.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4086'
Jun 19 21:20:39.834: INFO: stderr: "No resources found.\n"
Jun 19 21:20:39.834: INFO: stdout: ""
Jun 19 21:20:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=update-demo --namespace=kubectl-4086 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 21:20:39.912: INFO: stderr: ""
Jun 19 21:20:39.912: INFO: stdout: "update-demo-nautilus-7kzjz\nupdate-demo-nautilus-hq6fx\n"
Jun 19 21:20:40.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4086'
Jun 19 21:20:40.498: INFO: stderr: "No resources found.\n"
Jun 19 21:20:40.498: INFO: stdout: ""
Jun 19 21:20:40.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=update-demo --namespace=kubectl-4086 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 21:20:40.579: INFO: stderr: ""
Jun 19 21:20:40.579: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:20:40.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4086" for this suite.
Jun 19 21:20:46.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:20:46.662: INFO: namespace kubectl-4086 deletion completed in 6.078510197s

• [SLOW TEST:36.685 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:20:46.663: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:20:46.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-2383'
Jun 19 21:20:47.236: INFO: stderr: ""
Jun 19 21:20:47.236: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 19 21:20:47.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-2383'
Jun 19 21:20:47.630: INFO: stderr: ""
Jun 19 21:20:47.630: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 19 21:20:48.633: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:20:48.633: INFO: Found 0 / 1
Jun 19 21:20:49.633: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:20:49.633: INFO: Found 0 / 1
Jun 19 21:20:50.633: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:20:50.633: INFO: Found 0 / 1
Jun 19 21:20:51.633: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:20:51.633: INFO: Found 1 / 1
Jun 19 21:20:51.633: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 19 21:20:51.635: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 21:20:51.635: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 19 21:20:51.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 describe pod redis-master-6dxnx --namespace=kubectl-2383'
Jun 19 21:20:51.723: INFO: stderr: ""
Jun 19 21:20:51.723: INFO: stdout: "Name:           redis-master-6dxnx\nNamespace:      kubectl-2383\nPriority:       0\nNode:           k8s-pool1-37287165-vmss000000/10.240.0.34\nStart Time:     Wed, 19 Jun 2019 21:20:47 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             10.240.0.56\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://ab3d37fdff45e9311a108d54d69c48fe428fbadb91b01188240721a7229a4ee0\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 19 Jun 2019 21:20:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-99tt5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-99tt5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-99tt5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  4s    default-scheduler                       Successfully assigned kubectl-2383/redis-master-6dxnx to k8s-pool1-37287165-vmss000000\n  Normal  Pulled     2s    kubelet, k8s-pool1-37287165-vmss000000  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-pool1-37287165-vmss000000  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-pool1-37287165-vmss000000  Started container redis-master\n"
Jun 19 21:20:51.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 describe rc redis-master --namespace=kubectl-2383'
Jun 19 21:20:51.817: INFO: stderr: ""
Jun 19 21:20:51.817: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2383\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-6dxnx\n"
Jun 19 21:20:51.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 describe service redis-master --namespace=kubectl-2383'
Jun 19 21:20:51.912: INFO: stderr: ""
Jun 19 21:20:51.912: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2383\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.166.134\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.240.0.56:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 19 21:20:51.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 describe node k8s-master-37287165-0'
Jun 19 21:20:52.015: INFO: stderr: ""
Jun 19 21:20:52.015: INFO: stdout: "Name:               k8s-master-37287165-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westus2\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.azure.com/cluster=levo-1-15-5d0a8572\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master-37287165-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 19 Jun 2019 19:01:46 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 19 Jun 2019 21:20:46 +0000   Wed, 19 Jun 2019 19:01:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 19 Jun 2019 21:20:46 +0000   Wed, 19 Jun 2019 19:01:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 19 Jun 2019 21:20:46 +0000   Wed, 19 Jun 2019 19:01:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 19 Jun 2019 21:20:46 +0000   Wed, 19 Jun 2019 19:01:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    k8s-master-37287165-0\n  InternalIP:  10.255.255.5\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              30428648Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7113160Ki\n pods:                           30\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              28043041951\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6345160Ki\n pods:                           30\nSystem Info:\n Machine ID:                 f203d3fa3d0748c3bd4e201a7a8bf519\n System UUID:                97089D82-FF7A-1649-8449-E041C65F8F3B\n Boot ID:                    8eacfc74-5038-4b47-981c-1a3c8fbaa2b2\n Kernel Version:             4.15.0-1046-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.5\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nProviderID:                  azure:///subscriptions/01db32b1-e169-43b0-a791-de0e1ca5d8cd/resourceGroups/levo-1-15-5d0a8572/providers/Microsoft.Compute/virtualMachines/k8s-master-37287165-0\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-rsrgx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                azure-cni-networkmonitor-zb9cp                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         138m\n  kube-system                azure-ip-masq-agent-trdgt                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     138m\n  kube-system                kube-addon-manager-k8s-master-37287165-0                   5m (0%)       0 (0%)      50Mi (0%)        0 (0%)         137m\n  kube-system                kube-apiserver-k8s-master-37287165-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         137m\n  kube-system                kube-controller-manager-k8s-master-37287165-0              0 (0%)        0 (0%)      0 (0%)           0 (0%)         138m\n  kube-system                kube-proxy-6qmlz                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         138m\n  kube-system                kube-scheduler-k8s-master-37287165-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         138m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            155m (7%)   50m (2%)\n  memory                         100Mi (1%)  250Mi (4%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Jun 19 21:20:52.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 describe namespace kubectl-2383'
Jun 19 21:20:52.109: INFO: stderr: ""
Jun 19 21:20:52.109: INFO: stdout: "Name:         kubectl-2383\nLabels:       e2e-framework=kubectl\n              e2e-run=8818dd78-99e0-4c5d-b5a2-67ebdd68c3c4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:20:52.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2383" for this suite.
Jun 19 21:21:14.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:21:14.245: INFO: namespace kubectl-2383 deletion completed in 22.130429083s

• [SLOW TEST:27.582 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:21:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 19 21:21:24.491: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:21:24.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0619 21:21:24.491177      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3187" for this suite.
Jun 19 21:21:30.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:21:30.575: INFO: namespace gc-3187 deletion completed in 6.080412555s

• [SLOW TEST:16.329 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:21:30.575: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-a4e8079c-ca58-4ffd-8ab3-e629d5f6d8c0
STEP: Creating a pod to test consume secrets
Jun 19 21:21:30.730: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9" in namespace "projected-1285" to be "success or failure"
Jun 19 21:21:30.734: INFO: Pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.974339ms
Jun 19 21:21:32.737: INFO: Pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006995343s
Jun 19 21:21:34.740: INFO: Pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010256752s
Jun 19 21:21:36.743: INFO: Pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013900864s
STEP: Saw pod success
Jun 19 21:21:36.744: INFO: Pod "pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9" satisfied condition "success or failure"
Jun 19 21:21:36.746: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:21:36.762: INFO: Waiting for pod pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9 to disappear
Jun 19 21:21:36.766: INFO: Pod pod-projected-secrets-0426de63-809e-414e-9aa0-28f26e4cf1a9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:21:36.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1285" for this suite.
Jun 19 21:21:42.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:21:42.849: INFO: namespace projected-1285 deletion completed in 6.080330413s

• [SLOW TEST:12.275 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:21:42.850: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9742
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-9742
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9742
Jun 19 21:21:43.008: INFO: Found 0 stateful pods, waiting for 1
Jun 19 21:21:53.011: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 19 21:21:53.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:21:53.263: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:21:53.263: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:21:53.263: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 21:21:53.266: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 19 21:22:03.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 21:22:03.272: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:22:03.284: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:03.284: INFO: ss-0  k8s-pool1-37287165-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:03.284: INFO: 
Jun 19 21:22:03.284: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 19 21:22:04.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995733475s
Jun 19 21:22:05.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99263943s
Jun 19 21:22:06.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988865694s
Jun 19 21:22:07.298: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985150554s
Jun 19 21:22:08.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982435298s
Jun 19 21:22:09.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979217847s
Jun 19 21:22:10.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9755872s
Jun 19 21:22:11.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971448759s
Jun 19 21:22:12.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.908523ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9742
Jun 19 21:22:13.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:22:13.531: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 21:22:13.531: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 21:22:13.531: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 21:22:13.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:22:13.737: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 19 21:22:13.737: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 21:22:13.737: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 21:22:13.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:22:13.942: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 19 21:22:13.942: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 21:22:13.942: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 21:22:13.945: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 19 21:22:23.949: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:22:23.949: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:22:23.949: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 19 21:22:23.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:22:26.607: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:22:26.607: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:22:26.607: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 21:22:26.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:22:27.418: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:22:27.418: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:22:27.418: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 21:22:27.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:22:27.709: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:22:27.709: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:22:27.709: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 21:22:27.709: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:22:27.712: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 19 21:22:37.717: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 21:22:37.717: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 21:22:37.717: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 21:22:37.725: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:37.725: INFO: ss-0  k8s-pool1-37287165-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:37.725: INFO: ss-1  k8s-pool1-37287165-vmss000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:37.725: INFO: ss-2  k8s-pool1-37287165-vmss000002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:37.725: INFO: 
Jun 19 21:22:37.725: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 19 21:22:38.728: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:38.728: INFO: ss-0  k8s-pool1-37287165-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:38.728: INFO: ss-1  k8s-pool1-37287165-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:38.728: INFO: ss-2  k8s-pool1-37287165-vmss000002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:38.728: INFO: 
Jun 19 21:22:38.728: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 19 21:22:39.732: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:39.732: INFO: ss-0  k8s-pool1-37287165-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:39.732: INFO: ss-1  k8s-pool1-37287165-vmss000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:39.732: INFO: ss-2  k8s-pool1-37287165-vmss000002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:39.732: INFO: 
Jun 19 21:22:39.732: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 19 21:22:40.735: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:40.735: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:40.735: INFO: ss-1  k8s-pool1-37287165-vmss000001  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:40.735: INFO: ss-2  k8s-pool1-37287165-vmss000002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:40.735: INFO: 
Jun 19 21:22:40.735: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 19 21:22:41.738: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:41.738: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:41.739: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:41.739: INFO: 
Jun 19 21:22:41.739: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 19 21:22:42.742: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:42.742: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:42.742: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:42.742: INFO: 
Jun 19 21:22:42.742: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 19 21:22:43.745: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:43.745: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:43.745: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:43.745: INFO: 
Jun 19 21:22:43.745: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 19 21:22:44.748: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:44.748: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:44.749: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:44.749: INFO: 
Jun 19 21:22:44.749: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 19 21:22:45.752: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:45.752: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:45.752: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:45.752: INFO: 
Jun 19 21:22:45.752: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 19 21:22:46.755: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun 19 21:22:46.755: INFO: ss-0  k8s-pool1-37287165-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:21:43 +0000 UTC  }]
Jun 19 21:22:46.755: INFO: ss-2  k8s-pool1-37287165-vmss000002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:22:03 +0000 UTC  }]
Jun 19 21:22:46.756: INFO: 
Jun 19 21:22:46.756: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9742
Jun 19 21:22:47.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:22:47.887: INFO: rc: 1
Jun 19 21:22:47.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc003793110 exit status 1 <nil> <nil> true [0xc002aed528 0xc002aed540 0xc002aed558] [0xc002aed528 0xc002aed540 0xc002aed558] [0xc002aed538 0xc002aed550] [0x9d17b0 0x9d17b0] 0xc0022fa6c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jun 19 21:22:57.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:22:57.970: INFO: rc: 1
Jun 19 21:22:57.970: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003793470 exit status 1 <nil> <nil> true [0xc002aed560 0xc002aed578 0xc002aed590] [0xc002aed560 0xc002aed578 0xc002aed590] [0xc002aed570 0xc002aed588] [0x9d17b0 0x9d17b0] 0xc0022faa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:08.049: INFO: rc: 1
Jun 19 21:23:08.049: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003793830 exit status 1 <nil> <nil> true [0xc002aed598 0xc002aed5b0 0xc002aed5c8] [0xc002aed598 0xc002aed5b0 0xc002aed5c8] [0xc002aed5a8 0xc002aed5c0] [0x9d17b0 0x9d17b0] 0xc0022fae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:18.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:18.133: INFO: rc: 1
Jun 19 21:23:18.133: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003793b90 exit status 1 <nil> <nil> true [0xc002aed5d0 0xc002aed5e8 0xc002aed600] [0xc002aed5d0 0xc002aed5e8 0xc002aed600] [0xc002aed5e0 0xc002aed5f8] [0x9d17b0 0x9d17b0] 0xc0022fb2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:28.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:28.211: INFO: rc: 1
Jun 19 21:23:28.211: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e703c0 exit status 1 <nil> <nil> true [0xc001f16040 0xc001f160b8 0xc001f160f0] [0xc001f16040 0xc001f160b8 0xc001f160f0] [0xc001f160a0 0xc001f160d8] [0x9d17b0 0x9d17b0] 0xc0019e8600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:38.291: INFO: rc: 1
Jun 19 21:23:38.291: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e707b0 exit status 1 <nil> <nil> true [0xc001f16100 0xc001f161c8 0xc001f16258] [0xc001f16100 0xc001f161c8 0xc001f16258] [0xc001f16178 0xc001f16220] [0x9d17b0 0x9d17b0] 0xc0019e8c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:48.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:48.374: INFO: rc: 1
Jun 19 21:23:48.374: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e70b70 exit status 1 <nil> <nil> true [0xc001f16310 0xc001f16348 0xc001f163c8] [0xc001f16310 0xc001f16348 0xc001f163c8] [0xc001f16330 0xc001f16398] [0x9d17b0 0x9d17b0] 0xc0019e90e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:23:58.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:23:58.450: INFO: rc: 1
Jun 19 21:23:58.450: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e70f00 exit status 1 <nil> <nil> true [0xc001f16458 0xc001f16530 0xc001f16548] [0xc001f16458 0xc001f16530 0xc001f16548] [0xc001f164f8 0xc001f16540] [0x9d17b0 0x9d17b0] 0xc0019e9560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:08.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:08.523: INFO: rc: 1
Jun 19 21:24:08.524: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832450 exit status 1 <nil> <nil> true [0xc001778000 0xc001778018 0xc001778030] [0xc001778000 0xc001778018 0xc001778030] [0xc001778010 0xc001778028] [0x9d17b0 0x9d17b0] 0xc001b5c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:18.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:18.607: INFO: rc: 1
Jun 19 21:24:18.607: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e712f0 exit status 1 <nil> <nil> true [0xc001f16568 0xc001f165a0 0xc001f165f8] [0xc001f16568 0xc001f165a0 0xc001f165f8] [0xc001f16590 0xc001f165d8] [0x9d17b0 0x9d17b0] 0xc0019e9a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:28.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:28.683: INFO: rc: 1
Jun 19 21:24:28.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e71680 exit status 1 <nil> <nil> true [0xc001f16608 0xc001f16668 0xc001f166d0] [0xc001f16608 0xc001f16668 0xc001f166d0] [0xc001f16658 0xc001f166b8] [0x9d17b0 0x9d17b0] 0xc0019e9e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:38.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:38.757: INFO: rc: 1
Jun 19 21:24:38.757: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832840 exit status 1 <nil> <nil> true [0xc001778038 0xc001778070 0xc0017780a8] [0xc001778038 0xc001778070 0xc0017780a8] [0xc001778050 0xc001778090] [0x9d17b0 0x9d17b0] 0xc001b5cc00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:48.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:48.832: INFO: rc: 1
Jun 19 21:24:48.832: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832c00 exit status 1 <nil> <nil> true [0xc0017780c8 0xc0017780f0 0xc001778130] [0xc0017780c8 0xc0017780f0 0xc001778130] [0xc0017780e8 0xc001778110] [0x9d17b0 0x9d17b0] 0xc001b5d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:24:58.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:24:58.915: INFO: rc: 1
Jun 19 21:24:58.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001833050 exit status 1 <nil> <nil> true [0xc001778148 0xc001778190 0xc0017781b8] [0xc001778148 0xc001778190 0xc0017781b8] [0xc001778170 0xc0017781b0] [0x9d17b0 0x9d17b0] 0xc001b5d4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:08.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:08.995: INFO: rc: 1
Jun 19 21:25:08.995: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001833470 exit status 1 <nil> <nil> true [0xc0017781c0 0xc0017781f8 0xc001778210] [0xc0017781c0 0xc0017781f8 0xc001778210] [0xc0017781f0 0xc001778208] [0x9d17b0 0x9d17b0] 0xc001b5d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:18.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:19.075: INFO: rc: 1
Jun 19 21:25:19.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e71ad0 exit status 1 <nil> <nil> true [0xc001f166e0 0xc001f16700 0xc001f16730] [0xc001f166e0 0xc001f16700 0xc001f16730] [0xc001f166f8 0xc001f16720] [0x9d17b0 0x9d17b0] 0xc001596600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:29.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:29.168: INFO: rc: 1
Jun 19 21:25:29.168: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e71e90 exit status 1 <nil> <nil> true [0xc001f16768 0xc001f16798 0xc001f167e0] [0xc001f16768 0xc001f16798 0xc001f167e0] [0xc001f16788 0xc001f167d0] [0x9d17b0 0x9d17b0] 0xc0015972c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:39.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:39.248: INFO: rc: 1
Jun 19 21:25:39.248: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018323f0 exit status 1 <nil> <nil> true [0xc001778000 0xc001778018 0xc001778030] [0xc001778000 0xc001778018 0xc001778030] [0xc001778010 0xc001778028] [0x9d17b0 0x9d17b0] 0xc0019e8600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:49.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:49.324: INFO: rc: 1
Jun 19 21:25:49.324: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832870 exit status 1 <nil> <nil> true [0xc001778038 0xc001778070 0xc0017780a8] [0xc001778038 0xc001778070 0xc0017780a8] [0xc001778050 0xc001778090] [0x9d17b0 0x9d17b0] 0xc0019e8c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:25:59.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:25:59.398: INFO: rc: 1
Jun 19 21:25:59.398: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e70420 exit status 1 <nil> <nil> true [0xc001f16010 0xc001f160a0 0xc001f160d8] [0xc001f16010 0xc001f160a0 0xc001f160d8] [0xc001f16050 0xc001f160c8] [0x9d17b0 0x9d17b0] 0xc001b5c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:09.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:09.477: INFO: rc: 1
Jun 19 21:26:09.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e70870 exit status 1 <nil> <nil> true [0xc001f160f0 0xc001f16178 0xc001f16220] [0xc001f160f0 0xc001f16178 0xc001f16220] [0xc001f16140 0xc001f161e0] [0x9d17b0 0x9d17b0] 0xc001b5cc00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:19.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:19.565: INFO: rc: 1
Jun 19 21:26:19.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832bd0 exit status 1 <nil> <nil> true [0xc0017780c8 0xc0017780f0 0xc001778130] [0xc0017780c8 0xc0017780f0 0xc001778130] [0xc0017780e8 0xc001778110] [0x9d17b0 0x9d17b0] 0xc0019e90e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:29.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:29.637: INFO: rc: 1
Jun 19 21:26:29.637: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e70c90 exit status 1 <nil> <nil> true [0xc001f16258 0xc001f16330 0xc001f16398] [0xc001f16258 0xc001f16330 0xc001f16398] [0xc001f16328 0xc001f16378] [0x9d17b0 0x9d17b0] 0xc001b5d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:39.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:39.715: INFO: rc: 1
Jun 19 21:26:39.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001833080 exit status 1 <nil> <nil> true [0xc001778148 0xc001778190 0xc0017781b8] [0xc001778148 0xc001778190 0xc0017781b8] [0xc001778170 0xc0017781b0] [0x9d17b0 0x9d17b0] 0xc0019e9560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:49.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:49.809: INFO: rc: 1
Jun 19 21:26:49.809: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e710b0 exit status 1 <nil> <nil> true [0xc001f163c8 0xc001f164f8 0xc001f16540] [0xc001f163c8 0xc001f164f8 0xc001f16540] [0xc001f16490 0xc001f16538] [0x9d17b0 0x9d17b0] 0xc001b5d4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:26:59.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:26:59.883: INFO: rc: 1
Jun 19 21:26:59.883: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001833500 exit status 1 <nil> <nil> true [0xc0017781c0 0xc0017781f8 0xc001778210] [0xc0017781c0 0xc0017781f8 0xc001778210] [0xc0017781f0 0xc001778208] [0x9d17b0 0x9d17b0] 0xc0019e9a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:27:09.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:27:09.969: INFO: rc: 1
Jun 19 21:27:09.969: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e71470 exit status 1 <nil> <nil> true [0xc001f16548 0xc001f16590 0xc001f165d8] [0xc001f16548 0xc001f16590 0xc001f165d8] [0xc001f16580 0xc001f165c8] [0x9d17b0 0x9d17b0] 0xc001b5d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:27:19.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:27:20.048: INFO: rc: 1
Jun 19 21:27:20.048: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001833950 exit status 1 <nil> <nil> true [0xc001778228 0xc001778250 0xc001778268] [0xc001778228 0xc001778250 0xc001778268] [0xc001778248 0xc001778260] [0x9d17b0 0x9d17b0] 0xc0019e9e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:27:30.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:27:30.122: INFO: rc: 1
Jun 19 21:27:30.122: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e703c0 exit status 1 <nil> <nil> true [0xc001f16040 0xc001f160b8 0xc001f160f0] [0xc001f16040 0xc001f160b8 0xc001f160f0] [0xc001f160a0 0xc001f160d8] [0x9d17b0 0x9d17b0] 0xc001b5c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:27:40.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:27:40.199: INFO: rc: 1
Jun 19 21:27:40.199: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001832450 exit status 1 <nil> <nil> true [0xc001778000 0xc001778018 0xc001778030] [0xc001778000 0xc001778018 0xc001778030] [0xc001778010 0xc001778028] [0x9d17b0 0x9d17b0] 0xc0019e8600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 19 21:27:50.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-9742 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:27:50.281: INFO: rc: 1
Jun 19 21:27:50.281: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jun 19 21:27:50.281: INFO: Scaling statefulset ss to 0
Jun 19 21:27:50.289: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 19 21:27:50.291: INFO: Deleting all statefulset in ns statefulset-9742
Jun 19 21:27:50.293: INFO: Scaling statefulset ss to 0
Jun 19 21:27:50.300: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:27:50.302: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:27:50.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9742" for this suite.
Jun 19 21:27:56.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:27:56.403: INFO: namespace statefulset-9742 deletion completed in 6.085519557s

• [SLOW TEST:373.554 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:27:56.404: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 19 21:27:56.554: INFO: Waiting up to 5m0s for pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02" in namespace "emptydir-5019" to be "success or failure"
Jun 19 21:27:56.564: INFO: Pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02": Phase="Pending", Reason="", readiness=false. Elapsed: 9.823954ms
Jun 19 21:27:58.567: INFO: Pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01310779s
Jun 19 21:28:00.570: INFO: Pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016360232s
Jun 19 21:28:02.574: INFO: Pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020420465s
STEP: Saw pod success
Jun 19 21:28:02.574: INFO: Pod "pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02" satisfied condition "success or failure"
Jun 19 21:28:02.577: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02 container test-container: <nil>
STEP: delete the pod
Jun 19 21:28:02.599: INFO: Waiting for pod pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02 to disappear
Jun 19 21:28:02.603: INFO: Pod pod-a49af50b-833a-4c6d-8fbc-d9811ec34b02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:28:02.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5019" for this suite.
Jun 19 21:28:08.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:28:08.701: INFO: namespace emptydir-5019 deletion completed in 6.093575211s

• [SLOW TEST:12.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:28:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4742
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9753
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:28:15.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3551" for this suite.
Jun 19 21:28:21.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:28:21.292: INFO: namespace namespaces-3551 deletion completed in 6.080296882s
STEP: Destroying namespace "nsdeletetest-4742" for this suite.
Jun 19 21:28:21.294: INFO: Namespace nsdeletetest-4742 was already deleted
STEP: Destroying namespace "nsdeletetest-9753" for this suite.
Jun 19 21:28:27.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:28:27.370: INFO: namespace nsdeletetest-9753 deletion completed in 6.076457974s

• [SLOW TEST:18.670 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:28:27.371: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9093
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9093
STEP: Creating statefulset with conflicting port in namespace statefulset-9093
STEP: Waiting until pod test-pod will start running in namespace statefulset-9093
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9093
Jun 19 21:28:33.624: INFO: Observed stateful pod in namespace: statefulset-9093, name: ss-0, uid: e8992848-8cd1-4d6e-a729-3e9cafc3a983, status phase: Pending. Waiting for statefulset controller to delete.
Jun 19 21:28:33.802: INFO: Observed stateful pod in namespace: statefulset-9093, name: ss-0, uid: e8992848-8cd1-4d6e-a729-3e9cafc3a983, status phase: Failed. Waiting for statefulset controller to delete.
Jun 19 21:28:33.806: INFO: Observed stateful pod in namespace: statefulset-9093, name: ss-0, uid: e8992848-8cd1-4d6e-a729-3e9cafc3a983, status phase: Failed. Waiting for statefulset controller to delete.
Jun 19 21:28:33.810: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9093
STEP: Removing pod with conflicting port in namespace statefulset-9093
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9093 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 19 21:28:39.840: INFO: Deleting all statefulset in ns statefulset-9093
Jun 19 21:28:39.842: INFO: Scaling statefulset ss to 0
Jun 19 21:28:59.857: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:28:59.859: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:28:59.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9093" for this suite.
Jun 19 21:29:05.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:29:05.965: INFO: namespace statefulset-9093 deletion completed in 6.080970018s

• [SLOW TEST:38.594 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:29:05.965: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-78
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:29:06.116: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 19 21:29:11.119: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 19 21:29:11.119: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 19 21:29:17.149: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-78,SelfLink:/apis/apps/v1/namespaces/deployment-78/deployments/test-cleanup-deployment,UID:51dba18b-18c8-4422-a871-ad3646a1dbcc,ResourceVersion:18513,Generation:1,CreationTimestamp:2019-06-19 21:29:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-19 21:29:11 +0000 UTC 2019-06-19 21:29:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-19 21:29:15 +0000 UTC 2019-06-19 21:29:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 19 21:29:17.151: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-78,SelfLink:/apis/apps/v1/namespaces/deployment-78/replicasets/test-cleanup-deployment-55bbcbc84c,UID:f0b3e0f2-649a-4a69-8c00-dff80f23c6da,ResourceVersion:18502,Generation:1,CreationTimestamp:2019-06-19 21:29:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 51dba18b-18c8-4422-a871-ad3646a1dbcc 0xc003e59a17 0xc003e59a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 19 21:29:17.156: INFO: Pod "test-cleanup-deployment-55bbcbc84c-cd5nq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-cd5nq,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-78,SelfLink:/api/v1/namespaces/deployment-78/pods/test-cleanup-deployment-55bbcbc84c-cd5nq,UID:661ea030-3eaf-4cf9-819d-7ad190fe7a1f,ResourceVersion:18501,Generation:0,CreationTimestamp:2019-06-19 21:29:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c f0b3e0f2-649a-4a69-8c00-dff80f23c6da 0xc0027f1df7 0xc0027f1df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hjbvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hjbvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hjbvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027f1e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027f1e80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:29:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:29:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:29:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 21:29:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.72,StartTime:2019-06-19 21:29:11 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-19 21:29:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9e59a4b3cbd81ff22f8fc4d014d788b9cad01673295457bfe800b98e133ab9c9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:29:17.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-78" for this suite.
Jun 19 21:29:23.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:29:23.247: INFO: namespace deployment-78 deletion completed in 6.083982462s

• [SLOW TEST:17.282 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:29:23.249: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4617
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 19 21:29:23.405: INFO: Waiting up to 5m0s for pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba" in namespace "emptydir-4617" to be "success or failure"
Jun 19 21:29:23.416: INFO: Pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.76704ms
Jun 19 21:29:25.420: INFO: Pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014012635s
Jun 19 21:29:27.423: INFO: Pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017368732s
Jun 19 21:29:29.433: INFO: Pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027850326s
STEP: Saw pod success
Jun 19 21:29:29.436: INFO: Pod "pod-56432a2f-bb44-4670-a17e-525ff33b42ba" satisfied condition "success or failure"
Jun 19 21:29:29.438: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-56432a2f-bb44-4670-a17e-525ff33b42ba container test-container: <nil>
STEP: delete the pod
Jun 19 21:29:29.461: INFO: Waiting for pod pod-56432a2f-bb44-4670-a17e-525ff33b42ba to disappear
Jun 19 21:29:29.463: INFO: Pod pod-56432a2f-bb44-4670-a17e-525ff33b42ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:29:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4617" for this suite.
Jun 19 21:29:35.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:29:35.548: INFO: namespace emptydir-4617 deletion completed in 6.08157216s

• [SLOW TEST:12.299 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:29:35.548: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jun 19 21:29:35.695: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-497557601 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:29:35.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-390" for this suite.
Jun 19 21:29:41.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:29:41.853: INFO: namespace kubectl-390 deletion completed in 6.077912546s

• [SLOW TEST:6.305 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:29:41.854: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:29:42.002: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 19 21:29:43.029: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:29:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1479" for this suite.
Jun 19 21:29:50.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:29:50.124: INFO: namespace replication-controller-1479 deletion completed in 6.086699456s

• [SLOW TEST:8.270 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:29:50.124: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 19 21:29:50.297: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:50.301: INFO: Number of nodes with available pods: 0
Jun 19 21:29:50.301: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:29:51.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:51.308: INFO: Number of nodes with available pods: 0
Jun 19 21:29:51.308: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:29:52.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:52.308: INFO: Number of nodes with available pods: 0
Jun 19 21:29:52.308: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:29:53.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:53.308: INFO: Number of nodes with available pods: 0
Jun 19 21:29:53.308: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:29:54.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:54.307: INFO: Number of nodes with available pods: 0
Jun 19 21:29:54.307: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 21:29:55.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:55.307: INFO: Number of nodes with available pods: 2
Jun 19 21:29:55.307: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:29:56.305: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:56.308: INFO: Number of nodes with available pods: 3
Jun 19 21:29:56.308: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 19 21:29:56.327: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:56.330: INFO: Number of nodes with available pods: 2
Jun 19 21:29:56.330: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:29:57.335: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:57.338: INFO: Number of nodes with available pods: 2
Jun 19 21:29:57.338: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:29:58.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:58.337: INFO: Number of nodes with available pods: 2
Jun 19 21:29:58.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:29:59.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:29:59.337: INFO: Number of nodes with available pods: 2
Jun 19 21:29:59.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:00.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:00.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:00.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:01.335: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:01.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:01.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:02.335: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:02.338: INFO: Number of nodes with available pods: 2
Jun 19 21:30:02.338: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:03.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:03.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:03.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:04.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:04.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:04.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:05.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:05.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:05.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:06.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:06.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:06.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:07.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:07.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:07.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:08.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:08.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:08.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:09.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:09.338: INFO: Number of nodes with available pods: 2
Jun 19 21:30:09.338: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:10.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:10.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:10.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:11.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:11.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:11.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:12.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:12.336: INFO: Number of nodes with available pods: 2
Jun 19 21:30:12.336: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:13.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:13.337: INFO: Number of nodes with available pods: 2
Jun 19 21:30:13.337: INFO: Node k8s-pool1-37287165-vmss000001 is running more than one daemon pod
Jun 19 21:30:14.334: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 21:30:14.337: INFO: Number of nodes with available pods: 3
Jun 19 21:30:14.337: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4639, will wait for the garbage collector to delete the pods
Jun 19 21:30:14.397: INFO: Deleting DaemonSet.extensions daemon-set took: 4.69683ms
Jun 19 21:30:14.697: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.24456ms
Jun 19 21:30:30.100: INFO: Number of nodes with available pods: 0
Jun 19 21:30:30.100: INFO: Number of running nodes: 0, number of available pods: 0
Jun 19 21:30:30.102: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4639/daemonsets","resourceVersion":"18830"},"items":null}

Jun 19 21:30:30.104: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4639/pods","resourceVersion":"18830"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:30:30.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4639" for this suite.
Jun 19 21:30:36.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:30:36.197: INFO: namespace daemonsets-4639 deletion completed in 6.078618488s

• [SLOW TEST:46.073 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:30:36.200: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-a8ca10a1-db2e-42b4-9cc2-cf76698b5d0e
STEP: Creating secret with name secret-projected-all-test-volume-c4bc2ece-f5f7-43ad-b974-459209e378e1
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 19 21:30:36.374: INFO: Waiting up to 5m0s for pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe" in namespace "projected-5208" to be "success or failure"
Jun 19 21:30:36.385: INFO: Pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.835225ms
Jun 19 21:30:38.388: INFO: Pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014416643s
Jun 19 21:30:40.391: INFO: Pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01727246s
Jun 19 21:30:42.394: INFO: Pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020380776s
STEP: Saw pod success
Jun 19 21:30:42.394: INFO: Pod "projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe" satisfied condition "success or failure"
Jun 19 21:30:42.396: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 19 21:30:42.414: INFO: Waiting for pod projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe to disappear
Jun 19 21:30:42.418: INFO: Pod projected-volume-fdbcaf99-83cb-4c31-8c91-ce16b25b04fe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:30:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5208" for this suite.
Jun 19 21:30:48.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:30:48.532: INFO: namespace projected-5208 deletion completed in 6.11126456s

• [SLOW TEST:12.333 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:30:48.532: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 19 21:31:02.710: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:02.710: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:02.831: INFO: Exec stderr: ""
Jun 19 21:31:02.831: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:02.831: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:02.938: INFO: Exec stderr: ""
Jun 19 21:31:02.938: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:02.938: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.058: INFO: Exec stderr: ""
Jun 19 21:31:03.058: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.058: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.176: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 19 21:31:03.176: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.304: INFO: Exec stderr: ""
Jun 19 21:31:03.304: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.304: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.408: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 19 21:31:03.408: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.408: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.522: INFO: Exec stderr: ""
Jun 19 21:31:03.522: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.522: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.661: INFO: Exec stderr: ""
Jun 19 21:31:03.662: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.773: INFO: Exec stderr: ""
Jun 19 21:31:03.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2434 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 21:31:03.773: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 21:31:03.888: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:31:03.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2434" for this suite.
Jun 19 21:31:53.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:31:53.976: INFO: namespace e2e-kubelet-etc-hosts-2434 deletion completed in 50.084517417s

• [SLOW TEST:65.443 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:31:53.976: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 19 21:31:54.141: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jun 19 21:31:54.610: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 19 21:31:56.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:31:58.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:00.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:02.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:04.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:06.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:08.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:10.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:12.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:14.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:16.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:18.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:20.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:22.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:24.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696576714, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 21:32:27.496: INFO: Waited 819.899647ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:32:27.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8741" for this suite.
Jun 19 21:32:34.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:32:34.164: INFO: namespace aggregator-8741 deletion completed in 6.171726385s

• [SLOW TEST:40.188 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:32:34.164: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 19 21:32:34.316: INFO: Waiting up to 5m0s for pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c" in namespace "emptydir-6565" to be "success or failure"
Jun 19 21:32:34.320: INFO: Pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522148ms
Jun 19 21:32:36.323: INFO: Pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00663581s
Jun 19 21:32:38.326: INFO: Pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009687775s
Jun 19 21:32:40.329: INFO: Pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012770542s
STEP: Saw pod success
Jun 19 21:32:40.329: INFO: Pod "pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c" satisfied condition "success or failure"
Jun 19 21:32:40.331: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c container test-container: <nil>
STEP: delete the pod
Jun 19 21:32:40.346: INFO: Waiting for pod pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c to disappear
Jun 19 21:32:40.348: INFO: Pod pod-90b8fd2c-a46e-4f76-ab0b-4cddfcf7bd3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:32:40.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6565" for this suite.
Jun 19 21:32:46.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:32:46.431: INFO: namespace emptydir-6565 deletion completed in 6.079058889s

• [SLOW TEST:12.267 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:32:46.433: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:32:46.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09" in namespace "projected-7040" to be "success or failure"
Jun 19 21:32:46.588: INFO: Pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.72016ms
Jun 19 21:32:48.591: INFO: Pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005922735s
Jun 19 21:32:50.594: INFO: Pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008740317s
Jun 19 21:32:52.596: INFO: Pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011596102s
STEP: Saw pod success
Jun 19 21:32:52.596: INFO: Pod "downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09" satisfied condition "success or failure"
Jun 19 21:32:52.599: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09 container client-container: <nil>
STEP: delete the pod
Jun 19 21:32:52.620: INFO: Waiting for pod downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09 to disappear
Jun 19 21:32:52.623: INFO: Pod downwardapi-volume-a70a1767-5c96-48cb-9bfc-7406fdda2e09 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:32:52.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7040" for this suite.
Jun 19 21:32:58.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:32:58.711: INFO: namespace projected-7040 deletion completed in 6.083917859s

• [SLOW TEST:12.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:32:58.712: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 19 21:33:10.911: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 19 21:33:10.914: INFO: Pod pod-with-prestop-http-hook still exists
Jun 19 21:33:12.915: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 19 21:33:12.918: INFO: Pod pod-with-prestop-http-hook still exists
Jun 19 21:33:14.915: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 19 21:33:14.917: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:33:14.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-522" for this suite.
Jun 19 21:33:36.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:33:37.026: INFO: namespace container-lifecycle-hook-522 deletion completed in 22.096722175s

• [SLOW TEST:38.315 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:33:37.028: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 19 21:33:37.207: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 19 21:33:37.213: INFO: Waiting for terminating namespaces to be deleted...
Jun 19 21:33:37.216: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000000 before test
Jun 19 21:33:37.222: INFO: coredns-7f68dcdbdb-fzwx2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container coredns ready: true, restart count 0
Jun 19 21:33:37.222: INFO: blobfuse-flexvol-installer-9gm5x from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 21:33:37.222: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-vdlhn from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:37.222: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:37.222: INFO: azure-cni-networkmonitor-7s8d2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:37.222: INFO: kube-proxy-hzc6w from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:37.222: INFO: azure-ip-masq-agent-tg756 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:37.222: INFO: keyvault-flexvolume-lm2w8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 21:33:37.222: INFO: sonobuoy-e2e-job-944148fca6064c8c from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:37.222: INFO: 	Container e2e ready: true, restart count 0
Jun 19 21:33:37.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:37.222: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000001 before test
Jun 19 21:33:37.227: INFO: azure-cni-networkmonitor-852zj from kube-system started at 2019-06-19 19:02:06 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:37.227: INFO: blobfuse-flexvol-installer-44h2j from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 21:33:37.227: INFO: azure-ip-masq-agent-54llb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:37.227: INFO: keyvault-flexvolume-plrx8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 21:33:37.227: INFO: metrics-server-864ffbc5c-jj8rv from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container metrics-server ready: true, restart count 0
Jun 19 21:33:37.227: INFO: kube-proxy-bqn78 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:37.227: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-sqmlg from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:37.227: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:37.227: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:37.227: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000002 before test
Jun 19 21:33:37.233: INFO: azure-cni-networkmonitor-zc6s7 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:37.233: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-19 20:55:44 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 19 21:33:37.233: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-6hxg5 from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:37.233: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:37.233: INFO: kube-proxy-kdh48 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:37.233: INFO: blobfuse-flexvol-installer-lbfpx from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 21:33:37.233: INFO: azure-ip-masq-agent-djfsb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:37.233: INFO: keyvault-flexvolume-xgtzq from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:37.233: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node k8s-pool1-37287165-vmss000000
STEP: verifying the node has the label node k8s-pool1-37287165-vmss000001
STEP: verifying the node has the label node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod sonobuoy-e2e-job-944148fca6064c8c requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-6hxg5 requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-sqmlg requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-vdlhn requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod azure-cni-networkmonitor-7s8d2 requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod azure-cni-networkmonitor-852zj requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod azure-cni-networkmonitor-zc6s7 requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod azure-ip-masq-agent-54llb requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod azure-ip-masq-agent-djfsb requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod azure-ip-masq-agent-tg756 requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod blobfuse-flexvol-installer-44h2j requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod blobfuse-flexvol-installer-9gm5x requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod blobfuse-flexvol-installer-lbfpx requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod coredns-7f68dcdbdb-fzwx2 requesting resource cpu=100m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod keyvault-flexvolume-lm2w8 requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod keyvault-flexvolume-plrx8 requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod keyvault-flexvolume-xgtzq requesting resource cpu=50m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod kube-proxy-bqn78 requesting resource cpu=100m on Node k8s-pool1-37287165-vmss000001
Jun 19 21:33:37.299: INFO: Pod kube-proxy-hzc6w requesting resource cpu=100m on Node k8s-pool1-37287165-vmss000000
Jun 19 21:33:37.299: INFO: Pod kube-proxy-kdh48 requesting resource cpu=100m on Node k8s-pool1-37287165-vmss000002
Jun 19 21:33:37.299: INFO: Pod metrics-server-864ffbc5c-jj8rv requesting resource cpu=0m on Node k8s-pool1-37287165-vmss000001
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e.15a9b70e2a2b8256], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8107/filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e to k8s-pool1-37287165-vmss000002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e.15a9b70ec032defc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e.15a9b70efe2398c8], Reason = [Created], Message = [Created container filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e.15a9b70f10931b5d], Reason = [Started], Message = [Started container filler-pod-4974ee0c-7682-498d-a28d-224a44093f2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310.15a9b70e29d117b9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8107/filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310 to k8s-pool1-37287165-vmss000001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310.15a9b70eb950974b], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310.15a9b70ed90aa2cb], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310.15a9b70f1fac4d7a], Reason = [Created], Message = [Created container filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310.15a9b70f2c9fdc65], Reason = [Started], Message = [Started container filler-pod-da826f81-f5e7-4c7e-8713-e4e543f8b310]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924.15a9b70e292b2a23], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8107/filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924 to k8s-pool1-37287165-vmss000000]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924.15a9b70ea77ae8b7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924.15a9b70ee2f5d144], Reason = [Created], Message = [Created container filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924.15a9b70ef221e494], Reason = [Started], Message = [Started container filler-pod-f441a1b9-2175-46e2-beb1-9493d36f1924]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a9b70f90c526e4], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-pool1-37287165-vmss000002
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-pool1-37287165-vmss000000
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-pool1-37287165-vmss000001
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:33:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8107" for this suite.
Jun 19 21:33:50.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:33:50.478: INFO: namespace sched-pred-8107 deletion completed in 6.081471361s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:13.450 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:33:50.478: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jun 19 21:33:50.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 api-versions'
Jun 19 21:33:51.159: INFO: stderr: ""
Jun 19 21:33:51.159: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:33:51.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2271" for this suite.
Jun 19 21:33:57.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:33:57.292: INFO: namespace kubectl-2271 deletion completed in 6.124257053s

• [SLOW TEST:6.814 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:33:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 19 21:33:57.444: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 19 21:33:57.450: INFO: Waiting for terminating namespaces to be deleted...
Jun 19 21:33:57.452: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000000 before test
Jun 19 21:33:57.459: INFO: coredns-7f68dcdbdb-fzwx2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container coredns ready: true, restart count 0
Jun 19 21:33:57.459: INFO: blobfuse-flexvol-installer-9gm5x from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 21:33:57.459: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-vdlhn from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:57.459: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:57.459: INFO: azure-cni-networkmonitor-7s8d2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:57.459: INFO: kube-proxy-hzc6w from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:57.459: INFO: azure-ip-masq-agent-tg756 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:57.459: INFO: keyvault-flexvolume-lm2w8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 21:33:57.459: INFO: sonobuoy-e2e-job-944148fca6064c8c from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:57.459: INFO: 	Container e2e ready: true, restart count 0
Jun 19 21:33:57.459: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:57.459: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000001 before test
Jun 19 21:33:57.465: INFO: metrics-server-864ffbc5c-jj8rv from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container metrics-server ready: true, restart count 0
Jun 19 21:33:57.465: INFO: kube-proxy-bqn78 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:57.465: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-sqmlg from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:57.465: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:57.465: INFO: azure-cni-networkmonitor-852zj from kube-system started at 2019-06-19 19:02:06 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:57.465: INFO: blobfuse-flexvol-installer-44h2j from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 21:33:57.465: INFO: azure-ip-masq-agent-54llb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:57.465: INFO: keyvault-flexvolume-plrx8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.465: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 21:33:57.465: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000002 before test
Jun 19 21:33:57.471: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-6hxg5 from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 21:33:57.471: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 19 21:33:57.471: INFO: azure-cni-networkmonitor-zc6s7 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 21:33:57.471: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-19 20:55:44 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 19 21:33:57.471: INFO: azure-ip-masq-agent-djfsb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 21:33:57.471: INFO: keyvault-flexvolume-xgtzq from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 21:33:57.471: INFO: kube-proxy-kdh48 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 21:33:57.471: INFO: blobfuse-flexvol-installer-lbfpx from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 21:33:57.471: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a9b712dbf4a686], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:33:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4607" for this suite.
Jun 19 21:34:04.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:34:04.593: INFO: namespace sched-pred-4607 deletion completed in 6.093606324s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.302 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:34:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:34:04.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a" in namespace "downward-api-6013" to be "success or failure"
Jun 19 21:34:04.810: INFO: Pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.516046ms
Jun 19 21:34:06.813: INFO: Pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013256208s
Jun 19 21:34:08.816: INFO: Pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016084172s
Jun 19 21:34:10.819: INFO: Pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019421229s
STEP: Saw pod success
Jun 19 21:34:10.819: INFO: Pod "downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a" satisfied condition "success or failure"
Jun 19 21:34:10.822: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a container client-container: <nil>
STEP: delete the pod
Jun 19 21:34:10.839: INFO: Waiting for pod downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a to disappear
Jun 19 21:34:10.847: INFO: Pod downwardapi-volume-9af86205-b3df-440a-bd89-dd0d7ac2bf4a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:34:10.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6013" for this suite.
Jun 19 21:34:16.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:34:16.944: INFO: namespace downward-api-6013 deletion completed in 6.092083082s

• [SLOW TEST:12.350 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:34:16.945: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 19 21:34:57.116: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0619 21:34:57.116862      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:34:57.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1890" for this suite.
Jun 19 21:35:03.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:35:03.201: INFO: namespace gc-1890 deletion completed in 6.082232854s

• [SLOW TEST:46.257 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:35:03.202: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jun 19 21:35:03.349: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 19 21:35:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:07.988: INFO: stderr: ""
Jun 19 21:35:07.988: INFO: stdout: "service/redis-slave created\n"
Jun 19 21:35:07.989: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 19 21:35:07.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:08.424: INFO: stderr: ""
Jun 19 21:35:08.424: INFO: stdout: "service/redis-master created\n"
Jun 19 21:35:08.424: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 19 21:35:08.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:08.807: INFO: stderr: ""
Jun 19 21:35:08.807: INFO: stdout: "service/frontend created\n"
Jun 19 21:35:08.807: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 19 21:35:08.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:09.201: INFO: stderr: ""
Jun 19 21:35:09.201: INFO: stdout: "deployment.apps/frontend created\n"
Jun 19 21:35:09.201: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 19 21:35:09.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:09.601: INFO: stderr: ""
Jun 19 21:35:09.601: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 19 21:35:09.602: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 19 21:35:09.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-466'
Jun 19 21:35:10.500: INFO: stderr: ""
Jun 19 21:35:10.500: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 19 21:35:10.500: INFO: Waiting for all frontend pods to be Running.
Jun 19 21:35:55.552: INFO: Waiting for frontend to serve content.
Jun 19 21:35:55.577: INFO: Trying to add a new entry to the guestbook.
Jun 19 21:35:55.630: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 19 21:35:55.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:55.740: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:55.740: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 19 21:35:55.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:55.849: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:55.849: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 19 21:35:55.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:55.961: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:55.963: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 19 21:35:55.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:56.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:56.065: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 19 21:35:56.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:56.153: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:56.153: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 19 21:35:56.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-466'
Jun 19 21:35:56.230: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:35:56.230: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:35:56.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-466" for this suite.
Jun 19 21:36:34.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:36:34.316: INFO: namespace kubectl-466 deletion completed in 38.082306234s

• [SLOW TEST:91.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:36:34.317: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 19 21:36:34.716: INFO: Pod name wrapped-volume-race-1bee4c3e-f589-42ea-8ca9-69c3f77586c7: Found 0 pods out of 5
Jun 19 21:36:39.721: INFO: Pod name wrapped-volume-race-1bee4c3e-f589-42ea-8ca9-69c3f77586c7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1bee4c3e-f589-42ea-8ca9-69c3f77586c7 in namespace emptydir-wrapper-2081, will wait for the garbage collector to delete the pods
Jun 19 21:36:55.797: INFO: Deleting ReplicationController wrapped-volume-race-1bee4c3e-f589-42ea-8ca9-69c3f77586c7 took: 5.337122ms
Jun 19 21:36:56.097: INFO: Terminating ReplicationController wrapped-volume-race-1bee4c3e-f589-42ea-8ca9-69c3f77586c7 pods took: 300.630117ms
STEP: Creating RC which spawns configmap-volume pods
Jun 19 21:37:40.918: INFO: Pod name wrapped-volume-race-f70e83e9-bbcb-4b2b-b83b-5b4a5239f7a7: Found 0 pods out of 5
Jun 19 21:37:45.924: INFO: Pod name wrapped-volume-race-f70e83e9-bbcb-4b2b-b83b-5b4a5239f7a7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f70e83e9-bbcb-4b2b-b83b-5b4a5239f7a7 in namespace emptydir-wrapper-2081, will wait for the garbage collector to delete the pods
Jun 19 21:38:05.999: INFO: Deleting ReplicationController wrapped-volume-race-f70e83e9-bbcb-4b2b-b83b-5b4a5239f7a7 took: 5.131326ms
Jun 19 21:38:06.299: INFO: Terminating ReplicationController wrapped-volume-race-f70e83e9-bbcb-4b2b-b83b-5b4a5239f7a7 pods took: 300.17083ms
STEP: Creating RC which spawns configmap-volume pods
Jun 19 21:38:51.919: INFO: Pod name wrapped-volume-race-fa13e64e-562d-4898-af9f-0079132e101e: Found 0 pods out of 5
Jun 19 21:38:56.925: INFO: Pod name wrapped-volume-race-fa13e64e-562d-4898-af9f-0079132e101e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fa13e64e-562d-4898-af9f-0079132e101e in namespace emptydir-wrapper-2081, will wait for the garbage collector to delete the pods
Jun 19 21:39:17.001: INFO: Deleting ReplicationController wrapped-volume-race-fa13e64e-562d-4898-af9f-0079132e101e took: 5.743217ms
Jun 19 21:39:17.401: INFO: Terminating ReplicationController wrapped-volume-race-fa13e64e-562d-4898-af9f-0079132e101e pods took: 400.271281ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:40:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2081" for this suite.
Jun 19 21:40:09.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:40:09.253: INFO: namespace emptydir-wrapper-2081 deletion completed in 8.080385833s

• [SLOW TEST:214.937 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:40:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-afdec6d9-69dd-4699-95fd-0e1ea9a6f36f
STEP: Creating a pod to test consume secrets
Jun 19 21:40:09.410: INFO: Waiting up to 5m0s for pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6" in namespace "secrets-2773" to be "success or failure"
Jun 19 21:40:09.418: INFO: Pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955485ms
Jun 19 21:40:11.422: INFO: Pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011549586s
Jun 19 21:40:13.424: INFO: Pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014196401s
Jun 19 21:40:15.428: INFO: Pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017241611s
STEP: Saw pod success
Jun 19 21:40:15.428: INFO: Pod "pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6" satisfied condition "success or failure"
Jun 19 21:40:15.430: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6 container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:40:15.452: INFO: Waiting for pod pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6 to disappear
Jun 19 21:40:15.454: INFO: Pod pod-secrets-7b1651d8-48ea-4811-a6e8-63e3fbbb30b6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:40:15.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2773" for this suite.
Jun 19 21:40:21.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:40:21.537: INFO: namespace secrets-2773 deletion completed in 6.079728912s

• [SLOW TEST:12.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:40:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:40:21.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7091" for this suite.
Jun 19 21:40:27.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:40:27.785: INFO: namespace services-7091 deletion completed in 6.096231781s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.247 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:40:27.788: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:40:27.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2" in namespace "downward-api-9804" to be "success or failure"
Jun 19 21:40:27.960: INFO: Pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.080523ms
Jun 19 21:40:29.963: INFO: Pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021889943s
Jun 19 21:40:31.966: INFO: Pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025123459s
Jun 19 21:40:33.970: INFO: Pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028590571s
STEP: Saw pod success
Jun 19 21:40:33.970: INFO: Pod "downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2" satisfied condition "success or failure"
Jun 19 21:40:33.972: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2 container client-container: <nil>
STEP: delete the pod
Jun 19 21:40:33.996: INFO: Waiting for pod downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2 to disappear
Jun 19 21:40:33.998: INFO: Pod downwardapi-volume-be98741d-7cd4-46f7-bc35-0c873b5b10f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:40:33.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9804" for this suite.
Jun 19 21:40:40.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:40:40.083: INFO: namespace downward-api-9804 deletion completed in 6.080442127s

• [SLOW TEST:12.295 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:40:40.083: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jun 19 21:40:40.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-9114'
Jun 19 21:40:40.642: INFO: stderr: ""
Jun 19 21:40:40.642: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:40:40.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9114'
Jun 19 21:40:40.737: INFO: stderr: ""
Jun 19 21:40:40.737: INFO: stdout: "update-demo-nautilus-6k99k update-demo-nautilus-gpcfq "
Jun 19 21:40:40.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-6k99k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:40:40.826: INFO: stderr: ""
Jun 19 21:40:40.826: INFO: stdout: ""
Jun 19 21:40:40.826: INFO: update-demo-nautilus-6k99k is created but not running
Jun 19 21:40:45.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9114'
Jun 19 21:40:45.909: INFO: stderr: ""
Jun 19 21:40:45.909: INFO: stdout: "update-demo-nautilus-6k99k update-demo-nautilus-gpcfq "
Jun 19 21:40:45.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-6k99k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:40:45.984: INFO: stderr: ""
Jun 19 21:40:45.984: INFO: stdout: "true"
Jun 19 21:40:45.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-6k99k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:40:46.067: INFO: stderr: ""
Jun 19 21:40:46.067: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:40:46.067: INFO: validating pod update-demo-nautilus-6k99k
Jun 19 21:40:46.074: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:40:46.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:40:46.074: INFO: update-demo-nautilus-6k99k is verified up and running
Jun 19 21:40:46.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-gpcfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:40:46.155: INFO: stderr: ""
Jun 19 21:40:46.155: INFO: stdout: "true"
Jun 19 21:40:46.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-gpcfq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:40:46.234: INFO: stderr: ""
Jun 19 21:40:46.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:40:46.234: INFO: validating pod update-demo-nautilus-gpcfq
Jun 19 21:40:46.245: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:40:46.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:40:46.245: INFO: update-demo-nautilus-gpcfq is verified up and running
STEP: rolling-update to new replication controller
Jun 19 21:40:46.249: INFO: scanned /root for discovery docs: <nil>
Jun 19 21:40:46.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9114'
Jun 19 21:41:12.848: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 19 21:41:12.848: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:41:12.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9114'
Jun 19 21:41:12.932: INFO: stderr: ""
Jun 19 21:41:12.932: INFO: stdout: "update-demo-kitten-p8nbk update-demo-kitten-rlzct "
Jun 19 21:41:12.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-kitten-p8nbk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:41:13.010: INFO: stderr: ""
Jun 19 21:41:13.010: INFO: stdout: "true"
Jun 19 21:41:13.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-kitten-p8nbk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:41:13.082: INFO: stderr: ""
Jun 19 21:41:13.082: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 19 21:41:13.082: INFO: validating pod update-demo-kitten-p8nbk
Jun 19 21:41:13.088: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 19 21:41:13.088: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 19 21:41:13.088: INFO: update-demo-kitten-p8nbk is verified up and running
Jun 19 21:41:13.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-kitten-rlzct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:41:13.158: INFO: stderr: ""
Jun 19 21:41:13.158: INFO: stdout: "true"
Jun 19 21:41:13.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-kitten-rlzct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9114'
Jun 19 21:41:13.244: INFO: stderr: ""
Jun 19 21:41:13.244: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 19 21:41:13.244: INFO: validating pod update-demo-kitten-rlzct
Jun 19 21:41:13.251: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 19 21:41:13.251: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 19 21:41:13.251: INFO: update-demo-kitten-rlzct is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:41:13.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9114" for this suite.
Jun 19 21:41:35.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:41:35.346: INFO: namespace kubectl-9114 deletion completed in 22.091492009s

• [SLOW TEST:55.263 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:41:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-81236f88-1d4e-4fcc-9c3b-42c71a1f35b9
STEP: Creating a pod to test consume configMaps
Jun 19 21:41:35.508: INFO: Waiting up to 5m0s for pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d" in namespace "configmap-2821" to be "success or failure"
Jun 19 21:41:35.512: INFO: Pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399851ms
Jun 19 21:41:37.515: INFO: Pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006584395s
Jun 19 21:41:39.519: INFO: Pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010306631s
Jun 19 21:41:41.522: INFO: Pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013549776s
STEP: Saw pod success
Jun 19 21:41:41.522: INFO: Pod "pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d" satisfied condition "success or failure"
Jun 19 21:41:41.525: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:41:41.549: INFO: Waiting for pod pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d to disappear
Jun 19 21:41:41.553: INFO: Pod pod-configmaps-15f90b2a-0cf6-45af-b5ef-0434a89cc56d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:41:41.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2821" for this suite.
Jun 19 21:41:47.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:41:47.650: INFO: namespace configmap-2821 deletion completed in 6.090801362s

• [SLOW TEST:12.304 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:41:47.650: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-78fe7846-b665-4573-803b-7b0dbc89fe55
STEP: Creating a pod to test consume secrets
Jun 19 21:41:47.804: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96" in namespace "projected-2047" to be "success or failure"
Jun 19 21:41:47.817: INFO: Pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019525ms
Jun 19 21:41:49.820: INFO: Pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015132075s
Jun 19 21:41:51.823: INFO: Pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018211826s
Jun 19 21:41:53.826: INFO: Pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021666072s
STEP: Saw pod success
Jun 19 21:41:53.826: INFO: Pod "pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96" satisfied condition "success or failure"
Jun 19 21:41:53.828: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:41:53.849: INFO: Waiting for pod pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96 to disappear
Jun 19 21:41:53.852: INFO: Pod pod-projected-secrets-26b7cec4-14eb-4bf0-aa9a-cc6395248c96 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:41:53.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2047" for this suite.
Jun 19 21:41:59.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:41:59.939: INFO: namespace projected-2047 deletion completed in 6.083321286s

• [SLOW TEST:12.290 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:41:59.943: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:42:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5673" for this suite.
Jun 19 21:42:14.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:42:14.190: INFO: namespace kubelet-test-5673 deletion completed in 6.080219147s

• [SLOW TEST:14.250 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:42:14.192: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 19 21:42:14.346: INFO: Waiting up to 5m0s for pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808" in namespace "emptydir-7267" to be "success or failure"
Jun 19 21:42:14.350: INFO: Pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808": Phase="Pending", Reason="", readiness=false. Elapsed: 3.182254ms
Jun 19 21:42:16.353: INFO: Pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006893005s
Jun 19 21:42:18.357: INFO: Pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010217263s
Jun 19 21:42:20.360: INFO: Pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013447122s
STEP: Saw pod success
Jun 19 21:42:20.360: INFO: Pod "pod-3aebdb17-b48f-491b-8864-f300ea9c2808" satisfied condition "success or failure"
Jun 19 21:42:20.363: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-3aebdb17-b48f-491b-8864-f300ea9c2808 container test-container: <nil>
STEP: delete the pod
Jun 19 21:42:20.377: INFO: Waiting for pod pod-3aebdb17-b48f-491b-8864-f300ea9c2808 to disappear
Jun 19 21:42:20.389: INFO: Pod pod-3aebdb17-b48f-491b-8864-f300ea9c2808 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:42:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7267" for this suite.
Jun 19 21:42:26.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:42:26.473: INFO: namespace emptydir-7267 deletion completed in 6.081086248s

• [SLOW TEST:12.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:42:26.474: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 19 21:42:26.632: INFO: Waiting up to 5m0s for pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b" in namespace "emptydir-6645" to be "success or failure"
Jun 19 21:42:26.643: INFO: Pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.227552ms
Jun 19 21:42:28.646: INFO: Pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013259917s
Jun 19 21:42:30.649: INFO: Pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016384982s
Jun 19 21:42:32.652: INFO: Pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019624746s
STEP: Saw pod success
Jun 19 21:42:32.652: INFO: Pod "pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b" satisfied condition "success or failure"
Jun 19 21:42:32.655: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b container test-container: <nil>
STEP: delete the pod
Jun 19 21:42:32.670: INFO: Waiting for pod pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b to disappear
Jun 19 21:42:32.673: INFO: Pod pod-ee55d5a6-40e4-4feb-b8e2-e0721d28991b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:42:32.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6645" for this suite.
Jun 19 21:42:38.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:42:38.781: INFO: namespace emptydir-6645 deletion completed in 6.10458802s

• [SLOW TEST:12.307 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:42:38.781: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-cf22d359-2fdd-4885-a06d-64a084306e3d
STEP: Creating a pod to test consume secrets
Jun 19 21:42:38.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d" in namespace "projected-9430" to be "success or failure"
Jun 19 21:42:38.957: INFO: Pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.592617ms
Jun 19 21:42:40.960: INFO: Pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015089395s
Jun 19 21:42:42.963: INFO: Pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017885869s
Jun 19 21:42:44.965: INFO: Pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020573445s
STEP: Saw pod success
Jun 19 21:42:44.966: INFO: Pod "pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d" satisfied condition "success or failure"
Jun 19 21:42:44.968: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 21:42:44.994: INFO: Waiting for pod pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d to disappear
Jun 19 21:42:44.997: INFO: Pod pod-projected-secrets-880342fe-8843-4e4a-8641-fd4d7124836d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:42:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9430" for this suite.
Jun 19 21:42:51.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:42:51.107: INFO: namespace projected-9430 deletion completed in 6.106349508s

• [SLOW TEST:12.325 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:42:51.108: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-8d51a6e3-8dfa-4f3a-90ef-50d70617bd2c in namespace container-probe-2345
Jun 19 21:42:57.266: INFO: Started pod busybox-8d51a6e3-8dfa-4f3a-90ef-50d70617bd2c in namespace container-probe-2345
STEP: checking the pod's current state and verifying that restartCount is present
Jun 19 21:42:57.269: INFO: Initial restart count of pod busybox-8d51a6e3-8dfa-4f3a-90ef-50d70617bd2c is 0
Jun 19 21:43:47.357: INFO: Restart count of pod container-probe-2345/busybox-8d51a6e3-8dfa-4f3a-90ef-50d70617bd2c is now 1 (50.088448317s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:43:47.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2345" for this suite.
Jun 19 21:43:53.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:43:53.473: INFO: namespace container-probe-2345 deletion completed in 6.103507711s

• [SLOW TEST:62.365 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:43:53.474: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 21:43:59.652: INFO: Waiting up to 5m0s for pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c" in namespace "pods-4843" to be "success or failure"
Jun 19 21:43:59.667: INFO: Pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.753386ms
Jun 19 21:44:01.671: INFO: Pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018479672s
Jun 19 21:44:03.674: INFO: Pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021943162s
Jun 19 21:44:05.677: INFO: Pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024625465s
STEP: Saw pod success
Jun 19 21:44:05.677: INFO: Pod "client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c" satisfied condition "success or failure"
Jun 19 21:44:05.683: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c container env3cont: <nil>
STEP: delete the pod
Jun 19 21:44:05.713: INFO: Waiting for pod client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c to disappear
Jun 19 21:44:05.716: INFO: Pod client-envvars-4ee99307-495f-44ef-8c99-bf30ba6f8c6c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:44:05.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4843" for this suite.
Jun 19 21:44:55.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:44:55.804: INFO: namespace pods-4843 deletion completed in 50.084448093s

• [SLOW TEST:62.330 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:44:55.805: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 19 21:44:55.966: INFO: Waiting up to 5m0s for pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf" in namespace "downward-api-657" to be "success or failure"
Jun 19 21:44:55.989: INFO: Pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.639072ms
Jun 19 21:44:57.995: INFO: Pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028938937s
Jun 19 21:45:00.008: INFO: Pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042112103s
Jun 19 21:45:02.013: INFO: Pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046681894s
STEP: Saw pod success
Jun 19 21:45:02.013: INFO: Pod "downward-api-b2996f46-2000-41b4-b022-aeaf555014bf" satisfied condition "success or failure"
Jun 19 21:45:02.016: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downward-api-b2996f46-2000-41b4-b022-aeaf555014bf container dapi-container: <nil>
STEP: delete the pod
Jun 19 21:45:02.032: INFO: Waiting for pod downward-api-b2996f46-2000-41b4-b022-aeaf555014bf to disappear
Jun 19 21:45:02.034: INFO: Pod downward-api-b2996f46-2000-41b4-b022-aeaf555014bf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:45:02.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-657" for this suite.
Jun 19 21:45:08.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:45:08.114: INFO: namespace downward-api-657 deletion completed in 6.077102858s

• [SLOW TEST:12.310 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:45:08.115: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:45:08.274: INFO: Waiting up to 5m0s for pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41" in namespace "downward-api-1631" to be "success or failure"
Jun 19 21:45:08.287: INFO: Pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026726ms
Jun 19 21:45:10.290: INFO: Pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015020242s
Jun 19 21:45:12.293: INFO: Pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018324154s
Jun 19 21:45:14.296: INFO: Pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021517668s
STEP: Saw pod success
Jun 19 21:45:14.296: INFO: Pod "downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41" satisfied condition "success or failure"
Jun 19 21:45:14.299: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41 container client-container: <nil>
STEP: delete the pod
Jun 19 21:45:14.320: INFO: Waiting for pod downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41 to disappear
Jun 19 21:45:14.323: INFO: Pod downwardapi-volume-572dd8f6-a923-4bf4-98fe-b07627860f41 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:45:14.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1631" for this suite.
Jun 19 21:45:20.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:45:20.405: INFO: namespace downward-api-1631 deletion completed in 6.078903842s

• [SLOW TEST:12.291 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:45:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:45:20.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb" in namespace "projected-8329" to be "success or failure"
Jun 19 21:45:20.571: INFO: Pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.40195ms
Jun 19 21:45:22.575: INFO: Pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013474267s
Jun 19 21:45:24.578: INFO: Pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016822582s
Jun 19 21:45:26.581: INFO: Pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020006899s
STEP: Saw pod success
Jun 19 21:45:26.581: INFO: Pod "downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb" satisfied condition "success or failure"
Jun 19 21:45:26.583: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb container client-container: <nil>
STEP: delete the pod
Jun 19 21:45:26.619: INFO: Waiting for pod downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb to disappear
Jun 19 21:45:26.622: INFO: Pod downwardapi-volume-01bf3777-e01b-4b9d-9bf5-a502f30541fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:45:26.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8329" for this suite.
Jun 19 21:45:32.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:45:32.712: INFO: namespace projected-8329 deletion completed in 6.085463157s

• [SLOW TEST:12.306 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:45:32.712: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6671
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 19 21:45:32.879: INFO: Found 0 stateful pods, waiting for 3
Jun 19 21:45:42.883: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:45:42.883: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:45:42.883: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 19 21:45:52.883: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:45:52.883: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:45:52.883: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:45:52.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-6671 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:45:55.130: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:45:55.130: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:45:55.130: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 19 21:46:05.155: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 19 21:46:15.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-6671 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:46:15.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 21:46:15.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 21:46:15.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 21:46:25.385: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:46:25.385: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:25.385: INFO: Waiting for Pod statefulset-6671/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:25.385: INFO: Waiting for Pod statefulset-6671/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:35.390: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:46:35.390: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:35.390: INFO: Waiting for Pod statefulset-6671/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:45.391: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:46:45.391: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:46:55.391: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:46:55.391: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:47:05.392: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
STEP: Rolling back to a previous revision
Jun 19 21:47:15.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-6671 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 21:47:15.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 21:47:15.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 21:47:15.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 21:47:25.797: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 19 21:47:35.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-6671 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 21:47:36.019: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 21:47:36.019: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 21:47:36.019: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 21:47:46.033: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:47:46.034: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 19 21:47:46.034: INFO: Waiting for Pod statefulset-6671/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 19 21:47:56.039: INFO: Waiting for StatefulSet statefulset-6671/ss2 to complete update
Jun 19 21:47:56.039: INFO: Waiting for Pod statefulset-6671/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 19 21:48:06.039: INFO: Deleting all statefulset in ns statefulset-6671
Jun 19 21:48:06.041: INFO: Scaling statefulset ss2 to 0
Jun 19 21:48:26.060: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:48:26.062: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:48:26.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6671" for this suite.
Jun 19 21:48:32.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:48:32.171: INFO: namespace statefulset-6671 deletion completed in 6.096494614s

• [SLOW TEST:179.459 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:48:32.172: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0619 21:48:33.359675      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 19 21:48:33.359: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:48:33.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-884" for this suite.
Jun 19 21:48:39.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:48:39.444: INFO: namespace gc-884 deletion completed in 6.078242881s

• [SLOW TEST:7.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:48:39.444: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:48:39.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0" in namespace "projected-2785" to be "success or failure"
Jun 19 21:48:39.607: INFO: Pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365565ms
Jun 19 21:48:41.611: INFO: Pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012741721s
Jun 19 21:48:43.614: INFO: Pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01589808s
Jun 19 21:48:45.617: INFO: Pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019241437s
STEP: Saw pod success
Jun 19 21:48:45.617: INFO: Pod "downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0" satisfied condition "success or failure"
Jun 19 21:48:45.619: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0 container client-container: <nil>
STEP: delete the pod
Jun 19 21:48:45.642: INFO: Waiting for pod downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0 to disappear
Jun 19 21:48:45.645: INFO: Pod downwardapi-volume-82ad1eb8-6787-4d62-893e-cc1813abe4e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:48:45.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2785" for this suite.
Jun 19 21:48:51.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:48:51.732: INFO: namespace projected-2785 deletion completed in 6.082837921s

• [SLOW TEST:12.288 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:48:51.732: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:48:57.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7005" for this suite.
Jun 19 21:49:41.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:49:42.043: INFO: namespace kubelet-test-7005 deletion completed in 44.101791538s

• [SLOW TEST:50.310 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:49:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2ae3d1ec-65b4-42f8-9fd7-75f301de308b in namespace container-probe-6604
Jun 19 21:49:50.201: INFO: Started pod liveness-2ae3d1ec-65b4-42f8-9fd7-75f301de308b in namespace container-probe-6604
STEP: checking the pod's current state and verifying that restartCount is present
Jun 19 21:49:50.204: INFO: Initial restart count of pod liveness-2ae3d1ec-65b4-42f8-9fd7-75f301de308b is 0
Jun 19 21:50:06.228: INFO: Restart count of pod container-probe-6604/liveness-2ae3d1ec-65b4-42f8-9fd7-75f301de308b is now 1 (16.024041966s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:50:06.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6604" for this suite.
Jun 19 21:50:12.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:50:12.326: INFO: namespace container-probe-6604 deletion completed in 6.079598127s

• [SLOW TEST:30.283 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:50:12.327: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9487
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-6db79692-9401-47ec-b250-8e8f32a79640
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-6db79692-9401-47ec-b250-8e8f32a79640
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:51:48.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9487" for this suite.
Jun 19 21:52:02.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:52:02.936: INFO: namespace configmap-9487 deletion completed in 14.095252334s

• [SLOW TEST:110.609 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:52:02.937: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jun 19 21:52:03.095: INFO: Waiting up to 5m0s for pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12" in namespace "containers-9044" to be "success or failure"
Jun 19 21:52:03.098: INFO: Pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825042ms
Jun 19 21:52:05.101: INFO: Pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00673018s
Jun 19 21:52:07.104: INFO: Pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009651821s
Jun 19 21:52:09.108: INFO: Pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013317955s
STEP: Saw pod success
Jun 19 21:52:09.108: INFO: Pod "client-containers-7032d625-754d-4dd4-8f62-a306327d7b12" satisfied condition "success or failure"
Jun 19 21:52:09.110: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod client-containers-7032d625-754d-4dd4-8f62-a306327d7b12 container test-container: <nil>
STEP: delete the pod
Jun 19 21:52:09.125: INFO: Waiting for pod client-containers-7032d625-754d-4dd4-8f62-a306327d7b12 to disappear
Jun 19 21:52:09.128: INFO: Pod client-containers-7032d625-754d-4dd4-8f62-a306327d7b12 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:52:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9044" for this suite.
Jun 19 21:52:15.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:52:15.228: INFO: namespace containers-9044 deletion completed in 6.097584829s

• [SLOW TEST:12.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:52:15.229: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 19 21:52:15.374: INFO: PodSpec: initContainers in spec.initContainers
Jun 19 21:53:07.600: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fae8caf4-b591-4995-870e-31012ffd3d32", GenerateName:"", Namespace:"init-container-1461", SelfLink:"/api/v1/namespaces/init-container-1461/pods/pod-init-fae8caf4-b591-4995-870e-31012ffd3d32", UID:"b76a4bdc-6b97-4d96-8b1e-5366c1a53bc5", ResourceVersion:"23723", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63696577935, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"374537406"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-tnhtg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00284ae80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tnhtg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tnhtg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tnhtg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0006b8ca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-pool1-37287165-vmss000002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00230e0c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0006b8d50)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0006b8d70)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0006b8d78), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0006b8d7c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696577935, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696577935, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696577935, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696577935, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.96", PodIP:"10.240.0.112", StartTime:(*v1.Time)(0xc000869760), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a5a9a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a5aa10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://85fd7a39df22b55e09570ec6304db45bd912ae49f7ad127a66a387f6ebcd9eda"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0008697e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0008697c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:53:07.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1461" for this suite.
Jun 19 21:53:29.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:53:29.695: INFO: namespace init-container-1461 deletion completed in 22.082058885s

• [SLOW TEST:74.466 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:53:29.695: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-598ee149-48d4-4a9f-ae2f-ffcea7e88ee0
STEP: Creating a pod to test consume secrets
Jun 19 21:53:29.849: INFO: Waiting up to 5m0s for pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296" in namespace "secrets-3539" to be "success or failure"
Jun 19 21:53:29.853: INFO: Pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296": Phase="Pending", Reason="", readiness=false. Elapsed: 4.423534ms
Jun 19 21:53:31.856: INFO: Pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007531517s
Jun 19 21:53:33.860: INFO: Pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011110396s
Jun 19 21:53:35.863: INFO: Pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014430482s
STEP: Saw pod success
Jun 19 21:53:35.863: INFO: Pod "pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296" satisfied condition "success or failure"
Jun 19 21:53:35.866: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296 container secret-env-test: <nil>
STEP: delete the pod
Jun 19 21:53:35.894: INFO: Waiting for pod pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296 to disappear
Jun 19 21:53:35.898: INFO: Pod pod-secrets-f2ba3f5c-d6c1-445b-b1ae-7202752c8296 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:53:35.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3539" for this suite.
Jun 19 21:53:41.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:53:41.989: INFO: namespace secrets-3539 deletion completed in 6.087929217s

• [SLOW TEST:12.294 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:53:41.990: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jun 19 21:53:42.142: INFO: Waiting up to 5m0s for pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86" in namespace "var-expansion-1201" to be "success or failure"
Jun 19 21:53:42.154: INFO: Pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.240518ms
Jun 19 21:53:44.158: INFO: Pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015644215s
Jun 19 21:53:46.162: INFO: Pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020225199s
Jun 19 21:53:48.165: INFO: Pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022756815s
STEP: Saw pod success
Jun 19 21:53:48.165: INFO: Pod "var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86" satisfied condition "success or failure"
Jun 19 21:53:48.167: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86 container dapi-container: <nil>
STEP: delete the pod
Jun 19 21:53:48.190: INFO: Waiting for pod var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86 to disappear
Jun 19 21:53:48.198: INFO: Pod var-expansion-05e9a97d-acd4-4bc6-ba8d-84dba0dacd86 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:53:48.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1201" for this suite.
Jun 19 21:53:54.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:53:54.287: INFO: namespace var-expansion-1201 deletion completed in 6.085121816s

• [SLOW TEST:12.298 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:53:54.288: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-cd100452-3366-4c63-a1a7-179f5bb532f2
STEP: Creating a pod to test consume configMaps
Jun 19 21:53:54.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f" in namespace "configmap-7632" to be "success or failure"
Jun 19 21:53:54.450: INFO: Pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294851ms
Jun 19 21:53:56.453: INFO: Pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006287873s
Jun 19 21:53:58.456: INFO: Pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008844805s
Jun 19 21:54:00.459: INFO: Pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011687036s
STEP: Saw pod success
Jun 19 21:54:00.459: INFO: Pod "pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f" satisfied condition "success or failure"
Jun 19 21:54:00.461: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 21:54:00.477: INFO: Waiting for pod pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f to disappear
Jun 19 21:54:00.484: INFO: Pod pod-configmaps-cd2afa79-05cc-47c4-b620-15b67955931f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:54:00.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7632" for this suite.
Jun 19 21:54:06.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:54:06.581: INFO: namespace configmap-7632 deletion completed in 6.091927271s

• [SLOW TEST:12.293 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:54:06.581: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5130
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 19 21:54:06.735: INFO: Waiting up to 5m0s for pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6" in namespace "emptydir-5130" to be "success or failure"
Jun 19 21:54:06.738: INFO: Pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046154ms
Jun 19 21:54:08.741: INFO: Pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005936696s
Jun 19 21:54:10.743: INFO: Pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008775442s
Jun 19 21:54:12.747: INFO: Pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012371679s
STEP: Saw pod success
Jun 19 21:54:12.747: INFO: Pod "pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6" satisfied condition "success or failure"
Jun 19 21:54:12.749: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6 container test-container: <nil>
STEP: delete the pod
Jun 19 21:54:12.765: INFO: Waiting for pod pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6 to disappear
Jun 19 21:54:12.767: INFO: Pod pod-9d2f60da-7a1e-4c9b-b627-324573e5c2d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:54:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5130" for this suite.
Jun 19 21:54:18.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:54:18.878: INFO: namespace emptydir-5130 deletion completed in 6.107438594s

• [SLOW TEST:12.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:54:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 21:54:19.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1" in namespace "downward-api-2995" to be "success or failure"
Jun 19 21:54:19.047: INFO: Pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.386561ms
Jun 19 21:54:21.050: INFO: Pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012394719s
Jun 19 21:54:23.053: INFO: Pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014895887s
Jun 19 21:54:25.055: INFO: Pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017498957s
STEP: Saw pod success
Jun 19 21:54:25.055: INFO: Pod "downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1" satisfied condition "success or failure"
Jun 19 21:54:25.058: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1 container client-container: <nil>
STEP: delete the pod
Jun 19 21:54:25.084: INFO: Waiting for pod downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1 to disappear
Jun 19 21:54:25.087: INFO: Pod downwardapi-volume-0e2d1532-29ac-4cd7-8dff-0e120d0959f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:54:25.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2995" for this suite.
Jun 19 21:54:31.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:54:31.176: INFO: namespace downward-api-2995 deletion completed in 6.085995866s

• [SLOW TEST:12.298 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:54:31.176: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9718
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 19 21:54:31.344: INFO: Found 0 stateful pods, waiting for 3
Jun 19 21:54:41.348: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:54:41.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:54:41.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 19 21:54:51.351: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:54:51.351: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:54:51.351: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 19 21:54:51.378: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 19 21:55:01.415: INFO: Updating stateful set ss2
Jun 19 21:55:01.433: INFO: Waiting for Pod statefulset-9718/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jun 19 21:55:11.513: INFO: Found 2 stateful pods, waiting for 3
Jun 19 21:55:21.517: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:55:21.517: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:55:21.517: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 19 21:55:31.518: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:55:31.518: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 21:55:31.518: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 19 21:55:31.545: INFO: Updating stateful set ss2
Jun 19 21:55:31.572: INFO: Waiting for Pod statefulset-9718/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:55:41.595: INFO: Updating stateful set ss2
Jun 19 21:55:41.610: INFO: Waiting for StatefulSet statefulset-9718/ss2 to complete update
Jun 19 21:55:41.610: INFO: Waiting for Pod statefulset-9718/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 19 21:55:51.616: INFO: Waiting for StatefulSet statefulset-9718/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 19 21:56:01.616: INFO: Deleting all statefulset in ns statefulset-9718
Jun 19 21:56:01.620: INFO: Scaling statefulset ss2 to 0
Jun 19 21:56:31.638: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 21:56:31.641: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:56:31.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9718" for this suite.
Jun 19 21:56:37.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:56:37.740: INFO: namespace statefulset-9718 deletion completed in 6.081776506s

• [SLOW TEST:126.563 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:56:37.740: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:56:44.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7977" for this suite.
Jun 19 21:57:02.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:57:03.038: INFO: namespace replication-controller-7977 deletion completed in 18.108821221s

• [SLOW TEST:25.297 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:57:03.038: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 19 21:57:03.216: INFO: Waiting up to 5m0s for pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2" in namespace "emptydir-1128" to be "success or failure"
Jun 19 21:57:03.224: INFO: Pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.81487ms
Jun 19 21:57:05.228: INFO: Pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011908827s
Jun 19 21:57:07.231: INFO: Pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015214482s
Jun 19 21:57:09.234: INFO: Pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018202645s
STEP: Saw pod success
Jun 19 21:57:09.234: INFO: Pod "pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2" satisfied condition "success or failure"
Jun 19 21:57:09.236: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2 container test-container: <nil>
STEP: delete the pod
Jun 19 21:57:09.258: INFO: Waiting for pod pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2 to disappear
Jun 19 21:57:09.261: INFO: Pod pod-7bfba3a2-b642-4f7a-ad2a-ca7a7708aeb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:57:09.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1128" for this suite.
Jun 19 21:57:15.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:57:15.355: INFO: namespace emptydir-1128 deletion completed in 6.0903454s

• [SLOW TEST:12.317 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:57:15.355: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 19 21:57:22.062: INFO: Successfully updated pod "labelsupdateaa2e6a40-8448-4ac8-b67a-a98d13f5dbfa"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:57:24.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5592" for this suite.
Jun 19 21:57:46.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:57:46.168: INFO: namespace projected-5592 deletion completed in 22.082550551s

• [SLOW TEST:30.813 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:57:46.169: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 19 21:57:46.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-1510'
Jun 19 21:57:48.848: INFO: stderr: ""
Jun 19 21:57:48.848: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 19 21:57:48.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1510'
Jun 19 21:57:48.934: INFO: stderr: ""
Jun 19 21:57:48.934: INFO: stdout: "update-demo-nautilus-j6m92 update-demo-nautilus-zzlkk "
Jun 19 21:57:48.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-j6m92 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1510'
Jun 19 21:57:49.008: INFO: stderr: ""
Jun 19 21:57:49.008: INFO: stdout: ""
Jun 19 21:57:49.008: INFO: update-demo-nautilus-j6m92 is created but not running
Jun 19 21:57:54.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1510'
Jun 19 21:57:54.086: INFO: stderr: ""
Jun 19 21:57:54.086: INFO: stdout: "update-demo-nautilus-j6m92 update-demo-nautilus-zzlkk "
Jun 19 21:57:54.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-j6m92 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1510'
Jun 19 21:57:54.163: INFO: stderr: ""
Jun 19 21:57:54.163: INFO: stdout: "true"
Jun 19 21:57:54.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-j6m92 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1510'
Jun 19 21:57:54.244: INFO: stderr: ""
Jun 19 21:57:54.244: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:57:54.244: INFO: validating pod update-demo-nautilus-j6m92
Jun 19 21:57:54.250: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:57:54.250: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:57:54.250: INFO: update-demo-nautilus-j6m92 is verified up and running
Jun 19 21:57:54.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-zzlkk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1510'
Jun 19 21:57:54.329: INFO: stderr: ""
Jun 19 21:57:54.329: INFO: stdout: "true"
Jun 19 21:57:54.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods update-demo-nautilus-zzlkk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1510'
Jun 19 21:57:54.408: INFO: stderr: ""
Jun 19 21:57:54.408: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 19 21:57:54.408: INFO: validating pod update-demo-nautilus-zzlkk
Jun 19 21:57:54.414: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 19 21:57:54.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 19 21:57:54.414: INFO: update-demo-nautilus-zzlkk is verified up and running
STEP: using delete to clean up resources
Jun 19 21:57:54.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-1510'
Jun 19 21:57:54.491: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 21:57:54.491: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 19 21:57:54.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1510'
Jun 19 21:57:54.575: INFO: stderr: "No resources found.\n"
Jun 19 21:57:54.575: INFO: stdout: ""
Jun 19 21:57:54.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=update-demo --namespace=kubectl-1510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 21:57:54.652: INFO: stderr: ""
Jun 19 21:57:54.652: INFO: stdout: "update-demo-nautilus-j6m92\nupdate-demo-nautilus-zzlkk\n"
Jun 19 21:57:55.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1510'
Jun 19 21:57:55.233: INFO: stderr: "No resources found.\n"
Jun 19 21:57:55.233: INFO: stdout: ""
Jun 19 21:57:55.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=update-demo --namespace=kubectl-1510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 21:57:55.310: INFO: stderr: ""
Jun 19 21:57:55.310: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:57:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1510" for this suite.
Jun 19 21:58:01.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:58:01.399: INFO: namespace kubectl-1510 deletion completed in 6.084906117s

• [SLOW TEST:15.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:58:01.400: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:58:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7890" for this suite.
Jun 19 21:58:23.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 21:58:23.647: INFO: namespace pods-7890 deletion completed in 22.087869761s

• [SLOW TEST:22.247 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 21:58:23.647: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5850
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-0fe2c950-998e-4ada-a110-f9fd8fc971bc
STEP: Creating configMap with name cm-test-opt-upd-df734358-9953-4a92-863f-2ef3630000af
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0fe2c950-998e-4ada-a110-f9fd8fc971bc
STEP: Updating configmap cm-test-opt-upd-df734358-9953-4a92-863f-2ef3630000af
STEP: Creating configMap with name cm-test-opt-create-d5019c6d-c606-417c-88ec-082fc60a6918
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 21:59:48.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5850" for this suite.
Jun 19 22:00:10.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:00:10.287: INFO: namespace projected-5850 deletion completed in 22.079267747s

• [SLOW TEST:106.640 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:00:10.289: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:00:10.438: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:00:16.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9242" for this suite.
Jun 19 22:00:56.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:00:56.567: INFO: namespace pods-9242 deletion completed in 40.08553608s

• [SLOW TEST:46.278 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:00:56.569: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 19 22:01:10.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:10.766: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:12.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:12.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:14.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:14.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:16.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:16.770: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:18.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:18.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:20.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:20.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:22.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:22.770: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:24.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:24.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:26.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:26.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:28.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:28.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:30.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:30.770: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:32.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:32.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:34.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:34.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:36.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:36.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:38.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:38.769: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 19 22:01:40.766: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 19 22:01:40.769: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:01:40.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5588" for this suite.
Jun 19 22:02:02.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:02:02.857: INFO: namespace container-lifecycle-hook-5588 deletion completed in 22.084197593s

• [SLOW TEST:66.288 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:02:02.857: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8977
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-e90f0341-bb0d-4375-8041-c6e6d7fabe7d
STEP: Creating secret with name s-test-opt-upd-ba0220e9-d1ba-4b66-82a9-0567eac12cac
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e90f0341-bb0d-4375-8041-c6e6d7fabe7d
STEP: Updating secret s-test-opt-upd-ba0220e9-d1ba-4b66-82a9-0567eac12cac
STEP: Creating secret with name s-test-opt-create-7ab9ae38-e9c6-4bba-b222-8a32d80aaefd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:03:15.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8977" for this suite.
Jun 19 22:03:37.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:03:37.459: INFO: namespace projected-8977 deletion completed in 22.085254639s

• [SLOW TEST:94.602 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:03:37.460: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jun 19 22:03:37.606: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-497557601 proxy --unix-socket=/tmp/kubectl-proxy-unix208469568/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:03:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8395" for this suite.
Jun 19 22:03:43.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:03:43.747: INFO: namespace kubectl-8395 deletion completed in 6.084827223s

• [SLOW TEST:6.288 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:03:43.752: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 22:03:43.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7079'
Jun 19 22:03:43.982: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 19 22:03:43.982: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jun 19 22:03:46.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7079'
Jun 19 22:03:46.081: INFO: stderr: ""
Jun 19 22:03:46.081: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:03:46.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7079" for this suite.
Jun 19 22:03:52.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:03:52.172: INFO: namespace kubectl-7079 deletion completed in 6.082066375s

• [SLOW TEST:8.421 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:03:52.173: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 19 22:03:52.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25573,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 19 22:03:52.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25574,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 19 22:03:52.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25575,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 19 22:04:02.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25593,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 19 22:04:02.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25594,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 19 22:04:02.365: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3238,SelfLink:/api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed,UID:e75dc96f-58b1-4c37-9749-6e57ef8b51ab,ResourceVersion:25595,Generation:0,CreationTimestamp:2019-06-19 22:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:04:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3238" for this suite.
Jun 19 22:04:08.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:04:08.449: INFO: namespace watch-3238 deletion completed in 6.079535435s

• [SLOW TEST:16.275 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:04:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5982.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5982.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.195.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.195.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.195.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.195.6_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5982.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5982.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.195.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.195.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.195.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.195.6_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 22:04:36.680: INFO: Unable to read wheezy_udp@dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.683: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.686: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.689: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.710: INFO: Unable to read jessie_udp@dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.712: INFO: Unable to read jessie_tcp@dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.715: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.718: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local from pod dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f: the server could not find the requested resource (get pods dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f)
Jun 19 22:04:36.734: INFO: Lookups using dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f failed for: [wheezy_udp@dns-test-service.dns-5982.svc.cluster.local wheezy_tcp@dns-test-service.dns-5982.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local jessie_udp@dns-test-service.dns-5982.svc.cluster.local jessie_tcp@dns-test-service.dns-5982.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5982.svc.cluster.local]

Jun 19 22:04:41.796: INFO: DNS probes using dns-5982/dns-test-afa109f7-7744-4fce-8a25-afc4e4112e2f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:04:41.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5982" for this suite.
Jun 19 22:04:47.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:04:47.988: INFO: namespace dns-5982 deletion completed in 6.100937975s

• [SLOW TEST:39.538 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:04:47.988: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 22:04:56.173: INFO: DNS probes using dns-test-6f17937c-6660-41b4-9678-73ab0427e8f1 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 22:05:04.238: INFO: DNS probes using dns-test-98df7f51-06ab-4f7e-8a9d-412da968a0e3 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-907.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-907.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 22:05:12.305: INFO: DNS probes using dns-test-66eeb9ef-ac52-4a10-a8db-842aed589270 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:05:12.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-907" for this suite.
Jun 19 22:05:18.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:05:18.435: INFO: namespace dns-907 deletion completed in 6.081768792s

• [SLOW TEST:30.447 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:05:18.436: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 19 22:05:18.606: INFO: Waiting up to 5m0s for pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677" in namespace "downward-api-5999" to be "success or failure"
Jun 19 22:05:18.612: INFO: Pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677": Phase="Pending", Reason="", readiness=false. Elapsed: 5.996813ms
Jun 19 22:05:20.615: INFO: Pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009049798s
Jun 19 22:05:22.618: INFO: Pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012238982s
Jun 19 22:05:24.622: INFO: Pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015412768s
STEP: Saw pod success
Jun 19 22:05:24.622: INFO: Pod "downward-api-41834a66-7037-4dcf-ac01-0215640ca677" satisfied condition "success or failure"
Jun 19 22:05:24.624: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downward-api-41834a66-7037-4dcf-ac01-0215640ca677 container dapi-container: <nil>
STEP: delete the pod
Jun 19 22:05:24.640: INFO: Waiting for pod downward-api-41834a66-7037-4dcf-ac01-0215640ca677 to disappear
Jun 19 22:05:24.643: INFO: Pod downward-api-41834a66-7037-4dcf-ac01-0215640ca677 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:05:24.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5999" for this suite.
Jun 19 22:05:30.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:05:30.727: INFO: namespace downward-api-5999 deletion completed in 6.081070617s

• [SLOW TEST:12.291 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:05:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jun 19 22:05:30.879: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4845" to be "success or failure"
Jun 19 22:05:30.883: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.923543ms
Jun 19 22:05:32.886: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006990033s
Jun 19 22:05:34.888: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009808428s
Jun 19 22:05:36.892: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013158515s
Jun 19 22:05:38.895: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016512803s
STEP: Saw pod success
Jun 19 22:05:38.895: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 19 22:05:38.898: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 19 22:05:38.912: INFO: Waiting for pod pod-host-path-test to disappear
Jun 19 22:05:38.916: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:05:38.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4845" for this suite.
Jun 19 22:05:44.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:05:45.007: INFO: namespace hostpath-4845 deletion completed in 6.087433341s

• [SLOW TEST:14.279 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:05:45.008: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-15dbb529-2151-45aa-b546-ed9a3f33575d
STEP: Creating a pod to test consume configMaps
Jun 19 22:05:45.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd" in namespace "projected-8553" to be "success or failure"
Jun 19 22:05:45.191: INFO: Pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.454746ms
Jun 19 22:05:47.194: INFO: Pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020487942s
Jun 19 22:05:49.197: INFO: Pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023161044s
Jun 19 22:05:51.199: INFO: Pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025676449s
STEP: Saw pod success
Jun 19 22:05:51.200: INFO: Pod "pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd" satisfied condition "success or failure"
Jun 19 22:05:51.202: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:05:51.218: INFO: Waiting for pod pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd to disappear
Jun 19 22:05:51.220: INFO: Pod pod-projected-configmaps-fdeb3cac-62c7-449a-a6dc-56e65c937bdd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:05:51.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8553" for this suite.
Jun 19 22:05:57.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:05:57.317: INFO: namespace projected-8553 deletion completed in 6.087915747s

• [SLOW TEST:12.309 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:05:57.318: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 22:05:57.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1327'
Jun 19 22:05:57.559: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 19 22:05:57.559: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 19 22:05:57.567: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 19 22:05:57.574: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 19 22:05:57.579: INFO: scanned /root for discovery docs: <nil>
Jun 19 22:05:57.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1327'
Jun 19 22:06:14.335: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 19 22:06:14.335: INFO: stdout: "Created e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8\nScaling up e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 19 22:06:14.335: INFO: stdout: "Created e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8\nScaling up e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 19 22:06:14.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1327'
Jun 19 22:06:14.425: INFO: stderr: ""
Jun 19 22:06:14.425: INFO: stdout: "e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8-t2dm2 "
Jun 19 22:06:14.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8-t2dm2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1327'
Jun 19 22:06:14.521: INFO: stderr: ""
Jun 19 22:06:14.521: INFO: stdout: "true"
Jun 19 22:06:14.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8-t2dm2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1327'
Jun 19 22:06:14.605: INFO: stderr: ""
Jun 19 22:06:14.605: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 19 22:06:14.605: INFO: e2e-test-nginx-rc-6b936e61450736b11bc70df4b552b8f8-t2dm2 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jun 19 22:06:14.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete rc e2e-test-nginx-rc --namespace=kubectl-1327'
Jun 19 22:06:14.693: INFO: stderr: ""
Jun 19 22:06:14.693: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:06:14.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1327" for this suite.
Jun 19 22:06:36.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:06:36.792: INFO: namespace kubectl-1327 deletion completed in 22.09171646s

• [SLOW TEST:39.475 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:06:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 19 22:06:36.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1061,SelfLink:/api/v1/namespaces/watch-1061/configmaps/e2e-watch-test-resource-version,UID:f6005a9f-7d3a-470b-97d9-9f58e0227daa,ResourceVersion:26207,Generation:0,CreationTimestamp:2019-06-19 22:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 19 22:06:36.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1061,SelfLink:/api/v1/namespaces/watch-1061/configmaps/e2e-watch-test-resource-version,UID:f6005a9f-7d3a-470b-97d9-9f58e0227daa,ResourceVersion:26208,Generation:0,CreationTimestamp:2019-06-19 22:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:06:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1061" for this suite.
Jun 19 22:06:42.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:06:43.040: INFO: namespace watch-1061 deletion completed in 6.078323836s

• [SLOW TEST:6.247 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:06:43.040: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jun 19 22:06:43.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 create -f - --namespace=kubectl-917'
Jun 19 22:06:43.967: INFO: stderr: ""
Jun 19 22:06:43.967: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jun 19 22:06:44.971: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:44.971: INFO: Found 0 / 1
Jun 19 22:06:45.970: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:45.970: INFO: Found 0 / 1
Jun 19 22:06:46.971: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:46.971: INFO: Found 0 / 1
Jun 19 22:06:47.971: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:47.971: INFO: Found 0 / 1
Jun 19 22:06:48.970: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:48.970: INFO: Found 1 / 1
Jun 19 22:06:48.970: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 19 22:06:48.973: INFO: Selector matched 1 pods for map[app:redis]
Jun 19 22:06:48.973: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 19 22:06:48.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 logs redis-master-2fbgg redis-master --namespace=kubectl-917'
Jun 19 22:06:49.059: INFO: stderr: ""
Jun 19 22:06:49.059: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Jun 22:06:47.572 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Jun 22:06:47.572 # Server started, Redis version 3.2.12\n1:M 19 Jun 22:06:47.572 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Jun 22:06:47.572 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 19 22:06:49.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 log redis-master-2fbgg redis-master --namespace=kubectl-917 --tail=1'
Jun 19 22:06:49.151: INFO: stderr: ""
Jun 19 22:06:49.151: INFO: stdout: "1:M 19 Jun 22:06:47.572 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 19 22:06:49.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 log redis-master-2fbgg redis-master --namespace=kubectl-917 --limit-bytes=1'
Jun 19 22:06:49.236: INFO: stderr: ""
Jun 19 22:06:49.236: INFO: stdout: " "
STEP: exposing timestamps
Jun 19 22:06:49.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 log redis-master-2fbgg redis-master --namespace=kubectl-917 --tail=1 --timestamps'
Jun 19 22:06:49.317: INFO: stderr: ""
Jun 19 22:06:49.317: INFO: stdout: "2019-06-19T22:06:47.573056299Z 1:M 19 Jun 22:06:47.572 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 19 22:06:51.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 log redis-master-2fbgg redis-master --namespace=kubectl-917 --since=1s'
Jun 19 22:06:51.904: INFO: stderr: ""
Jun 19 22:06:51.904: INFO: stdout: ""
Jun 19 22:06:51.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 log redis-master-2fbgg redis-master --namespace=kubectl-917 --since=24h'
Jun 19 22:06:52.002: INFO: stderr: ""
Jun 19 22:06:52.002: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Jun 22:06:47.572 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Jun 22:06:47.572 # Server started, Redis version 3.2.12\n1:M 19 Jun 22:06:47.572 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Jun 22:06:47.572 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jun 19 22:06:52.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete --grace-period=0 --force -f - --namespace=kubectl-917'
Jun 19 22:06:52.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 19 22:06:52.091: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 19 22:06:52.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get rc,svc -l name=nginx --no-headers --namespace=kubectl-917'
Jun 19 22:06:52.180: INFO: stderr: "No resources found.\n"
Jun 19 22:06:52.180: INFO: stdout: ""
Jun 19 22:06:52.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 get pods -l name=nginx --namespace=kubectl-917 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 19 22:06:52.254: INFO: stderr: ""
Jun 19 22:06:52.254: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:06:52.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-917" for this suite.
Jun 19 22:07:14.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:07:14.360: INFO: namespace kubectl-917 deletion completed in 22.10193925s

• [SLOW TEST:31.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:07:14.360: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7710
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e845e6d0-468d-497f-8c99-68dbb6c1ab20
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-e845e6d0-468d-497f-8c99-68dbb6c1ab20
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:08:32.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7710" for this suite.
Jun 19 22:08:54.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:08:54.906: INFO: namespace projected-7710 deletion completed in 22.093343598s

• [SLOW TEST:100.546 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:08:54.906: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:09:01.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6689" for this suite.
Jun 19 22:09:41.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:09:41.168: INFO: namespace kubelet-test-6689 deletion completed in 40.084844174s

• [SLOW TEST:46.262 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:09:41.169: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 19 22:09:47.337: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0619 22:09:47.337534      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:09:47.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1076" for this suite.
Jun 19 22:09:53.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:09:53.431: INFO: namespace gc-1076 deletion completed in 6.090293824s

• [SLOW TEST:12.261 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:09:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:09:53.575: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:09:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9299" for this suite.
Jun 19 22:10:43.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:10:43.795: INFO: namespace pods-9299 deletion completed in 44.086876763s

• [SLOW TEST:50.364 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:10:43.797: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 19 22:10:43.963: INFO: Waiting up to 5m0s for pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13" in namespace "emptydir-2775" to be "success or failure"
Jun 19 22:10:43.980: INFO: Pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13": Phase="Pending", Reason="", readiness=false. Elapsed: 16.860155ms
Jun 19 22:10:45.983: INFO: Pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019485541s
Jun 19 22:10:47.986: INFO: Pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022417924s
Jun 19 22:10:49.989: INFO: Pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025379106s
STEP: Saw pod success
Jun 19 22:10:49.989: INFO: Pod "pod-968aa82f-7355-423a-b5c1-256dd0525f13" satisfied condition "success or failure"
Jun 19 22:10:49.991: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-968aa82f-7355-423a-b5c1-256dd0525f13 container test-container: <nil>
STEP: delete the pod
Jun 19 22:10:50.013: INFO: Waiting for pod pod-968aa82f-7355-423a-b5c1-256dd0525f13 to disappear
Jun 19 22:10:50.015: INFO: Pod pod-968aa82f-7355-423a-b5c1-256dd0525f13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:10:50.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2775" for this suite.
Jun 19 22:10:56.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:10:56.103: INFO: namespace emptydir-2775 deletion completed in 6.084995742s

• [SLOW TEST:12.307 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:10:56.104: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9952/configmap-test-6ea13429-62c8-4691-a88e-a5b0e8661ab2
STEP: Creating a pod to test consume configMaps
Jun 19 22:10:56.312: INFO: Waiting up to 5m0s for pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42" in namespace "configmap-9952" to be "success or failure"
Jun 19 22:10:56.315: INFO: Pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.254453ms
Jun 19 22:10:58.318: INFO: Pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006399934s
Jun 19 22:11:00.322: INFO: Pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010438103s
Jun 19 22:11:02.326: INFO: Pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013760282s
STEP: Saw pod success
Jun 19 22:11:02.326: INFO: Pod "pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42" satisfied condition "success or failure"
Jun 19 22:11:02.327: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42 container env-test: <nil>
STEP: delete the pod
Jun 19 22:11:02.344: INFO: Waiting for pod pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42 to disappear
Jun 19 22:11:02.346: INFO: Pod pod-configmaps-b141c9aa-b713-439d-affe-ae349ebd1d42 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:11:02.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9952" for this suite.
Jun 19 22:11:08.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:11:08.435: INFO: namespace configmap-9952 deletion completed in 6.085225347s

• [SLOW TEST:12.331 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:11:08.436: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 19 22:11:15.618: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:11:16.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6137" for this suite.
Jun 19 22:11:38.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:11:38.739: INFO: namespace replicaset-6137 deletion completed in 22.103202561s

• [SLOW TEST:30.304 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:11:38.741: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jun 19 22:11:38.909: INFO: Waiting up to 5m0s for pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154" in namespace "containers-4526" to be "success or failure"
Jun 19 22:11:38.912: INFO: Pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126655ms
Jun 19 22:11:40.915: INFO: Pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005726952s
Jun 19 22:11:42.917: INFO: Pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008225451s
Jun 19 22:11:44.920: INFO: Pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011561139s
STEP: Saw pod success
Jun 19 22:11:44.920: INFO: Pod "client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154" satisfied condition "success or failure"
Jun 19 22:11:44.922: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154 container test-container: <nil>
STEP: delete the pod
Jun 19 22:11:44.946: INFO: Waiting for pod client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154 to disappear
Jun 19 22:11:44.949: INFO: Pod client-containers-8f9fe304-b2f5-4794-8d6d-331c940cd154 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:11:44.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4526" for this suite.
Jun 19 22:11:50.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:11:51.035: INFO: namespace containers-4526 deletion completed in 6.083263899s

• [SLOW TEST:12.295 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:11:51.036: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 22:11:51.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-340'
Jun 19 22:11:53.357: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 19 22:11:53.357: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 19 22:11:53.373: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-9t844]
Jun 19 22:11:53.373: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-9t844" in namespace "kubectl-340" to be "running and ready"
Jun 19 22:11:53.377: INFO: Pod "e2e-test-nginx-rc-9t844": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885743ms
Jun 19 22:11:55.380: INFO: Pod "e2e-test-nginx-rc-9t844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00671884s
Jun 19 22:11:57.387: INFO: Pod "e2e-test-nginx-rc-9t844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013746076s
Jun 19 22:11:59.390: INFO: Pod "e2e-test-nginx-rc-9t844": Phase="Running", Reason="", readiness=true. Elapsed: 6.01680147s
Jun 19 22:11:59.390: INFO: Pod "e2e-test-nginx-rc-9t844" satisfied condition "running and ready"
Jun 19 22:11:59.390: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-9t844]
Jun 19 22:11:59.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 logs rc/e2e-test-nginx-rc --namespace=kubectl-340'
Jun 19 22:11:59.483: INFO: stderr: ""
Jun 19 22:11:59.483: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jun 19 22:11:59.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete rc e2e-test-nginx-rc --namespace=kubectl-340'
Jun 19 22:11:59.564: INFO: stderr: ""
Jun 19 22:11:59.564: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:11:59.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-340" for this suite.
Jun 19 22:12:05.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:12:05.656: INFO: namespace kubectl-340 deletion completed in 6.086771157s

• [SLOW TEST:14.619 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:12:05.656: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 19 22:12:10.830: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:12:10.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8158" for this suite.
Jun 19 22:12:16.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:12:16.946: INFO: namespace container-runtime-8158 deletion completed in 6.092095186s

• [SLOW TEST:11.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:12:16.947: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-338773ad-6d22-439e-901a-ae24b003b726
STEP: Creating a pod to test consume secrets
Jun 19 22:12:17.104: INFO: Waiting up to 5m0s for pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a" in namespace "secrets-1719" to be "success or failure"
Jun 19 22:12:17.107: INFO: Pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992856ms
Jun 19 22:12:19.110: INFO: Pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006003155s
Jun 19 22:12:21.113: INFO: Pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009157751s
Jun 19 22:12:23.116: INFO: Pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012504045s
STEP: Saw pod success
Jun 19 22:12:23.116: INFO: Pod "pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a" satisfied condition "success or failure"
Jun 19 22:12:23.118: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 22:12:23.136: INFO: Waiting for pod pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a to disappear
Jun 19 22:12:23.145: INFO: Pod pod-secrets-db659342-39db-4015-85f8-14ffc9be1b2a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:12:23.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1719" for this suite.
Jun 19 22:12:29.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:12:29.244: INFO: namespace secrets-1719 deletion completed in 6.096200733s

• [SLOW TEST:12.297 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:12:29.245: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jun 19 22:12:29.402: INFO: Waiting up to 5m0s for pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92" in namespace "containers-6416" to be "success or failure"
Jun 19 22:12:29.409: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.51569ms
Jun 19 22:12:31.412: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010466692s
Jun 19 22:12:33.415: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013345994s
Jun 19 22:12:35.418: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016082299s
Jun 19 22:12:37.421: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019295098s
STEP: Saw pod success
Jun 19 22:12:37.421: INFO: Pod "client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92" satisfied condition "success or failure"
Jun 19 22:12:37.423: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92 container test-container: <nil>
STEP: delete the pod
Jun 19 22:12:37.439: INFO: Waiting for pod client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92 to disappear
Jun 19 22:12:37.441: INFO: Pod client-containers-efb97e1e-9446-42c3-97d2-8442337d2f92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:12:37.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6416" for this suite.
Jun 19 22:12:43.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:12:43.528: INFO: namespace containers-6416 deletion completed in 6.083742521s

• [SLOW TEST:14.283 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:12:43.528: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:12:43.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b" in namespace "projected-4293" to be "success or failure"
Jun 19 22:12:43.696: INFO: Pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.501446ms
Jun 19 22:12:45.699: INFO: Pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020785345s
Jun 19 22:12:47.702: INFO: Pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023397654s
Jun 19 22:12:49.705: INFO: Pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02690885s
STEP: Saw pod success
Jun 19 22:12:49.705: INFO: Pod "downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b" satisfied condition "success or failure"
Jun 19 22:12:49.708: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b container client-container: <nil>
STEP: delete the pod
Jun 19 22:12:49.722: INFO: Waiting for pod downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b to disappear
Jun 19 22:12:49.725: INFO: Pod downwardapi-volume-99d5dee9-0472-4fea-b149-c3264f9a199b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:12:49.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4293" for this suite.
Jun 19 22:12:55.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:12:55.815: INFO: namespace projected-4293 deletion completed in 6.085755898s

• [SLOW TEST:12.287 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:12:55.815: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-c2l9
STEP: Creating a pod to test atomic-volume-subpath
Jun 19 22:12:55.977: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c2l9" in namespace "subpath-6274" to be "success or failure"
Jun 19 22:12:55.985: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.689374ms
Jun 19 22:12:57.990: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013369654s
Jun 19 22:12:59.993: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016865852s
Jun 19 22:13:01.997: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 6.020312251s
Jun 19 22:13:04.000: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 8.023594653s
Jun 19 22:13:06.004: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 10.026977054s
Jun 19 22:13:08.007: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 12.030180657s
Jun 19 22:13:10.010: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 14.03343106s
Jun 19 22:13:12.014: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 16.037103358s
Jun 19 22:13:14.017: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 18.040518259s
Jun 19 22:13:16.020: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 20.043713564s
Jun 19 22:13:18.024: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 22.047343063s
Jun 19 22:13:20.027: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Running", Reason="", readiness=true. Elapsed: 24.050625567s
Jun 19 22:13:22.031: INFO: Pod "pod-subpath-test-configmap-c2l9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.054242267s
STEP: Saw pod success
Jun 19 22:13:22.031: INFO: Pod "pod-subpath-test-configmap-c2l9" satisfied condition "success or failure"
Jun 19 22:13:22.033: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-subpath-test-configmap-c2l9 container test-container-subpath-configmap-c2l9: <nil>
STEP: delete the pod
Jun 19 22:13:22.049: INFO: Waiting for pod pod-subpath-test-configmap-c2l9 to disappear
Jun 19 22:13:22.052: INFO: Pod pod-subpath-test-configmap-c2l9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c2l9
Jun 19 22:13:22.052: INFO: Deleting pod "pod-subpath-test-configmap-c2l9" in namespace "subpath-6274"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:13:22.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6274" for this suite.
Jun 19 22:13:28.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:13:28.138: INFO: namespace subpath-6274 deletion completed in 6.079459204s

• [SLOW TEST:32.322 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:13:28.138: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 19 22:13:34.842: INFO: Successfully updated pod "annotationupdatec8d52805-2c45-461e-9f2b-511c7613137c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:13:36.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3271" for this suite.
Jun 19 22:13:58.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:13:58.945: INFO: namespace downward-api-3271 deletion completed in 22.080264853s

• [SLOW TEST:30.807 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:13:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:13:59.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 version'
Jun 19 22:13:59.293: INFO: stderr: ""
Jun 19 22:13:59.293: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:13:59.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9671" for this suite.
Jun 19 22:14:05.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:14:05.386: INFO: namespace kubectl-9671 deletion completed in 6.088096496s

• [SLOW TEST:6.441 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:14:05.387: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:14:05.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826" in namespace "downward-api-9063" to be "success or failure"
Jun 19 22:14:05.550: INFO: Pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400622ms
Jun 19 22:14:07.553: INFO: Pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008498036s
Jun 19 22:14:09.557: INFO: Pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012097943s
Jun 19 22:14:11.559: INFO: Pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014820262s
STEP: Saw pod success
Jun 19 22:14:11.559: INFO: Pod "downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826" satisfied condition "success or failure"
Jun 19 22:14:11.562: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826 container client-container: <nil>
STEP: delete the pod
Jun 19 22:14:11.578: INFO: Waiting for pod downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826 to disappear
Jun 19 22:14:11.580: INFO: Pod downwardapi-volume-18dac357-c0ce-4b3f-a1cd-547b44c9c826 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:14:11.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9063" for this suite.
Jun 19 22:14:17.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:14:17.671: INFO: namespace downward-api-9063 deletion completed in 6.088373097s

• [SLOW TEST:12.285 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:14:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8686
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 19 22:14:17.822: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 19 22:14:41.905: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.108&port=8080&tries=1'] Namespace:pod-network-test-8686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:14:41.905: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:14:42.030: INFO: Waiting for endpoints: map[]
Jun 19 22:14:42.032: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.73&port=8080&tries=1'] Namespace:pod-network-test-8686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:14:42.032: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:14:42.162: INFO: Waiting for endpoints: map[]
Jun 19 22:14:42.165: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.80:8080/dial?request=hostName&protocol=http&host=10.240.0.61&port=8080&tries=1'] Namespace:pod-network-test-8686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:14:42.165: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:14:42.297: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:14:42.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8686" for this suite.
Jun 19 22:15:04.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:15:04.382: INFO: namespace pod-network-test-8686 deletion completed in 22.081406535s

• [SLOW TEST:46.711 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:15:04.383: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:15:37.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3200" for this suite.
Jun 19 22:15:43.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:15:43.852: INFO: namespace container-runtime-3200 deletion completed in 6.080283547s

• [SLOW TEST:39.470 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:15:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jun 19 22:15:44.014: INFO: Waiting up to 5m0s for pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e" in namespace "var-expansion-3789" to be "success or failure"
Jun 19 22:15:44.021: INFO: Pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.9182ms
Jun 19 22:15:46.024: INFO: Pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009712731s
Jun 19 22:15:48.027: INFO: Pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012875857s
Jun 19 22:15:50.031: INFO: Pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016336378s
STEP: Saw pod success
Jun 19 22:15:50.031: INFO: Pod "var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e" satisfied condition "success or failure"
Jun 19 22:15:50.033: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e container dapi-container: <nil>
STEP: delete the pod
Jun 19 22:15:50.048: INFO: Waiting for pod var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e to disappear
Jun 19 22:15:50.051: INFO: Pod var-expansion-f5d337ca-160e-46fc-9619-8311af29a35e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:15:50.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3789" for this suite.
Jun 19 22:15:56.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:15:56.164: INFO: namespace var-expansion-3789 deletion completed in 6.108790238s

• [SLOW TEST:12.309 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:15:56.165: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1647
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:16:24.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1501" for this suite.
Jun 19 22:16:30.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:16:30.768: INFO: namespace namespaces-1501 deletion completed in 6.081415647s
STEP: Destroying namespace "nsdeletetest-7399" for this suite.
Jun 19 22:16:30.770: INFO: Namespace nsdeletetest-7399 was already deleted
STEP: Destroying namespace "nsdeletetest-1647" for this suite.
Jun 19 22:16:36.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:16:36.850: INFO: namespace nsdeletetest-1647 deletion completed in 6.079351779s

• [SLOW TEST:40.684 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:16:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:17:37.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9323" for this suite.
Jun 19 22:17:59.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:17:59.097: INFO: namespace container-probe-9323 deletion completed in 22.089539226s

• [SLOW TEST:82.247 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:17:59.097: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1211
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 19 22:18:07.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec pod-sharedvolume-eb115b08-9c3e-40f0-ab2c-6f2b598262cb -c busybox-main-container --namespace=emptydir-1211 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 19 22:18:07.467: INFO: stderr: ""
Jun 19 22:18:07.467: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:18:07.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1211" for this suite.
Jun 19 22:18:13.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:18:13.553: INFO: namespace emptydir-1211 deletion completed in 6.081086982s

• [SLOW TEST:14.456 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:18:13.553: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-nl4n
STEP: Creating a pod to test atomic-volume-subpath
Jun 19 22:18:13.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nl4n" in namespace "subpath-2061" to be "success or failure"
Jun 19 22:18:13.746: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634889ms
Jun 19 22:18:15.748: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010440735s
Jun 19 22:18:17.751: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013357779s
Jun 19 22:18:19.754: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 6.015982628s
Jun 19 22:18:21.757: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 8.019203568s
Jun 19 22:18:23.760: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 10.022420308s
Jun 19 22:18:25.763: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 12.025533451s
Jun 19 22:18:27.767: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 14.028773491s
Jun 19 22:18:29.769: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 16.031554738s
Jun 19 22:18:31.772: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 18.034498083s
Jun 19 22:18:33.776: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 20.037989421s
Jun 19 22:18:35.779: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 22.041256961s
Jun 19 22:18:37.782: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Running", Reason="", readiness=true. Elapsed: 24.044510602s
Jun 19 22:18:39.785: INFO: Pod "pod-subpath-test-downwardapi-nl4n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.047431848s
STEP: Saw pod success
Jun 19 22:18:39.785: INFO: Pod "pod-subpath-test-downwardapi-nl4n" satisfied condition "success or failure"
Jun 19 22:18:39.788: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-subpath-test-downwardapi-nl4n container test-container-subpath-downwardapi-nl4n: <nil>
STEP: delete the pod
Jun 19 22:18:39.817: INFO: Waiting for pod pod-subpath-test-downwardapi-nl4n to disappear
Jun 19 22:18:39.822: INFO: Pod pod-subpath-test-downwardapi-nl4n no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nl4n
Jun 19 22:18:39.822: INFO: Deleting pod "pod-subpath-test-downwardapi-nl4n" in namespace "subpath-2061"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:18:39.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2061" for this suite.
Jun 19 22:18:45.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:18:45.914: INFO: namespace subpath-2061 deletion completed in 6.080886793s

• [SLOW TEST:32.361 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:18:45.914: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:18:46.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05" in namespace "projected-9146" to be "success or failure"
Jun 19 22:18:46.076: INFO: Pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05": Phase="Pending", Reason="", readiness=false. Elapsed: 10.858043ms
Jun 19 22:18:48.080: INFO: Pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014167584s
Jun 19 22:18:50.083: INFO: Pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017442026s
Jun 19 22:18:52.086: INFO: Pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020175975s
STEP: Saw pod success
Jun 19 22:18:52.086: INFO: Pod "downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05" satisfied condition "success or failure"
Jun 19 22:18:52.088: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05 container client-container: <nil>
STEP: delete the pod
Jun 19 22:18:52.114: INFO: Waiting for pod downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05 to disappear
Jun 19 22:18:52.117: INFO: Pod downwardapi-volume-318767b6-6741-4307-8dd5-e87c5ae54b05 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:18:52.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9146" for this suite.
Jun 19 22:18:58.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:18:58.228: INFO: namespace projected-9146 deletion completed in 6.10752611s

• [SLOW TEST:12.314 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:18:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 19 22:18:58.446: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:19:19.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8479" for this suite.
Jun 19 22:19:25.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:19:25.964: INFO: namespace pods-8479 deletion completed in 6.079602621s

• [SLOW TEST:27.736 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:19:25.965: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-g4r9
STEP: Creating a pod to test atomic-volume-subpath
Jun 19 22:19:26.128: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g4r9" in namespace "subpath-4532" to be "success or failure"
Jun 19 22:19:26.130: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700361ms
Jun 19 22:19:28.133: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005912007s
Jun 19 22:19:30.136: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008547061s
Jun 19 22:19:32.139: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 6.011371112s
Jun 19 22:19:34.142: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 8.014726056s
Jun 19 22:19:36.145: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 10.017527109s
Jun 19 22:19:38.149: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 12.020962852s
Jun 19 22:19:40.152: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 14.024110899s
Jun 19 22:19:42.155: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 16.026935251s
Jun 19 22:19:44.158: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 18.030391495s
Jun 19 22:19:46.161: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 20.033571242s
Jun 19 22:19:48.164: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 22.03675639s
Jun 19 22:19:50.168: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Running", Reason="", readiness=true. Elapsed: 24.040182934s
Jun 19 22:19:52.170: INFO: Pod "pod-subpath-test-secret-g4r9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.04280159s
STEP: Saw pod success
Jun 19 22:19:52.170: INFO: Pod "pod-subpath-test-secret-g4r9" satisfied condition "success or failure"
Jun 19 22:19:52.173: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-subpath-test-secret-g4r9 container test-container-subpath-secret-g4r9: <nil>
STEP: delete the pod
Jun 19 22:19:52.188: INFO: Waiting for pod pod-subpath-test-secret-g4r9 to disappear
Jun 19 22:19:52.190: INFO: Pod pod-subpath-test-secret-g4r9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-g4r9
Jun 19 22:19:52.190: INFO: Deleting pod "pod-subpath-test-secret-g4r9" in namespace "subpath-4532"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:19:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4532" for this suite.
Jun 19 22:19:58.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:19:58.280: INFO: namespace subpath-4532 deletion completed in 6.085330145s

• [SLOW TEST:32.315 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:19:58.281: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:20:04.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9378" for this suite.
Jun 19 22:20:10.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:20:10.583: INFO: namespace emptydir-wrapper-9378 deletion completed in 6.083351177s

• [SLOW TEST:12.302 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:20:10.584: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4163/configmap-test-b16fed42-13c3-44e2-884f-4735f902837c
STEP: Creating a pod to test consume configMaps
Jun 19 22:20:10.789: INFO: Waiting up to 5m0s for pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b" in namespace "configmap-4163" to be "success or failure"
Jun 19 22:20:10.803: INFO: Pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.111996ms
Jun 19 22:20:12.807: INFO: Pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018042734s
Jun 19 22:20:14.810: INFO: Pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020848089s
Jun 19 22:20:16.813: INFO: Pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024357634s
STEP: Saw pod success
Jun 19 22:20:16.813: INFO: Pod "pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b" satisfied condition "success or failure"
Jun 19 22:20:16.816: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b container env-test: <nil>
STEP: delete the pod
Jun 19 22:20:16.843: INFO: Waiting for pod pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b to disappear
Jun 19 22:20:16.848: INFO: Pod pod-configmaps-3abf065d-f24f-4098-b1e6-e3d71d50eb5b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:20:16.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4163" for this suite.
Jun 19 22:20:22.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:20:22.935: INFO: namespace configmap-4163 deletion completed in 6.083515877s

• [SLOW TEST:12.351 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:20:22.936: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-4081c385-743c-4c5e-a634-22152577eaee
STEP: Creating a pod to test consume secrets
Jun 19 22:20:23.092: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512" in namespace "projected-6408" to be "success or failure"
Jun 19 22:20:23.096: INFO: Pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414451ms
Jun 19 22:20:25.098: INFO: Pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006135108s
Jun 19 22:20:27.102: INFO: Pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010259444s
Jun 19 22:20:29.106: INFO: Pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013816089s
STEP: Saw pod success
Jun 19 22:20:29.106: INFO: Pod "pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512" satisfied condition "success or failure"
Jun 19 22:20:29.108: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 19 22:20:29.123: INFO: Waiting for pod pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512 to disappear
Jun 19 22:20:29.131: INFO: Pod pod-projected-secrets-dfc52504-40ba-4458-adb8-d33582467512 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:20:29.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6408" for this suite.
Jun 19 22:20:35.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:20:35.218: INFO: namespace projected-6408 deletion completed in 6.083190084s

• [SLOW TEST:12.282 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:20:35.222: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:21:03.388: INFO: Container started at 2019-06-19 22:20:39 +0000 UTC, pod became ready at 2019-06-19 22:21:03 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:21:03.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9337" for this suite.
Jun 19 22:21:25.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:21:25.481: INFO: namespace container-probe-9337 deletion completed in 22.088660808s

• [SLOW TEST:50.258 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:21:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 19 22:21:30.673: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:21:30.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9358" for this suite.
Jun 19 22:21:36.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:21:36.781: INFO: namespace container-runtime-9358 deletion completed in 6.090303393s

• [SLOW TEST:11.300 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:21:36.784: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 19 22:21:36.932: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 19 22:21:36.939: INFO: Waiting for terminating namespaces to be deleted...
Jun 19 22:21:36.941: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000000 before test
Jun 19 22:21:36.946: INFO: coredns-7f68dcdbdb-fzwx2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container coredns ready: true, restart count 0
Jun 19 22:21:36.946: INFO: blobfuse-flexvol-installer-9gm5x from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 22:21:36.946: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-vdlhn from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 19 22:21:36.946: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 19 22:21:36.946: INFO: azure-cni-networkmonitor-7s8d2 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 22:21:36.946: INFO: kube-proxy-hzc6w from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 22:21:36.946: INFO: azure-ip-masq-agent-tg756 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 22:21:36.946: INFO: keyvault-flexvolume-lm2w8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 22:21:36.946: INFO: sonobuoy-e2e-job-944148fca6064c8c from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 22:21:36.946: INFO: 	Container e2e ready: true, restart count 0
Jun 19 22:21:36.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 19 22:21:36.946: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000001 before test
Jun 19 22:21:36.952: INFO: azure-cni-networkmonitor-852zj from kube-system started at 2019-06-19 19:02:06 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.952: INFO: 	Container azure-cnms ready: true, restart count 0
Jun 19 22:21:36.952: INFO: blobfuse-flexvol-installer-44h2j from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.952: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 22:21:36.952: INFO: azure-ip-masq-agent-54llb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.952: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 22:21:36.953: INFO: keyvault-flexvolume-plrx8 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.953: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 22:21:36.953: INFO: metrics-server-864ffbc5c-jj8rv from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.953: INFO: 	Container metrics-server ready: true, restart count 0
Jun 19 22:21:36.953: INFO: kube-proxy-bqn78 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.953: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 22:21:36.953: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-sqmlg from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 22:21:36.953: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 19 22:21:36.953: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 19 22:21:36.953: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-37287165-vmss000002 before test
Jun 19 22:21:36.958: INFO: blobfuse-flexvol-installer-lbfpx from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
Jun 19 22:21:36.958: INFO: azure-ip-masq-agent-djfsb from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Jun 19 22:21:36.958: INFO: keyvault-flexvolume-xgtzq from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
Jun 19 22:21:36.958: INFO: kube-proxy-kdh48 from kube-system started at 2019-06-19 19:02:08 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 19 22:21:36.958: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-19 20:55:44 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 19 22:21:36.958: INFO: sonobuoy-systemd-logs-daemon-set-8670c8299a3d4458-6hxg5 from heptio-sonobuoy started at 2019-06-19 20:55:56 +0000 UTC (2 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 19 22:21:36.958: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 19 22:21:36.958: INFO: azure-cni-networkmonitor-zc6s7 from kube-system started at 2019-06-19 19:02:05 +0000 UTC (1 container statuses recorded)
Jun 19 22:21:36.958: INFO: 	Container azure-cnms ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-602af4c3-dc23-4e4d-81b2-6629d3c30a2a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-602af4c3-dc23-4e4d-81b2-6629d3c30a2a off the node k8s-pool1-37287165-vmss000001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-602af4c3-dc23-4e4d-81b2-6629d3c30a2a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:21:49.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2567" for this suite.
Jun 19 22:22:01.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:22:01.137: INFO: namespace sched-pred-2567 deletion completed in 12.09313866s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:24.354 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:22:01.137: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 19 22:22:01.285: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:22:11.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-207" for this suite.
Jun 19 22:22:33.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:22:33.467: INFO: namespace init-container-207 deletion completed in 22.085745693s

• [SLOW TEST:32.330 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:22:33.467: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 19 22:22:33.623: INFO: Waiting up to 5m0s for pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740" in namespace "downward-api-7785" to be "success or failure"
Jun 19 22:22:33.632: INFO: Pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740": Phase="Pending", Reason="", readiness=false. Elapsed: 9.782758ms
Jun 19 22:22:35.635: INFO: Pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265502s
Jun 19 22:22:37.638: INFO: Pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015231287s
Jun 19 22:22:39.641: INFO: Pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018150249s
STEP: Saw pod success
Jun 19 22:22:39.641: INFO: Pod "downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740" satisfied condition "success or failure"
Jun 19 22:22:39.643: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740 container dapi-container: <nil>
STEP: delete the pod
Jun 19 22:22:39.665: INFO: Waiting for pod downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740 to disappear
Jun 19 22:22:39.668: INFO: Pod downward-api-855da1d9-5452-4f4a-8afe-a5f674c34740 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:22:39.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7785" for this suite.
Jun 19 22:22:45.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:22:45.756: INFO: namespace downward-api-7785 deletion completed in 6.084490489s

• [SLOW TEST:12.289 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:22:45.758: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:22:45.929: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 19 22:22:45.942: INFO: Number of nodes with available pods: 0
Jun 19 22:22:45.942: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 19 22:22:45.962: INFO: Number of nodes with available pods: 0
Jun 19 22:22:45.962: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:46.965: INFO: Number of nodes with available pods: 0
Jun 19 22:22:46.965: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:47.966: INFO: Number of nodes with available pods: 0
Jun 19 22:22:47.966: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:48.964: INFO: Number of nodes with available pods: 0
Jun 19 22:22:48.964: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:49.965: INFO: Number of nodes with available pods: 0
Jun 19 22:22:49.965: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:50.967: INFO: Number of nodes with available pods: 1
Jun 19 22:22:50.967: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 19 22:22:50.987: INFO: Number of nodes with available pods: 1
Jun 19 22:22:50.987: INFO: Number of running nodes: 0, number of available pods: 1
Jun 19 22:22:51.990: INFO: Number of nodes with available pods: 0
Jun 19 22:22:51.990: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 19 22:22:51.997: INFO: Number of nodes with available pods: 0
Jun 19 22:22:51.997: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:53.002: INFO: Number of nodes with available pods: 0
Jun 19 22:22:53.002: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:54.001: INFO: Number of nodes with available pods: 0
Jun 19 22:22:54.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:55.007: INFO: Number of nodes with available pods: 0
Jun 19 22:22:55.007: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:56.000: INFO: Number of nodes with available pods: 0
Jun 19 22:22:56.000: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:57.001: INFO: Number of nodes with available pods: 0
Jun 19 22:22:57.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:58.001: INFO: Number of nodes with available pods: 0
Jun 19 22:22:58.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:22:59.001: INFO: Number of nodes with available pods: 0
Jun 19 22:22:59.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:00.001: INFO: Number of nodes with available pods: 0
Jun 19 22:23:00.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:01.001: INFO: Number of nodes with available pods: 0
Jun 19 22:23:01.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:02.003: INFO: Number of nodes with available pods: 0
Jun 19 22:23:02.003: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:03.000: INFO: Number of nodes with available pods: 0
Jun 19 22:23:03.000: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:04.000: INFO: Number of nodes with available pods: 0
Jun 19 22:23:04.000: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:05.001: INFO: Number of nodes with available pods: 0
Jun 19 22:23:05.001: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:23:06.001: INFO: Number of nodes with available pods: 1
Jun 19 22:23:06.001: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1310, will wait for the garbage collector to delete the pods
Jun 19 22:23:06.066: INFO: Deleting DaemonSet.extensions daemon-set took: 7.251695ms
Jun 19 22:23:06.366: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.207248ms
Jun 19 22:23:20.868: INFO: Number of nodes with available pods: 0
Jun 19 22:23:20.869: INFO: Number of running nodes: 0, number of available pods: 0
Jun 19 22:23:20.871: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1310/daemonsets","resourceVersion":"29111"},"items":null}

Jun 19 22:23:20.873: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1310/pods","resourceVersion":"29111"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:23:20.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1310" for this suite.
Jun 19 22:23:26.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:23:26.984: INFO: namespace daemonsets-1310 deletion completed in 6.088992729s

• [SLOW TEST:41.226 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:23:26.985: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-0705b823-31c5-4096-b4fd-04b3bdc41173
STEP: Creating a pod to test consume configMaps
Jun 19 22:23:27.143: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d" in namespace "configmap-3042" to be "success or failure"
Jun 19 22:23:27.154: INFO: Pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.97734ms
Jun 19 22:23:29.157: INFO: Pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013678708s
Jun 19 22:23:31.160: INFO: Pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016809101s
Jun 19 22:23:33.163: INFO: Pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020033919s
STEP: Saw pod success
Jun 19 22:23:33.163: INFO: Pod "pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d" satisfied condition "success or failure"
Jun 19 22:23:33.166: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:23:33.183: INFO: Waiting for pod pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d to disappear
Jun 19 22:23:33.192: INFO: Pod pod-configmaps-d4aa6535-c8de-490d-bd65-3a99c639e27d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:23:33.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3042" for this suite.
Jun 19 22:23:39.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:23:39.291: INFO: namespace configmap-3042 deletion completed in 6.096050509s

• [SLOW TEST:12.306 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:23:39.292: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-c464d1ff-0190-4d9c-b12e-e2262ea17725
STEP: Creating a pod to test consume configMaps
Jun 19 22:23:39.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1" in namespace "projected-9833" to be "success or failure"
Jun 19 22:23:39.464: INFO: Pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281596ms
Jun 19 22:23:41.468: INFO: Pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01056651s
Jun 19 22:23:43.471: INFO: Pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013743625s
Jun 19 22:23:45.478: INFO: Pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020677985s
STEP: Saw pod success
Jun 19 22:23:45.478: INFO: Pod "pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1" satisfied condition "success or failure"
Jun 19 22:23:45.480: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:23:45.502: INFO: Waiting for pod pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1 to disappear
Jun 19 22:23:45.504: INFO: Pod pod-projected-configmaps-901aa76d-a7c2-4534-9f14-b812565501b1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:23:45.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9833" for this suite.
Jun 19 22:23:51.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:23:51.586: INFO: namespace projected-9833 deletion completed in 6.078528147s

• [SLOW TEST:12.294 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:23:51.587: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 19 22:23:57.773: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-497557601 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 19 22:24:02.849: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:24:02.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7968" for this suite.
Jun 19 22:24:08.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:24:08.939: INFO: namespace pods-7968 deletion completed in 6.084328544s

• [SLOW TEST:17.352 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:24:08.939: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jun 19 22:24:09.611: INFO: created pod pod-service-account-defaultsa
Jun 19 22:24:09.611: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 19 22:24:09.619: INFO: created pod pod-service-account-mountsa
Jun 19 22:24:09.619: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 19 22:24:09.629: INFO: created pod pod-service-account-nomountsa
Jun 19 22:24:09.629: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 19 22:24:09.635: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 19 22:24:09.635: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 19 22:24:09.644: INFO: created pod pod-service-account-mountsa-mountspec
Jun 19 22:24:09.644: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 19 22:24:09.653: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 19 22:24:09.653: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 19 22:24:09.657: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 19 22:24:09.657: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 19 22:24:09.669: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 19 22:24:09.669: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 19 22:24:09.673: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 19 22:24:09.673: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:24:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8248" for this suite.
Jun 19 22:24:31.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:24:31.774: INFO: namespace svcaccounts-8248 deletion completed in 22.087352859s

• [SLOW TEST:22.835 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:24:31.774: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jun 19 22:24:37.949: INFO: Pod pod-hostip-0a368265-c74c-4eda-87da-2f848e859852 has hostIP: 10.240.0.65
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:24:37.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9851" for this suite.
Jun 19 22:24:59.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:25:00.030: INFO: namespace pods-9851 deletion completed in 22.078201284s

• [SLOW TEST:28.256 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:25:00.030: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 19 22:25:06.226: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:25:06.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3245" for this suite.
Jun 19 22:25:12.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:25:12.325: INFO: namespace container-runtime-3245 deletion completed in 6.079000855s

• [SLOW TEST:12.295 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:25:12.326: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 19 22:25:19.023: INFO: Successfully updated pod "pod-update-e2d5c4f0-b824-427e-b7ce-623204cba152"
STEP: verifying the updated pod is in kubernetes
Jun 19 22:25:19.036: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:25:19.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9394" for this suite.
Jun 19 22:25:41.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:25:41.122: INFO: namespace pods-9394 deletion completed in 22.083566462s

• [SLOW TEST:28.796 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:25:41.123: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-6pj9
STEP: Creating a pod to test atomic-volume-subpath
Jun 19 22:25:41.278: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6pj9" in namespace "subpath-7564" to be "success or failure"
Jun 19 22:25:41.281: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.677061ms
Jun 19 22:25:43.284: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005606439s
Jun 19 22:25:45.287: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008641015s
Jun 19 22:25:47.290: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 6.011831689s
Jun 19 22:25:49.293: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 8.014963162s
Jun 19 22:25:51.297: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 10.018278732s
Jun 19 22:25:53.300: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 12.021120509s
Jun 19 22:25:55.302: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 14.023679489s
Jun 19 22:25:57.305: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 16.026579363s
Jun 19 22:25:59.307: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.028983144s
Jun 19 22:26:01.311: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 20.032271412s
Jun 19 22:26:03.314: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 22.035164385s
Jun 19 22:26:05.317: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Running", Reason="", readiness=true. Elapsed: 24.038097457s
Jun 19 22:26:07.320: INFO: Pod "pod-subpath-test-projected-6pj9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.041258824s
STEP: Saw pod success
Jun 19 22:26:07.320: INFO: Pod "pod-subpath-test-projected-6pj9" satisfied condition "success or failure"
Jun 19 22:26:07.322: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-subpath-test-projected-6pj9 container test-container-subpath-projected-6pj9: <nil>
STEP: delete the pod
Jun 19 22:26:07.346: INFO: Waiting for pod pod-subpath-test-projected-6pj9 to disappear
Jun 19 22:26:07.348: INFO: Pod pod-subpath-test-projected-6pj9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6pj9
Jun 19 22:26:07.348: INFO: Deleting pod "pod-subpath-test-projected-6pj9" in namespace "subpath-7564"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:26:07.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7564" for this suite.
Jun 19 22:26:13.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:26:13.432: INFO: namespace subpath-7564 deletion completed in 6.079083298s

• [SLOW TEST:32.310 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:26:13.434: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-93db25dc-38bc-49fc-b2f3-1f349ed37b7f
STEP: Creating a pod to test consume configMaps
Jun 19 22:26:13.590: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de" in namespace "projected-7101" to be "success or failure"
Jun 19 22:26:13.600: INFO: Pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de": Phase="Pending", Reason="", readiness=false. Elapsed: 10.38555ms
Jun 19 22:26:15.603: INFO: Pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013523716s
Jun 19 22:26:17.606: INFO: Pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016389685s
Jun 19 22:26:19.609: INFO: Pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019382152s
STEP: Saw pod success
Jun 19 22:26:19.609: INFO: Pod "pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de" satisfied condition "success or failure"
Jun 19 22:26:19.611: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:26:19.637: INFO: Waiting for pod pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de to disappear
Jun 19 22:26:19.639: INFO: Pod pod-projected-configmaps-7259df7f-c347-4f97-bf95-262273f3e3de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:26:19.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7101" for this suite.
Jun 19 22:26:25.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:26:25.723: INFO: namespace projected-7101 deletion completed in 6.079118087s

• [SLOW TEST:12.289 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:26:25.723: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 19 22:26:25.881: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-watch-closed,UID:c47f90c0-6401-4529-a92a-66017b3967ce,ResourceVersion:29732,Generation:0,CreationTimestamp:2019-06-19 22:26:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 19 22:26:25.881: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-watch-closed,UID:c47f90c0-6401-4529-a92a-66017b3967ce,ResourceVersion:29733,Generation:0,CreationTimestamp:2019-06-19 22:26:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 19 22:26:25.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-watch-closed,UID:c47f90c0-6401-4529-a92a-66017b3967ce,ResourceVersion:29734,Generation:0,CreationTimestamp:2019-06-19 22:26:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 19 22:26:25.891: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-watch-closed,UID:c47f90c0-6401-4529-a92a-66017b3967ce,ResourceVersion:29735,Generation:0,CreationTimestamp:2019-06-19 22:26:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:26:25.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8715" for this suite.
Jun 19 22:26:31.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:26:31.975: INFO: namespace watch-8715 deletion completed in 6.080189766s

• [SLOW TEST:6.251 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:26:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-11c02bab-7ec5-44bb-9074-5bb5cada3c89
STEP: Creating a pod to test consume secrets
Jun 19 22:26:32.146: INFO: Waiting up to 5m0s for pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00" in namespace "secrets-4801" to be "success or failure"
Jun 19 22:26:32.165: INFO: Pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00": Phase="Pending", Reason="", readiness=false. Elapsed: 19.026126ms
Jun 19 22:26:34.168: INFO: Pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022417383s
Jun 19 22:26:36.171: INFO: Pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025543444s
Jun 19 22:26:38.175: INFO: Pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029095097s
STEP: Saw pod success
Jun 19 22:26:38.175: INFO: Pod "pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00" satisfied condition "success or failure"
Jun 19 22:26:38.177: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00 container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 22:26:38.201: INFO: Waiting for pod pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00 to disappear
Jun 19 22:26:38.203: INFO: Pod pod-secrets-f991d37b-6387-44f9-b8db-614aad8fdf00 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:26:38.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4801" for this suite.
Jun 19 22:26:44.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:26:44.299: INFO: namespace secrets-4801 deletion completed in 6.092862874s

• [SLOW TEST:12.323 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:26:44.299: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:26:44.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6" in namespace "downward-api-4885" to be "success or failure"
Jun 19 22:26:44.520: INFO: Pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.69946ms
Jun 19 22:26:46.523: INFO: Pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013155713s
Jun 19 22:26:48.526: INFO: Pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015922775s
Jun 19 22:26:50.529: INFO: Pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019432926s
STEP: Saw pod success
Jun 19 22:26:50.529: INFO: Pod "downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6" satisfied condition "success or failure"
Jun 19 22:26:50.532: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6 container client-container: <nil>
STEP: delete the pod
Jun 19 22:26:50.554: INFO: Waiting for pod downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6 to disappear
Jun 19 22:26:50.556: INFO: Pod downwardapi-volume-249862c4-c21e-4c3b-a577-94b832adaaa6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:26:50.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4885" for this suite.
Jun 19 22:26:56.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:26:56.641: INFO: namespace downward-api-4885 deletion completed in 6.081901922s

• [SLOW TEST:12.342 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:26:56.642: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:26:56.801: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de" in namespace "projected-3517" to be "success or failure"
Jun 19 22:26:56.810: INFO: Pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.758774ms
Jun 19 22:26:58.813: INFO: Pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011348636s
Jun 19 22:27:00.816: INFO: Pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01447159s
Jun 19 22:27:02.819: INFO: Pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01777294s
STEP: Saw pod success
Jun 19 22:27:02.819: INFO: Pod "downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de" satisfied condition "success or failure"
Jun 19 22:27:02.821: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de container client-container: <nil>
STEP: delete the pod
Jun 19 22:27:02.836: INFO: Waiting for pod downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de to disappear
Jun 19 22:27:02.839: INFO: Pod downwardapi-volume-3b0f2c84-baf4-4e0a-ac8b-31f93e46f1de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:27:02.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3517" for this suite.
Jun 19 22:27:08.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:27:08.943: INFO: namespace projected-3517 deletion completed in 6.099221063s

• [SLOW TEST:12.302 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:27:08.944: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 19 22:27:09.144: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:27:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2873" for this suite.
Jun 19 22:27:24.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:27:24.303: INFO: namespace init-container-2873 deletion completed in 6.097263679s

• [SLOW TEST:15.359 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:27:24.303: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 19 22:27:24.475: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:24.477: INFO: Number of nodes with available pods: 0
Jun 19 22:27:24.477: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:27:25.481: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:25.487: INFO: Number of nodes with available pods: 0
Jun 19 22:27:25.487: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:27:26.482: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:26.485: INFO: Number of nodes with available pods: 0
Jun 19 22:27:26.485: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:27:27.481: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:27.485: INFO: Number of nodes with available pods: 0
Jun 19 22:27:27.485: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:27:28.482: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:28.485: INFO: Number of nodes with available pods: 0
Jun 19 22:27:28.485: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:27:29.481: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:29.484: INFO: Number of nodes with available pods: 3
Jun 19 22:27:29.484: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 19 22:27:29.495: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:27:29.499: INFO: Number of nodes with available pods: 3
Jun 19 22:27:29.499: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8754, will wait for the garbage collector to delete the pods
Jun 19 22:27:30.576: INFO: Deleting DaemonSet.extensions daemon-set took: 6.9207ms
Jun 19 22:27:30.876: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.265875ms
Jun 19 22:27:34.879: INFO: Number of nodes with available pods: 0
Jun 19 22:27:34.879: INFO: Number of running nodes: 0, number of available pods: 0
Jun 19 22:27:34.881: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8754/daemonsets","resourceVersion":"30031"},"items":null}

Jun 19 22:27:34.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8754/pods","resourceVersion":"30032"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:27:34.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8754" for this suite.
Jun 19 22:27:40.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:27:40.999: INFO: namespace daemonsets-8754 deletion completed in 6.086568322s

• [SLOW TEST:16.695 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:27:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-0363fbee-1a43-42db-88f7-c4e757edb503
STEP: Creating a pod to test consume configMaps
Jun 19 22:27:41.177: INFO: Waiting up to 5m0s for pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143" in namespace "configmap-5650" to be "success or failure"
Jun 19 22:27:41.185: INFO: Pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143": Phase="Pending", Reason="", readiness=false. Elapsed: 8.410679ms
Jun 19 22:27:43.188: INFO: Pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010930431s
Jun 19 22:27:45.191: INFO: Pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013736779s
Jun 19 22:27:47.194: INFO: Pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016598325s
STEP: Saw pod success
Jun 19 22:27:47.194: INFO: Pod "pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143" satisfied condition "success or failure"
Jun 19 22:27:47.196: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:27:47.217: INFO: Waiting for pod pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143 to disappear
Jun 19 22:27:47.222: INFO: Pod pod-configmaps-61d81408-1b79-433c-ba85-e2d24f1e5143 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:27:47.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5650" for this suite.
Jun 19 22:27:53.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:27:53.314: INFO: namespace configmap-5650 deletion completed in 6.088926379s

• [SLOW TEST:12.315 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:27:53.314: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 19 22:27:58.494: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:27:58.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5591" for this suite.
Jun 19 22:28:04.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:28:04.609: INFO: namespace container-runtime-5591 deletion completed in 6.094349593s

• [SLOW TEST:11.295 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:28:04.610: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:28:04.755: INFO: Creating deployment "nginx-deployment"
Jun 19 22:28:04.759: INFO: Waiting for observed generation 1
Jun 19 22:28:06.764: INFO: Waiting for all required pods to come up
Jun 19 22:28:06.767: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 19 22:28:16.779: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 19 22:28:16.784: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 19 22:28:16.791: INFO: Updating deployment nginx-deployment
Jun 19 22:28:16.791: INFO: Waiting for observed generation 2
Jun 19 22:28:18.804: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 19 22:28:18.810: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 19 22:28:18.812: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 19 22:28:18.818: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 19 22:28:18.818: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 19 22:28:18.820: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 19 22:28:18.824: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 19 22:28:18.824: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 19 22:28:18.829: INFO: Updating deployment nginx-deployment
Jun 19 22:28:18.829: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 19 22:28:18.836: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 19 22:28:18.841: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 19 22:28:18.866: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8010,SelfLink:/apis/apps/v1/namespaces/deployment-8010/deployments/nginx-deployment,UID:54dc4332-72bc-405b-a21d-d5ee101540f5,ResourceVersion:30365,Generation:3,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-06-19 22:28:16 +0000 UTC 2019-06-19 22:28:04 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-06-19 22:28:18 +0000 UTC 2019-06-19 22:28:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 19 22:28:18.876: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-8010,SelfLink:/apis/apps/v1/namespaces/deployment-8010/replicasets/nginx-deployment-55fb7cb77f,UID:7c5bfb61-2c91-42d9-aa57-ba18ed267bd8,ResourceVersion:30362,Generation:3,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 54dc4332-72bc-405b-a21d-d5ee101540f5 0xc001905ad7 0xc001905ad8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 22:28:18.876: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 19 22:28:18.876: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-8010,SelfLink:/apis/apps/v1/namespaces/deployment-8010/replicasets/nginx-deployment-7b8c6f4498,UID:7246b4c2-2b75-464e-8641-9b090a794fc6,ResourceVersion:30360,Generation:3,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 54dc4332-72bc-405b-a21d-d5ee101540f5 0xc001905bd7 0xc001905bd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-5sn2n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5sn2n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-5sn2n,UID:0964e6a2-23d1-4be3-b3fc-41987b745b9e,ResourceVersion:30334,Generation:0,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22787 0xc002e22788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e227f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.96,PodIP:,StartTime:2019-06-19 22:28:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-7xcll" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7xcll,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-7xcll,UID:8bd9d86a-a3c1-487b-bf7b-43e6efe37d54,ResourceVersion:30353,Generation:0,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e228e0 0xc002e228e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.96,PodIP:,StartTime:2019-06-19 22:28:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-bz8df" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bz8df,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-bz8df,UID:74103984-739e-4211-8ebc-119d771667ec,ResourceVersion:30380,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22a40 0xc002e22a41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-h7ppw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-h7ppw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-h7ppw,UID:0def7d62-089f-4b0f-a770-2f67ea848103,ResourceVersion:30327,Generation:0,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22b50 0xc002e22b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:,StartTime:2019-06-19 22:28:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-mlfnp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mlfnp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-mlfnp,UID:4327caf7-9279-40ab-99d1-45f22c9caa5b,ResourceVersion:30378,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22cb0 0xc002e22cb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.892: INFO: Pod "nginx-deployment-55fb7cb77f-q6v4f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q6v4f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-q6v4f,UID:ad181aca-d2d0-4580-b313-760c62145d01,ResourceVersion:30328,Generation:0,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22dc0 0xc002e22dc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-06-19 22:28:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-55fb7cb77f-slb6p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-slb6p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-slb6p,UID:9de92e3b-cdc2-435d-a50d-7a90acc91d15,ResourceVersion:30374,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e22f30 0xc002e22f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e22fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e22fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-55fb7cb77f-xbtkv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xbtkv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-55fb7cb77f-xbtkv,UID:00091118-fbf6-4110-a5ec-dc28da13b3e0,ResourceVersion:30349,Generation:0,CreationTimestamp:2019-06-19 22:28:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7c5bfb61-2c91-42d9-aa57-ba18ed267bd8 0xc002e23027 0xc002e23028}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e230a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e230c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:,StartTime:2019-06-19 22:28:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-7b8c6f4498-55x9m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-55x9m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-55x9m,UID:df4b9920-3cb5-4d56-83fa-bf224305fbda,ResourceVersion:30257,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e231a0 0xc002e231a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.92,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2cb73ebadd6f04f20d8ac7d3387f08d0000e7f80c9153dc694f1bb9dba490cb2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-7b8c6f4498-5rxkx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5rxkx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-5rxkx,UID:6dcaf8e5-047c-4872-a8d4-aa5c0de0ded3,ResourceVersion:30276,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e232f0 0xc002e232f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.70,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c3e7ef398bef3fd636e7366b540a6df757b8823d73394654ccaf6c6fd8b97d72}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-7b8c6f4498-88hn8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-88hn8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-88hn8,UID:b6bcfa8c-a946-4afc-9700-7a12c3fc8674,ResourceVersion:30381,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23460 0xc002e23461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e234d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e234f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.893: INFO: Pod "nginx-deployment-7b8c6f4498-8hwfl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8hwfl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-8hwfl,UID:42f56b21-7b8f-438d-8878-fcd72aad6c73,ResourceVersion:30265,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23570 0xc002e23571}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e235d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e235f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.39,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d2b58c3320d6f3a4f2eff2e43acf0f63b9d1d27313013cb20ddb1b1296653382}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-987jd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-987jd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-987jd,UID:31c1c9c5-2658-4d64-a175-cda5a787bcf8,ResourceVersion:30379,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e236d0 0xc002e236d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-gdxnt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gdxnt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-gdxnt,UID:9c95f731-50d6-4019-9ca1-ec69243fb8a8,ResourceVersion:30373,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e237d0 0xc002e237d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-l8srk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l8srk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-l8srk,UID:be0dbe7b-3b18-4548-ab4d-2b465e16cf16,ResourceVersion:30364,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e238b7 0xc002e238b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-mwnz6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mwnz6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-mwnz6,UID:05631b99-b7b5-4b49-938c-0a1bb9f2783d,ResourceVersion:30262,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e239c0 0xc002e239c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.40,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9b5f892e65ef1202852776121d64e87c87b66cc677e7a7e6df0a90d54b9ebfdf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-p7x6d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p7x6d,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-p7x6d,UID:12704985-8a87-4af4-8439-13af123cd5c7,ResourceVersion:30286,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23b10 0xc002e23b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.96,PodIP:10.240.0.109,StartTime:2019-06-19 22:28:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6bc9141da54fecf3bcd65ff044ef9c4c0b996ef4d22e255b62aca31ff6b7be41}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.894: INFO: Pod "nginx-deployment-7b8c6f4498-r4gwz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r4gwz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-r4gwz,UID:1270d447-91dd-49af-8b79-7022e36e75b4,ResourceVersion:30376,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23c60 0xc002e23c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.895: INFO: Pod "nginx-deployment-7b8c6f4498-tlkvj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tlkvj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-tlkvj,UID:f42347aa-d2c4-46c4-8f3f-4c69bc17eb7a,ResourceVersion:30292,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23d60 0xc002e23d61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.96,PodIP:10.240.0.116,StartTime:2019-06-19 22:28:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ab7d1d0dfbaa85ea06a947193c1362e21c358b10606ba286ab1859910d6cd9ed}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.895: INFO: Pod "nginx-deployment-7b8c6f4498-wp265" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wp265,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-wp265,UID:9e15b731-572d-4eb1-bc2a-79f5fbbdbc59,ResourceVersion:30268,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc002e23eb0 0xc002e23eb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e23f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e23f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.54,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f93cb57ac4f539aa25f3964870a76f56359453183aade45f3fa501c8de580d25}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.895: INFO: Pod "nginx-deployment-7b8c6f4498-xthnn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xthnn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-xthnn,UID:f16e2bfe-c492-4bf0-910a-7905efe72cd1,ResourceVersion:30273,Generation:0,CreationTimestamp:2019-06-19 22:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc0024ae000 0xc0024ae001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ae060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ae080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:04 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.75,StartTime:2019-06-19 22:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-19 22:28:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://28187c456a8ea5f8e50b405b0ad5e2b53c18c329c822a0fa276bfeb0a6953bda}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 19 22:28:18.895: INFO: Pod "nginx-deployment-7b8c6f4498-z6vvm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z6vvm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8010,SelfLink:/api/v1/namespaces/deployment-8010/pods/nginx-deployment-7b8c6f4498-z6vvm,UID:94f15822-19a1-467b-95b3-99a03bcfd6c1,ResourceVersion:30382,Generation:0,CreationTimestamp:2019-06-19 22:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7246b4c2-2b75-464e-8641-9b090a794fc6 0xc0024ae150 0xc0024ae151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gwwlr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gwwlr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gwwlr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ae1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ae1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:28:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:28:18.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8010" for this suite.
Jun 19 22:28:26.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:28:27.050: INFO: namespace deployment-8010 deletion completed in 8.116613536s

• [SLOW TEST:22.441 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:28:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 19 22:28:41.785: INFO: Successfully updated pod "annotationupdate9b060663-a085-450e-b99a-753d7414437a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:28:43.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9068" for this suite.
Jun 19 22:29:05.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:29:05.893: INFO: namespace projected-9068 deletion completed in 22.083267896s

• [SLOW TEST:38.841 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:29:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 19 22:29:16.060: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0619 22:29:16.060843      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:29:16.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5915" for this suite.
Jun 19 22:29:22.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:29:22.153: INFO: namespace gc-5915 deletion completed in 6.089389014s

• [SLOW TEST:16.261 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:29:22.159: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:29:22.313: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 19 22:29:27.316: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 19 22:29:27.317: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 19 22:29:29.324: INFO: Creating deployment "test-rollover-deployment"
Jun 19 22:29:29.336: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 19 22:29:31.344: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 19 22:29:31.354: INFO: Ensure that both replica sets have 1 created replica
Jun 19 22:29:31.358: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 19 22:29:31.364: INFO: Updating deployment test-rollover-deployment
Jun 19 22:29:31.364: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 19 22:29:33.375: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 19 22:29:33.380: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 19 22:29:33.391: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:33.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580171, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:35.395: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:35.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580171, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:37.396: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:37.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580175, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:39.395: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:39.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580175, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:41.396: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:41.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580175, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:43.396: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:43.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580175, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:45.396: INFO: all replica sets need to contain the pod-template-hash label
Jun 19 22:29:45.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580175, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696580169, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:29:47.400: INFO: 
Jun 19 22:29:47.400: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 19 22:29:47.407: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6762,SelfLink:/apis/apps/v1/namespaces/deployment-6762/deployments/test-rollover-deployment,UID:88b6f529-1b36-493d-bbf2-a9317e7b1680,ResourceVersion:30903,Generation:2,CreationTimestamp:2019-06-19 22:29:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-19 22:29:29 +0000 UTC 2019-06-19 22:29:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-19 22:29:45 +0000 UTC 2019-06-19 22:29:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 19 22:29:47.409: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-6762,SelfLink:/apis/apps/v1/namespaces/deployment-6762/replicasets/test-rollover-deployment-854595fc44,UID:09398150-96fc-488c-87ca-aa43907130fc,ResourceVersion:30893,Generation:2,CreationTimestamp:2019-06-19 22:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 88b6f529-1b36-493d-bbf2-a9317e7b1680 0xc001904ae7 0xc001904ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 19 22:29:47.409: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 19 22:29:47.409: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6762,SelfLink:/apis/apps/v1/namespaces/deployment-6762/replicasets/test-rollover-controller,UID:a24c0499-44eb-438c-9184-1fb937c82db3,ResourceVersion:30902,Generation:2,CreationTimestamp:2019-06-19 22:29:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 88b6f529-1b36-493d-bbf2-a9317e7b1680 0xc001904a17 0xc001904a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 22:29:47.410: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-6762,SelfLink:/apis/apps/v1/namespaces/deployment-6762/replicasets/test-rollover-deployment-9b8b997cf,UID:2a0eb7f4-5378-41c1-a078-323d585f25ee,ResourceVersion:30861,Generation:2,CreationTimestamp:2019-06-19 22:29:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 88b6f529-1b36-493d-bbf2-a9317e7b1680 0xc001904d30 0xc001904d31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 22:29:47.413: INFO: Pod "test-rollover-deployment-854595fc44-kddfw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-kddfw,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-6762,SelfLink:/api/v1/namespaces/deployment-6762/pods/test-rollover-deployment-854595fc44-kddfw,UID:3c34fd6a-336d-43c9-9f8b-b65dce527b42,ResourceVersion:30875,Generation:0,CreationTimestamp:2019-06-19 22:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 09398150-96fc-488c-87ca-aa43907130fc 0xc003d862b7 0xc003d862b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lst57 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lst57,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lst57 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003d86320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003d86340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:29:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:29:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:29:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.73,StartTime:2019-06-19 22:29:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-19 22:29:35 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://a05039bd40e4bd954c872bff00fc5172a22f43e70892a39b8719cf8460da0413}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:29:47.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6762" for this suite.
Jun 19 22:29:53.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:29:53.501: INFO: namespace deployment-6762 deletion completed in 6.084651064s

• [SLOW TEST:31.342 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:29:53.501: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:29:53.667: INFO: (0) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.867716ms)
Jun 19 22:29:53.670: INFO: (1) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.220054ms)
Jun 19 22:29:53.673: INFO: (2) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.329452ms)
Jun 19 22:29:53.676: INFO: (3) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.236554ms)
Jun 19 22:29:53.680: INFO: (4) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.079356ms)
Jun 19 22:29:53.683: INFO: (5) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.358351ms)
Jun 19 22:29:53.687: INFO: (6) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.705246ms)
Jun 19 22:29:53.690: INFO: (7) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.007757ms)
Jun 19 22:29:53.693: INFO: (8) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.273653ms)
Jun 19 22:29:53.696: INFO: (9) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.305452ms)
Jun 19 22:29:53.699: INFO: (10) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.983356ms)
Jun 19 22:29:53.703: INFO: (11) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.661247ms)
Jun 19 22:29:53.707: INFO: (12) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.722246ms)
Jun 19 22:29:53.711: INFO: (13) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.572749ms)
Jun 19 22:29:53.714: INFO: (14) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.614048ms)
Jun 19 22:29:53.718: INFO: (15) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.560848ms)
Jun 19 22:29:53.721: INFO: (16) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.122855ms)
Jun 19 22:29:53.724: INFO: (17) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.110455ms)
Jun 19 22:29:53.728: INFO: (18) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.865944ms)
Jun 19 22:29:53.731: INFO: (19) /api/v1/nodes/k8s-pool1-37287165-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.42265ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:29:53.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3292" for this suite.
Jun 19 22:29:59.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:29:59.813: INFO: namespace proxy-3292 deletion completed in 6.078902143s

• [SLOW TEST:6.313 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:29:59.813: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jun 19 22:29:59.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 cluster-info'
Jun 19 22:30:02.233: INFO: stderr: ""
Jun 19 22:30:02.233: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:30:02.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4861" for this suite.
Jun 19 22:30:08.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:30:08.334: INFO: namespace kubectl-4861 deletion completed in 6.096311588s

• [SLOW TEST:8.520 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:30:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:30:08.529: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4d9c347c-d16f-4123-8f60-032e88b3b5bf", Controller:(*bool)(0xc0012f069e), BlockOwnerDeletion:(*bool)(0xc0012f069f)}}
Jun 19 22:30:08.534: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"96eb1887-1255-4c96-b490-dfc62750c897", Controller:(*bool)(0xc003d43f2e), BlockOwnerDeletion:(*bool)(0xc003d43f2f)}}
Jun 19 22:30:08.543: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0eb4c947-ed02-47a6-973b-66b589b93652", Controller:(*bool)(0xc0012f087e), BlockOwnerDeletion:(*bool)(0xc0012f087f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:30:13.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8096" for this suite.
Jun 19 22:30:19.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:30:19.632: INFO: namespace gc-8096 deletion completed in 6.07769155s

• [SLOW TEST:11.298 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:30:19.633: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 19 22:30:19.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3100'
Jun 19 22:30:19.871: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 19 22:30:19.871: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jun 19 22:30:21.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3100'
Jun 19 22:30:21.958: INFO: stderr: ""
Jun 19 22:30:21.958: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:30:21.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3100" for this suite.
Jun 19 22:30:27.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:30:28.056: INFO: namespace kubectl-3100 deletion completed in 6.092348834s

• [SLOW TEST:8.423 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:30:28.057: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 19 22:30:36.230: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-5f6982f2-3248-43fa-849b-8e49e09a6c24,GenerateName:,Namespace:events-3554,SelfLink:/api/v1/namespaces/events-3554/pods/send-events-5f6982f2-3248-43fa-849b-8e49e09a6c24,UID:f3fb0ca5-77ca-42b2-a3e3-0a9742e73b9b,ResourceVersion:31121,Generation:0,CreationTimestamp:2019-06-19 22:30:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 203539966,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wqw4h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wqw4h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-wqw4h true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029a14f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029a1510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:30:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:30:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:30:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:30:28 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.96,PodIP:10.240.0.121,StartTime:2019-06-19 22:30:28 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-19 22:30:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://18af85774fce2781dfb3f5dd81f6858481190f09bfa0efbf8f912cf2e4957738}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 19 22:30:38.233: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 19 22:30:40.237: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:30:40.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3554" for this suite.
Jun 19 22:31:20.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:31:20.350: INFO: namespace events-3554 deletion completed in 40.099467657s

• [SLOW TEST:52.294 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:31:20.352: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-bzlfw in namespace proxy-2429
I0619 22:31:20.526035      16 runners.go:180] Created replication controller with name: proxy-service-bzlfw, namespace: proxy-2429, replica count: 1
I0619 22:31:21.577003      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:22.577224      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:23.577419      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:24.577689      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:25.577933      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:26.578200      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0619 22:31:27.578451      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0619 22:31:28.578646      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0619 22:31:29.578904      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0619 22:31:30.579093      16 runners.go:180] proxy-service-bzlfw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 19 22:31:30.581: INFO: setup took 10.077249813s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 14.742887ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 14.383392ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 13.8379ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 14.439192ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 14.478591ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 14.001598ms)
Jun 19 22:31:30.596: INFO: (0) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 14.738288ms)
Jun 19 22:31:30.599: INFO: (0) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 16.802658ms)
Jun 19 22:31:30.602: INFO: (0) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 20.12231ms)
Jun 19 22:31:30.603: INFO: (0) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 21.018497ms)
Jun 19 22:31:30.603: INFO: (0) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 21.086596ms)
Jun 19 22:31:30.603: INFO: (0) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 22.089481ms)
Jun 19 22:31:30.605: INFO: (0) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 22.622573ms)
Jun 19 22:31:30.605: INFO: (0) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 23.335664ms)
Jun 19 22:31:30.606: INFO: (0) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 23.312664ms)
Jun 19 22:31:30.607: INFO: (0) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 25.966225ms)
Jun 19 22:31:30.611: INFO: (1) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 4.101541ms)
Jun 19 22:31:30.612: INFO: (1) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 4.609034ms)
Jun 19 22:31:30.615: INFO: (1) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 7.764488ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 8.584476ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 8.871072ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 9.02637ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 8.790473ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 9.459863ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.803473ms)
Jun 19 22:31:30.617: INFO: (1) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 9.284466ms)
Jun 19 22:31:30.622: INFO: (1) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 13.709403ms)
Jun 19 22:31:30.622: INFO: (1) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 13.837201ms)
Jun 19 22:31:30.622: INFO: (1) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 14.438592ms)
Jun 19 22:31:30.622: INFO: (1) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 14.694688ms)
Jun 19 22:31:30.623: INFO: (1) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 15.196381ms)
Jun 19 22:31:30.623: INFO: (1) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 15.023783ms)
Jun 19 22:31:30.629: INFO: (2) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 6.036813ms)
Jun 19 22:31:30.630: INFO: (2) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 7.005399ms)
Jun 19 22:31:30.630: INFO: (2) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 7.171597ms)
Jun 19 22:31:30.630: INFO: (2) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 7.239095ms)
Jun 19 22:31:30.631: INFO: (2) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 7.67459ms)
Jun 19 22:31:30.649: INFO: (2) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 25.790328ms)
Jun 19 22:31:30.649: INFO: (2) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 25.912026ms)
Jun 19 22:31:30.649: INFO: (2) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 25.773328ms)
Jun 19 22:31:30.649: INFO: (2) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 25.724529ms)
Jun 19 22:31:30.649: INFO: (2) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 25.870027ms)
Jun 19 22:31:30.650: INFO: (2) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 26.768514ms)
Jun 19 22:31:30.650: INFO: (2) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 26.819813ms)
Jun 19 22:31:30.652: INFO: (2) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 28.695186ms)
Jun 19 22:31:30.652: INFO: (2) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 28.995082ms)
Jun 19 22:31:30.653: INFO: (2) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 29.263878ms)
Jun 19 22:31:30.653: INFO: (2) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 30.404861ms)
Jun 19 22:31:30.674: INFO: (3) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 20.719701ms)
Jun 19 22:31:30.675: INFO: (3) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 20.621002ms)
Jun 19 22:31:30.677: INFO: (3) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 22.89307ms)
Jun 19 22:31:30.677: INFO: (3) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 22.761172ms)
Jun 19 22:31:30.677: INFO: (3) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 23.453162ms)
Jun 19 22:31:30.677: INFO: (3) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 22.613573ms)
Jun 19 22:31:30.678: INFO: (3) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 23.801157ms)
Jun 19 22:31:30.678: INFO: (3) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 24.082252ms)
Jun 19 22:31:30.678: INFO: (3) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 24.399048ms)
Jun 19 22:31:30.678: INFO: (3) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 24.817742ms)
Jun 19 22:31:30.679: INFO: (3) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 24.045353ms)
Jun 19 22:31:30.679: INFO: (3) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 24.602745ms)
Jun 19 22:31:30.679: INFO: (3) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 24.794042ms)
Jun 19 22:31:30.679: INFO: (3) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 24.700543ms)
Jun 19 22:31:30.680: INFO: (3) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 25.261736ms)
Jun 19 22:31:30.680: INFO: (3) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 25.199937ms)
Jun 19 22:31:30.687: INFO: (4) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 7.554191ms)
Jun 19 22:31:30.688: INFO: (4) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 7.65469ms)
Jun 19 22:31:30.690: INFO: (4) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 9.74096ms)
Jun 19 22:31:30.690: INFO: (4) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 9.873957ms)
Jun 19 22:31:30.693: INFO: (4) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 12.398122ms)
Jun 19 22:31:30.694: INFO: (4) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 13.599404ms)
Jun 19 22:31:30.694: INFO: (4) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 13.123311ms)
Jun 19 22:31:30.694: INFO: (4) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 14.199595ms)
Jun 19 22:31:30.695: INFO: (4) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 14.266394ms)
Jun 19 22:31:30.695: INFO: (4) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 14.262795ms)
Jun 19 22:31:30.698: INFO: (4) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 17.819843ms)
Jun 19 22:31:30.698: INFO: (4) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 17.500148ms)
Jun 19 22:31:30.699: INFO: (4) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 17.841242ms)
Jun 19 22:31:30.699: INFO: (4) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 18.595432ms)
Jun 19 22:31:30.699: INFO: (4) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 18.657031ms)
Jun 19 22:31:30.699: INFO: (4) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 18.785929ms)
Jun 19 22:31:30.705: INFO: (5) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 6.040213ms)
Jun 19 22:31:30.705: INFO: (5) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 5.899914ms)
Jun 19 22:31:30.706: INFO: (5) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 6.394707ms)
Jun 19 22:31:30.710: INFO: (5) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 10.561747ms)
Jun 19 22:31:30.711: INFO: (5) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 11.313437ms)
Jun 19 22:31:30.711: INFO: (5) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.291137ms)
Jun 19 22:31:30.712: INFO: (5) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 12.588918ms)
Jun 19 22:31:30.712: INFO: (5) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 12.190024ms)
Jun 19 22:31:30.712: INFO: (5) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.019127ms)
Jun 19 22:31:30.713: INFO: (5) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 13.519305ms)
Jun 19 22:31:30.713: INFO: (5) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 13.522305ms)
Jun 19 22:31:30.714: INFO: (5) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 13.9067ms)
Jun 19 22:31:30.714: INFO: (5) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 14.648688ms)
Jun 19 22:31:30.714: INFO: (5) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 14.029598ms)
Jun 19 22:31:30.714: INFO: (5) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 14.118196ms)
Jun 19 22:31:30.714: INFO: (5) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 14.091097ms)
Jun 19 22:31:30.719: INFO: (6) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 4.775032ms)
Jun 19 22:31:30.721: INFO: (6) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 6.696204ms)
Jun 19 22:31:30.724: INFO: (6) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 8.788974ms)
Jun 19 22:31:30.724: INFO: (6) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.396978ms)
Jun 19 22:31:30.724: INFO: (6) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 9.265467ms)
Jun 19 22:31:30.725: INFO: (6) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 9.535663ms)
Jun 19 22:31:30.725: INFO: (6) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 9.992556ms)
Jun 19 22:31:30.725: INFO: (6) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 9.925357ms)
Jun 19 22:31:30.727: INFO: (6) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 12.158825ms)
Jun 19 22:31:30.727: INFO: (6) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 12.405721ms)
Jun 19 22:31:30.727: INFO: (6) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 12.021226ms)
Jun 19 22:31:30.728: INFO: (6) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.805715ms)
Jun 19 22:31:30.728: INFO: (6) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.968313ms)
Jun 19 22:31:30.729: INFO: (6) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 14.077397ms)
Jun 19 22:31:30.730: INFO: (6) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 14.290994ms)
Jun 19 22:31:30.730: INFO: (6) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 15.036283ms)
Jun 19 22:31:30.735: INFO: (7) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 4.606934ms)
Jun 19 22:31:30.736: INFO: (7) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 5.446721ms)
Jun 19 22:31:30.738: INFO: (7) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 7.279795ms)
Jun 19 22:31:30.738: INFO: (7) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 7.63389ms)
Jun 19 22:31:30.740: INFO: (7) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 8.705874ms)
Jun 19 22:31:30.742: INFO: (7) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 10.876043ms)
Jun 19 22:31:30.742: INFO: (7) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 10.730245ms)
Jun 19 22:31:30.742: INFO: (7) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 11.324336ms)
Jun 19 22:31:30.742: INFO: (7) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 12.115825ms)
Jun 19 22:31:30.743: INFO: (7) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 12.490119ms)
Jun 19 22:31:30.743: INFO: (7) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 11.908229ms)
Jun 19 22:31:30.743: INFO: (7) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 12.030927ms)
Jun 19 22:31:30.743: INFO: (7) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.940628ms)
Jun 19 22:31:30.743: INFO: (7) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.780216ms)
Jun 19 22:31:30.744: INFO: (7) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 12.825115ms)
Jun 19 22:31:30.744: INFO: (7) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 12.729517ms)
Jun 19 22:31:30.749: INFO: (8) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 4.428336ms)
Jun 19 22:31:30.752: INFO: (8) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 7.802088ms)
Jun 19 22:31:30.754: INFO: (8) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 10.44615ms)
Jun 19 22:31:30.756: INFO: (8) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.08114ms)
Jun 19 22:31:30.756: INFO: (8) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 10.817644ms)
Jun 19 22:31:30.756: INFO: (8) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 11.202739ms)
Jun 19 22:31:30.756: INFO: (8) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 11.543533ms)
Jun 19 22:31:30.756: INFO: (8) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 11.639632ms)
Jun 19 22:31:30.757: INFO: (8) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.277322ms)
Jun 19 22:31:30.758: INFO: (8) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 13.383507ms)
Jun 19 22:31:30.758: INFO: (8) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 12.812015ms)
Jun 19 22:31:30.759: INFO: (8) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 14.234994ms)
Jun 19 22:31:30.759: INFO: (8) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 13.932999ms)
Jun 19 22:31:30.759: INFO: (8) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 14.061697ms)
Jun 19 22:31:30.759: INFO: (8) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 14.150596ms)
Jun 19 22:31:30.760: INFO: (8) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 15.341078ms)
Jun 19 22:31:30.765: INFO: (9) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 5.391922ms)
Jun 19 22:31:30.772: INFO: (9) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 12.242724ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.319423ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 12.434121ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 12.142325ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.275923ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 12.006527ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 12.602519ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 12.188924ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 12.156925ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 12.46482ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 12.017727ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 11.981127ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.081126ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 12.609118ms)
Jun 19 22:31:30.773: INFO: (9) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 12.365322ms)
Jun 19 22:31:30.783: INFO: (10) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 9.364665ms)
Jun 19 22:31:30.783: INFO: (10) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 9.255067ms)
Jun 19 22:31:30.783: INFO: (10) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 9.628461ms)
Jun 19 22:31:30.784: INFO: (10) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 10.654747ms)
Jun 19 22:31:30.784: INFO: (10) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 10.500649ms)
Jun 19 22:31:30.785: INFO: (10) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 11.325336ms)
Jun 19 22:31:30.785: INFO: (10) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 11.259337ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 12.585619ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.558519ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 13.023012ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 12.864314ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 12.571919ms)
Jun 19 22:31:30.786: INFO: (10) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 12.473921ms)
Jun 19 22:31:30.789: INFO: (10) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 15.162581ms)
Jun 19 22:31:30.789: INFO: (10) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 15.372878ms)
Jun 19 22:31:30.789: INFO: (10) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 15.390178ms)
Jun 19 22:31:30.796: INFO: (11) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 6.500406ms)
Jun 19 22:31:30.796: INFO: (11) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 6.609705ms)
Jun 19 22:31:30.797: INFO: (11) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 7.62029ms)
Jun 19 22:31:30.798: INFO: (11) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 8.651675ms)
Jun 19 22:31:30.798: INFO: (11) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.113783ms)
Jun 19 22:31:30.801: INFO: (11) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 10.622047ms)
Jun 19 22:31:30.801: INFO: (11) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 10.35035ms)
Jun 19 22:31:30.801: INFO: (11) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 10.632446ms)
Jun 19 22:31:30.801: INFO: (11) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 10.925643ms)
Jun 19 22:31:30.803: INFO: (11) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 12.548919ms)
Jun 19 22:31:30.806: INFO: (11) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 16.870956ms)
Jun 19 22:31:30.806: INFO: (11) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 16.091468ms)
Jun 19 22:31:30.808: INFO: (11) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 17.728544ms)
Jun 19 22:31:30.808: INFO: (11) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 18.472933ms)
Jun 19 22:31:30.809: INFO: (11) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 18.780629ms)
Jun 19 22:31:30.809: INFO: (11) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 19.133224ms)
Jun 19 22:31:30.817: INFO: (12) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 7.691889ms)
Jun 19 22:31:30.817: INFO: (12) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 7.709089ms)
Jun 19 22:31:30.817: INFO: (12) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 7.818187ms)
Jun 19 22:31:30.819: INFO: (12) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 10.189153ms)
Jun 19 22:31:30.821: INFO: (12) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 11.858428ms)
Jun 19 22:31:30.822: INFO: (12) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 13.21181ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 14.022397ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 13.973299ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 13.8679ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 14.125096ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 13.922599ms)
Jun 19 22:31:30.823: INFO: (12) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 14.161496ms)
Jun 19 22:31:30.824: INFO: (12) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 14.760487ms)
Jun 19 22:31:30.824: INFO: (12) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 15.097382ms)
Jun 19 22:31:30.824: INFO: (12) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 15.26928ms)
Jun 19 22:31:30.824: INFO: (12) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 15.285079ms)
Jun 19 22:31:30.830: INFO: (13) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 6.010513ms)
Jun 19 22:31:30.832: INFO: (13) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 7.663089ms)
Jun 19 22:31:30.833: INFO: (13) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.814373ms)
Jun 19 22:31:30.833: INFO: (13) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 8.661075ms)
Jun 19 22:31:30.834: INFO: (13) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 9.214167ms)
Jun 19 22:31:30.835: INFO: (13) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 10.689546ms)
Jun 19 22:31:30.836: INFO: (13) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.551033ms)
Jun 19 22:31:30.836: INFO: (13) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 11.628933ms)
Jun 19 22:31:30.837: INFO: (13) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 11.560633ms)
Jun 19 22:31:30.838: INFO: (13) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 13.511605ms)
Jun 19 22:31:30.838: INFO: (13) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 13.325407ms)
Jun 19 22:31:30.839: INFO: (13) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 14.252894ms)
Jun 19 22:31:30.839: INFO: (13) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 13.726502ms)
Jun 19 22:31:30.839: INFO: (13) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 14.166296ms)
Jun 19 22:31:30.839: INFO: (13) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 14.307293ms)
Jun 19 22:31:30.839: INFO: (13) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 14.751387ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 11.06994ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 10.476149ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 10.268552ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 10.970142ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 10.951842ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 11.09334ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 10.330151ms)
Jun 19 22:31:30.851: INFO: (14) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 10.962541ms)
Jun 19 22:31:30.852: INFO: (14) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.341036ms)
Jun 19 22:31:30.852: INFO: (14) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 11.81083ms)
Jun 19 22:31:30.852: INFO: (14) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 11.73753ms)
Jun 19 22:31:30.853: INFO: (14) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 12.438021ms)
Jun 19 22:31:30.855: INFO: (14) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 14.312393ms)
Jun 19 22:31:30.855: INFO: (14) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 14.711388ms)
Jun 19 22:31:30.855: INFO: (14) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 14.258194ms)
Jun 19 22:31:30.855: INFO: (14) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 14.312794ms)
Jun 19 22:31:30.859: INFO: (15) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 4.394636ms)
Jun 19 22:31:30.863: INFO: (15) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 7.63779ms)
Jun 19 22:31:30.863: INFO: (15) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 7.60629ms)
Jun 19 22:31:30.863: INFO: (15) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 7.863686ms)
Jun 19 22:31:30.867: INFO: (15) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 10.901643ms)
Jun 19 22:31:30.868: INFO: (15) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.201624ms)
Jun 19 22:31:30.869: INFO: (15) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 13.517305ms)
Jun 19 22:31:30.869: INFO: (15) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 14.131396ms)
Jun 19 22:31:30.869: INFO: (15) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 13.928399ms)
Jun 19 22:31:30.870: INFO: (15) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 14.004697ms)
Jun 19 22:31:30.870: INFO: (15) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 15.28398ms)
Jun 19 22:31:30.870: INFO: (15) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 14.486591ms)
Jun 19 22:31:30.870: INFO: (15) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 14.605189ms)
Jun 19 22:31:30.871: INFO: (15) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 15.737273ms)
Jun 19 22:31:30.871: INFO: (15) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 15.740773ms)
Jun 19 22:31:30.872: INFO: (15) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 15.543176ms)
Jun 19 22:31:30.879: INFO: (16) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 6.965999ms)
Jun 19 22:31:30.880: INFO: (16) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 7.540591ms)
Jun 19 22:31:30.883: INFO: (16) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 10.575348ms)
Jun 19 22:31:30.883: INFO: (16) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 10.498649ms)
Jun 19 22:31:30.883: INFO: (16) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 11.332037ms)
Jun 19 22:31:30.883: INFO: (16) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 10.927442ms)
Jun 19 22:31:30.884: INFO: (16) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 11.634532ms)
Jun 19 22:31:30.884: INFO: (16) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 11.718831ms)
Jun 19 22:31:30.884: INFO: (16) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 12.339622ms)
Jun 19 22:31:30.884: INFO: (16) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 12.094025ms)
Jun 19 22:31:30.885: INFO: (16) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 12.665217ms)
Jun 19 22:31:30.887: INFO: (16) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 15.21018ms)
Jun 19 22:31:30.887: INFO: (16) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 14.888285ms)
Jun 19 22:31:30.887: INFO: (16) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 15.002583ms)
Jun 19 22:31:30.887: INFO: (16) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 15.167481ms)
Jun 19 22:31:30.887: INFO: (16) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 15.004384ms)
Jun 19 22:31:30.893: INFO: (17) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 5.711817ms)
Jun 19 22:31:30.893: INFO: (17) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 5.598519ms)
Jun 19 22:31:30.895: INFO: (17) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 8.052184ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 9.957456ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 10.215553ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 10.39485ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 9.985656ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 10.644547ms)
Jun 19 22:31:30.898: INFO: (17) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 10.750345ms)
Jun 19 22:31:30.899: INFO: (17) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 10.967241ms)
Jun 19 22:31:30.899: INFO: (17) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 11.370236ms)
Jun 19 22:31:30.899: INFO: (17) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 11.434935ms)
Jun 19 22:31:30.900: INFO: (17) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 11.807829ms)
Jun 19 22:31:30.900: INFO: (17) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 11.826229ms)
Jun 19 22:31:30.901: INFO: (17) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 12.51632ms)
Jun 19 22:31:30.901: INFO: (17) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.634118ms)
Jun 19 22:31:30.906: INFO: (18) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 5.096326ms)
Jun 19 22:31:30.907: INFO: (18) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 6.094412ms)
Jun 19 22:31:30.910: INFO: (18) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 8.831473ms)
Jun 19 22:31:30.910: INFO: (18) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.627975ms)
Jun 19 22:31:30.910: INFO: (18) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 8.876272ms)
Jun 19 22:31:30.911: INFO: (18) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 10.176053ms)
Jun 19 22:31:30.912: INFO: (18) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 10.820244ms)
Jun 19 22:31:30.913: INFO: (18) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 11.521834ms)
Jun 19 22:31:30.913: INFO: (18) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 11.649732ms)
Jun 19 22:31:30.917: INFO: (18) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 16.63916ms)
Jun 19 22:31:30.918: INFO: (18) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 17.013754ms)
Jun 19 22:31:30.918: INFO: (18) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 17.090853ms)
Jun 19 22:31:30.919: INFO: (18) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 17.642246ms)
Jun 19 22:31:30.919: INFO: (18) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 18.141838ms)
Jun 19 22:31:30.919: INFO: (18) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 17.754043ms)
Jun 19 22:31:30.919: INFO: (18) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 18.230037ms)
Jun 19 22:31:30.928: INFO: (19) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 8.904972ms)
Jun 19 22:31:30.928: INFO: (19) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:460/proxy/: tls baz (200; 8.860472ms)
Jun 19 22:31:30.929: INFO: (19) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn/proxy/rewriteme">test</a> (200; 9.332465ms)
Jun 19 22:31:30.930: INFO: (19) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 10.448949ms)
Jun 19 22:31:30.930: INFO: (19) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:462/proxy/: tls qux (200; 10.37635ms)
Jun 19 22:31:30.932: INFO: (19) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:160/proxy/: foo (200; 12.993412ms)
Jun 19 22:31:30.932: INFO: (19) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname1/proxy/: tls baz (200; 12.814515ms)
Jun 19 22:31:30.932: INFO: (19) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">... (200; 12.588218ms)
Jun 19 22:31:30.933: INFO: (19) /api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/proxy-service-bzlfw-9mpcn:1080/proxy/rewriteme">test<... (200; 13.322107ms)
Jun 19 22:31:30.933: INFO: (19) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname1/proxy/: foo (200; 13.571505ms)
Jun 19 22:31:30.933: INFO: (19) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname1/proxy/: foo (200; 13.8713ms)
Jun 19 22:31:30.934: INFO: (19) /api/v1/namespaces/proxy-2429/pods/http:proxy-service-bzlfw-9mpcn:162/proxy/: bar (200; 14.641389ms)
Jun 19 22:31:30.934: INFO: (19) /api/v1/namespaces/proxy-2429/services/proxy-service-bzlfw:portname2/proxy/: bar (200; 14.56379ms)
Jun 19 22:31:30.934: INFO: (19) /api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/: <a href="/api/v1/namespaces/proxy-2429/pods/https:proxy-service-bzlfw-9mpcn:443/proxy/tlsrewritem... (200; 14.54059ms)
Jun 19 22:31:30.934: INFO: (19) /api/v1/namespaces/proxy-2429/services/https:proxy-service-bzlfw:tlsportname2/proxy/: tls qux (200; 15.202981ms)
Jun 19 22:31:30.936: INFO: (19) /api/v1/namespaces/proxy-2429/services/http:proxy-service-bzlfw:portname2/proxy/: bar (200; 16.327965ms)
STEP: deleting ReplicationController proxy-service-bzlfw in namespace proxy-2429, will wait for the garbage collector to delete the pods
Jun 19 22:31:30.993: INFO: Deleting ReplicationController proxy-service-bzlfw took: 4.624634ms
Jun 19 22:31:31.293: INFO: Terminating ReplicationController proxy-service-bzlfw pods took: 300.239368ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:31:40.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2429" for this suite.
Jun 19 22:31:46.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:31:46.202: INFO: namespace proxy-2429 deletion completed in 6.103268638s

• [SLOW TEST:25.850 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:31:46.202: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-202c595a-14ec-4078-9a8d-518f652ca8b3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:31:46.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4301" for this suite.
Jun 19 22:31:52.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:31:52.436: INFO: namespace configmap-4301 deletion completed in 6.085592891s

• [SLOW TEST:6.235 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:31:52.437: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4799
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4799
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4799
Jun 19 22:31:52.609: INFO: Found 0 stateful pods, waiting for 1
Jun 19 22:32:02.613: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 19 22:32:02.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 22:32:02.835: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 22:32:02.835: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 22:32:02.835: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 22:32:02.837: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 19 22:32:12.841: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 22:32:12.841: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 22:32:12.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
Jun 19 22:32:13.863: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990714965s
Jun 19 22:32:14.867: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987138647s
Jun 19 22:32:15.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983244934s
Jun 19 22:32:16.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979785815s
Jun 19 22:32:17.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97671009s
Jun 19 22:32:18.880: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973838763s
Jun 19 22:32:19.889: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970285245s
Jun 19 22:32:20.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.961747399s
Jun 19 22:32:21.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.889586ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4799
Jun 19 22:32:22.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 22:32:23.119: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 22:32:23.119: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 22:32:23.119: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 22:32:23.122: INFO: Found 1 stateful pods, waiting for 3
Jun 19 22:32:33.125: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 22:32:33.125: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 19 22:32:33.125: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 19 22:32:33.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 22:32:33.343: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 22:32:33.343: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 22:32:33.343: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 22:32:33.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 22:32:33.578: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 22:32:33.578: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 22:32:33.578: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 22:32:33.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 19 22:32:33.798: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 19 22:32:33.798: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 19 22:32:33.798: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 19 22:32:33.798: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 22:32:33.801: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 19 22:32:43.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 22:32:43.807: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 22:32:43.807: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 19 22:32:43.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
Jun 19 22:32:44.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996785579s
Jun 19 22:32:45.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992867369s
Jun 19 22:32:46.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988255068s
Jun 19 22:32:47.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984520855s
Jun 19 22:32:48.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981204336s
Jun 19 22:32:49.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977490322s
Jun 19 22:32:50.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974049705s
Jun 19 22:32:51.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970650488s
Jun 19 22:32:52.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.782791ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4799
Jun 19 22:32:53.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 22:32:54.066: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 22:32:54.066: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 22:32:54.066: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 22:32:54.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 22:32:54.307: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 22:32:54.308: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 22:32:54.308: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 22:32:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-497557601 exec --namespace=statefulset-4799 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 19 22:32:54.514: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 19 22:32:54.514: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 19 22:32:54.514: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 19 22:32:54.514: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 19 22:33:24.526: INFO: Deleting all statefulset in ns statefulset-4799
Jun 19 22:33:24.528: INFO: Scaling statefulset ss to 0
Jun 19 22:33:24.535: INFO: Waiting for statefulset status.replicas updated to 0
Jun 19 22:33:24.537: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:33:24.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4799" for this suite.
Jun 19 22:33:30.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:33:30.637: INFO: namespace statefulset-4799 deletion completed in 6.086149043s

• [SLOW TEST:98.201 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:33:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9002
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 19 22:33:30.784: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 19 22:34:00.877: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.69:8080/dial?request=hostName&protocol=udp&host=10.240.0.112&port=8081&tries=1'] Namespace:pod-network-test-9002 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:34:00.877: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:34:01.009: INFO: Waiting for endpoints: map[]
Jun 19 22:34:01.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.69:8080/dial?request=hostName&protocol=udp&host=10.240.0.75&port=8081&tries=1'] Namespace:pod-network-test-9002 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:34:01.012: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:34:01.143: INFO: Waiting for endpoints: map[]
Jun 19 22:34:01.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.69:8080/dial?request=hostName&protocol=udp&host=10.240.0.35&port=8081&tries=1'] Namespace:pod-network-test-9002 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:34:01.146: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:34:01.302: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:34:01.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9002" for this suite.
Jun 19 22:34:23.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:34:23.384: INFO: namespace pod-network-test-9002 deletion completed in 22.078584128s

• [SLOW TEST:52.745 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:34:23.386: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-65b8f3cf-a90e-4c4f-9b8b-66ecec5f2f66
STEP: Creating a pod to test consume secrets
Jun 19 22:34:23.558: INFO: Waiting up to 5m0s for pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba" in namespace "secrets-6854" to be "success or failure"
Jun 19 22:34:23.568: INFO: Pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba": Phase="Pending", Reason="", readiness=false. Elapsed: 9.798759ms
Jun 19 22:34:25.571: INFO: Pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012872637s
Jun 19 22:34:27.574: INFO: Pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016215111s
Jun 19 22:34:29.577: INFO: Pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019297488s
STEP: Saw pod success
Jun 19 22:34:29.577: INFO: Pod "pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba" satisfied condition "success or failure"
Jun 19 22:34:29.580: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba container secret-volume-test: <nil>
STEP: delete the pod
Jun 19 22:34:29.605: INFO: Waiting for pod pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba to disappear
Jun 19 22:34:29.607: INFO: Pod pod-secrets-8453cb63-49a5-4c5a-be41-b9c3e5a64dba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:34:29.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6854" for this suite.
Jun 19 22:34:35.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:34:35.697: INFO: namespace secrets-6854 deletion completed in 6.087627599s

• [SLOW TEST:12.312 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:34:35.698: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jun 19 22:34:35.850: INFO: Waiting up to 5m0s for pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea" in namespace "containers-3391" to be "success or failure"
Jun 19 22:34:35.867: INFO: Pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea": Phase="Pending", Reason="", readiness=false. Elapsed: 16.748459ms
Jun 19 22:34:37.872: INFO: Pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021010518s
Jun 19 22:34:39.874: INFO: Pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023851198s
Jun 19 22:34:41.878: INFO: Pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027174771s
STEP: Saw pod success
Jun 19 22:34:41.878: INFO: Pod "client-containers-21053356-ed72-409e-92b6-e48737dad0ea" satisfied condition "success or failure"
Jun 19 22:34:41.880: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod client-containers-21053356-ed72-409e-92b6-e48737dad0ea container test-container: <nil>
STEP: delete the pod
Jun 19 22:34:41.901: INFO: Waiting for pod client-containers-21053356-ed72-409e-92b6-e48737dad0ea to disappear
Jun 19 22:34:41.904: INFO: Pod client-containers-21053356-ed72-409e-92b6-e48737dad0ea no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:34:41.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3391" for this suite.
Jun 19 22:34:47.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:34:47.988: INFO: namespace containers-3391 deletion completed in 6.080687796s

• [SLOW TEST:12.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:34:47.991: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1021
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 19 22:34:48.195: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 19 22:35:16.286: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:35:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:35:17.411: INFO: Found all expected endpoints: [netserver-0]
Jun 19 22:35:17.415: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:35:17.415: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:35:18.532: INFO: Found all expected endpoints: [netserver-1]
Jun 19 22:35:18.542: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.88 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 19 22:35:18.542: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
Jun 19 22:35:19.669: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:35:19.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1021" for this suite.
Jun 19 22:35:41.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:35:41.763: INFO: namespace pod-network-test-1021 deletion completed in 22.09031327s

• [SLOW TEST:53.772 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:35:41.763: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-41ddd275-dc76-4cf9-ab06-0a499d4b5998
STEP: Creating a pod to test consume configMaps
Jun 19 22:35:41.919: INFO: Waiting up to 5m0s for pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024" in namespace "configmap-4935" to be "success or failure"
Jun 19 22:35:41.928: INFO: Pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024": Phase="Pending", Reason="", readiness=false. Elapsed: 9.204067ms
Jun 19 22:35:43.932: INFO: Pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012709131s
Jun 19 22:35:45.935: INFO: Pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015743702s
Jun 19 22:35:47.939: INFO: Pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019882056s
STEP: Saw pod success
Jun 19 22:35:47.939: INFO: Pod "pod-configmaps-01a862da-852f-4205-84d1-106e884b2024" satisfied condition "success or failure"
Jun 19 22:35:47.942: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-configmaps-01a862da-852f-4205-84d1-106e884b2024 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 19 22:35:47.966: INFO: Waiting for pod pod-configmaps-01a862da-852f-4205-84d1-106e884b2024 to disappear
Jun 19 22:35:47.969: INFO: Pod pod-configmaps-01a862da-852f-4205-84d1-106e884b2024 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:35:47.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4935" for this suite.
Jun 19 22:35:53.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:35:54.056: INFO: namespace configmap-4935 deletion completed in 6.084170826s

• [SLOW TEST:12.293 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:35:54.064: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-06908e65-d76f-4407-b170-de23490ff8b5 in namespace container-probe-3956
Jun 19 22:36:00.262: INFO: Started pod busybox-06908e65-d76f-4407-b170-de23490ff8b5 in namespace container-probe-3956
STEP: checking the pod's current state and verifying that restartCount is present
Jun 19 22:36:00.264: INFO: Initial restart count of pod busybox-06908e65-d76f-4407-b170-de23490ff8b5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:40:00.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3956" for this suite.
Jun 19 22:40:06.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:40:06.791: INFO: namespace container-probe-3956 deletion completed in 6.095702304s

• [SLOW TEST:252.727 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:40:06.795: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 19 22:40:18.992: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:18.996: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:20.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:21.000: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:22.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:22.999: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:24.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:24.999: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:26.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:26.999: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:28.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:28.999: INFO: Pod pod-with-poststart-http-hook still exists
Jun 19 22:40:30.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 19 22:40:30.999: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:40:31.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1406" for this suite.
Jun 19 22:40:53.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:40:53.089: INFO: namespace container-lifecycle-hook-1406 deletion completed in 22.085540695s

• [SLOW TEST:46.294 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:40:53.090: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8288
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8288 to expose endpoints map[]
Jun 19 22:40:53.285: INFO: Get endpoints failed (6.691604ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 19 22:40:54.288: INFO: successfully validated that service endpoint-test2 in namespace services-8288 exposes endpoints map[] (1.009732906s elapsed)
STEP: Creating pod pod1 in namespace services-8288
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8288 to expose endpoints map[pod1:[80]]
Jun 19 22:40:58.324: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.030764541s elapsed, will retry)
Jun 19 22:40:59.328: INFO: successfully validated that service endpoint-test2 in namespace services-8288 exposes endpoints map[pod1:[80]] (5.035512919s elapsed)
STEP: Creating pod pod2 in namespace services-8288
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8288 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 19 22:41:03.382: INFO: Unexpected endpoints: found map[e6f38f8a-5f7d-438a-89ea-011d16e662bf:[80]], expected map[pod1:[80] pod2:[80]] (4.049543369s elapsed, will retry)
Jun 19 22:41:04.390: INFO: successfully validated that service endpoint-test2 in namespace services-8288 exposes endpoints map[pod1:[80] pod2:[80]] (5.0575473s elapsed)
STEP: Deleting pod pod1 in namespace services-8288
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8288 to expose endpoints map[pod2:[80]]
Jun 19 22:41:04.419: INFO: successfully validated that service endpoint-test2 in namespace services-8288 exposes endpoints map[pod2:[80]] (22.259478ms elapsed)
STEP: Deleting pod pod2 in namespace services-8288
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8288 to expose endpoints map[]
Jun 19 22:41:04.437: INFO: successfully validated that service endpoint-test2 in namespace services-8288 exposes endpoints map[] (8.495777ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:41:04.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8288" for this suite.
Jun 19 22:41:10.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:41:10.560: INFO: namespace services-8288 deletion completed in 6.091313257s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:17.470 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:41:10.561: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:41:10.752: INFO: Create a RollingUpdate DaemonSet
Jun 19 22:41:10.755: INFO: Check that daemon pods launch on every node of the cluster
Jun 19 22:41:10.760: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:10.765: INFO: Number of nodes with available pods: 0
Jun 19 22:41:10.765: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:41:11.769: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:11.772: INFO: Number of nodes with available pods: 0
Jun 19 22:41:11.772: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:41:12.769: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:12.771: INFO: Number of nodes with available pods: 0
Jun 19 22:41:12.771: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:41:13.770: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:13.772: INFO: Number of nodes with available pods: 0
Jun 19 22:41:13.772: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:41:14.769: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:14.771: INFO: Number of nodes with available pods: 0
Jun 19 22:41:14.771: INFO: Node k8s-pool1-37287165-vmss000000 is running more than one daemon pod
Jun 19 22:41:15.775: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:15.778: INFO: Number of nodes with available pods: 2
Jun 19 22:41:15.778: INFO: Node k8s-pool1-37287165-vmss000002 is running more than one daemon pod
Jun 19 22:41:16.769: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:16.772: INFO: Number of nodes with available pods: 3
Jun 19 22:41:16.772: INFO: Number of running nodes: 3, number of available pods: 3
Jun 19 22:41:16.772: INFO: Update the DaemonSet to trigger a rollout
Jun 19 22:41:16.777: INFO: Updating DaemonSet daemon-set
Jun 19 22:41:20.790: INFO: Roll back the DaemonSet before rollout is complete
Jun 19 22:41:20.797: INFO: Updating DaemonSet daemon-set
Jun 19 22:41:20.797: INFO: Make sure DaemonSet rollback is complete
Jun 19 22:41:20.811: INFO: Wrong image for pod: daemon-set-9s9xw. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 19 22:41:20.811: INFO: Pod daemon-set-9s9xw is not available
Jun 19 22:41:20.816: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:21.820: INFO: Wrong image for pod: daemon-set-9s9xw. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 19 22:41:21.820: INFO: Pod daemon-set-9s9xw is not available
Jun 19 22:41:21.823: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:22.820: INFO: Wrong image for pod: daemon-set-9s9xw. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 19 22:41:22.820: INFO: Pod daemon-set-9s9xw is not available
Jun 19 22:41:22.823: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:23.820: INFO: Wrong image for pod: daemon-set-9s9xw. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 19 22:41:23.820: INFO: Pod daemon-set-9s9xw is not available
Jun 19 22:41:23.825: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:24.820: INFO: Wrong image for pod: daemon-set-9s9xw. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 19 22:41:24.820: INFO: Pod daemon-set-9s9xw is not available
Jun 19 22:41:24.824: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 19 22:41:25.820: INFO: Pod daemon-set-ldp77 is not available
Jun 19 22:41:25.823: INFO: DaemonSet pods can't tolerate node k8s-master-37287165-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7013, will wait for the garbage collector to delete the pods
Jun 19 22:41:25.885: INFO: Deleting DaemonSet.extensions daemon-set took: 4.746331ms
Jun 19 22:41:26.185: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.166762ms
Jun 19 22:41:40.888: INFO: Number of nodes with available pods: 0
Jun 19 22:41:40.888: INFO: Number of running nodes: 0, number of available pods: 0
Jun 19 22:41:40.890: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7013/daemonsets","resourceVersion":"32912"},"items":null}

Jun 19 22:41:40.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7013/pods","resourceVersion":"32912"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:41:40.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7013" for this suite.
Jun 19 22:41:46.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:41:46.986: INFO: namespace daemonsets-7013 deletion completed in 6.081656692s

• [SLOW TEST:36.425 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:41:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 19 22:41:47.140: INFO: Waiting up to 5m0s for pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e" in namespace "downward-api-503" to be "success or failure"
Jun 19 22:41:47.143: INFO: Pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.79146ms
Jun 19 22:41:49.147: INFO: Pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006621295s
Jun 19 22:41:51.150: INFO: Pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00981204s
Jun 19 22:41:53.153: INFO: Pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012821887s
STEP: Saw pod success
Jun 19 22:41:53.153: INFO: Pod "downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e" satisfied condition "success or failure"
Jun 19 22:41:53.156: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e container dapi-container: <nil>
STEP: delete the pod
Jun 19 22:41:53.173: INFO: Waiting for pod downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e to disappear
Jun 19 22:41:53.176: INFO: Pod downward-api-a525d0a9-6f38-4387-a368-66fde3f5dd5e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:41:53.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-503" for this suite.
Jun 19 22:41:59.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:41:59.261: INFO: namespace downward-api-503 deletion completed in 6.081542592s

• [SLOW TEST:12.275 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:41:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:41:59.404: INFO: Creating ReplicaSet my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17
Jun 19 22:41:59.411: INFO: Pod name my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17: Found 0 pods out of 1
Jun 19 22:42:04.414: INFO: Pod name my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17: Found 1 pods out of 1
Jun 19 22:42:04.414: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17" is running
Jun 19 22:42:04.417: INFO: Pod "my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17-w6nbx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 22:41:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 22:42:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 22:42:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-19 22:41:59 +0000 UTC Reason: Message:}])
Jun 19 22:42:04.417: INFO: Trying to dial the pod
Jun 19 22:42:09.428: INFO: Controller my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17: Got expected result from replica 1 [my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17-w6nbx]: "my-hostname-basic-89986123-f384-4c5f-87ad-d09b3d44bb17-w6nbx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:42:09.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6025" for this suite.
Jun 19 22:42:15.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:42:15.518: INFO: namespace replicaset-6025 deletion completed in 6.086116224s

• [SLOW TEST:16.257 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:42:15.520: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 19 22:42:27.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:27.744: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:29.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:29.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:31.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:31.747: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:33.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:33.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:35.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:35.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:37.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:37.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:39.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:39.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:41.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:41.747: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:43.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:43.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:45.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:45.747: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:47.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:47.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:49.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:49.748: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 19 22:42:51.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 19 22:42:51.748: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:42:51.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2932" for this suite.
Jun 19 22:43:13.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:43:13.877: INFO: namespace container-lifecycle-hook-2932 deletion completed in 22.09213423s

• [SLOW TEST:58.359 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:43:13.878: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 19 22:43:14.037: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 19 22:43:19.039: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:43:19.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4886" for this suite.
Jun 19 22:43:25.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:43:25.167: INFO: namespace replication-controller-4886 deletion completed in 6.097654449s

• [SLOW TEST:11.289 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:43:25.168: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-443
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:43:25.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f" in namespace "projected-443" to be "success or failure"
Jun 19 22:43:25.328: INFO: Pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092668ms
Jun 19 22:43:27.332: INFO: Pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013069097s
Jun 19 22:43:29.335: INFO: Pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016031841s
Jun 19 22:43:31.338: INFO: Pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019436378s
STEP: Saw pod success
Jun 19 22:43:31.338: INFO: Pod "downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f" satisfied condition "success or failure"
Jun 19 22:43:31.340: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f container client-container: <nil>
STEP: delete the pod
Jun 19 22:43:31.364: INFO: Waiting for pod downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f to disappear
Jun 19 22:43:31.366: INFO: Pod downwardapi-volume-13759a55-acf2-4f59-afd9-d3526c20e83f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:43:31.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-443" for this suite.
Jun 19 22:43:37.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:43:37.469: INFO: namespace projected-443 deletion completed in 6.094949286s

• [SLOW TEST:12.301 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:43:37.470: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6956
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-ae4c20ef-c6a7-4de5-a4d0-b8796a80c068
STEP: Creating secret with name s-test-opt-upd-42b7feb2-2af8-4eeb-8b20-1fd90394a94d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ae4c20ef-c6a7-4de5-a4d0-b8796a80c068
STEP: Updating secret s-test-opt-upd-42b7feb2-2af8-4eeb-8b20-1fd90394a94d
STEP: Creating secret with name s-test-opt-create-84ef8711-8553-4807-a4fc-7aa8743e8de5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:43:49.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6956" for this suite.
Jun 19 22:44:11.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:44:11.816: INFO: namespace secrets-6956 deletion completed in 22.089732241s

• [SLOW TEST:34.346 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:44:11.816: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5705
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:44:11.962: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:44:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5705" for this suite.
Jun 19 22:44:19.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:44:19.122: INFO: namespace custom-resource-definition-5705 deletion completed in 6.082704059s

• [SLOW TEST:7.306 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:44:19.122: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5331.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5331.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 19 22:44:27.319: INFO: DNS probes using dns-5331/dns-test-96aab069-8da9-4b9e-9fb0-f525d893eb54 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:44:27.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5331" for this suite.
Jun 19 22:44:33.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:44:33.430: INFO: namespace dns-5331 deletion completed in 6.093130507s

• [SLOW TEST:14.308 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:44:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6144, will wait for the garbage collector to delete the pods
Jun 19 22:44:39.644: INFO: Deleting Job.batch foo took: 4.055241ms
Jun 19 22:44:39.945: INFO: Terminating Job.batch foo pods took: 300.158561ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:45:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6144" for this suite.
Jun 19 22:45:26.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:45:27.050: INFO: namespace job-6144 deletion completed in 6.082453656s

• [SLOW TEST:53.620 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:45:27.051: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 19 22:45:27.211: INFO: Waiting up to 5m0s for pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c" in namespace "emptydir-8196" to be "success or failure"
Jun 19 22:45:27.215: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44135ms
Jun 19 22:45:29.220: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008789755s
Jun 19 22:45:31.224: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012043491s
Jun 19 22:45:33.227: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015527123s
Jun 19 22:45:35.230: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018956556s
STEP: Saw pod success
Jun 19 22:45:35.231: INFO: Pod "pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c" satisfied condition "success or failure"
Jun 19 22:45:35.233: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c container test-container: <nil>
STEP: delete the pod
Jun 19 22:45:35.259: INFO: Waiting for pod pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c to disappear
Jun 19 22:45:35.261: INFO: Pod pod-7ad1bc72-d7c6-4b7d-bb36-c559388e4d3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:45:35.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8196" for this suite.
Jun 19 22:45:41.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:45:41.347: INFO: namespace emptydir-8196 deletion completed in 6.081902862s

• [SLOW TEST:14.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:45:41.349: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:45:41.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18" in namespace "downward-api-5049" to be "success or failure"
Jun 19 22:45:41.517: INFO: Pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18": Phase="Pending", Reason="", readiness=false. Elapsed: 6.25751ms
Jun 19 22:45:43.522: INFO: Pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010708127s
Jun 19 22:45:45.525: INFO: Pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014344457s
Jun 19 22:45:47.528: INFO: Pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017073699s
STEP: Saw pod success
Jun 19 22:45:47.528: INFO: Pod "downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18" satisfied condition "success or failure"
Jun 19 22:45:47.531: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18 container client-container: <nil>
STEP: delete the pod
Jun 19 22:45:47.547: INFO: Waiting for pod downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18 to disappear
Jun 19 22:45:47.552: INFO: Pod downwardapi-volume-953b0d19-1e8e-4032-a011-ac7406927b18 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:45:47.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5049" for this suite.
Jun 19 22:45:53.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:45:53.657: INFO: namespace downward-api-5049 deletion completed in 6.100923186s

• [SLOW TEST:12.308 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:45:53.659: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 19 22:46:00.355: INFO: Successfully updated pod "labelsupdate1d67ccb4-823a-4fcd-b629-b5a9b5b4dde1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:46:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8878" for this suite.
Jun 19 22:46:24.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:46:24.488: INFO: namespace downward-api-8878 deletion completed in 22.11642801s

• [SLOW TEST:30.829 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:46:24.489: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 19 22:46:24.670: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172" in namespace "projected-9572" to be "success or failure"
Jun 19 22:46:24.672: INFO: Pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610962ms
Jun 19 22:46:26.675: INFO: Pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005568701s
Jun 19 22:46:28.679: INFO: Pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009267028s
Jun 19 22:46:30.682: INFO: Pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012397163s
STEP: Saw pod success
Jun 19 22:46:30.682: INFO: Pod "downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172" satisfied condition "success or failure"
Jun 19 22:46:30.684: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172 container client-container: <nil>
STEP: delete the pod
Jun 19 22:46:30.701: INFO: Waiting for pod downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172 to disappear
Jun 19 22:46:30.704: INFO: Pod downwardapi-volume-9d60dd6d-ec2a-40a4-a748-a7548e9ac172 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:46:30.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9572" for this suite.
Jun 19 22:46:36.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:46:36.794: INFO: namespace projected-9572 deletion completed in 6.087170182s

• [SLOW TEST:12.306 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:46:36.795: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 19 22:46:36.958: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 19 22:46:36.967: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 19 22:46:41.972: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 19 22:46:41.972: INFO: Creating deployment "test-rolling-update-deployment"
Jun 19 22:46:41.975: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 19 22:46:41.981: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 19 22:46:43.987: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 19 22:46:43.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581201, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:46:45.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581202, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696581201, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 19 22:46:47.992: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 19 22:46:48.002: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-7662,SelfLink:/apis/apps/v1/namespaces/deployment-7662/deployments/test-rolling-update-deployment,UID:f6b3ae39-3ed5-4abe-99d3-5798b7abb566,ResourceVersion:33910,Generation:1,CreationTimestamp:2019-06-19 22:46:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-19 22:46:42 +0000 UTC 2019-06-19 22:46:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-19 22:46:46 +0000 UTC 2019-06-19 22:46:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 19 22:46:48.004: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-7662,SelfLink:/apis/apps/v1/namespaces/deployment-7662/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:b8d92f3d-9901-43aa-83b2-1790cea08dc2,ResourceVersion:33899,Generation:1,CreationTimestamp:2019-06-19 22:46:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f6b3ae39-3ed5-4abe-99d3-5798b7abb566 0xc0029a14e7 0xc0029a14e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 19 22:46:48.004: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 19 22:46:48.004: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-7662,SelfLink:/apis/apps/v1/namespaces/deployment-7662/replicasets/test-rolling-update-controller,UID:61902501-26c9-4914-9b28-245e6a79d451,ResourceVersion:33908,Generation:2,CreationTimestamp:2019-06-19 22:46:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f6b3ae39-3ed5-4abe-99d3-5798b7abb566 0xc0029a1417 0xc0029a1418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 19 22:46:48.006: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-bw4mp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-bw4mp,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-7662,SelfLink:/api/v1/namespaces/deployment-7662/pods/test-rolling-update-deployment-79f6b9d75c-bw4mp,UID:537538d3-4496-412f-9300-882c219967c3,ResourceVersion:33898,Generation:0,CreationTimestamp:2019-06-19 22:46:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c b8d92f3d-9901-43aa-83b2-1790cea08dc2 0xc003a06d47 0xc003a06d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5vrb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5vrb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5vrb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-37287165-vmss000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a06db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a06dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:46:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:46:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:46:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-19 22:46:42 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.65,PodIP:10.240.0.87,StartTime:2019-06-19 22:46:42 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-19 22:46:45 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3ebac4eed2e5e212925053836dab789b949384932225d85f8a1a22ad40f98b28}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:46:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7662" for this suite.
Jun 19 22:46:54.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:46:54.110: INFO: namespace deployment-7662 deletion completed in 6.098428717s

• [SLOW TEST:17.316 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:46:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5a453a60-f4a3-449d-81eb-b801e24aeef4
STEP: Creating a pod to test consume secrets
Jun 19 22:46:54.276: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1" in namespace "projected-7986" to be "success or failure"
Jun 19 22:46:54.279: INFO: Pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71386ms
Jun 19 22:46:56.283: INFO: Pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006136291s
Jun 19 22:46:58.286: INFO: Pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009214327s
Jun 19 22:47:00.290: INFO: Pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013187149s
STEP: Saw pod success
Jun 19 22:47:00.290: INFO: Pod "pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1" satisfied condition "success or failure"
Jun 19 22:47:00.292: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000001 pod pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 19 22:47:00.316: INFO: Waiting for pod pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1 to disappear
Jun 19 22:47:00.319: INFO: Pod pod-projected-secrets-5b0b4e5e-7826-4765-8f99-05db90afaff1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:47:00.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7986" for this suite.
Jun 19 22:47:06.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:47:06.409: INFO: namespace projected-7986 deletion completed in 6.085866998s

• [SLOW TEST:12.296 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:47:06.409: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-3035
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3035
STEP: Deleting pre-stop pod
Jun 19 22:47:25.608: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:47:25.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3035" for this suite.
Jun 19 22:48:03.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:48:03.712: INFO: namespace prestop-3035 deletion completed in 38.087608234s

• [SLOW TEST:57.302 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:48:03.712: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-22
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 19 22:48:03.870: INFO: Waiting up to 5m0s for pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04" in namespace "emptydir-22" to be "success or failure"
Jun 19 22:48:03.903: INFO: Pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04": Phase="Pending", Reason="", readiness=false. Elapsed: 33.352417ms
Jun 19 22:48:05.906: INFO: Pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036152555s
Jun 19 22:48:07.909: INFO: Pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039395987s
Jun 19 22:48:09.912: INFO: Pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042716717s
STEP: Saw pod success
Jun 19 22:48:09.912: INFO: Pod "pod-426e1fa0-7952-4538-99d2-abf2d4d52d04" satisfied condition "success or failure"
Jun 19 22:48:09.915: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000002 pod pod-426e1fa0-7952-4538-99d2-abf2d4d52d04 container test-container: <nil>
STEP: delete the pod
Jun 19 22:48:09.937: INFO: Waiting for pod pod-426e1fa0-7952-4538-99d2-abf2d4d52d04 to disappear
Jun 19 22:48:09.939: INFO: Pod pod-426e1fa0-7952-4538-99d2-abf2d4d52d04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:48:09.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-22" for this suite.
Jun 19 22:48:15.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:48:16.031: INFO: namespace emptydir-22 deletion completed in 6.089335843s

• [SLOW TEST:12.320 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:48:16.035: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1179
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 19 22:48:16.199: INFO: Waiting up to 5m0s for pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde" in namespace "emptydir-1179" to be "success or failure"
Jun 19 22:48:16.203: INFO: Pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15874ms
Jun 19 22:48:18.206: INFO: Pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007432471s
Jun 19 22:48:20.212: INFO: Pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013786257s
Jun 19 22:48:22.216: INFO: Pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017450882s
STEP: Saw pod success
Jun 19 22:48:22.216: INFO: Pod "pod-608422ca-563f-4203-9d08-fe1c444d1fde" satisfied condition "success or failure"
Jun 19 22:48:22.219: INFO: Trying to get logs from node k8s-pool1-37287165-vmss000000 pod pod-608422ca-563f-4203-9d08-fe1c444d1fde container test-container: <nil>
STEP: delete the pod
Jun 19 22:48:22.247: INFO: Waiting for pod pod-608422ca-563f-4203-9d08-fe1c444d1fde to disappear
Jun 19 22:48:22.250: INFO: Pod pod-608422ca-563f-4203-9d08-fe1c444d1fde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:48:22.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1179" for this suite.
Jun 19 22:48:28.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:48:28.340: INFO: namespace emptydir-1179 deletion completed in 6.086518284s

• [SLOW TEST:12.306 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 19 22:48:28.341: INFO: >>> kubeConfig: /tmp/kubeconfig-497557601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-d620b6f0-d9c1-47d1-98a0-c425218d7518
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 19 22:48:28.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8702" for this suite.
Jun 19 22:48:34.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 19 22:48:34.590: INFO: namespace secrets-8702 deletion completed in 6.089980933s

• [SLOW TEST:6.249 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSJun 19 22:48:34.590: INFO: Running AfterSuite actions on all nodes
Jun 19 22:48:34.590: INFO: Running AfterSuite actions on node 1
Jun 19 22:48:34.590: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 6708.619 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h51m50.366062855s
Test Suite Passed
