I1114 23:04:21.854354      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-687135821
I1114 23:04:21.855285      16 e2e.go:241] Starting e2e run "b98ff6b4-fcf2-438e-83e1-d7aabb79e69e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1573772659 - Will randomize all specs
Will run 215 of 4413 specs

Nov 14 23:04:22.422: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:04:22.428: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 14 23:04:22.490: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 14 23:04:22.552: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 14 23:04:22.552: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Nov 14 23:04:22.552: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 14 23:04:22.569: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 14 23:04:22.570: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Nov 14 23:04:22.570: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'magnum-auto-healer' (0 seconds elapsed)
Nov 14 23:04:22.570: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Nov 14 23:04:22.570: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Nov 14 23:04:22.570: INFO: e2e test version: v1.15.2
Nov 14 23:04:22.573: INFO: kube-apiserver version: v1.15.2
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:04:22.581: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
Nov 14 23:04:22.641: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Nov 14 23:04:22.661: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-63e1fbd2-7bd0-4129-9b9f-28def43b6b65
STEP: Creating a pod to test consume secrets
Nov 14 23:04:22.856: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff" in namespace "projected-6660" to be "success or failure"
Nov 14 23:04:22.868: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.695896ms
Nov 14 23:04:24.872: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015450274s
Nov 14 23:04:26.878: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021563953s
Nov 14 23:04:28.890: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033906662s
Nov 14 23:04:30.896: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040116182s
Nov 14 23:04:32.906: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049869223s
Nov 14 23:04:34.912: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.055591838s
STEP: Saw pod success
Nov 14 23:04:34.912: INFO: Pod "pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff" satisfied condition "success or failure"
Nov 14 23:04:34.915: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:04:34.970: INFO: Waiting for pod pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff to disappear
Nov 14 23:04:34.979: INFO: Pod pod-projected-secrets-dafdfc31-726a-484d-8023-3d3c26ef37ff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:04:34.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6660" for this suite.
Nov 14 23:04:41.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:04:41.289: INFO: namespace projected-6660 deletion completed in 6.296932068s

• [SLOW TEST:18.709 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:04:41.298: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:04:41.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15" in namespace "downward-api-1764" to be "success or failure"
Nov 14 23:04:41.483: INFO: Pod "downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15": Phase="Pending", Reason="", readiness=false. Elapsed: 7.549077ms
Nov 14 23:04:43.491: INFO: Pod "downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014667885s
Nov 14 23:04:45.499: INFO: Pod "downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023402353s
STEP: Saw pod success
Nov 14 23:04:45.500: INFO: Pod "downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15" satisfied condition "success or failure"
Nov 14 23:04:45.506: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15 container client-container: <nil>
STEP: delete the pod
Nov 14 23:04:45.545: INFO: Waiting for pod downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15 to disappear
Nov 14 23:04:45.549: INFO: Pod downwardapi-volume-11b6a5a7-b107-4e11-b145-8e6c71ad3c15 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:04:45.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1764" for this suite.
Nov 14 23:04:51.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:04:51.749: INFO: namespace downward-api-1764 deletion completed in 6.188513671s

• [SLOW TEST:10.452 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:04:51.756: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-e98184ff-c368-47d2-9f0c-7ec3a2fbbccc
STEP: Creating a pod to test consume configMaps
Nov 14 23:04:52.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f" in namespace "configmap-2476" to be "success or failure"
Nov 14 23:04:52.024: INFO: Pod "pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.962724ms
Nov 14 23:04:54.028: INFO: Pod "pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023495819s
Nov 14 23:04:56.045: INFO: Pod "pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040086283s
STEP: Saw pod success
Nov 14 23:04:56.045: INFO: Pod "pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f" satisfied condition "success or failure"
Nov 14 23:04:56.047: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f container configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:04:56.075: INFO: Waiting for pod pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f to disappear
Nov 14 23:04:56.081: INFO: Pod pod-configmaps-78f2c682-9426-4c21-a8b4-61f7117a9f0f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:04:56.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2476" for this suite.
Nov 14 23:05:02.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:05:02.244: INFO: namespace configmap-2476 deletion completed in 6.157932334s

• [SLOW TEST:10.489 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:05:02.246: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:05:02.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043" in namespace "downward-api-2642" to be "success or failure"
Nov 14 23:05:02.488: INFO: Pod "downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043": Phase="Pending", Reason="", readiness=false. Elapsed: 8.555231ms
Nov 14 23:05:04.511: INFO: Pod "downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043": Phase="Running", Reason="", readiness=true. Elapsed: 2.032040196s
Nov 14 23:05:06.517: INFO: Pod "downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037742596s
STEP: Saw pod success
Nov 14 23:05:06.517: INFO: Pod "downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043" satisfied condition "success or failure"
Nov 14 23:05:06.521: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043 container client-container: <nil>
STEP: delete the pod
Nov 14 23:05:06.558: INFO: Waiting for pod downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043 to disappear
Nov 14 23:05:06.565: INFO: Pod downwardapi-volume-6450ac87-8dd8-4ae6-a5d9-26ac1859c043 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:05:06.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2642" for this suite.
Nov 14 23:05:12.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:05:12.789: INFO: namespace downward-api-2642 deletion completed in 6.208778692s

• [SLOW TEST:10.544 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:05:12.809: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1321
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 14 23:05:12.979: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 14 23:05:55.128: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1321 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:05:55.129: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:05:56.380: INFO: Found all expected endpoints: [netserver-0]
Nov 14 23:05:56.384: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1321 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:05:56.384: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:05:57.589: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:05:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1321" for this suite.
Nov 14 23:06:19.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:06:19.804: INFO: namespace pod-network-test-1321 deletion completed in 22.208061944s

• [SLOW TEST:66.996 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:06:19.813: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 14 23:06:19.993: INFO: Waiting up to 5m0s for pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a" in namespace "emptydir-4702" to be "success or failure"
Nov 14 23:06:20.002: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.824004ms
Nov 14 23:06:22.005: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254233s
Nov 14 23:06:24.014: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021163067s
Nov 14 23:06:26.019: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025778223s
Nov 14 23:06:28.025: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031806271s
Nov 14 23:06:30.029: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036163389s
Nov 14 23:06:32.034: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041347877s
Nov 14 23:06:34.052: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.059023253s
STEP: Saw pod success
Nov 14 23:06:34.052: INFO: Pod "pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a" satisfied condition "success or failure"
Nov 14 23:06:34.070: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a container test-container: <nil>
STEP: delete the pod
Nov 14 23:06:34.114: INFO: Waiting for pod pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a to disappear
Nov 14 23:06:34.123: INFO: Pod pod-9c5d46ae-0e02-41c3-af33-33c89d6d017a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:06:34.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4702" for this suite.
Nov 14 23:06:40.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:06:40.329: INFO: namespace emptydir-4702 deletion completed in 6.200080514s

• [SLOW TEST:20.518 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:06:40.340: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 14 23:06:40.544: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:06:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2765" for this suite.
Nov 14 23:06:59.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:06:59.808: INFO: namespace init-container-2765 deletion completed in 6.287300245s

• [SLOW TEST:19.469 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:06:59.811: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Nov 14 23:07:00.036: INFO: Waiting up to 5m0s for pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff" in namespace "containers-2036" to be "success or failure"
Nov 14 23:07:00.043: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.683999ms
Nov 14 23:07:02.057: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020502513s
Nov 14 23:07:04.061: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024795115s
Nov 14 23:07:06.068: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031328823s
Nov 14 23:07:08.073: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036907602s
Nov 14 23:07:10.077: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.041115938s
Nov 14 23:07:12.085: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.049010792s
STEP: Saw pod success
Nov 14 23:07:12.099: INFO: Pod "client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff" satisfied condition "success or failure"
Nov 14 23:07:12.105: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff container test-container: <nil>
STEP: delete the pod
Nov 14 23:07:12.150: INFO: Waiting for pod client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff to disappear
Nov 14 23:07:12.156: INFO: Pod client-containers-ce3bc064-36cb-4643-addd-9a0a1fd0caff no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:07:12.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2036" for this suite.
Nov 14 23:07:18.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:07:18.426: INFO: namespace containers-2036 deletion completed in 6.261236859s

• [SLOW TEST:18.616 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:07:18.428: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Nov 14 23:07:18.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 --namespace=kubectl-1716 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov 14 23:07:30.903: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Nov 14 23:07:30.903: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:07:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1716" for this suite.
Nov 14 23:07:39.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:07:39.480: INFO: namespace kubectl-1716 deletion completed in 6.352786756s

• [SLOW TEST:21.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:07:39.481: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:07:39.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a" in namespace "downward-api-672" to be "success or failure"
Nov 14 23:07:39.687: INFO: Pod "downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.405989ms
Nov 14 23:07:41.693: INFO: Pod "downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017292318s
Nov 14 23:07:43.699: INFO: Pod "downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02284091s
STEP: Saw pod success
Nov 14 23:07:43.699: INFO: Pod "downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a" satisfied condition "success or failure"
Nov 14 23:07:43.702: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a container client-container: <nil>
STEP: delete the pod
Nov 14 23:07:43.734: INFO: Waiting for pod downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a to disappear
Nov 14 23:07:43.741: INFO: Pod downwardapi-volume-75ef0be0-07af-47bd-9333-a403cca4179a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:07:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-672" for this suite.
Nov 14 23:07:49.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:07:50.153: INFO: namespace downward-api-672 deletion completed in 6.40470322s

• [SLOW TEST:10.671 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:07:50.155: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 14 23:07:50.403: INFO: Waiting up to 5m0s for pod "pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9" in namespace "emptydir-9173" to be "success or failure"
Nov 14 23:07:50.415: INFO: Pod "pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779462ms
Nov 14 23:07:52.420: INFO: Pod "pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016983388s
Nov 14 23:07:54.425: INFO: Pod "pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021663368s
STEP: Saw pod success
Nov 14 23:07:54.425: INFO: Pod "pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9" satisfied condition "success or failure"
Nov 14 23:07:54.428: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9 container test-container: <nil>
STEP: delete the pod
Nov 14 23:07:54.480: INFO: Waiting for pod pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9 to disappear
Nov 14 23:07:54.486: INFO: Pod pod-5e552a2f-abd1-4d44-9cc4-a8fcb44751f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:07:54.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9173" for this suite.
Nov 14 23:08:00.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:08:00.702: INFO: namespace emptydir-9173 deletion completed in 6.212295542s

• [SLOW TEST:10.548 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:08:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8009
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 14 23:08:00.885: INFO: Waiting up to 5m0s for pod "pod-570382fa-1a70-469f-843d-fdcb7279286a" in namespace "emptydir-8009" to be "success or failure"
Nov 14 23:08:00.896: INFO: Pod "pod-570382fa-1a70-469f-843d-fdcb7279286a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.941173ms
Nov 14 23:08:02.901: INFO: Pod "pod-570382fa-1a70-469f-843d-fdcb7279286a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015157543s
Nov 14 23:08:04.906: INFO: Pod "pod-570382fa-1a70-469f-843d-fdcb7279286a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020718838s
STEP: Saw pod success
Nov 14 23:08:04.906: INFO: Pod "pod-570382fa-1a70-469f-843d-fdcb7279286a" satisfied condition "success or failure"
Nov 14 23:08:04.909: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-570382fa-1a70-469f-843d-fdcb7279286a container test-container: <nil>
STEP: delete the pod
Nov 14 23:08:04.968: INFO: Waiting for pod pod-570382fa-1a70-469f-843d-fdcb7279286a to disappear
Nov 14 23:08:04.972: INFO: Pod pod-570382fa-1a70-469f-843d-fdcb7279286a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:08:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8009" for this suite.
Nov 14 23:08:10.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:08:11.204: INFO: namespace emptydir-8009 deletion completed in 6.226321555s

• [SLOW TEST:10.499 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:08:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-b74441b1-f20d-4f6d-b871-4a37ed3d7078
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:08:11.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5135" for this suite.
Nov 14 23:08:17.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:08:17.561: INFO: namespace configmap-5135 deletion completed in 6.169583102s

• [SLOW TEST:6.352 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:08:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:08:17.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed" in namespace "projected-7984" to be "success or failure"
Nov 14 23:08:17.811: INFO: Pod "downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.057452ms
Nov 14 23:08:19.817: INFO: Pod "downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018507426s
Nov 14 23:08:21.824: INFO: Pod "downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025162777s
STEP: Saw pod success
Nov 14 23:08:21.824: INFO: Pod "downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed" satisfied condition "success or failure"
Nov 14 23:08:21.831: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed container client-container: <nil>
STEP: delete the pod
Nov 14 23:08:21.879: INFO: Waiting for pod downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed to disappear
Nov 14 23:08:21.888: INFO: Pod downwardapi-volume-b93cf1c5-532c-49e1-96b7-7a179c7672ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:08:21.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7984" for this suite.
Nov 14 23:08:27.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:08:28.097: INFO: namespace projected-7984 deletion completed in 6.200973096s

• [SLOW TEST:10.534 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:08:28.099: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-05dcb460-3ab0-41e6-9ff9-ca8e8ffdf7f5
STEP: Creating a pod to test consume configMaps
Nov 14 23:08:28.274: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41" in namespace "projected-5775" to be "success or failure"
Nov 14 23:08:28.405: INFO: Pod "pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41": Phase="Pending", Reason="", readiness=false. Elapsed: 130.938349ms
Nov 14 23:08:30.410: INFO: Pod "pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136224515s
Nov 14 23:08:32.415: INFO: Pod "pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.140613025s
STEP: Saw pod success
Nov 14 23:08:32.415: INFO: Pod "pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41" satisfied condition "success or failure"
Nov 14 23:08:32.419: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:08:32.461: INFO: Waiting for pod pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41 to disappear
Nov 14 23:08:32.467: INFO: Pod pod-projected-configmaps-99968edf-bc26-471f-bb63-ed43dc885d41 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:08:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5775" for this suite.
Nov 14 23:08:38.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:08:38.670: INFO: namespace projected-5775 deletion completed in 6.197948254s

• [SLOW TEST:10.571 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:08:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 14 23:08:51.752: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:51.752: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:51.960: INFO: Exec stderr: ""
Nov 14 23:08:51.960: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:51.960: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:52.134: INFO: Exec stderr: ""
Nov 14 23:08:52.134: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:52.134: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:52.303: INFO: Exec stderr: ""
Nov 14 23:08:52.303: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:52.303: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:52.488: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 14 23:08:52.488: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:52.488: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:52.666: INFO: Exec stderr: ""
Nov 14 23:08:52.666: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:52.866: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 14 23:08:52.866: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:52.866: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:53.066: INFO: Exec stderr: ""
Nov 14 23:08:53.066: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:53.066: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:53.240: INFO: Exec stderr: ""
Nov 14 23:08:53.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:53.240: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:53.390: INFO: Exec stderr: ""
Nov 14 23:08:53.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6099 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:08:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:08:53.576: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:08:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6099" for this suite.
Nov 14 23:09:39.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:09:39.816: INFO: namespace e2e-kubelet-etc-hosts-6099 deletion completed in 46.232537945s

• [SLOW TEST:61.141 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:09:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:09:40.023: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 14 23:09:40.039: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 14 23:09:45.045: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 14 23:09:53.057: INFO: Creating deployment "test-rolling-update-deployment"
Nov 14 23:09:53.066: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 14 23:09:53.074: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 14 23:09:55.085: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 14 23:09:55.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:09:57.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:09:59.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:10:01.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:10:03.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:10:05.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709369793, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:10:07.093: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 14 23:10:07.107: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5512,SelfLink:/apis/apps/v1/namespaces/deployment-5512/deployments/test-rolling-update-deployment,UID:43b6c260-505d-4338-95cf-924730ba35bc,ResourceVersion:4024,Generation:1,CreationTimestamp:2019-11-14 23:09:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-14 23:09:53 +0000 UTC 2019-11-14 23:09:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-14 23:10:05 +0000 UTC 2019-11-14 23:09:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 14 23:10:07.112: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5512,SelfLink:/apis/apps/v1/namespaces/deployment-5512/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:8abc5706-23bd-4cb8-8f1c-a79ba931248b,ResourceVersion:4013,Generation:1,CreationTimestamp:2019-11-14 23:09:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 43b6c260-505d-4338-95cf-924730ba35bc 0xc002351ed7 0xc002351ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 14 23:10:07.112: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 14 23:10:07.112: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5512,SelfLink:/apis/apps/v1/namespaces/deployment-5512/replicasets/test-rolling-update-controller,UID:c476840e-9115-42a8-b29d-47a33107a30e,ResourceVersion:4023,Generation:2,CreationTimestamp:2019-11-14 23:09:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 43b6c260-505d-4338-95cf-924730ba35bc 0xc002351dff 0xc002351e10}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 14 23:10:07.116: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-n749v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-n749v,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5512,SelfLink:/api/v1/namespaces/deployment-5512/pods/test-rolling-update-deployment-79f6b9d75c-n749v,UID:f65d9744-00e6-4aeb-9f1b-eb2b0da6f83a,ResourceVersion:4012,Generation:0,CreationTimestamp:2019-11-14 23:09:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.22/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 8abc5706-23bd-4cb8-8f1c-a79ba931248b 0xc0027fe7c7 0xc0027fe7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xk4fw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xk4fw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xk4fw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027fe830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027fe850}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-14 23:09:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-14 23:10:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-14 23:10:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-14 23:09:53 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.22,StartTime:2019-11-14 23:09:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-14 23:10:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://bb9195133a44e16b7d4fe24cd73d3009888641a3fba89b1ac4119645b2854c24}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:10:07.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5512" for this suite.
Nov 14 23:10:15.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:10:15.363: INFO: namespace deployment-5512 deletion completed in 8.235987149s

• [SLOW TEST:35.546 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:10:15.366: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Nov 14 23:10:25.599: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:10:25.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1114 23:10:25.598986      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2310" for this suite.
Nov 14 23:10:31.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:10:31.790: INFO: namespace gc-2310 deletion completed in 6.182060471s

• [SLOW TEST:16.425 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:10:31.792: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Nov 14 23:10:32.171: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-687135821 proxy --unix-socket=/tmp/kubectl-proxy-unix514962760/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:10:32.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7137" for this suite.
Nov 14 23:10:38.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:10:38.552: INFO: namespace kubectl-7137 deletion completed in 6.279803447s

• [SLOW TEST:6.759 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:10:38.556: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-860.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-860.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-860.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-860.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:11:46.765: INFO: Unable to read wheezy_udp@PodARecord from pod dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64: the server could not find the requested resource (get pods dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64)
Nov 14 23:11:46.769: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64: the server could not find the requested resource (get pods dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64)
Nov 14 23:11:46.781: INFO: Unable to read jessie_udp@PodARecord from pod dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64: the server could not find the requested resource (get pods dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64)
Nov 14 23:11:46.786: INFO: Unable to read jessie_tcp@PodARecord from pod dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64: the server could not find the requested resource (get pods dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64)
Nov 14 23:11:46.786: INFO: Lookups using dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Nov 14 23:11:51.823: INFO: DNS probes using dns-860/dns-test-6e2d44a4-b3df-486d-80fb-05e3829bde64 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:11:51.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-860" for this suite.
Nov 14 23:11:57.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:11:58.175: INFO: namespace dns-860 deletion completed in 6.269890104s

• [SLOW TEST:79.619 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:11:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f
Nov 14 23:11:58.439: INFO: Pod name my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f: Found 0 pods out of 1
Nov 14 23:12:03.451: INFO: Pod name my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f: Found 1 pods out of 1
Nov 14 23:12:03.451: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f" are running
Nov 14 23:12:11.461: INFO: Pod "my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f-sg4kp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-14 23:11:58 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-14 23:11:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-14 23:11:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-14 23:11:58 +0000 UTC Reason: Message:}])
Nov 14 23:12:11.462: INFO: Trying to dial the pod
Nov 14 23:12:16.478: INFO: Controller my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f: Got expected result from replica 1 [my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f-sg4kp]: "my-hostname-basic-d2989b7c-d1bb-4626-bb0c-fc67b0a6c30f-sg4kp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:12:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1201" for this suite.
Nov 14 23:12:22.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:12:22.653: INFO: namespace replication-controller-1201 deletion completed in 6.168540893s

• [SLOW TEST:24.475 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:12:22.657: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 14 23:12:22.853: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3758,SelfLink:/api/v1/namespaces/watch-3758/configmaps/e2e-watch-test-resource-version,UID:99993aca-fbfd-41b7-9f8d-3521129434f7,ResourceVersion:4504,Generation:0,CreationTimestamp:2019-11-14 23:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 14 23:12:22.853: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3758,SelfLink:/api/v1/namespaces/watch-3758/configmaps/e2e-watch-test-resource-version,UID:99993aca-fbfd-41b7-9f8d-3521129434f7,ResourceVersion:4505,Generation:0,CreationTimestamp:2019-11-14 23:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:12:22.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3758" for this suite.
Nov 14 23:12:28.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:12:29.074: INFO: namespace watch-3758 deletion completed in 6.21480175s

• [SLOW TEST:6.417 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:12:29.075: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cc5b4eb7-3849-40fd-9f6e-8717f891adb6
STEP: Creating a pod to test consume configMaps
Nov 14 23:12:29.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132" in namespace "configmap-6812" to be "success or failure"
Nov 14 23:12:29.275: INFO: Pod "pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132": Phase="Pending", Reason="", readiness=false. Elapsed: 17.052876ms
Nov 14 23:12:31.287: INFO: Pod "pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029177295s
Nov 14 23:12:33.291: INFO: Pod "pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033612649s
STEP: Saw pod success
Nov 14 23:12:33.292: INFO: Pod "pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132" satisfied condition "success or failure"
Nov 14 23:12:33.294: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:12:33.369: INFO: Waiting for pod pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132 to disappear
Nov 14 23:12:33.385: INFO: Pod pod-configmaps-da7bf180-fd23-48b1-a7b9-89790ad4f132 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:12:33.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6812" for this suite.
Nov 14 23:12:39.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:12:39.572: INFO: namespace configmap-6812 deletion completed in 6.180857811s

• [SLOW TEST:10.497 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:12:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:12:39.774: INFO: (0) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 12.336379ms)
Nov 14 23:12:39.782: INFO: (1) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.952569ms)
Nov 14 23:12:39.788: INFO: (2) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.87992ms)
Nov 14 23:12:39.795: INFO: (3) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.309861ms)
Nov 14 23:12:39.802: INFO: (4) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.524175ms)
Nov 14 23:12:39.807: INFO: (5) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.59688ms)
Nov 14 23:12:39.811: INFO: (6) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.210348ms)
Nov 14 23:12:39.816: INFO: (7) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.355455ms)
Nov 14 23:12:39.821: INFO: (8) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.669788ms)
Nov 14 23:12:39.827: INFO: (9) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.7963ms)
Nov 14 23:12:39.833: INFO: (10) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.437602ms)
Nov 14 23:12:39.839: INFO: (11) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.368013ms)
Nov 14 23:12:39.846: INFO: (12) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.344032ms)
Nov 14 23:12:39.852: INFO: (13) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.079078ms)
Nov 14 23:12:39.858: INFO: (14) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.353391ms)
Nov 14 23:12:39.863: INFO: (15) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.689219ms)
Nov 14 23:12:39.868: INFO: (16) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.639156ms)
Nov 14 23:12:39.872: INFO: (17) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.737072ms)
Nov 14 23:12:39.877: INFO: (18) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.187433ms)
Nov 14 23:12:39.882: INFO: (19) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.253961ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:12:39.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9548" for this suite.
Nov 14 23:12:45.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:12:46.124: INFO: namespace proxy-9548 deletion completed in 6.237825311s

• [SLOW TEST:6.551 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:12:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Nov 14 23:12:46.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-2489'
Nov 14 23:12:47.944: INFO: stderr: ""
Nov 14 23:12:47.944: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 14 23:12:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2489'
Nov 14 23:12:48.100: INFO: stderr: ""
Nov 14 23:12:48.100: INFO: stdout: "update-demo-nautilus-pwcsd update-demo-nautilus-qnp26 "
Nov 14 23:12:48.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-pwcsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:12:48.255: INFO: stderr: ""
Nov 14 23:12:48.255: INFO: stdout: ""
Nov 14 23:12:48.255: INFO: update-demo-nautilus-pwcsd is created but not running
Nov 14 23:12:53.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2489'
Nov 14 23:12:53.423: INFO: stderr: ""
Nov 14 23:12:53.423: INFO: stdout: "update-demo-nautilus-pwcsd update-demo-nautilus-qnp26 "
Nov 14 23:12:53.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-pwcsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:12:53.571: INFO: stderr: ""
Nov 14 23:12:53.571: INFO: stdout: ""
Nov 14 23:12:53.571: INFO: update-demo-nautilus-pwcsd is created but not running
Nov 14 23:12:58.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2489'
Nov 14 23:12:58.730: INFO: stderr: ""
Nov 14 23:12:58.730: INFO: stdout: "update-demo-nautilus-pwcsd update-demo-nautilus-qnp26 "
Nov 14 23:12:58.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-pwcsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:12:58.883: INFO: stderr: ""
Nov 14 23:12:58.883: INFO: stdout: ""
Nov 14 23:12:58.884: INFO: update-demo-nautilus-pwcsd is created but not running
Nov 14 23:13:03.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2489'
Nov 14 23:13:04.025: INFO: stderr: ""
Nov 14 23:13:04.025: INFO: stdout: "update-demo-nautilus-pwcsd update-demo-nautilus-qnp26 "
Nov 14 23:13:04.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-pwcsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:04.168: INFO: stderr: ""
Nov 14 23:13:04.168: INFO: stdout: "true"
Nov 14 23:13:04.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-pwcsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:04.310: INFO: stderr: ""
Nov 14 23:13:04.310: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:13:04.310: INFO: validating pod update-demo-nautilus-pwcsd
Nov 14 23:13:04.320: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:13:04.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:13:04.321: INFO: update-demo-nautilus-pwcsd is verified up and running
Nov 14 23:13:04.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-qnp26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:04.464: INFO: stderr: ""
Nov 14 23:13:04.464: INFO: stdout: "true"
Nov 14 23:13:04.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-qnp26 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:04.603: INFO: stderr: ""
Nov 14 23:13:04.603: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:13:04.603: INFO: validating pod update-demo-nautilus-qnp26
Nov 14 23:13:04.612: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:13:04.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:13:04.612: INFO: update-demo-nautilus-qnp26 is verified up and running
STEP: rolling-update to new replication controller
Nov 14 23:13:04.618: INFO: scanned /root for discovery docs: <nil>
Nov 14 23:13:04.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2489'
Nov 14 23:13:47.752: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 14 23:13:47.752: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 14 23:13:47.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2489'
Nov 14 23:13:47.907: INFO: stderr: ""
Nov 14 23:13:47.908: INFO: stdout: "update-demo-kitten-twgqj update-demo-kitten-xr8lb "
Nov 14 23:13:47.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-kitten-twgqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:48.044: INFO: stderr: ""
Nov 14 23:13:48.044: INFO: stdout: "true"
Nov 14 23:13:48.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-kitten-twgqj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:48.197: INFO: stderr: ""
Nov 14 23:13:48.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 14 23:13:48.198: INFO: validating pod update-demo-kitten-twgqj
Nov 14 23:13:48.210: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 14 23:13:48.211: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 14 23:13:48.211: INFO: update-demo-kitten-twgqj is verified up and running
Nov 14 23:13:48.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-kitten-xr8lb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:48.350: INFO: stderr: ""
Nov 14 23:13:48.350: INFO: stdout: "true"
Nov 14 23:13:48.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-kitten-xr8lb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2489'
Nov 14 23:13:48.487: INFO: stderr: ""
Nov 14 23:13:48.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 14 23:13:48.487: INFO: validating pod update-demo-kitten-xr8lb
Nov 14 23:13:48.500: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 14 23:13:48.500: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 14 23:13:48.500: INFO: update-demo-kitten-xr8lb is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:13:48.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2489" for this suite.
Nov 14 23:14:10.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:14:10.759: INFO: namespace kubectl-2489 deletion completed in 22.234075621s

• [SLOW TEST:84.632 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:14:10.767: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:14:11.014: INFO: Create a RollingUpdate DaemonSet
Nov 14 23:14:11.020: INFO: Check that daemon pods launch on every node of the cluster
Nov 14 23:14:11.032: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:11.038: INFO: Number of nodes with available pods: 0
Nov 14 23:14:11.038: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:14:12.043: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:12.046: INFO: Number of nodes with available pods: 0
Nov 14 23:14:12.046: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:14:13.044: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:13.048: INFO: Number of nodes with available pods: 1
Nov 14 23:14:13.048: INFO: Node k8s-100-3imo44lif6er-minion-1 is running more than one daemon pod
Nov 14 23:14:14.061: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:14.072: INFO: Number of nodes with available pods: 2
Nov 14 23:14:14.073: INFO: Number of running nodes: 2, number of available pods: 2
Nov 14 23:14:14.073: INFO: Update the DaemonSet to trigger a rollout
Nov 14 23:14:14.086: INFO: Updating DaemonSet daemon-set
Nov 14 23:14:19.124: INFO: Roll back the DaemonSet before rollout is complete
Nov 14 23:14:19.133: INFO: Updating DaemonSet daemon-set
Nov 14 23:14:19.133: INFO: Make sure DaemonSet rollback is complete
Nov 14 23:14:19.147: INFO: Wrong image for pod: daemon-set-rsnvr. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov 14 23:14:19.147: INFO: Pod daemon-set-rsnvr is not available
Nov 14 23:14:19.161: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:20.166: INFO: Wrong image for pod: daemon-set-rsnvr. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov 14 23:14:20.166: INFO: Pod daemon-set-rsnvr is not available
Nov 14 23:14:20.170: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:14:21.166: INFO: Pod daemon-set-qlzxx is not available
Nov 14 23:14:21.172: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4364, will wait for the garbage collector to delete the pods
Nov 14 23:14:21.257: INFO: Deleting DaemonSet.extensions daemon-set took: 19.557765ms
Nov 14 23:14:22.358: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100570605s
Nov 14 23:15:26.266: INFO: Number of nodes with available pods: 0
Nov 14 23:15:26.266: INFO: Number of running nodes: 0, number of available pods: 0
Nov 14 23:15:26.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4364/daemonsets","resourceVersion":"5180"},"items":null}

Nov 14 23:15:26.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4364/pods","resourceVersion":"5180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:15:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4364" for this suite.
Nov 14 23:15:32.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:15:32.547: INFO: namespace daemonsets-4364 deletion completed in 6.240439296s

• [SLOW TEST:81.781 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:15:32.552: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-j7qxj in namespace proxy-7317
I1114 23:15:32.749180      16 runners.go:180] Created replication controller with name: proxy-service-j7qxj, namespace: proxy-7317, replica count: 1
I1114 23:15:33.802696      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:34.803069      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:35.803397      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:36.803730      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:37.804006      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:38.804237      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:39.804787      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:40.805055      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:41.805386      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:42.805751      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:43.806543      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:44.806728      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:45.806967      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:46.807260      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:47.807710      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:48.808024      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:49.808458      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:50.808967      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:51.809470      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:52.809946      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:53.810237      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:54.810721      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:55.811139      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:56.811588      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:57.811953      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:58.812246      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:15:59.812811      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:00.813118      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:01.813486      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:02.814051      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:03.814349      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:04.814613      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:05.814873      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:06.815123      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:07.815518      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:08.815803      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:09.816058      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:10.816382      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:11.816754      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:12.817089      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:13.817409      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:14.817661      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:15.817883      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:16.818153      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:17.818832      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:18.819522      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:19.820069      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:20.820441      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:21.820917      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:22.821224      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:23.821489      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:24.821770      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:25.822023      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:26.822290      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:27.822580      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:28.822869      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:29.823164      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:30.823549      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:31.823814      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:32.824069      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1114 23:16:33.824373      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1114 23:16:34.824801      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1114 23:16:35.825188      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1114 23:16:36.825549      16 runners.go:180] proxy-service-j7qxj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 14 23:16:36.833: INFO: setup took 1m4.12621636s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 14 23:16:36.897: INFO: (0) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 62.341143ms)
Nov 14 23:16:36.900: INFO: (0) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 66.298084ms)
Nov 14 23:16:36.900: INFO: (0) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 66.000401ms)
Nov 14 23:16:36.900: INFO: (0) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 66.615834ms)
Nov 14 23:16:36.901: INFO: (0) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 66.280408ms)
Nov 14 23:16:36.907: INFO: (0) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 73.266347ms)
Nov 14 23:16:36.907: INFO: (0) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 73.291954ms)
Nov 14 23:16:36.909: INFO: (0) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 74.858203ms)
Nov 14 23:16:36.909: INFO: (0) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 75.082178ms)
Nov 14 23:16:36.909: INFO: (0) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 74.87688ms)
Nov 14 23:16:36.909: INFO: (0) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 75.103792ms)
Nov 14 23:16:36.916: INFO: (0) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 82.719092ms)
Nov 14 23:16:36.918: INFO: (0) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 83.758542ms)
Nov 14 23:16:36.920: INFO: (0) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 86.059306ms)
Nov 14 23:16:36.920: INFO: (0) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 86.255185ms)
Nov 14 23:16:36.921: INFO: (0) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 86.42051ms)
Nov 14 23:16:36.926: INFO: (1) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 5.199215ms)
Nov 14 23:16:36.933: INFO: (1) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 12.365941ms)
Nov 14 23:16:36.940: INFO: (1) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 18.501466ms)
Nov 14 23:16:36.941: INFO: (1) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 19.029728ms)
Nov 14 23:16:36.941: INFO: (1) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 17.940867ms)
Nov 14 23:16:36.941: INFO: (1) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 18.698572ms)
Nov 14 23:16:36.941: INFO: (1) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 18.078004ms)
Nov 14 23:16:36.942: INFO: (1) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 20.270871ms)
Nov 14 23:16:36.942: INFO: (1) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 18.838903ms)
Nov 14 23:16:36.943: INFO: (1) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 21.200254ms)
Nov 14 23:16:36.943: INFO: (1) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 20.137068ms)
Nov 14 23:16:36.943: INFO: (1) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 19.88239ms)
Nov 14 23:16:36.943: INFO: (1) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 20.90524ms)
Nov 14 23:16:36.944: INFO: (1) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 21.644432ms)
Nov 14 23:16:36.944: INFO: (1) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 22.227203ms)
Nov 14 23:16:36.946: INFO: (1) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 22.296079ms)
Nov 14 23:16:36.963: INFO: (2) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 16.28865ms)
Nov 14 23:16:36.964: INFO: (2) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 18.322997ms)
Nov 14 23:16:36.964: INFO: (2) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 17.911352ms)
Nov 14 23:16:36.964: INFO: (2) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 18.051484ms)
Nov 14 23:16:36.965: INFO: (2) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 19.090314ms)
Nov 14 23:16:36.965: INFO: (2) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 18.697296ms)
Nov 14 23:16:36.965: INFO: (2) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 19.38367ms)
Nov 14 23:16:36.966: INFO: (2) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 19.855973ms)
Nov 14 23:16:36.966: INFO: (2) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 20.064305ms)
Nov 14 23:16:36.967: INFO: (2) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 20.888054ms)
Nov 14 23:16:36.967: INFO: (2) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 20.59087ms)
Nov 14 23:16:36.968: INFO: (2) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 21.766635ms)
Nov 14 23:16:36.969: INFO: (2) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 22.359863ms)
Nov 14 23:16:36.969: INFO: (2) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 22.630841ms)
Nov 14 23:16:36.969: INFO: (2) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 22.672726ms)
Nov 14 23:16:36.969: INFO: (2) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 22.905092ms)
Nov 14 23:16:36.983: INFO: (3) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 12.904679ms)
Nov 14 23:16:36.987: INFO: (3) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 17.397063ms)
Nov 14 23:16:36.988: INFO: (3) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 17.880115ms)
Nov 14 23:16:36.988: INFO: (3) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 17.914846ms)
Nov 14 23:16:36.988: INFO: (3) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 18.099053ms)
Nov 14 23:16:36.988: INFO: (3) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 18.109606ms)
Nov 14 23:16:36.988: INFO: (3) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 17.997136ms)
Nov 14 23:16:36.989: INFO: (3) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 19.982196ms)
Nov 14 23:16:36.991: INFO: (3) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 21.095581ms)
Nov 14 23:16:36.991: INFO: (3) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 21.463052ms)
Nov 14 23:16:36.992: INFO: (3) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 21.730108ms)
Nov 14 23:16:36.992: INFO: (3) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 22.263708ms)
Nov 14 23:16:36.993: INFO: (3) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 22.835753ms)
Nov 14 23:16:36.993: INFO: (3) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 23.162716ms)
Nov 14 23:16:36.994: INFO: (3) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 24.091139ms)
Nov 14 23:16:36.994: INFO: (3) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 24.835849ms)
Nov 14 23:16:37.009: INFO: (4) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 13.617816ms)
Nov 14 23:16:37.009: INFO: (4) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 12.700802ms)
Nov 14 23:16:37.009: INFO: (4) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.440595ms)
Nov 14 23:16:37.009: INFO: (4) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 13.683072ms)
Nov 14 23:16:37.010: INFO: (4) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.764592ms)
Nov 14 23:16:37.010: INFO: (4) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 15.927456ms)
Nov 14 23:16:37.010: INFO: (4) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 14.892135ms)
Nov 14 23:16:37.011: INFO: (4) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 16.581841ms)
Nov 14 23:16:37.011: INFO: (4) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 14.873113ms)
Nov 14 23:16:37.011: INFO: (4) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 16.360388ms)
Nov 14 23:16:37.012: INFO: (4) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 15.047425ms)
Nov 14 23:16:37.012: INFO: (4) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 15.833497ms)
Nov 14 23:16:37.015: INFO: (4) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 17.860363ms)
Nov 14 23:16:37.015: INFO: (4) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 18.017272ms)
Nov 14 23:16:37.016: INFO: (4) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 18.997607ms)
Nov 14 23:16:37.017: INFO: (4) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 20.846022ms)
Nov 14 23:16:37.025: INFO: (5) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 6.692044ms)
Nov 14 23:16:37.029: INFO: (5) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 10.135964ms)
Nov 14 23:16:37.030: INFO: (5) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 10.623672ms)
Nov 14 23:16:37.030: INFO: (5) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 11.58037ms)
Nov 14 23:16:37.031: INFO: (5) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 12.576516ms)
Nov 14 23:16:37.031: INFO: (5) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 11.577042ms)
Nov 14 23:16:37.031: INFO: (5) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 12.195336ms)
Nov 14 23:16:37.031: INFO: (5) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 13.088145ms)
Nov 14 23:16:37.034: INFO: (5) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.363203ms)
Nov 14 23:16:37.035: INFO: (5) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 16.597958ms)
Nov 14 23:16:37.035: INFO: (5) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 14.840132ms)
Nov 14 23:16:37.036: INFO: (5) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 16.01094ms)
Nov 14 23:16:37.036: INFO: (5) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 15.347479ms)
Nov 14 23:16:37.036: INFO: (5) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 17.181827ms)
Nov 14 23:16:37.036: INFO: (5) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 15.830033ms)
Nov 14 23:16:37.036: INFO: (5) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 15.74149ms)
Nov 14 23:16:37.041: INFO: (6) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 4.959853ms)
Nov 14 23:16:37.048: INFO: (6) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 10.273119ms)
Nov 14 23:16:37.048: INFO: (6) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 11.716839ms)
Nov 14 23:16:37.050: INFO: (6) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 13.185777ms)
Nov 14 23:16:37.055: INFO: (6) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 18.23568ms)
Nov 14 23:16:37.056: INFO: (6) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 18.987934ms)
Nov 14 23:16:37.056: INFO: (6) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 19.073508ms)
Nov 14 23:16:37.056: INFO: (6) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 19.245465ms)
Nov 14 23:16:37.056: INFO: (6) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 19.488885ms)
Nov 14 23:16:37.057: INFO: (6) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 19.937361ms)
Nov 14 23:16:37.057: INFO: (6) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 19.993546ms)
Nov 14 23:16:37.057: INFO: (6) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 19.849402ms)
Nov 14 23:16:37.059: INFO: (6) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 21.767884ms)
Nov 14 23:16:37.059: INFO: (6) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 21.623364ms)
Nov 14 23:16:37.059: INFO: (6) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 22.137802ms)
Nov 14 23:16:37.059: INFO: (6) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 22.081085ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 14.142367ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 14.794686ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 14.148222ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 13.998861ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.90869ms)
Nov 14 23:16:37.074: INFO: (7) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 15.085282ms)
Nov 14 23:16:37.075: INFO: (7) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 14.432169ms)
Nov 14 23:16:37.076: INFO: (7) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 15.228469ms)
Nov 14 23:16:37.079: INFO: (7) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 19.077653ms)
Nov 14 23:16:37.079: INFO: (7) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 18.724791ms)
Nov 14 23:16:37.086: INFO: (7) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 25.155863ms)
Nov 14 23:16:37.086: INFO: (7) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 25.464294ms)
Nov 14 23:16:37.087: INFO: (7) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 26.221753ms)
Nov 14 23:16:37.087: INFO: (7) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 25.700282ms)
Nov 14 23:16:37.089: INFO: (7) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 27.704722ms)
Nov 14 23:16:37.089: INFO: (7) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 27.924361ms)
Nov 14 23:16:37.101: INFO: (8) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 11.254294ms)
Nov 14 23:16:37.101: INFO: (8) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 11.590457ms)
Nov 14 23:16:37.102: INFO: (8) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 11.048023ms)
Nov 14 23:16:37.102: INFO: (8) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 11.920437ms)
Nov 14 23:16:37.103: INFO: (8) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.374627ms)
Nov 14 23:16:37.103: INFO: (8) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 13.667595ms)
Nov 14 23:16:37.104: INFO: (8) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 15.38338ms)
Nov 14 23:16:37.105: INFO: (8) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 14.802646ms)
Nov 14 23:16:37.105: INFO: (8) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.181101ms)
Nov 14 23:16:37.105: INFO: (8) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 13.575666ms)
Nov 14 23:16:37.105: INFO: (8) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 14.035251ms)
Nov 14 23:16:37.105: INFO: (8) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 14.561301ms)
Nov 14 23:16:37.107: INFO: (8) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 16.57081ms)
Nov 14 23:16:37.108: INFO: (8) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 17.50078ms)
Nov 14 23:16:37.108: INFO: (8) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 16.692663ms)
Nov 14 23:16:37.108: INFO: (8) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 16.314156ms)
Nov 14 23:16:37.117: INFO: (9) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 8.467845ms)
Nov 14 23:16:37.122: INFO: (9) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 14.175996ms)
Nov 14 23:16:37.122: INFO: (9) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 14.085018ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 13.566921ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 14.533213ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 14.475525ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 14.323329ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 15.087804ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 14.43826ms)
Nov 14 23:16:37.123: INFO: (9) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 14.043209ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 15.09622ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 15.028484ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.289657ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 15.003807ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.602834ms)
Nov 14 23:16:37.124: INFO: (9) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 14.876345ms)
Nov 14 23:16:37.135: INFO: (10) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 9.197282ms)
Nov 14 23:16:37.136: INFO: (10) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 10.038436ms)
Nov 14 23:16:37.136: INFO: (10) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 10.929525ms)
Nov 14 23:16:37.136: INFO: (10) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 10.870135ms)
Nov 14 23:16:37.138: INFO: (10) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.046381ms)
Nov 14 23:16:37.139: INFO: (10) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 13.322035ms)
Nov 14 23:16:37.139: INFO: (10) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 12.924217ms)
Nov 14 23:16:37.140: INFO: (10) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 14.33367ms)
Nov 14 23:16:37.141: INFO: (10) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 16.370045ms)
Nov 14 23:16:37.142: INFO: (10) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 17.12074ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 16.616101ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 16.989307ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 16.495929ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 16.930535ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 18.77854ms)
Nov 14 23:16:37.143: INFO: (10) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 17.142132ms)
Nov 14 23:16:37.153: INFO: (11) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 8.940668ms)
Nov 14 23:16:37.153: INFO: (11) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 9.195433ms)
Nov 14 23:16:37.155: INFO: (11) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 9.356616ms)
Nov 14 23:16:37.158: INFO: (11) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.001197ms)
Nov 14 23:16:37.158: INFO: (11) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 11.962832ms)
Nov 14 23:16:37.159: INFO: (11) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 12.481779ms)
Nov 14 23:16:37.159: INFO: (11) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 12.132215ms)
Nov 14 23:16:37.160: INFO: (11) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 14.125676ms)
Nov 14 23:16:37.160: INFO: (11) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 15.245329ms)
Nov 14 23:16:37.161: INFO: (11) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.30823ms)
Nov 14 23:16:37.161: INFO: (11) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 15.944195ms)
Nov 14 23:16:37.161: INFO: (11) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 16.775166ms)
Nov 14 23:16:37.161: INFO: (11) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 16.467896ms)
Nov 14 23:16:37.161: INFO: (11) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 17.151098ms)
Nov 14 23:16:37.162: INFO: (11) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 14.849494ms)
Nov 14 23:16:37.159: INFO: (11) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 13.501345ms)
Nov 14 23:16:37.167: INFO: (12) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 5.482659ms)
Nov 14 23:16:37.171: INFO: (12) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 9.406193ms)
Nov 14 23:16:37.175: INFO: (12) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.554895ms)
Nov 14 23:16:37.175: INFO: (12) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 13.34981ms)
Nov 14 23:16:37.176: INFO: (12) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 13.665908ms)
Nov 14 23:16:37.177: INFO: (12) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 14.710499ms)
Nov 14 23:16:37.178: INFO: (12) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 15.47417ms)
Nov 14 23:16:37.178: INFO: (12) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 16.115587ms)
Nov 14 23:16:37.179: INFO: (12) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 16.855738ms)
Nov 14 23:16:37.179: INFO: (12) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 16.444865ms)
Nov 14 23:16:37.180: INFO: (12) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 16.840173ms)
Nov 14 23:16:37.180: INFO: (12) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 17.391927ms)
Nov 14 23:16:37.180: INFO: (12) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 17.619118ms)
Nov 14 23:16:37.180: INFO: (12) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 17.810394ms)
Nov 14 23:16:37.180: INFO: (12) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 18.090135ms)
Nov 14 23:16:37.181: INFO: (12) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 18.641787ms)
Nov 14 23:16:37.187: INFO: (13) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 6.193242ms)
Nov 14 23:16:37.191: INFO: (13) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 8.758419ms)
Nov 14 23:16:37.192: INFO: (13) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 9.955693ms)
Nov 14 23:16:37.192: INFO: (13) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 10.36814ms)
Nov 14 23:16:37.192: INFO: (13) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 10.814387ms)
Nov 14 23:16:37.193: INFO: (13) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 12.086107ms)
Nov 14 23:16:37.193: INFO: (13) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 11.941495ms)
Nov 14 23:16:37.193: INFO: (13) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 11.804863ms)
Nov 14 23:16:37.194: INFO: (13) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 11.968914ms)
Nov 14 23:16:37.194: INFO: (13) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 12.284189ms)
Nov 14 23:16:37.194: INFO: (13) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 12.853297ms)
Nov 14 23:16:37.195: INFO: (13) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.99202ms)
Nov 14 23:16:37.196: INFO: (13) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 14.341458ms)
Nov 14 23:16:37.196: INFO: (13) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 14.562223ms)
Nov 14 23:16:37.196: INFO: (13) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.324089ms)
Nov 14 23:16:37.196: INFO: (13) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 14.523051ms)
Nov 14 23:16:37.202: INFO: (14) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 6.084705ms)
Nov 14 23:16:37.204: INFO: (14) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 7.750178ms)
Nov 14 23:16:37.206: INFO: (14) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 8.99519ms)
Nov 14 23:16:37.206: INFO: (14) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 8.382245ms)
Nov 14 23:16:37.207: INFO: (14) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 8.18125ms)
Nov 14 23:16:37.207: INFO: (14) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 8.793834ms)
Nov 14 23:16:37.208: INFO: (14) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 11.10403ms)
Nov 14 23:16:37.209: INFO: (14) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 10.41766ms)
Nov 14 23:16:37.209: INFO: (14) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 11.822391ms)
Nov 14 23:16:37.210: INFO: (14) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.351683ms)
Nov 14 23:16:37.210: INFO: (14) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 13.00026ms)
Nov 14 23:16:37.211: INFO: (14) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 13.809543ms)
Nov 14 23:16:37.211: INFO: (14) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 13.382034ms)
Nov 14 23:16:37.211: INFO: (14) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 14.193836ms)
Nov 14 23:16:37.211: INFO: (14) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 14.041059ms)
Nov 14 23:16:37.211: INFO: (14) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 13.691ms)
Nov 14 23:16:37.228: INFO: (15) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 15.586155ms)
Nov 14 23:16:37.229: INFO: (15) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 16.927107ms)
Nov 14 23:16:37.229: INFO: (15) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 17.364808ms)
Nov 14 23:16:37.230: INFO: (15) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 16.764304ms)
Nov 14 23:16:37.230: INFO: (15) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 18.653868ms)
Nov 14 23:16:37.231: INFO: (15) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 19.099852ms)
Nov 14 23:16:37.232: INFO: (15) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 18.921057ms)
Nov 14 23:16:37.232: INFO: (15) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 19.927873ms)
Nov 14 23:16:37.233: INFO: (15) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 19.616924ms)
Nov 14 23:16:37.233: INFO: (15) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 20.409989ms)
Nov 14 23:16:37.233: INFO: (15) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 20.746308ms)
Nov 14 23:16:37.233: INFO: (15) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 20.509215ms)
Nov 14 23:16:37.233: INFO: (15) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 20.483594ms)
Nov 14 23:16:37.235: INFO: (15) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 21.80425ms)
Nov 14 23:16:37.235: INFO: (15) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 23.074036ms)
Nov 14 23:16:37.237: INFO: (15) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 23.520759ms)
Nov 14 23:16:37.248: INFO: (16) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 9.438483ms)
Nov 14 23:16:37.249: INFO: (16) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 11.189519ms)
Nov 14 23:16:37.250: INFO: (16) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 11.805708ms)
Nov 14 23:16:37.250: INFO: (16) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 11.851813ms)
Nov 14 23:16:37.251: INFO: (16) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 12.385503ms)
Nov 14 23:16:37.251: INFO: (16) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 14.40573ms)
Nov 14 23:16:37.252: INFO: (16) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 14.962245ms)
Nov 14 23:16:37.252: INFO: (16) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 13.463276ms)
Nov 14 23:16:37.252: INFO: (16) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 15.755609ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 14.84388ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 14.751109ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 15.698856ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 14.18982ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 15.655948ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 15.578896ms)
Nov 14 23:16:37.253: INFO: (16) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 15.595671ms)
Nov 14 23:16:37.263: INFO: (17) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 8.813931ms)
Nov 14 23:16:37.264: INFO: (17) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 9.708958ms)
Nov 14 23:16:37.264: INFO: (17) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 10.79684ms)
Nov 14 23:16:37.264: INFO: (17) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 10.617378ms)
Nov 14 23:16:37.264: INFO: (17) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 11.074796ms)
Nov 14 23:16:37.265: INFO: (17) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 10.408938ms)
Nov 14 23:16:37.265: INFO: (17) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 11.100828ms)
Nov 14 23:16:37.265: INFO: (17) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 11.124099ms)
Nov 14 23:16:37.265: INFO: (17) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 9.896615ms)
Nov 14 23:16:37.267: INFO: (17) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 12.122346ms)
Nov 14 23:16:37.267: INFO: (17) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 13.268679ms)
Nov 14 23:16:37.267: INFO: (17) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 12.189775ms)
Nov 14 23:16:37.268: INFO: (17) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 13.443915ms)
Nov 14 23:16:37.268: INFO: (17) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 12.901837ms)
Nov 14 23:16:37.269: INFO: (17) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 13.807241ms)
Nov 14 23:16:37.269: INFO: (17) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 14.081442ms)
Nov 14 23:16:37.281: INFO: (18) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 11.220544ms)
Nov 14 23:16:37.283: INFO: (18) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 13.136512ms)
Nov 14 23:16:37.283: INFO: (18) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 13.361679ms)
Nov 14 23:16:37.286: INFO: (18) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 16.708233ms)
Nov 14 23:16:37.286: INFO: (18) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 15.063383ms)
Nov 14 23:16:37.286: INFO: (18) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 16.027272ms)
Nov 14 23:16:37.288: INFO: (18) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 15.970136ms)
Nov 14 23:16:37.288: INFO: (18) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 16.001671ms)
Nov 14 23:16:37.288: INFO: (18) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 16.634776ms)
Nov 14 23:16:37.288: INFO: (18) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 16.281368ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 17.160048ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 18.183267ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 18.133828ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 17.626951ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 18.28529ms)
Nov 14 23:16:37.289: INFO: (18) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 16.853998ms)
Nov 14 23:16:37.298: INFO: (19) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg/proxy/rewriteme">test</a> (200; 8.956283ms)
Nov 14 23:16:37.299: INFO: (19) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:460/proxy/: tls baz (200; 8.728922ms)
Nov 14 23:16:37.300: INFO: (19) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:462/proxy/: tls qux (200; 9.995144ms)
Nov 14 23:16:37.301: INFO: (19) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:1080/proxy/rewriteme">test<... (200; 9.945448ms)
Nov 14 23:16:37.313: INFO: (19) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:162/proxy/: bar (200; 22.288317ms)
Nov 14 23:16:37.317: INFO: (19) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname2/proxy/: bar (200; 26.236487ms)
Nov 14 23:16:37.319: INFO: (19) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:1080/proxy/rewriteme">... (200; 28.322947ms)
Nov 14 23:16:37.325: INFO: (19) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname1/proxy/: foo (200; 35.077075ms)
Nov 14 23:16:37.327: INFO: (19) /api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/: <a href="/api/v1/namespaces/proxy-7317/pods/https:proxy-service-j7qxj-822wg:443/proxy/tlsrewritem... (200; 35.957825ms)
Nov 14 23:16:37.327: INFO: (19) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname1/proxy/: tls baz (200; 36.491077ms)
Nov 14 23:16:37.327: INFO: (19) /api/v1/namespaces/proxy-7317/services/http:proxy-service-j7qxj:portname1/proxy/: foo (200; 36.74151ms)
Nov 14 23:16:37.327: INFO: (19) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:160/proxy/: foo (200; 36.883339ms)
Nov 14 23:16:37.328: INFO: (19) /api/v1/namespaces/proxy-7317/services/proxy-service-j7qxj:portname2/proxy/: bar (200; 36.872111ms)
Nov 14 23:16:37.330: INFO: (19) /api/v1/namespaces/proxy-7317/pods/proxy-service-j7qxj-822wg:160/proxy/: foo (200; 40.140368ms)
Nov 14 23:16:37.330: INFO: (19) /api/v1/namespaces/proxy-7317/services/https:proxy-service-j7qxj:tlsportname2/proxy/: tls qux (200; 39.741141ms)
Nov 14 23:16:37.331: INFO: (19) /api/v1/namespaces/proxy-7317/pods/http:proxy-service-j7qxj-822wg:162/proxy/: bar (200; 39.838747ms)
STEP: deleting ReplicationController proxy-service-j7qxj in namespace proxy-7317, will wait for the garbage collector to delete the pods
Nov 14 23:16:37.411: INFO: Deleting ReplicationController proxy-service-j7qxj took: 16.604693ms
Nov 14 23:16:38.111: INFO: Terminating ReplicationController proxy-service-j7qxj pods took: 700.553409ms
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:16:40.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7317" for this suite.
Nov 14 23:16:46.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:16:46.368: INFO: namespace proxy-7317 deletion completed in 6.245967111s

• [SLOW TEST:73.816 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:16:46.375: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8c7a28b9-7a90-41bf-a91b-2c0a19e64656
STEP: Creating a pod to test consume secrets
Nov 14 23:16:46.556: INFO: Waiting up to 5m0s for pod "pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a" in namespace "secrets-2932" to be "success or failure"
Nov 14 23:16:46.566: INFO: Pod "pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.320622ms
Nov 14 23:16:48.572: INFO: Pod "pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01587955s
Nov 14 23:16:50.577: INFO: Pod "pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021219056s
STEP: Saw pod success
Nov 14 23:16:50.578: INFO: Pod "pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a" satisfied condition "success or failure"
Nov 14 23:16:50.581: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a container secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:16:50.616: INFO: Waiting for pod pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a to disappear
Nov 14 23:16:50.621: INFO: Pod pod-secrets-937990a9-f763-4bd8-9e03-d3c119488e3a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:16:50.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2932" for this suite.
Nov 14 23:16:56.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:16:56.805: INFO: namespace secrets-2932 deletion completed in 6.177506486s

• [SLOW TEST:10.431 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:16:56.810: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:17:02.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6349" for this suite.
Nov 14 23:17:08.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:17:08.805: INFO: namespace watch-6349 deletion completed in 6.235267543s

• [SLOW TEST:11.996 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:17:08.808: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4573
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4610
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:18:15.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2047" for this suite.
Nov 14 23:18:21.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:18:21.732: INFO: namespace namespaces-2047 deletion completed in 6.222656178s
STEP: Destroying namespace "nsdeletetest-4573" for this suite.
Nov 14 23:18:21.736: INFO: Namespace nsdeletetest-4573 was already deleted
STEP: Destroying namespace "nsdeletetest-4610" for this suite.
Nov 14 23:18:28.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:18:28.281: INFO: namespace nsdeletetest-4610 deletion completed in 6.545027348s

• [SLOW TEST:79.473 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:18:28.283: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4435
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-84fb625d-0a23-43d1-babe-e694e6dd8803
STEP: Creating secret with name s-test-opt-upd-5c74af7a-5742-44f8-9d52-620a392d46ff
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-84fb625d-0a23-43d1-babe-e694e6dd8803
STEP: Updating secret s-test-opt-upd-5c74af7a-5742-44f8-9d52-620a392d46ff
STEP: Creating secret with name s-test-opt-create-61a7fe52-7060-4ab8-b676-e241d0e9e9d6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:19:43.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4435" for this suite.
Nov 14 23:20:13.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:20:13.957: INFO: namespace secrets-4435 deletion completed in 30.239535998s

• [SLOW TEST:105.674 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:20:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Nov 14 23:20:14.144: INFO: Waiting up to 5m0s for pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c" in namespace "containers-4394" to be "success or failure"
Nov 14 23:20:14.151: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400372ms
Nov 14 23:20:16.159: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014823515s
Nov 14 23:20:18.163: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019370906s
Nov 14 23:20:20.169: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c": Phase="Running", Reason="", readiness=true. Elapsed: 6.024731735s
Nov 14 23:20:22.174: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029702168s
STEP: Saw pod success
Nov 14 23:20:22.174: INFO: Pod "client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c" satisfied condition "success or failure"
Nov 14 23:20:22.178: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c container test-container: <nil>
STEP: delete the pod
Nov 14 23:20:22.225: INFO: Waiting for pod client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c to disappear
Nov 14 23:20:22.231: INFO: Pod client-containers-256a5307-c41a-44bf-a650-7b13fb98f62c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:20:22.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4394" for this suite.
Nov 14 23:20:28.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:20:28.454: INFO: namespace containers-4394 deletion completed in 6.216558078s

• [SLOW TEST:14.495 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:20:28.463: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Nov 14 23:20:28.706: INFO: namespace kubectl-771
Nov 14 23:20:28.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-771'
Nov 14 23:20:30.287: INFO: stderr: ""
Nov 14 23:20:30.287: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 14 23:20:31.297: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:20:31.297: INFO: Found 0 / 1
Nov 14 23:20:32.294: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:20:32.294: INFO: Found 1 / 1
Nov 14 23:20:32.294: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 14 23:20:32.298: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:20:32.298: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 14 23:20:32.298: INFO: wait on redis-master startup in kubectl-771 
Nov 14 23:20:32.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 logs redis-master-jfskr redis-master --namespace=kubectl-771'
Nov 14 23:20:32.477: INFO: stderr: ""
Nov 14 23:20:32.477: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Nov 23:20:31.970 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Nov 23:20:31.970 # Server started, Redis version 3.2.12\n1:M 14 Nov 23:20:31.971 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Nov 23:20:31.971 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Nov 14 23:20:32.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-771'
Nov 14 23:20:32.653: INFO: stderr: ""
Nov 14 23:20:32.653: INFO: stdout: "service/rm2 exposed\n"
Nov 14 23:20:32.668: INFO: Service rm2 in namespace kubectl-771 found.
STEP: exposing service
Nov 14 23:20:34.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-771'
Nov 14 23:20:34.846: INFO: stderr: ""
Nov 14 23:20:34.846: INFO: stdout: "service/rm3 exposed\n"
Nov 14 23:20:34.856: INFO: Service rm3 in namespace kubectl-771 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:20:36.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-771" for this suite.
Nov 14 23:21:14.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:21:15.087: INFO: namespace kubectl-771 deletion completed in 38.195136675s

• [SLOW TEST:46.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:21:15.094: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:21:15.256: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 14 23:21:17.301: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:21:18.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2489" for this suite.
Nov 14 23:21:24.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:21:24.520: INFO: namespace replication-controller-2489 deletion completed in 6.20140048s

• [SLOW TEST:9.427 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:21:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:21:24.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1" in namespace "downward-api-5383" to be "success or failure"
Nov 14 23:21:24.829: INFO: Pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 136.159288ms
Nov 14 23:21:26.836: INFO: Pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.143232975s
Nov 14 23:21:28.842: INFO: Pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148795023s
Nov 14 23:21:30.849: INFO: Pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.155965045s
STEP: Saw pod success
Nov 14 23:21:30.849: INFO: Pod "downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1" satisfied condition "success or failure"
Nov 14 23:21:30.854: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1 container client-container: <nil>
STEP: delete the pod
Nov 14 23:21:30.889: INFO: Waiting for pod downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1 to disappear
Nov 14 23:21:30.898: INFO: Pod downwardapi-volume-3a1a15c1-9a49-4649-b032-fd6099b86ba1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:21:30.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5383" for this suite.
Nov 14 23:21:36.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:21:37.147: INFO: namespace downward-api-5383 deletion completed in 6.233286814s

• [SLOW TEST:12.626 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:21:37.153: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:22:07.417: INFO: Container started at 2019-11-14 23:21:49 +0000 UTC, pod became ready at 2019-11-14 23:22:06 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:22:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3731" for this suite.
Nov 14 23:22:29.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:22:29.649: INFO: namespace container-probe-3731 deletion completed in 22.220474392s

• [SLOW TEST:52.496 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:22:29.654: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-0c086aa6-9fe9-4f53-af33-5664d9465ef4
STEP: Creating a pod to test consume configMaps
Nov 14 23:22:29.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6" in namespace "configmap-4653" to be "success or failure"
Nov 14 23:22:29.901: INFO: Pod "pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.787991ms
Nov 14 23:22:31.907: INFO: Pod "pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01997566s
Nov 14 23:22:33.912: INFO: Pod "pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025047424s
STEP: Saw pod success
Nov 14 23:22:33.913: INFO: Pod "pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6" satisfied condition "success or failure"
Nov 14 23:22:33.915: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:22:33.949: INFO: Waiting for pod pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6 to disappear
Nov 14 23:22:33.959: INFO: Pod pod-configmaps-7adfbe39-5ac4-4699-be5d-6deb78a625b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:22:33.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4653" for this suite.
Nov 14 23:22:39.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:22:40.136: INFO: namespace configmap-4653 deletion completed in 6.170098346s

• [SLOW TEST:10.482 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:22:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:22:44.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-98" for this suite.
Nov 14 23:23:44.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:23:44.574: INFO: namespace kubelet-test-98 deletion completed in 1m0.198223406s

• [SLOW TEST:64.433 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:23:44.576: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:23:44.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 version'
Nov 14 23:23:44.886: INFO: stderr: ""
Nov 14 23:23:44.886: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:23:26Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:15:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:23:44.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3099" for this suite.
Nov 14 23:23:50.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:23:51.074: INFO: namespace kubectl-3099 deletion completed in 6.179125312s

• [SLOW TEST:6.498 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:23:51.076: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Nov 14 23:23:51.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-8212'
Nov 14 23:23:51.707: INFO: stderr: ""
Nov 14 23:23:51.707: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 14 23:23:51.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:23:51.861: INFO: stderr: ""
Nov 14 23:23:51.861: INFO: stdout: "update-demo-nautilus-nrzl9 update-demo-nautilus-vqvjp "
Nov 14 23:23:51.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-nrzl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:23:52.074: INFO: stderr: ""
Nov 14 23:23:52.074: INFO: stdout: ""
Nov 14 23:23:52.074: INFO: update-demo-nautilus-nrzl9 is created but not running
Nov 14 23:23:57.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:23:57.232: INFO: stderr: ""
Nov 14 23:23:57.233: INFO: stdout: "update-demo-nautilus-nrzl9 update-demo-nautilus-vqvjp "
Nov 14 23:23:57.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-nrzl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:23:57.401: INFO: stderr: ""
Nov 14 23:23:57.401: INFO: stdout: "true"
Nov 14 23:23:57.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-nrzl9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:23:57.540: INFO: stderr: ""
Nov 14 23:23:57.540: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:23:57.540: INFO: validating pod update-demo-nautilus-nrzl9
Nov 14 23:23:57.548: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:23:57.548: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:23:57.548: INFO: update-demo-nautilus-nrzl9 is verified up and running
Nov 14 23:23:57.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:23:57.707: INFO: stderr: ""
Nov 14 23:23:57.707: INFO: stdout: "true"
Nov 14 23:23:57.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:23:57.836: INFO: stderr: ""
Nov 14 23:23:57.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:23:57.836: INFO: validating pod update-demo-nautilus-vqvjp
Nov 14 23:23:57.844: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:23:57.844: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:23:57.844: INFO: update-demo-nautilus-vqvjp is verified up and running
STEP: scaling down the replication controller
Nov 14 23:23:57.851: INFO: scanned /root for discovery docs: <nil>
Nov 14 23:23:57.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8212'
Nov 14 23:23:59.063: INFO: stderr: ""
Nov 14 23:23:59.063: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 14 23:23:59.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:23:59.213: INFO: stderr: ""
Nov 14 23:23:59.213: INFO: stdout: "update-demo-nautilus-nrzl9 update-demo-nautilus-vqvjp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 14 23:24:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:04.369: INFO: stderr: ""
Nov 14 23:24:04.369: INFO: stdout: "update-demo-nautilus-vqvjp "
Nov 14 23:24:04.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:04.507: INFO: stderr: ""
Nov 14 23:24:04.507: INFO: stdout: "true"
Nov 14 23:24:04.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:04.651: INFO: stderr: ""
Nov 14 23:24:04.651: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:24:04.651: INFO: validating pod update-demo-nautilus-vqvjp
Nov 14 23:24:04.657: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:24:04.657: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:24:04.657: INFO: update-demo-nautilus-vqvjp is verified up and running
STEP: scaling up the replication controller
Nov 14 23:24:04.662: INFO: scanned /root for discovery docs: <nil>
Nov 14 23:24:04.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8212'
Nov 14 23:24:05.853: INFO: stderr: ""
Nov 14 23:24:05.853: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 14 23:24:05.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:06.012: INFO: stderr: ""
Nov 14 23:24:06.012: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:06.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:06.564: INFO: stderr: ""
Nov 14 23:24:06.564: INFO: stdout: ""
Nov 14 23:24:06.564: INFO: update-demo-nautilus-ljpbs is created but not running
Nov 14 23:24:11.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:11.721: INFO: stderr: ""
Nov 14 23:24:11.721: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:11.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:11.878: INFO: stderr: ""
Nov 14 23:24:11.878: INFO: stdout: ""
Nov 14 23:24:11.878: INFO: update-demo-nautilus-ljpbs is created but not running
Nov 14 23:24:16.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:17.013: INFO: stderr: ""
Nov 14 23:24:17.013: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:17.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:17.140: INFO: stderr: ""
Nov 14 23:24:17.140: INFO: stdout: ""
Nov 14 23:24:17.140: INFO: update-demo-nautilus-ljpbs is created but not running
Nov 14 23:24:22.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:22.309: INFO: stderr: ""
Nov 14 23:24:22.309: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:22.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:22.479: INFO: stderr: ""
Nov 14 23:24:22.479: INFO: stdout: ""
Nov 14 23:24:22.479: INFO: update-demo-nautilus-ljpbs is created but not running
Nov 14 23:24:27.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:27.647: INFO: stderr: ""
Nov 14 23:24:27.647: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:27.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:27.778: INFO: stderr: ""
Nov 14 23:24:27.778: INFO: stdout: ""
Nov 14 23:24:27.778: INFO: update-demo-nautilus-ljpbs is created but not running
Nov 14 23:24:32.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8212'
Nov 14 23:24:32.929: INFO: stderr: ""
Nov 14 23:24:32.929: INFO: stdout: "update-demo-nautilus-ljpbs update-demo-nautilus-vqvjp "
Nov 14 23:24:32.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:33.054: INFO: stderr: ""
Nov 14 23:24:33.054: INFO: stdout: "true"
Nov 14 23:24:33.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-ljpbs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:33.189: INFO: stderr: ""
Nov 14 23:24:33.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:24:33.189: INFO: validating pod update-demo-nautilus-ljpbs
Nov 14 23:24:33.208: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:24:33.208: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:24:33.208: INFO: update-demo-nautilus-ljpbs is verified up and running
Nov 14 23:24:33.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:33.376: INFO: stderr: ""
Nov 14 23:24:33.376: INFO: stdout: "true"
Nov 14 23:24:33.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-vqvjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8212'
Nov 14 23:24:33.529: INFO: stderr: ""
Nov 14 23:24:33.529: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 14 23:24:33.529: INFO: validating pod update-demo-nautilus-vqvjp
Nov 14 23:24:33.536: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 14 23:24:33.536: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 14 23:24:33.536: INFO: update-demo-nautilus-vqvjp is verified up and running
STEP: using delete to clean up resources
Nov 14 23:24:33.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-8212'
Nov 14 23:24:33.700: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 14 23:24:33.700: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 14 23:24:33.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8212'
Nov 14 23:24:33.867: INFO: stderr: "No resources found.\n"
Nov 14 23:24:33.867: INFO: stdout: ""
Nov 14 23:24:33.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -l name=update-demo --namespace=kubectl-8212 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 14 23:24:34.029: INFO: stderr: ""
Nov 14 23:24:34.029: INFO: stdout: "update-demo-nautilus-ljpbs\nupdate-demo-nautilus-vqvjp\n"
Nov 14 23:24:34.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8212'
Nov 14 23:24:34.742: INFO: stderr: "No resources found.\n"
Nov 14 23:24:34.742: INFO: stdout: ""
Nov 14 23:24:34.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -l name=update-demo --namespace=kubectl-8212 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 14 23:24:34.934: INFO: stderr: ""
Nov 14 23:24:34.934: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:24:34.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8212" for this suite.
Nov 14 23:24:56.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:24:57.163: INFO: namespace kubectl-8212 deletion completed in 22.220710176s

• [SLOW TEST:66.089 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:24:57.172: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7d8a3b5e-a496-44d5-bb35-71c25a4ac8a7
STEP: Creating a pod to test consume configMaps
Nov 14 23:24:57.351: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5" in namespace "projected-9356" to be "success or failure"
Nov 14 23:24:57.358: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.387654ms
Nov 14 23:24:59.364: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034706s
Nov 14 23:25:01.369: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01852818s
Nov 14 23:25:03.375: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024255692s
Nov 14 23:25:05.382: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03116067s
Nov 14 23:25:07.387: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036355593s
Nov 14 23:25:09.396: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045224517s
Nov 14 23:25:11.402: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051359336s
Nov 14 23:25:13.407: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.056331487s
Nov 14 23:25:15.413: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.061960416s
Nov 14 23:25:17.417: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.066012851s
Nov 14 23:25:19.423: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.072139624s
Nov 14 23:25:21.429: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.077839754s
Nov 14 23:25:23.434: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.083023257s
Nov 14 23:25:25.441: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.090593285s
Nov 14 23:25:27.451: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.100210765s
Nov 14 23:25:29.457: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.1065267s
Nov 14 23:25:31.465: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.114191895s
Nov 14 23:25:33.472: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.120848342s
Nov 14 23:25:35.478: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.126678263s
Nov 14 23:25:37.482: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.131278813s
Nov 14 23:25:39.487: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.136301584s
Nov 14 23:25:41.491: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.140105833s
Nov 14 23:25:43.497: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.145857701s
Nov 14 23:25:45.505: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.153751658s
Nov 14 23:25:47.510: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.159542105s
Nov 14 23:25:49.516: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.164714086s
Nov 14 23:25:51.521: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.170026445s
Nov 14 23:25:53.529: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.177865966s
Nov 14 23:25:55.534: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.182641648s
Nov 14 23:25:57.543: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m0.191736864s
STEP: Saw pod success
Nov 14 23:25:57.543: INFO: Pod "pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5" satisfied condition "success or failure"
Nov 14 23:25:57.547: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:25:57.592: INFO: Waiting for pod pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5 to disappear
Nov 14 23:25:57.604: INFO: Pod pod-projected-configmaps-450611b5-0054-4ed6-9154-71fb4bb7b2b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:25:57.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9356" for this suite.
Nov 14 23:26:03.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:26:03.821: INFO: namespace projected-9356 deletion completed in 6.210297684s

• [SLOW TEST:66.650 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:26:03.825: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:26:04.004: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37" in namespace "downward-api-4795" to be "success or failure"
Nov 14 23:26:04.014: INFO: Pod "downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37": Phase="Pending", Reason="", readiness=false. Elapsed: 9.714435ms
Nov 14 23:26:06.019: INFO: Pod "downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014665165s
STEP: Saw pod success
Nov 14 23:26:06.020: INFO: Pod "downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37" satisfied condition "success or failure"
Nov 14 23:26:06.022: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37 container client-container: <nil>
STEP: delete the pod
Nov 14 23:26:06.062: INFO: Waiting for pod downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37 to disappear
Nov 14 23:26:06.069: INFO: Pod downwardapi-volume-5bd1b55d-8015-4d56-bbd0-3e9f91404d37 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:26:06.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4795" for this suite.
Nov 14 23:26:12.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:26:12.252: INFO: namespace downward-api-4795 deletion completed in 6.175017401s

• [SLOW TEST:8.427 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:26:12.254: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Nov 14 23:26:12.535: INFO: Waiting up to 5m0s for pod "client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da" in namespace "containers-6870" to be "success or failure"
Nov 14 23:26:12.553: INFO: Pod "client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da": Phase="Pending", Reason="", readiness=false. Elapsed: 17.233831ms
Nov 14 23:26:14.558: INFO: Pod "client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.022556574s
Nov 14 23:26:16.562: INFO: Pod "client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026678052s
STEP: Saw pod success
Nov 14 23:26:16.562: INFO: Pod "client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da" satisfied condition "success or failure"
Nov 14 23:26:16.566: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da container test-container: <nil>
STEP: delete the pod
Nov 14 23:26:16.600: INFO: Waiting for pod client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da to disappear
Nov 14 23:26:16.608: INFO: Pod client-containers-b6ca492a-3a90-4a64-b940-714a4f80c1da no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:26:16.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6870" for this suite.
Nov 14 23:26:22.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:26:22.807: INFO: namespace containers-6870 deletion completed in 6.190768872s

• [SLOW TEST:10.553 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:26:22.815: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:26:23.004: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 14 23:26:23.039: INFO: Number of nodes with available pods: 0
Nov 14 23:26:23.039: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 14 23:26:23.073: INFO: Number of nodes with available pods: 0
Nov 14 23:26:23.073: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:24.078: INFO: Number of nodes with available pods: 0
Nov 14 23:26:24.078: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:25.080: INFO: Number of nodes with available pods: 0
Nov 14 23:26:25.080: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:26.080: INFO: Number of nodes with available pods: 0
Nov 14 23:26:26.080: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:27.079: INFO: Number of nodes with available pods: 0
Nov 14 23:26:27.080: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:28.079: INFO: Number of nodes with available pods: 0
Nov 14 23:26:28.079: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:29.078: INFO: Number of nodes with available pods: 1
Nov 14 23:26:29.078: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 14 23:26:29.121: INFO: Number of nodes with available pods: 1
Nov 14 23:26:29.121: INFO: Number of running nodes: 0, number of available pods: 1
Nov 14 23:26:30.125: INFO: Number of nodes with available pods: 0
Nov 14 23:26:30.125: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 14 23:26:30.824: INFO: Number of nodes with available pods: 0
Nov 14 23:26:30.824: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:31.830: INFO: Number of nodes with available pods: 0
Nov 14 23:26:31.830: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:32.839: INFO: Number of nodes with available pods: 0
Nov 14 23:26:32.839: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:33.831: INFO: Number of nodes with available pods: 0
Nov 14 23:26:33.831: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:34.830: INFO: Number of nodes with available pods: 0
Nov 14 23:26:34.830: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:35.831: INFO: Number of nodes with available pods: 0
Nov 14 23:26:35.831: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:26:36.832: INFO: Number of nodes with available pods: 1
Nov 14 23:26:36.832: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7070, will wait for the garbage collector to delete the pods
Nov 14 23:26:36.905: INFO: Deleting DaemonSet.extensions daemon-set took: 9.38513ms
Nov 14 23:26:37.705: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.425887ms
Nov 14 23:26:41.611: INFO: Number of nodes with available pods: 0
Nov 14 23:26:41.612: INFO: Number of running nodes: 0, number of available pods: 0
Nov 14 23:26:41.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7070/daemonsets","resourceVersion":"7459"},"items":null}

Nov 14 23:26:41.618: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7070/pods","resourceVersion":"7459"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:26:41.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7070" for this suite.
Nov 14 23:26:47.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:26:47.915: INFO: namespace daemonsets-7070 deletion completed in 6.259641026s

• [SLOW TEST:25.101 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:26:47.934: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1114 23:27:18.195721      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 14 23:27:18.196: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:27:18.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3843" for this suite.
Nov 14 23:27:26.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:27:26.434: INFO: namespace gc-3843 deletion completed in 8.231231889s

• [SLOW TEST:38.501 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:27:26.444: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-d546
STEP: Creating a pod to test atomic-volume-subpath
Nov 14 23:27:27.057: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d546" in namespace "subpath-1330" to be "success or failure"
Nov 14 23:27:27.088: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Pending", Reason="", readiness=false. Elapsed: 30.465938ms
Nov 14 23:27:29.231: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173895251s
Nov 14 23:27:31.240: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182721345s
Nov 14 23:27:33.246: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188544971s
Nov 14 23:27:35.251: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Pending", Reason="", readiness=false. Elapsed: 8.193378214s
Nov 14 23:27:37.258: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 10.200386369s
Nov 14 23:27:39.266: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 12.208980237s
Nov 14 23:27:41.275: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 14.217599782s
Nov 14 23:27:43.282: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 16.224539167s
Nov 14 23:27:45.287: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 18.229559231s
Nov 14 23:27:47.293: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 20.235844701s
Nov 14 23:27:49.298: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 22.240894896s
Nov 14 23:27:51.577: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 24.520023476s
Nov 14 23:27:53.582: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 26.524404414s
Nov 14 23:27:55.587: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 28.529801345s
Nov 14 23:27:57.595: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 30.537844045s
Nov 14 23:27:59.599: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 32.541933634s
Nov 14 23:28:01.608: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 34.55098815s
Nov 14 23:28:03.614: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 36.556890289s
Nov 14 23:28:05.626: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 38.568303062s
Nov 14 23:28:07.632: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 40.574493558s
Nov 14 23:28:09.639: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 42.581536191s
Nov 14 23:28:11.646: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 44.588775613s
Nov 14 23:28:13.651: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 46.59412431s
Nov 14 23:28:15.826: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 48.768398932s
Nov 14 23:28:17.830: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 50.772417898s
Nov 14 23:28:19.836: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 52.778934071s
Nov 14 23:28:21.842: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 54.785229902s
Nov 14 23:28:23.851: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 56.793526201s
Nov 14 23:28:25.862: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 58.805084242s
Nov 14 23:28:27.871: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 1m0.813631081s
Nov 14 23:28:29.877: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Running", Reason="", readiness=true. Elapsed: 1m2.81952768s
Nov 14 23:28:31.886: INFO: Pod "pod-subpath-test-configmap-d546": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m4.829223883s
STEP: Saw pod success
Nov 14 23:28:31.887: INFO: Pod "pod-subpath-test-configmap-d546" satisfied condition "success or failure"
Nov 14 23:28:31.909: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-subpath-test-configmap-d546 container test-container-subpath-configmap-d546: <nil>
STEP: delete the pod
Nov 14 23:28:32.376: INFO: Waiting for pod pod-subpath-test-configmap-d546 to disappear
Nov 14 23:28:32.400: INFO: Pod pod-subpath-test-configmap-d546 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d546
Nov 14 23:28:32.400: INFO: Deleting pod "pod-subpath-test-configmap-d546" in namespace "subpath-1330"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:28:32.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1330" for this suite.
Nov 14 23:28:38.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:28:38.636: INFO: namespace subpath-1330 deletion completed in 6.222276153s

• [SLOW TEST:72.193 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:28:38.641: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:28:42.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4895" for this suite.
Nov 14 23:29:04.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:29:05.196: INFO: namespace replication-controller-4895 deletion completed in 22.221717219s

• [SLOW TEST:26.556 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:29:05.202: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:29:05.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3577" for this suite.
Nov 14 23:29:27.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:29:27.657: INFO: namespace pods-3577 deletion completed in 22.245553142s

• [SLOW TEST:22.455 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:29:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7808
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 14 23:29:53.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec pod-sharedvolume-3266bd51-b645-4d3f-acba-9abf97f19a29 -c busybox-main-container --namespace=emptydir-7808 -- cat /usr/share/volumeshare/shareddata.txt'
Nov 14 23:29:54.303: INFO: stderr: ""
Nov 14 23:29:54.304: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:29:54.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7808" for this suite.
Nov 14 23:30:00.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:30:00.576: INFO: namespace emptydir-7808 deletion completed in 6.263960048s

• [SLOW TEST:32.914 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:30:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:30:00.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596" in namespace "projected-5094" to be "success or failure"
Nov 14 23:30:00.856: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 42.814798ms
Nov 14 23:30:02.863: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049594544s
Nov 14 23:30:04.866: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053351588s
Nov 14 23:30:06.871: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058114902s
Nov 14 23:30:08.876: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063167846s
Nov 14 23:30:10.882: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 10.068944243s
Nov 14 23:30:12.889: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 12.07553624s
Nov 14 23:30:14.893: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079552211s
Nov 14 23:30:16.898: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 16.085232444s
Nov 14 23:30:18.902: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 18.089248088s
Nov 14 23:30:20.908: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 20.095050877s
Nov 14 23:30:22.913: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 22.099819037s
Nov 14 23:30:24.918: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 24.104665309s
Nov 14 23:30:26.924: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 26.110955184s
Nov 14 23:30:28.933: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 28.12029513s
Nov 14 23:30:30.941: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 30.128144s
Nov 14 23:30:32.946: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 32.132703806s
Nov 14 23:30:34.954: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Pending", Reason="", readiness=false. Elapsed: 34.140588589s
Nov 14 23:30:36.966: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.152727806s
STEP: Saw pod success
Nov 14 23:30:36.966: INFO: Pod "downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596" satisfied condition "success or failure"
Nov 14 23:30:36.977: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596 container client-container: <nil>
STEP: delete the pod
Nov 14 23:30:37.022: INFO: Waiting for pod downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596 to disappear
Nov 14 23:30:37.027: INFO: Pod downwardapi-volume-61bf6e44-c339-4e47-aac6-3476552cd596 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:30:37.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5094" for this suite.
Nov 14 23:30:43.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:30:43.262: INFO: namespace projected-5094 deletion completed in 6.229209412s

• [SLOW TEST:42.685 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:30:43.271: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:30:43.442: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776" in namespace "projected-4920" to be "success or failure"
Nov 14 23:30:43.460: INFO: Pod "downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776": Phase="Pending", Reason="", readiness=false. Elapsed: 17.977641ms
Nov 14 23:30:45.470: INFO: Pod "downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027522839s
Nov 14 23:30:47.477: INFO: Pod "downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034402766s
STEP: Saw pod success
Nov 14 23:30:47.477: INFO: Pod "downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776" satisfied condition "success or failure"
Nov 14 23:30:47.482: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776 container client-container: <nil>
STEP: delete the pod
Nov 14 23:30:47.511: INFO: Waiting for pod downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776 to disappear
Nov 14 23:30:47.516: INFO: Pod downwardapi-volume-b202ad45-4869-4cef-a484-333c323aa776 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:30:47.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4920" for this suite.
Nov 14 23:30:53.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:30:53.733: INFO: namespace projected-4920 deletion completed in 6.208458149s

• [SLOW TEST:10.462 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:30:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 14 23:30:53.957: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:53.961: INFO: Number of nodes with available pods: 0
Nov 14 23:30:53.961: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:30:54.975: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:54.978: INFO: Number of nodes with available pods: 0
Nov 14 23:30:54.978: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:30:55.974: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:55.981: INFO: Number of nodes with available pods: 0
Nov 14 23:30:55.981: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:30:56.979: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:56.982: INFO: Number of nodes with available pods: 2
Nov 14 23:30:56.982: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 14 23:30:57.030: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:57.039: INFO: Number of nodes with available pods: 1
Nov 14 23:30:57.039: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:30:58.046: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:58.050: INFO: Number of nodes with available pods: 1
Nov 14 23:30:58.050: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:30:59.045: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:30:59.050: INFO: Number of nodes with available pods: 1
Nov 14 23:30:59.050: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:00.045: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:00.049: INFO: Number of nodes with available pods: 1
Nov 14 23:31:00.049: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:01.046: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:01.050: INFO: Number of nodes with available pods: 1
Nov 14 23:31:01.050: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:02.047: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:02.051: INFO: Number of nodes with available pods: 1
Nov 14 23:31:02.051: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:03.046: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:03.051: INFO: Number of nodes with available pods: 1
Nov 14 23:31:03.051: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:04.065: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:04.071: INFO: Number of nodes with available pods: 1
Nov 14 23:31:04.071: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:05.046: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:05.050: INFO: Number of nodes with available pods: 1
Nov 14 23:31:05.050: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:06.049: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:06.058: INFO: Number of nodes with available pods: 1
Nov 14 23:31:06.058: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:07.047: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:07.051: INFO: Number of nodes with available pods: 1
Nov 14 23:31:07.051: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:08.047: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:08.051: INFO: Number of nodes with available pods: 1
Nov 14 23:31:08.051: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:09.043: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:09.047: INFO: Number of nodes with available pods: 1
Nov 14 23:31:09.047: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:10.046: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:10.051: INFO: Number of nodes with available pods: 1
Nov 14 23:31:10.051: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:31:11.047: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:31:11.052: INFO: Number of nodes with available pods: 2
Nov 14 23:31:11.052: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5346, will wait for the garbage collector to delete the pods
Nov 14 23:31:11.120: INFO: Deleting DaemonSet.extensions daemon-set took: 8.674164ms
Nov 14 23:31:11.820: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.668997ms
Nov 14 23:31:14.731: INFO: Number of nodes with available pods: 0
Nov 14 23:31:14.731: INFO: Number of running nodes: 0, number of available pods: 0
Nov 14 23:31:14.734: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5346/daemonsets","resourceVersion":"8356"},"items":null}

Nov 14 23:31:14.736: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5346/pods","resourceVersion":"8356"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:31:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5346" for this suite.
Nov 14 23:31:20.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:31:21.020: INFO: namespace daemonsets-5346 deletion completed in 6.268664755s

• [SLOW TEST:27.285 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:31:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 14 23:31:21.270: INFO: Waiting up to 5m0s for pod "downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2" in namespace "downward-api-279" to be "success or failure"
Nov 14 23:31:21.371: INFO: Pod "downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 101.050309ms
Nov 14 23:31:23.378: INFO: Pod "downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10810837s
Nov 14 23:31:25.383: INFO: Pod "downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113252733s
STEP: Saw pod success
Nov 14 23:31:25.383: INFO: Pod "downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2" satisfied condition "success or failure"
Nov 14 23:31:25.385: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2 container dapi-container: <nil>
STEP: delete the pod
Nov 14 23:31:25.419: INFO: Waiting for pod downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2 to disappear
Nov 14 23:31:25.432: INFO: Pod downward-api-9a4e45c8-002b-4e10-9fca-50735282a7b2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:31:25.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-279" for this suite.
Nov 14 23:31:31.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:31:31.637: INFO: namespace downward-api-279 deletion completed in 6.197821066s

• [SLOW TEST:10.609 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:31:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 14 23:31:31.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-5693'
Nov 14 23:31:34.221: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 14 23:31:34.221: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Nov 14 23:31:36.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5693'
Nov 14 23:31:36.418: INFO: stderr: ""
Nov 14 23:31:36.418: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:31:36.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5693" for this suite.
Nov 14 23:31:42.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:31:42.639: INFO: namespace kubectl-5693 deletion completed in 6.206043469s

• [SLOW TEST:11.001 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:31:42.645: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 14 23:31:42.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1503'
Nov 14 23:31:42.988: INFO: stderr: ""
Nov 14 23:31:42.988: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Nov 14 23:31:53.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pod e2e-test-nginx-pod --namespace=kubectl-1503 -o json'
Nov 14 23:31:53.200: INFO: stderr: ""
Nov 14 23:31:53.200: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.2.58/32\"\n        },\n        \"creationTimestamp\": \"2019-11-14T23:31:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1503\",\n        \"resourceVersion\": \"8529\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1503/pods/e2e-test-nginx-pod\",\n        \"uid\": \"5a70db48-618d-4a7e-a670-f22fda0b0829\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-b4k5j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-100-3imo44lif6er-minion-0\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-b4k5j\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-b4k5j\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-14T23:31:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-14T23:31:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-14T23:31:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-14T23:31:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://de0fe958c8dd797c14b5af894c401d6902f0b2a627aef8421a6fa6b0fa443160\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-11-14T23:31:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.2.58\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-11-14T23:31:42Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 14 23:31:53.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 replace -f - --namespace=kubectl-1503'
Nov 14 23:31:53.558: INFO: stderr: ""
Nov 14 23:31:53.558: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Nov 14 23:31:53.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete pods e2e-test-nginx-pod --namespace=kubectl-1503'
Nov 14 23:31:58.383: INFO: stderr: ""
Nov 14 23:31:58.387: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:31:58.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1503" for this suite.
Nov 14 23:32:04.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:32:04.636: INFO: namespace kubectl-1503 deletion completed in 6.237733878s

• [SLOW TEST:21.991 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:32:04.640: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 14 23:32:04.867: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 14 23:32:04.877: INFO: Waiting for terminating namespaces to be deleted...
Nov 14 23:32:04.883: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-0 before test
Nov 14 23:32:04.897: INFO: calico-node-ntzpq from kube-system started at 2019-11-14 22:58:21 +0000 UTC (2 container statuses recorded)
Nov 14 23:32:04.897: INFO: 	Container calico-node ready: true, restart count 0
Nov 14 23:32:04.897: INFO: 	Container install-cni ready: true, restart count 0
Nov 14 23:32:04.898: INFO: node-exporter-w9wqv from prometheus-monitoring started at 2019-11-14 22:58:23 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.898: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 14 23:32:04.898: INFO: npd-wvz4s from kube-system started at 2019-11-14 22:58:24 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.898: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 14 23:32:04.898: INFO: sonobuoy from sonobuoy started at 2019-11-14 23:02:29 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.898: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 14 23:32:04.898: INFO: sonobuoy-e2e-job-65028defe7584f98 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:32:04.899: INFO: 	Container e2e ready: true, restart count 0
Nov 14 23:32:04.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:32:04.899: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-hqkrc from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:32:04.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:32:04.899: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 14 23:32:04.899: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-1 before test
Nov 14 23:32:04.918: INFO: npd-g7tcr from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 14 23:32:04.918: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-6wwn9 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:32:04.918: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 14 23:32:04.918: INFO: node-exporter-hmjtl from prometheus-monitoring started at 2019-11-14 22:56:57 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 14 23:32:04.918: INFO: calico-node-2mxq7 from kube-system started at 2019-11-14 22:56:57 +0000 UTC (2 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container calico-node ready: true, restart count 0
Nov 14 23:32:04.918: INFO: 	Container install-cni ready: true, restart count 0
Nov 14 23:32:04.918: INFO: prometheus-ff4dc5bfd-27sxj from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container prometheus ready: true, restart count 0
Nov 14 23:32:04.918: INFO: kube-dns-autoscaler-97b76b9b4-grhf6 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container autoscaler ready: true, restart count 0
Nov 14 23:32:04.918: INFO: heapster-bbf96fd9d-fnrqj from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container heapster ready: true, restart count 0
Nov 14 23:32:04.918: INFO: grafana-85c5975df9-7hxpg from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container grafana ready: true, restart count 0
Nov 14 23:32:04.918: INFO: coredns-646fbb9987-g28vb from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container coredns ready: true, restart count 0
Nov 14 23:32:04.918: INFO: kubernetes-dashboard-f456bc54b-nl4gf from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 14 23:32:04.918: INFO: coredns-646fbb9987-c6bz4 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:32:04.918: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15d72b690bc0dcc5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:32:05.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-455" for this suite.
Nov 14 23:32:11.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:32:12.168: INFO: namespace sched-pred-455 deletion completed in 6.191208871s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.528 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:32:12.178: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 14 23:32:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Nov 14 23:32:13.081: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 14 23:32:15.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:17.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:19.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:21.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:23.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:25.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:27.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:29.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:31.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:33.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:35.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:37.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:39.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:41.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:43.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:45.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:47.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:49.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:51.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:53.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:55.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:57.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:32:59.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:33:01.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:33:03.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709371133, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 14 23:33:06.766: INFO: Waited 1.551670122s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:33:07.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4464" for this suite.
Nov 14 23:33:13.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:33:13.907: INFO: namespace aggregator-4464 deletion completed in 6.32077615s

• [SLOW TEST:61.730 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:33:13.915: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-829add63-6e5c-4b21-b929-bde9f453e452
STEP: Creating a pod to test consume configMaps
Nov 14 23:33:14.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940" in namespace "configmap-6090" to be "success or failure"
Nov 14 23:33:14.143: INFO: Pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005605ms
Nov 14 23:33:16.149: INFO: Pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014153991s
Nov 14 23:33:18.153: INFO: Pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018445353s
Nov 14 23:33:20.160: INFO: Pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025379207s
STEP: Saw pod success
Nov 14 23:33:20.160: INFO: Pod "pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940" satisfied condition "success or failure"
Nov 14 23:33:20.164: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:33:20.201: INFO: Waiting for pod pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940 to disappear
Nov 14 23:33:20.206: INFO: Pod pod-configmaps-f7a33142-339b-4941-9d9c-4f1a0359b940 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:33:20.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6090" for this suite.
Nov 14 23:33:26.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:33:26.460: INFO: namespace configmap-6090 deletion completed in 6.248575881s

• [SLOW TEST:12.545 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:33:26.461: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7923
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-370cf327-f551-4cb6-8192-81a4d533f520
STEP: Creating configMap with name cm-test-opt-upd-0cbe56b0-eb3c-4bf7-a99e-28a9a5ca0fa5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-370cf327-f551-4cb6-8192-81a4d533f520
STEP: Updating configmap cm-test-opt-upd-0cbe56b0-eb3c-4bf7-a99e-28a9a5ca0fa5
STEP: Creating configMap with name cm-test-opt-create-95659252-7a1c-4505-950f-9e3d4c017153
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:33:36.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7923" for this suite.
Nov 14 23:33:58.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:33:59.120: INFO: namespace projected-7923 deletion completed in 22.268172319s

• [SLOW TEST:32.660 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:33:59.130: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 14 23:33:59.634: INFO: Waiting up to 5m0s for pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e" in namespace "downward-api-3137" to be "success or failure"
Nov 14 23:33:59.648: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.840142ms
Nov 14 23:34:01.652: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017824459s
Nov 14 23:34:03.658: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023775843s
Nov 14 23:34:05.663: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029286604s
Nov 14 23:34:07.680: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045598812s
STEP: Saw pod success
Nov 14 23:34:07.680: INFO: Pod "downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e" satisfied condition "success or failure"
Nov 14 23:34:07.685: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e container dapi-container: <nil>
STEP: delete the pod
Nov 14 23:34:07.738: INFO: Waiting for pod downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e to disappear
Nov 14 23:34:07.743: INFO: Pod downward-api-02c80f5b-b59f-40d0-ae87-789557963b0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:34:07.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3137" for this suite.
Nov 14 23:34:13.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:34:13.988: INFO: namespace downward-api-3137 deletion completed in 6.238881971s

• [SLOW TEST:14.862 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:34:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:35:14.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2541" for this suite.
Nov 14 23:35:36.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:35:36.503: INFO: namespace container-probe-2541 deletion completed in 22.289838618s

• [SLOW TEST:82.504 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:35:36.510: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Nov 14 23:35:36.720: INFO: Waiting up to 5m0s for pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1" in namespace "var-expansion-3316" to be "success or failure"
Nov 14 23:35:36.730: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.324066ms
Nov 14 23:35:38.797: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076680458s
Nov 14 23:35:40.803: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082516457s
Nov 14 23:35:42.811: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090240391s
Nov 14 23:35:44.818: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096728122s
STEP: Saw pod success
Nov 14 23:35:44.818: INFO: Pod "var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1" satisfied condition "success or failure"
Nov 14 23:35:44.821: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1 container dapi-container: <nil>
STEP: delete the pod
Nov 14 23:35:44.877: INFO: Waiting for pod var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1 to disappear
Nov 14 23:35:44.883: INFO: Pod var-expansion-eb9639f0-6a8f-43ae-bd49-789820c4b5e1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:35:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3316" for this suite.
Nov 14 23:35:50.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:35:51.104: INFO: namespace var-expansion-3316 deletion completed in 6.21324888s

• [SLOW TEST:14.595 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:35:51.114: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:35:51.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f" in namespace "projected-5707" to be "success or failure"
Nov 14 23:35:51.340: INFO: Pod "downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.425571ms
Nov 14 23:35:53.362: INFO: Pod "downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032650961s
Nov 14 23:35:55.367: INFO: Pod "downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037470627s
STEP: Saw pod success
Nov 14 23:35:55.367: INFO: Pod "downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f" satisfied condition "success or failure"
Nov 14 23:35:55.370: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f container client-container: <nil>
STEP: delete the pod
Nov 14 23:35:55.411: INFO: Waiting for pod downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f to disappear
Nov 14 23:35:55.416: INFO: Pod downwardapi-volume-57254905-bdac-4486-aa45-d43139439b4f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:35:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5707" for this suite.
Nov 14 23:36:01.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:36:01.627: INFO: namespace projected-5707 deletion completed in 6.206352713s

• [SLOW TEST:10.513 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:36:01.628: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-fb00906e-c875-435a-97b1-7878bd077eb8
STEP: Creating a pod to test consume secrets
Nov 14 23:36:01.823: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875" in namespace "projected-8620" to be "success or failure"
Nov 14 23:36:01.837: INFO: Pod "pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875": Phase="Pending", Reason="", readiness=false. Elapsed: 13.198168ms
Nov 14 23:36:03.845: INFO: Pod "pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021238413s
Nov 14 23:36:05.851: INFO: Pod "pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027988989s
STEP: Saw pod success
Nov 14 23:36:05.852: INFO: Pod "pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875" satisfied condition "success or failure"
Nov 14 23:36:05.855: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875 container secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:36:05.882: INFO: Waiting for pod pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875 to disappear
Nov 14 23:36:05.887: INFO: Pod pod-projected-secrets-9d7a910b-f6a8-427d-8f6f-68bdb5812875 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:36:05.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8620" for this suite.
Nov 14 23:36:11.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:36:12.120: INFO: namespace projected-8620 deletion completed in 6.227425604s

• [SLOW TEST:10.493 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:36:12.122: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8374.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8374.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:36:18.356: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8374/dns-test-54fba734-5fe3-419e-8705-00000b24681d: the server could not find the requested resource (get pods dns-test-54fba734-5fe3-419e-8705-00000b24681d)
Nov 14 23:36:18.360: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8374/dns-test-54fba734-5fe3-419e-8705-00000b24681d: the server could not find the requested resource (get pods dns-test-54fba734-5fe3-419e-8705-00000b24681d)
Nov 14 23:36:18.392: INFO: Unable to read jessie_udp@PodARecord from pod dns-8374/dns-test-54fba734-5fe3-419e-8705-00000b24681d: the server could not find the requested resource (get pods dns-test-54fba734-5fe3-419e-8705-00000b24681d)
Nov 14 23:36:18.399: INFO: Lookups using dns-8374/dns-test-54fba734-5fe3-419e-8705-00000b24681d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord]

Nov 14 23:36:23.449: INFO: DNS probes using dns-8374/dns-test-54fba734-5fe3-419e-8705-00000b24681d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:36:23.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8374" for this suite.
Nov 14 23:36:29.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:36:29.788: INFO: namespace dns-8374 deletion completed in 6.255768765s

• [SLOW TEST:17.666 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:36:29.792: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 14 23:36:30.017: INFO: Waiting up to 5m0s for pod "downward-api-a6743166-6f6a-4d9d-9380-77a288550b56" in namespace "downward-api-9946" to be "success or failure"
Nov 14 23:36:30.023: INFO: Pod "downward-api-a6743166-6f6a-4d9d-9380-77a288550b56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.612043ms
Nov 14 23:36:32.027: INFO: Pod "downward-api-a6743166-6f6a-4d9d-9380-77a288550b56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010084822s
Nov 14 23:36:34.032: INFO: Pod "downward-api-a6743166-6f6a-4d9d-9380-77a288550b56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015040836s
STEP: Saw pod success
Nov 14 23:36:34.032: INFO: Pod "downward-api-a6743166-6f6a-4d9d-9380-77a288550b56" satisfied condition "success or failure"
Nov 14 23:36:34.035: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downward-api-a6743166-6f6a-4d9d-9380-77a288550b56 container dapi-container: <nil>
STEP: delete the pod
Nov 14 23:36:34.097: INFO: Waiting for pod downward-api-a6743166-6f6a-4d9d-9380-77a288550b56 to disappear
Nov 14 23:36:34.105: INFO: Pod downward-api-a6743166-6f6a-4d9d-9380-77a288550b56 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:36:34.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9946" for this suite.
Nov 14 23:36:40.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:36:40.347: INFO: namespace downward-api-9946 deletion completed in 6.226564479s

• [SLOW TEST:10.555 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:36:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:36:40.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574" in namespace "downward-api-9324" to be "success or failure"
Nov 14 23:36:40.561: INFO: Pod "downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574": Phase="Pending", Reason="", readiness=false. Elapsed: 27.869258ms
Nov 14 23:36:42.569: INFO: Pod "downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035450043s
Nov 14 23:36:44.573: INFO: Pod "downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040024473s
STEP: Saw pod success
Nov 14 23:36:44.573: INFO: Pod "downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574" satisfied condition "success or failure"
Nov 14 23:36:44.576: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574 container client-container: <nil>
STEP: delete the pod
Nov 14 23:36:44.607: INFO: Waiting for pod downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574 to disappear
Nov 14 23:36:44.613: INFO: Pod downwardapi-volume-00d2be5d-dca9-4c01-9b18-07e7c7d97574 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:36:44.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9324" for this suite.
Nov 14 23:36:50.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:36:50.811: INFO: namespace downward-api-9324 deletion completed in 6.193799258s

• [SLOW TEST:10.463 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:36:50.821: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Nov 14 23:36:51.002: INFO: Waiting up to 5m0s for pod "client-containers-d00ff632-f226-4efa-8739-acf18b8de091" in namespace "containers-4222" to be "success or failure"
Nov 14 23:36:51.014: INFO: Pod "client-containers-d00ff632-f226-4efa-8739-acf18b8de091": Phase="Pending", Reason="", readiness=false. Elapsed: 12.672169ms
Nov 14 23:36:53.164: INFO: Pod "client-containers-d00ff632-f226-4efa-8739-acf18b8de091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162222113s
Nov 14 23:36:55.169: INFO: Pod "client-containers-d00ff632-f226-4efa-8739-acf18b8de091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.167091917s
STEP: Saw pod success
Nov 14 23:36:55.169: INFO: Pod "client-containers-d00ff632-f226-4efa-8739-acf18b8de091" satisfied condition "success or failure"
Nov 14 23:36:55.172: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod client-containers-d00ff632-f226-4efa-8739-acf18b8de091 container test-container: <nil>
STEP: delete the pod
Nov 14 23:36:55.201: INFO: Waiting for pod client-containers-d00ff632-f226-4efa-8739-acf18b8de091 to disappear
Nov 14 23:36:55.212: INFO: Pod client-containers-d00ff632-f226-4efa-8739-acf18b8de091 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:36:55.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4222" for this suite.
Nov 14 23:37:01.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:37:01.403: INFO: namespace containers-4222 deletion completed in 6.181749566s

• [SLOW TEST:10.582 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:37:01.411: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-rbww
STEP: Creating a pod to test atomic-volume-subpath
Nov 14 23:37:01.660: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rbww" in namespace "subpath-950" to be "success or failure"
Nov 14 23:37:01.674: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Pending", Reason="", readiness=false. Elapsed: 14.328715ms
Nov 14 23:37:03.683: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023222678s
Nov 14 23:37:05.687: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 4.027322832s
Nov 14 23:37:07.691: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 6.031238343s
Nov 14 23:37:09.697: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 8.037237592s
Nov 14 23:37:11.705: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 10.045227076s
Nov 14 23:37:13.713: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 12.052452254s
Nov 14 23:37:15.717: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 14.057394086s
Nov 14 23:37:17.723: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 16.062797649s
Nov 14 23:37:19.728: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 18.067668313s
Nov 14 23:37:21.732: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 20.072424081s
Nov 14 23:37:23.739: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Running", Reason="", readiness=true. Elapsed: 22.079217127s
Nov 14 23:37:25.746: INFO: Pod "pod-subpath-test-downwardapi-rbww": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.08599976s
STEP: Saw pod success
Nov 14 23:37:25.746: INFO: Pod "pod-subpath-test-downwardapi-rbww" satisfied condition "success or failure"
Nov 14 23:37:25.750: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-subpath-test-downwardapi-rbww container test-container-subpath-downwardapi-rbww: <nil>
STEP: delete the pod
Nov 14 23:37:25.836: INFO: Waiting for pod pod-subpath-test-downwardapi-rbww to disappear
Nov 14 23:37:25.845: INFO: Pod pod-subpath-test-downwardapi-rbww no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rbww
Nov 14 23:37:25.846: INFO: Deleting pod "pod-subpath-test-downwardapi-rbww" in namespace "subpath-950"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:37:25.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-950" for this suite.
Nov 14 23:37:31.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:37:32.139: INFO: namespace subpath-950 deletion completed in 6.278962328s

• [SLOW TEST:30.728 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:37:32.143: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-d3f05c9d-1787-474c-a56e-b78c0fb06ed5
STEP: Creating a pod to test consume secrets
Nov 14 23:37:32.333: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95" in namespace "projected-1871" to be "success or failure"
Nov 14 23:37:32.339: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 5.869069ms
Nov 14 23:37:34.348: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01460568s
Nov 14 23:37:36.352: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018944562s
Nov 14 23:37:38.356: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022251137s
Nov 14 23:37:40.361: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027851167s
Nov 14 23:37:42.374: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.040776153s
STEP: Saw pod success
Nov 14 23:37:42.374: INFO: Pod "pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95" satisfied condition "success or failure"
Nov 14 23:37:42.383: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:37:42.436: INFO: Waiting for pod pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95 to disappear
Nov 14 23:37:42.444: INFO: Pod pod-projected-secrets-18e9024f-28e1-4013-a63d-7fd336ba6c95 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:37:42.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1871" for this suite.
Nov 14 23:37:48.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:37:48.657: INFO: namespace projected-1871 deletion completed in 6.207160086s

• [SLOW TEST:16.515 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:37:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-806
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-806
STEP: Deleting pre-stop pod
Nov 14 23:38:17.961: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:38:17.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-806" for this suite.
Nov 14 23:39:04.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:39:04.270: INFO: namespace prestop-806 deletion completed in 46.25523379s

• [SLOW TEST:75.612 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:39:04.276: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 14 23:39:04.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1799'
Nov 14 23:39:04.729: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 14 23:39:04.729: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Nov 14 23:39:06.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1799'
Nov 14 23:39:07.038: INFO: stderr: ""
Nov 14 23:39:07.038: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:39:07.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1799" for this suite.
Nov 14 23:41:09.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:41:09.320: INFO: namespace kubectl-1799 deletion completed in 2m2.274703279s

• [SLOW TEST:125.045 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:41:09.329: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:41:09.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0" in namespace "projected-9528" to be "success or failure"
Nov 14 23:41:09.554: INFO: Pod "downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.281649ms
Nov 14 23:41:11.561: INFO: Pod "downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02604006s
Nov 14 23:41:13.575: INFO: Pod "downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039891114s
STEP: Saw pod success
Nov 14 23:41:13.576: INFO: Pod "downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0" satisfied condition "success or failure"
Nov 14 23:41:13.580: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0 container client-container: <nil>
STEP: delete the pod
Nov 14 23:41:13.629: INFO: Waiting for pod downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0 to disappear
Nov 14 23:41:13.635: INFO: Pod downwardapi-volume-64245838-93f3-4347-8f11-f6d18ec841a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:41:13.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9528" for this suite.
Nov 14 23:41:19.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:41:19.819: INFO: namespace projected-9528 deletion completed in 6.177367941s

• [SLOW TEST:10.491 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:41:19.826: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 14 23:41:30.066: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:41:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6388" for this suite.
Nov 14 23:41:36.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:41:36.291: INFO: namespace container-runtime-6388 deletion completed in 6.193247681s

• [SLOW TEST:16.465 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:41:36.292: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Nov 14 23:41:36.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-5891'
Nov 14 23:41:37.951: INFO: stderr: ""
Nov 14 23:41:37.952: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Nov 14 23:41:38.960: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:41:38.960: INFO: Found 0 / 1
Nov 14 23:41:39.961: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:41:39.961: INFO: Found 0 / 1
Nov 14 23:41:40.956: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:41:40.956: INFO: Found 0 / 1
Nov 14 23:41:41.957: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:41:41.957: INFO: Found 1 / 1
Nov 14 23:41:41.957: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 14 23:41:41.960: INFO: Selector matched 1 pods for map[app:redis]
Nov 14 23:41:41.960: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Nov 14 23:41:41.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 logs redis-master-nr42x redis-master --namespace=kubectl-5891'
Nov 14 23:41:42.129: INFO: stderr: ""
Nov 14 23:41:42.129: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Nov 23:41:40.885 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Nov 23:41:40.885 # Server started, Redis version 3.2.12\n1:M 14 Nov 23:41:40.885 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Nov 23:41:40.891 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Nov 14 23:41:42.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 log redis-master-nr42x redis-master --namespace=kubectl-5891 --tail=1'
Nov 14 23:41:42.293: INFO: stderr: ""
Nov 14 23:41:42.293: INFO: stdout: "1:M 14 Nov 23:41:40.891 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Nov 14 23:41:42.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 log redis-master-nr42x redis-master --namespace=kubectl-5891 --limit-bytes=1'
Nov 14 23:41:42.449: INFO: stderr: ""
Nov 14 23:41:42.449: INFO: stdout: " "
STEP: exposing timestamps
Nov 14 23:41:42.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 log redis-master-nr42x redis-master --namespace=kubectl-5891 --tail=1 --timestamps'
Nov 14 23:41:42.614: INFO: stderr: ""
Nov 14 23:41:42.614: INFO: stdout: "2019-11-14T23:41:40.892719055Z 1:M 14 Nov 23:41:40.891 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Nov 14 23:41:45.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 log redis-master-nr42x redis-master --namespace=kubectl-5891 --since=1s'
Nov 14 23:41:45.287: INFO: stderr: ""
Nov 14 23:41:45.287: INFO: stdout: ""
Nov 14 23:41:45.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 log redis-master-nr42x redis-master --namespace=kubectl-5891 --since=24h'
Nov 14 23:41:45.453: INFO: stderr: ""
Nov 14 23:41:45.453: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Nov 23:41:40.885 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Nov 23:41:40.885 # Server started, Redis version 3.2.12\n1:M 14 Nov 23:41:40.885 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Nov 23:41:40.891 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Nov 14 23:41:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-5891'
Nov 14 23:41:45.649: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 14 23:41:45.649: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Nov 14 23:41:45.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get rc,svc -l name=nginx --no-headers --namespace=kubectl-5891'
Nov 14 23:41:45.822: INFO: stderr: "No resources found.\n"
Nov 14 23:41:45.822: INFO: stdout: ""
Nov 14 23:41:45.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -l name=nginx --namespace=kubectl-5891 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 14 23:41:45.954: INFO: stderr: ""
Nov 14 23:41:45.954: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:41:45.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5891" for this suite.
Nov 14 23:41:51.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:41:52.209: INFO: namespace kubectl-5891 deletion completed in 6.246807451s

• [SLOW TEST:15.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:41:52.213: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 14 23:41:57.504: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:41:57.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6103" for this suite.
Nov 14 23:42:19.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:42:19.767: INFO: namespace replicaset-6103 deletion completed in 22.213878051s

• [SLOW TEST:27.555 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:42:19.769: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 14 23:42:20.021: INFO: Waiting up to 5m0s for pod "pod-b8c82769-e477-4701-86ef-71605c748cc6" in namespace "emptydir-4692" to be "success or failure"
Nov 14 23:42:20.037: INFO: Pod "pod-b8c82769-e477-4701-86ef-71605c748cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.684007ms
Nov 14 23:42:22.045: INFO: Pod "pod-b8c82769-e477-4701-86ef-71605c748cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023646573s
Nov 14 23:42:24.090: INFO: Pod "pod-b8c82769-e477-4701-86ef-71605c748cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068697838s
STEP: Saw pod success
Nov 14 23:42:24.090: INFO: Pod "pod-b8c82769-e477-4701-86ef-71605c748cc6" satisfied condition "success or failure"
Nov 14 23:42:24.109: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-b8c82769-e477-4701-86ef-71605c748cc6 container test-container: <nil>
STEP: delete the pod
Nov 14 23:42:24.149: INFO: Waiting for pod pod-b8c82769-e477-4701-86ef-71605c748cc6 to disappear
Nov 14 23:42:24.158: INFO: Pod pod-b8c82769-e477-4701-86ef-71605c748cc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:42:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4692" for this suite.
Nov 14 23:42:30.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:42:30.400: INFO: namespace emptydir-4692 deletion completed in 6.234252034s

• [SLOW TEST:10.632 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:42:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:42:30.591: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 14 23:42:30.605: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:30.612: INFO: Number of nodes with available pods: 0
Nov 14 23:42:30.612: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:31.620: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:31.626: INFO: Number of nodes with available pods: 0
Nov 14 23:42:31.626: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:32.620: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:32.627: INFO: Number of nodes with available pods: 0
Nov 14 23:42:32.627: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:33.618: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:33.621: INFO: Number of nodes with available pods: 1
Nov 14 23:42:33.621: INFO: Node k8s-100-3imo44lif6er-minion-1 is running more than one daemon pod
Nov 14 23:42:34.619: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:34.623: INFO: Number of nodes with available pods: 2
Nov 14 23:42:34.623: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 14 23:42:34.671: INFO: Wrong image for pod: daemon-set-d2ltp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:34.671: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:34.687: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:35.693: INFO: Wrong image for pod: daemon-set-d2ltp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:35.693: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:35.697: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:36.695: INFO: Wrong image for pod: daemon-set-d2ltp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:36.695: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:36.701: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:37.694: INFO: Wrong image for pod: daemon-set-d2ltp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:37.694: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:37.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:38.692: INFO: Wrong image for pod: daemon-set-d2ltp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:38.692: INFO: Pod daemon-set-d2ltp is not available
Nov 14 23:42:38.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:38.696: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:39.695: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:39.695: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:39.702: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:40.694: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:40.694: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:40.700: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:41.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:41.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:41.697: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:42.693: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:42.693: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:42.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:43.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:43.693: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:43.697: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:44.693: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:44.693: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:44.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:45.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:45.693: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:45.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:46.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:46.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:46.697: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:47.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:47.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:47.697: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:48.694: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:48.694: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:48.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:49.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:49.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:49.696: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:50.692: INFO: Pod daemon-set-22fn6 is not available
Nov 14 23:42:50.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:50.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:51.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:51.696: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:52.692: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:52.698: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:53.694: INFO: Wrong image for pod: daemon-set-nx5tj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 14 23:42:53.694: INFO: Pod daemon-set-nx5tj is not available
Nov 14 23:42:53.700: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:54.694: INFO: Pod daemon-set-d4jb8 is not available
Nov 14 23:42:54.701: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 14 23:42:54.706: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:54.710: INFO: Number of nodes with available pods: 1
Nov 14 23:42:54.710: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:55.729: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:55.740: INFO: Number of nodes with available pods: 1
Nov 14 23:42:55.740: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:56.720: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:56.724: INFO: Number of nodes with available pods: 1
Nov 14 23:42:56.724: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:42:57.717: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:42:57.721: INFO: Number of nodes with available pods: 2
Nov 14 23:42:57.721: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6490, will wait for the garbage collector to delete the pods
Nov 14 23:42:57.803: INFO: Deleting DaemonSet.extensions daemon-set took: 9.951062ms
Nov 14 23:42:58.504: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.463708ms
Nov 14 23:43:11.616: INFO: Number of nodes with available pods: 0
Nov 14 23:43:11.616: INFO: Number of running nodes: 0, number of available pods: 0
Nov 14 23:43:11.626: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6490/daemonsets","resourceVersion":"10874"},"items":null}

Nov 14 23:43:11.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6490/pods","resourceVersion":"10874"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:43:11.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6490" for this suite.
Nov 14 23:43:17.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:43:18.215: INFO: namespace daemonsets-6490 deletion completed in 6.564032185s

• [SLOW TEST:47.812 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:43:18.218: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Nov 14 23:43:22.611: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-687135821 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 14 23:43:27.773: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:43:27.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7612" for this suite.
Nov 14 23:43:33.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:43:34.023: INFO: namespace pods-7612 deletion completed in 6.226804843s

• [SLOW TEST:15.805 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:43:34.027: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 14 23:43:43.529: INFO: Successfully updated pod "annotationupdateba05d399-e416-4802-9a29-e8f1fddbf937"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:43:45.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9109" for this suite.
Nov 14 23:44:07.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:44:07.832: INFO: namespace downward-api-9109 deletion completed in 22.261281878s

• [SLOW TEST:33.806 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:44:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:44:08.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552" in namespace "projected-7146" to be "success or failure"
Nov 14 23:44:08.035: INFO: Pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552": Phase="Pending", Reason="", readiness=false. Elapsed: 7.012425ms
Nov 14 23:44:10.040: INFO: Pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879491s
Nov 14 23:44:12.045: INFO: Pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016494083s
Nov 14 23:44:14.061: INFO: Pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032680233s
STEP: Saw pod success
Nov 14 23:44:14.061: INFO: Pod "downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552" satisfied condition "success or failure"
Nov 14 23:44:14.064: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552 container client-container: <nil>
STEP: delete the pod
Nov 14 23:44:14.105: INFO: Waiting for pod downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552 to disappear
Nov 14 23:44:14.118: INFO: Pod downwardapi-volume-e1ffdb28-9d0b-4586-9a02-49ce343ec552 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:44:14.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7146" for this suite.
Nov 14 23:44:20.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:44:20.297: INFO: namespace projected-7146 deletion completed in 6.171247677s

• [SLOW TEST:12.456 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:44:20.297: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 14 23:44:20.553: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8270,SelfLink:/api/v1/namespaces/watch-8270/configmaps/e2e-watch-test-watch-closed,UID:2b8bee4b-2b0d-4608-951e-e854acb945f5,ResourceVersion:11144,Generation:0,CreationTimestamp:2019-11-14 23:44:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 14 23:44:20.554: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8270,SelfLink:/api/v1/namespaces/watch-8270/configmaps/e2e-watch-test-watch-closed,UID:2b8bee4b-2b0d-4608-951e-e854acb945f5,ResourceVersion:11145,Generation:0,CreationTimestamp:2019-11-14 23:44:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 14 23:44:20.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8270,SelfLink:/api/v1/namespaces/watch-8270/configmaps/e2e-watch-test-watch-closed,UID:2b8bee4b-2b0d-4608-951e-e854acb945f5,ResourceVersion:11146,Generation:0,CreationTimestamp:2019-11-14 23:44:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 14 23:44:20.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8270,SelfLink:/api/v1/namespaces/watch-8270/configmaps/e2e-watch-test-watch-closed,UID:2b8bee4b-2b0d-4608-951e-e854acb945f5,ResourceVersion:11147,Generation:0,CreationTimestamp:2019-11-14 23:44:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:44:20.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8270" for this suite.
Nov 14 23:44:26.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:44:26.776: INFO: namespace watch-8270 deletion completed in 6.199886139s

• [SLOW TEST:6.479 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:44:26.777: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Nov 14 23:44:30.979: INFO: Pod pod-hostip-6a410225-204f-4b23-95f3-31f221f1dbae has hostIP: 10.0.0.6
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:44:30.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5870" for this suite.
Nov 14 23:44:53.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:44:53.206: INFO: namespace pods-5870 deletion completed in 22.221091299s

• [SLOW TEST:26.430 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:44:53.210: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 14 23:44:53.393: INFO: Waiting up to 5m0s for pod "pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5" in namespace "emptydir-5731" to be "success or failure"
Nov 14 23:44:53.401: INFO: Pod "pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387573ms
Nov 14 23:44:55.406: INFO: Pod "pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011516607s
Nov 14 23:44:57.411: INFO: Pod "pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016338779s
STEP: Saw pod success
Nov 14 23:44:57.411: INFO: Pod "pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5" satisfied condition "success or failure"
Nov 14 23:44:57.414: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5 container test-container: <nil>
STEP: delete the pod
Nov 14 23:44:57.454: INFO: Waiting for pod pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5 to disappear
Nov 14 23:44:57.458: INFO: Pod pod-0c4799f2-1f24-4101-9a1f-a0a86ca25ee5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:44:57.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5731" for this suite.
Nov 14 23:45:03.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:45:03.665: INFO: namespace emptydir-5731 deletion completed in 6.200739675s

• [SLOW TEST:10.455 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:45:03.681: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1114 23:45:13.983234      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 14 23:45:13.984: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:45:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3199" for this suite.
Nov 14 23:45:20.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:45:20.172: INFO: namespace gc-3199 deletion completed in 6.180415923s

• [SLOW TEST:16.492 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:45:20.186: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 14 23:45:20.703: INFO: Pod name wrapped-volume-race-0b65e6b5-dba1-4ef7-b80a-9a9d0203832e: Found 0 pods out of 5
Nov 14 23:45:25.724: INFO: Pod name wrapped-volume-race-0b65e6b5-dba1-4ef7-b80a-9a9d0203832e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0b65e6b5-dba1-4ef7-b80a-9a9d0203832e in namespace emptydir-wrapper-1834, will wait for the garbage collector to delete the pods
Nov 14 23:45:45.835: INFO: Deleting ReplicationController wrapped-volume-race-0b65e6b5-dba1-4ef7-b80a-9a9d0203832e took: 13.71246ms
Nov 14 23:45:46.536: INFO: Terminating ReplicationController wrapped-volume-race-0b65e6b5-dba1-4ef7-b80a-9a9d0203832e pods took: 700.481853ms
STEP: Creating RC which spawns configmap-volume pods
Nov 14 23:46:31.762: INFO: Pod name wrapped-volume-race-0cf72f30-1828-4c3d-a481-3e5b32ebfd12: Found 0 pods out of 5
Nov 14 23:46:36.774: INFO: Pod name wrapped-volume-race-0cf72f30-1828-4c3d-a481-3e5b32ebfd12: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0cf72f30-1828-4c3d-a481-3e5b32ebfd12 in namespace emptydir-wrapper-1834, will wait for the garbage collector to delete the pods
Nov 14 23:46:50.896: INFO: Deleting ReplicationController wrapped-volume-race-0cf72f30-1828-4c3d-a481-3e5b32ebfd12 took: 12.327676ms
Nov 14 23:46:51.696: INFO: Terminating ReplicationController wrapped-volume-race-0cf72f30-1828-4c3d-a481-3e5b32ebfd12 pods took: 800.350487ms
STEP: Creating RC which spawns configmap-volume pods
Nov 14 23:47:41.843: INFO: Pod name wrapped-volume-race-2e3ff201-dc76-4fc2-84da-49d5387c22ce: Found 0 pods out of 5
Nov 14 23:47:46.853: INFO: Pod name wrapped-volume-race-2e3ff201-dc76-4fc2-84da-49d5387c22ce: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2e3ff201-dc76-4fc2-84da-49d5387c22ce in namespace emptydir-wrapper-1834, will wait for the garbage collector to delete the pods
Nov 14 23:48:00.959: INFO: Deleting ReplicationController wrapped-volume-race-2e3ff201-dc76-4fc2-84da-49d5387c22ce took: 10.853432ms
Nov 14 23:48:01.690: INFO: Terminating ReplicationController wrapped-volume-race-2e3ff201-dc76-4fc2-84da-49d5387c22ce pods took: 730.976553ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:48:42.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1834" for this suite.
Nov 14 23:48:50.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:48:50.950: INFO: namespace emptydir-wrapper-1834 deletion completed in 8.289000313s

• [SLOW TEST:210.765 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:48:50.955: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1627/configmap-test-586133aa-0755-48c7-8c10-dc94b7800fff
STEP: Creating a pod to test consume configMaps
Nov 14 23:48:51.181: INFO: Waiting up to 5m0s for pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5" in namespace "configmap-1627" to be "success or failure"
Nov 14 23:48:51.204: INFO: Pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.370466ms
Nov 14 23:48:53.225: INFO: Pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043600616s
Nov 14 23:48:55.230: INFO: Pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.049035522s
Nov 14 23:48:57.235: INFO: Pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053845078s
STEP: Saw pod success
Nov 14 23:48:57.235: INFO: Pod "pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5" satisfied condition "success or failure"
Nov 14 23:48:57.238: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5 container env-test: <nil>
STEP: delete the pod
Nov 14 23:48:57.277: INFO: Waiting for pod pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5 to disappear
Nov 14 23:48:57.280: INFO: Pod pod-configmaps-325e5f04-b5ed-48a6-a075-c963fdafa5f5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:48:57.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1627" for this suite.
Nov 14 23:49:03.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:49:03.486: INFO: namespace configmap-1627 deletion completed in 6.201984197s

• [SLOW TEST:12.532 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:49:03.489: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7756
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:49:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:49:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7756" for this suite.
Nov 14 23:49:45.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:49:45.957: INFO: namespace pods-7756 deletion completed in 38.210823206s

• [SLOW TEST:42.469 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:49:45.971: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 14 23:49:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3780'
Nov 14 23:49:46.330: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 14 23:49:46.330: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Nov 14 23:49:46.355: INFO: scanned /root for discovery docs: <nil>
Nov 14 23:49:46.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3780'
Nov 14 23:50:02.286: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 14 23:50:02.287: INFO: stdout: "Created e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7\nScaling up e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Nov 14 23:50:02.287: INFO: stdout: "Created e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7\nScaling up e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Nov 14 23:50:02.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3780'
Nov 14 23:50:02.453: INFO: stderr: ""
Nov 14 23:50:02.453: INFO: stdout: "e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7-h759w "
Nov 14 23:50:02.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7-h759w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3780'
Nov 14 23:50:02.606: INFO: stderr: ""
Nov 14 23:50:02.606: INFO: stdout: "true"
Nov 14 23:50:02.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7-h759w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3780'
Nov 14 23:50:02.758: INFO: stderr: ""
Nov 14 23:50:02.758: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Nov 14 23:50:02.758: INFO: e2e-test-nginx-rc-ec805ec15b34efc81b4142143c44bfa7-h759w is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Nov 14 23:50:02.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete rc e2e-test-nginx-rc --namespace=kubectl-3780'
Nov 14 23:50:02.913: INFO: stderr: ""
Nov 14 23:50:02.913: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:50:02.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3780" for this suite.
Nov 14 23:50:08.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:50:11.125: INFO: namespace kubectl-3780 deletion completed in 8.167665459s

• [SLOW TEST:25.155 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:50:11.127: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Nov 14 23:50:11.308: INFO: Waiting up to 5m0s for pod "var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210" in namespace "var-expansion-7845" to be "success or failure"
Nov 14 23:50:11.316: INFO: Pod "var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210": Phase="Pending", Reason="", readiness=false. Elapsed: 8.360816ms
Nov 14 23:50:13.321: INFO: Pod "var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013226641s
STEP: Saw pod success
Nov 14 23:50:13.321: INFO: Pod "var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210" satisfied condition "success or failure"
Nov 14 23:50:13.324: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210 container dapi-container: <nil>
STEP: delete the pod
Nov 14 23:50:13.355: INFO: Waiting for pod var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210 to disappear
Nov 14 23:50:13.360: INFO: Pod var-expansion-5d7bfcb1-bf96-4857-951c-47a761b02210 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:50:13.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7845" for this suite.
Nov 14 23:50:19.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:50:19.585: INFO: namespace var-expansion-7845 deletion completed in 6.217513485s

• [SLOW TEST:8.458 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:50:19.587: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 14 23:50:19.748: INFO: PodSpec: initContainers in spec.initContainers
Nov 14 23:51:06.295: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f2c24fca-b94d-4410-b660-50ba639f90cb", GenerateName:"", Namespace:"init-container-2700", SelfLink:"/api/v1/namespaces/init-container-2700/pods/pod-init-f2c24fca-b94d-4410-b660-50ba639f90cb", UID:"a5d0813e-8d78-4a0b-8933-f4a139ddd95a", ResourceVersion:"13380", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63709372219, loc:(*time.Location)(0x80bfa40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"748490308"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.2.115/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9llvg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002a84f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9llvg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9llvg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9llvg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028b4d38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-100-3imo44lif6er-minion-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0027905a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028b4dc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028b4de0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0028b4de8), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372219, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372219, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372219, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372219, loc:(*time.Location)(0x80bfa40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.6", PodIP:"192.168.2.115", StartTime:(*v1.Time)(0xc0019a78a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0028531f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002853260)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9c68f3e7aadc1e31d34b4909d0e6441cf084360c924ca159ed6bc72730791128"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0019a7920), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0019a78e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:51:06.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2700" for this suite.
Nov 14 23:51:28.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:51:28.570: INFO: namespace init-container-2700 deletion completed in 22.246412832s

• [SLOW TEST:68.984 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:51:28.578: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1825
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9f0d24f3-cf3f-4aef-a951-93f9f05369c2
STEP: Creating secret with name s-test-opt-upd-acc5004f-356d-4f98-9ec4-eaa86c88d0f5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9f0d24f3-cf3f-4aef-a951-93f9f05369c2
STEP: Updating secret s-test-opt-upd-acc5004f-356d-4f98-9ec4-eaa86c88d0f5
STEP: Creating secret with name s-test-opt-create-51a8b22c-c33a-4350-83ad-2ff1f80314d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:51:35.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1825" for this suite.
Nov 14 23:51:57.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:51:57.255: INFO: namespace projected-1825 deletion completed in 22.180839022s

• [SLOW TEST:28.678 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:51:57.256: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 14 23:51:57.442: INFO: Waiting up to 5m0s for pod "pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22" in namespace "emptydir-5402" to be "success or failure"
Nov 14 23:51:57.459: INFO: Pod "pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22": Phase="Pending", Reason="", readiness=false. Elapsed: 16.727401ms
Nov 14 23:51:59.476: INFO: Pod "pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033692674s
Nov 14 23:52:01.480: INFO: Pod "pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038149802s
STEP: Saw pod success
Nov 14 23:52:01.480: INFO: Pod "pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22" satisfied condition "success or failure"
Nov 14 23:52:01.483: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22 container test-container: <nil>
STEP: delete the pod
Nov 14 23:52:01.523: INFO: Waiting for pod pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22 to disappear
Nov 14 23:52:01.528: INFO: Pod pod-d5288724-0ffd-4ad9-8fa8-2e81adca2e22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:52:01.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5402" for this suite.
Nov 14 23:52:07.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:52:07.773: INFO: namespace emptydir-5402 deletion completed in 6.239377443s

• [SLOW TEST:10.517 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:52:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1114 23:52:14.717135      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 14 23:52:14.717: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:52:14.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5387" for this suite.
Nov 14 23:52:20.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:52:20.963: INFO: namespace gc-5387 deletion completed in 6.237666892s

• [SLOW TEST:13.187 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:52:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Nov 14 23:52:21.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 cluster-info'
Nov 14 23:52:21.789: INFO: stderr: ""
Nov 14 23:52:21.790: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:52:21.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8908" for this suite.
Nov 14 23:52:27.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:52:27.985: INFO: namespace kubectl-8908 deletion completed in 6.190230856s

• [SLOW TEST:7.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:52:27.989: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-278.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 104.16.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.16.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.16.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.16.104_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-278.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-278.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 104.16.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.16.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.16.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.16.104_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:52:32.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.239: INFO: Unable to read wheezy_tcp@dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.246: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.251: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.268: INFO: Unable to read wheezy_udp@PodARecord from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.274: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.284: INFO: Unable to read jessie_udp@dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.291: INFO: Unable to read jessie_tcp@dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.298: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.303: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.315: INFO: Unable to read jessie_udp@PodARecord from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.319: INFO: Unable to read jessie_tcp@PodARecord from pod dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053: the server could not find the requested resource (get pods dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053)
Nov 14 23:52:32.327: INFO: Lookups using dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053 failed for: [wheezy_udp@dns-test-service.dns-278.svc.cluster.local wheezy_tcp@dns-test-service.dns-278.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-278.svc.cluster.local jessie_tcp@dns-test-service.dns-278.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-278.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-278.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Nov 14 23:52:37.416: INFO: DNS probes using dns-278/dns-test-f5621595-9452-4bcc-8cf7-353f03ce9053 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:52:37.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-278" for this suite.
Nov 14 23:52:43.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:52:43.780: INFO: namespace dns-278 deletion completed in 6.217262519s

• [SLOW TEST:15.792 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:52:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-a878c9c1-fa6b-417d-a132-e6691463fec5
STEP: Creating a pod to test consume secrets
Nov 14 23:52:43.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8" in namespace "projected-9419" to be "success or failure"
Nov 14 23:52:43.964: INFO: Pod "pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.117163ms
Nov 14 23:52:45.969: INFO: Pod "pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012010281s
Nov 14 23:52:47.974: INFO: Pod "pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016472929s
STEP: Saw pod success
Nov 14 23:52:47.974: INFO: Pod "pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8" satisfied condition "success or failure"
Nov 14 23:52:47.976: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:52:48.006: INFO: Waiting for pod pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8 to disappear
Nov 14 23:52:48.012: INFO: Pod pod-projected-secrets-0dbc93f8-3008-43d0-b477-34aced774fc8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:52:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9419" for this suite.
Nov 14 23:52:54.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:52:54.220: INFO: namespace projected-9419 deletion completed in 6.19019941s

• [SLOW TEST:10.438 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:52:54.222: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4778
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 14 23:52:54.427: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 14 23:53:26.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.127:8080/dial?request=hostName&protocol=udp&host=192.168.1.31&port=8081&tries=1'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:53:26.712: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:53:26.918: INFO: Waiting for endpoints: map[]
Nov 14 23:53:26.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.127:8080/dial?request=hostName&protocol=udp&host=192.168.2.126&port=8081&tries=1'] Namespace:pod-network-test-4778 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:53:26.921: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:53:27.180: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:53:27.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4778" for this suite.
Nov 14 23:53:49.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:53:49.447: INFO: namespace pod-network-test-4778 deletion completed in 22.260356623s

• [SLOW TEST:55.226 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:53:49.452: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-dl4l
STEP: Creating a pod to test atomic-volume-subpath
Nov 14 23:53:49.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dl4l" in namespace "subpath-2959" to be "success or failure"
Nov 14 23:53:49.705: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643623ms
Nov 14 23:53:51.715: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.016414231s
Nov 14 23:53:53.719: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 4.020695971s
Nov 14 23:53:55.723: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 6.024623811s
Nov 14 23:53:57.728: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 8.029962047s
Nov 14 23:53:59.732: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 10.033943533s
Nov 14 23:54:01.769: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 12.070239947s
Nov 14 23:54:03.773: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 14.074885995s
Nov 14 23:54:05.779: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 16.080938338s
Nov 14 23:54:08.101: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 18.402684989s
Nov 14 23:54:10.107: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 20.408571077s
Nov 14 23:54:12.212: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Running", Reason="", readiness=true. Elapsed: 22.513538652s
Nov 14 23:54:14.217: INFO: Pod "pod-subpath-test-projected-dl4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.518696779s
STEP: Saw pod success
Nov 14 23:54:14.217: INFO: Pod "pod-subpath-test-projected-dl4l" satisfied condition "success or failure"
Nov 14 23:54:14.220: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-subpath-test-projected-dl4l container test-container-subpath-projected-dl4l: <nil>
STEP: delete the pod
Nov 14 23:54:14.271: INFO: Waiting for pod pod-subpath-test-projected-dl4l to disappear
Nov 14 23:54:14.276: INFO: Pod pod-subpath-test-projected-dl4l no longer exists
STEP: Deleting pod pod-subpath-test-projected-dl4l
Nov 14 23:54:14.276: INFO: Deleting pod "pod-subpath-test-projected-dl4l" in namespace "subpath-2959"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:54:14.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2959" for this suite.
Nov 14 23:54:20.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:54:20.510: INFO: namespace subpath-2959 deletion completed in 6.226002622s

• [SLOW TEST:31.059 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:54:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 14 23:54:20.767: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c881c880-a4d2-4f83-8369-94fe7127fd06", Controller:(*bool)(0xc0023fe366), BlockOwnerDeletion:(*bool)(0xc0023fe367)}}
Nov 14 23:54:20.787: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7f144107-7a7d-474d-9bff-c6d4462fea33", Controller:(*bool)(0xc001805856), BlockOwnerDeletion:(*bool)(0xc001805857)}}
Nov 14 23:54:20.797: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6254ace9-3a8d-4433-a438-efcf9deb6f6c", Controller:(*bool)(0xc0018059f6), BlockOwnerDeletion:(*bool)(0xc0018059f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:54:25.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7487" for this suite.
Nov 14 23:54:31.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:54:32.085: INFO: namespace gc-7487 deletion completed in 6.262295484s

• [SLOW TEST:11.573 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:54:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 14 23:54:32.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14360,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 14 23:54:32.339: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14361,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 14 23:54:32.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14362,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 14 23:54:42.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14386,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 14 23:54:42.388: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14387,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov 14 23:54:42.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3002,SelfLink:/api/v1/namespaces/watch-3002/configmaps/e2e-watch-test-label-changed,UID:f9f427b2-e07a-40fc-92a9-8d953d67ad4b,ResourceVersion:14388,Generation:0,CreationTimestamp:2019-11-14 23:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:54:42.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3002" for this suite.
Nov 14 23:54:48.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:54:48.569: INFO: namespace watch-3002 deletion completed in 6.175058211s

• [SLOW TEST:16.483 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:54:48.578: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-02b571ea-75f3-43e3-856a-2f5eb05ef8bc
STEP: Creating a pod to test consume secrets
Nov 14 23:54:48.934: INFO: Waiting up to 5m0s for pod "pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566" in namespace "secrets-3760" to be "success or failure"
Nov 14 23:54:48.943: INFO: Pod "pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566": Phase="Pending", Reason="", readiness=false. Elapsed: 9.258722ms
Nov 14 23:54:50.948: INFO: Pod "pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01432147s
Nov 14 23:54:52.953: INFO: Pod "pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019146606s
STEP: Saw pod success
Nov 14 23:54:52.953: INFO: Pod "pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566" satisfied condition "success or failure"
Nov 14 23:54:52.956: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566 container secret-volume-test: <nil>
STEP: delete the pod
Nov 14 23:54:53.025: INFO: Waiting for pod pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566 to disappear
Nov 14 23:54:53.034: INFO: Pod pod-secrets-2a170005-d60c-4e88-ba2a-30caeae82566 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:54:53.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3760" for this suite.
Nov 14 23:54:59.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:54:59.208: INFO: namespace secrets-3760 deletion completed in 6.169052469s

• [SLOW TEST:10.631 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:54:59.210: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 14 23:54:59.440: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 14 23:54:59.452: INFO: Waiting for terminating namespaces to be deleted...
Nov 14 23:54:59.457: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-0 before test
Nov 14 23:54:59.470: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-hqkrc from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:54:59.470: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 14 23:54:59.470: INFO: sonobuoy-e2e-job-65028defe7584f98 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container e2e ready: true, restart count 0
Nov 14 23:54:59.470: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:54:59.470: INFO: calico-node-ntzpq from kube-system started at 2019-11-14 22:58:21 +0000 UTC (2 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container calico-node ready: true, restart count 0
Nov 14 23:54:59.470: INFO: 	Container install-cni ready: true, restart count 0
Nov 14 23:54:59.470: INFO: node-exporter-w9wqv from prometheus-monitoring started at 2019-11-14 22:58:23 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 14 23:54:59.470: INFO: npd-wvz4s from kube-system started at 2019-11-14 22:58:24 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 14 23:54:59.470: INFO: sonobuoy from sonobuoy started at 2019-11-14 23:02:29 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.470: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 14 23:54:59.470: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-1 before test
Nov 14 23:54:59.506: INFO: node-exporter-hmjtl from prometheus-monitoring started at 2019-11-14 22:56:57 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.506: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 14 23:54:59.507: INFO: calico-node-2mxq7 from kube-system started at 2019-11-14 22:56:57 +0000 UTC (2 container statuses recorded)
Nov 14 23:54:59.507: INFO: 	Container calico-node ready: true, restart count 0
Nov 14 23:54:59.507: INFO: 	Container install-cni ready: true, restart count 0
Nov 14 23:54:59.507: INFO: prometheus-ff4dc5bfd-27sxj from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.507: INFO: 	Container prometheus ready: true, restart count 0
Nov 14 23:54:59.507: INFO: heapster-bbf96fd9d-fnrqj from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.508: INFO: 	Container heapster ready: true, restart count 0
Nov 14 23:54:59.508: INFO: npd-g7tcr from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.508: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 14 23:54:59.508: INFO: grafana-85c5975df9-7hxpg from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.508: INFO: 	Container grafana ready: true, restart count 0
Nov 14 23:54:59.508: INFO: coredns-646fbb9987-g28vb from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.508: INFO: 	Container coredns ready: true, restart count 0
Nov 14 23:54:59.509: INFO: kube-dns-autoscaler-97b76b9b4-grhf6 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.509: INFO: 	Container autoscaler ready: true, restart count 0
Nov 14 23:54:59.509: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-6wwn9 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 14 23:54:59.509: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 14 23:54:59.509: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 14 23:54:59.510: INFO: kubernetes-dashboard-f456bc54b-nl4gf from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.510: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 14 23:54:59.510: INFO: coredns-646fbb9987-c6bz4 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 14 23:54:59.510: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node k8s-100-3imo44lif6er-minion-0
STEP: verifying the node has the label node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.592: INFO: Pod calico-node-2mxq7 requesting resource cpu=250m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.592: INFO: Pod calico-node-ntzpq requesting resource cpu=250m on Node k8s-100-3imo44lif6er-minion-0
Nov 14 23:54:59.592: INFO: Pod coredns-646fbb9987-c6bz4 requesting resource cpu=100m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.593: INFO: Pod coredns-646fbb9987-g28vb requesting resource cpu=100m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.593: INFO: Pod heapster-bbf96fd9d-fnrqj requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.593: INFO: Pod kube-dns-autoscaler-97b76b9b4-grhf6 requesting resource cpu=20m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.593: INFO: Pod kubernetes-dashboard-f456bc54b-nl4gf requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.593: INFO: Pod npd-g7tcr requesting resource cpu=20m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.594: INFO: Pod npd-wvz4s requesting resource cpu=20m on Node k8s-100-3imo44lif6er-minion-0
Nov 14 23:54:59.594: INFO: Pod grafana-85c5975df9-7hxpg requesting resource cpu=100m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.594: INFO: Pod node-exporter-hmjtl requesting resource cpu=10m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.594: INFO: Pod node-exporter-w9wqv requesting resource cpu=10m on Node k8s-100-3imo44lif6er-minion-0
Nov 14 23:54:59.594: INFO: Pod prometheus-ff4dc5bfd-27sxj requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.595: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-0
Nov 14 23:54:59.595: INFO: Pod sonobuoy-e2e-job-65028defe7584f98 requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-0
Nov 14 23:54:59.595: INFO: Pod sonobuoy-systemd-logs-daemon-set-524408101ba247a5-6wwn9 requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-1
Nov 14 23:54:59.595: INFO: Pod sonobuoy-systemd-logs-daemon-set-524408101ba247a5-hqkrc requesting resource cpu=0m on Node k8s-100-3imo44lif6er-minion-0
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39.15d72ca91bdb7467], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9515/filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39 to k8s-100-3imo44lif6er-minion-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39.15d72ca9630c5911], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39.15d72ca979c75fcd], Reason = [Created], Message = [Created container filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39.15d72ca9830a435a], Reason = [Started], Message = [Started container filler-pod-819c7f4c-8a66-44cd-99f1-af8790895d39]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6.15d72ca91c9fa509], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9515/filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6 to k8s-100-3imo44lif6er-minion-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6.15d72ca95f0a1d46], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6.15d72cab63c67756], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6.15d72caba869ad8d], Reason = [Created], Message = [Created container filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6.15d72cabb0023ec2], Reason = [Started], Message = [Started container filler-pod-c3d3be4e-2ae0-4002-a783-84121b7600c6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15d72cabea1dadab], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node k8s-100-3imo44lif6er-minion-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-100-3imo44lif6er-minion-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:55:12.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9515" for this suite.
Nov 14 23:55:18.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:55:18.925: INFO: namespace sched-pred-9515 deletion completed in 6.177216893s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:19.716 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:55:18.930: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 14 23:55:19.194: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:19.223: INFO: Number of nodes with available pods: 0
Nov 14 23:55:19.223: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:20.229: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:20.232: INFO: Number of nodes with available pods: 0
Nov 14 23:55:20.232: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:21.230: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:21.233: INFO: Number of nodes with available pods: 0
Nov 14 23:55:21.233: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:22.229: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:22.232: INFO: Number of nodes with available pods: 0
Nov 14 23:55:22.232: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:23.231: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:23.244: INFO: Number of nodes with available pods: 0
Nov 14 23:55:23.245: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:24.228: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:24.231: INFO: Number of nodes with available pods: 1
Nov 14 23:55:24.231: INFO: Node k8s-100-3imo44lif6er-minion-0 is running more than one daemon pod
Nov 14 23:55:25.229: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:25.232: INFO: Number of nodes with available pods: 2
Nov 14 23:55:25.232: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 14 23:55:25.259: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:25.269: INFO: Number of nodes with available pods: 1
Nov 14 23:55:25.269: INFO: Node k8s-100-3imo44lif6er-minion-1 is running more than one daemon pod
Nov 14 23:55:26.276: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:26.280: INFO: Number of nodes with available pods: 1
Nov 14 23:55:26.281: INFO: Node k8s-100-3imo44lif6er-minion-1 is running more than one daemon pod
Nov 14 23:55:27.275: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:27.280: INFO: Number of nodes with available pods: 1
Nov 14 23:55:27.280: INFO: Node k8s-100-3imo44lif6er-minion-1 is running more than one daemon pod
Nov 14 23:55:28.278: INFO: DaemonSet pods can't tolerate node k8s-100-3imo44lif6er-master-0 with taints [{Key:CriticalAddonsOnly Value:True Effect:NoSchedule TimeAdded:<nil>} {Key:dedicated Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 14 23:55:28.283: INFO: Number of nodes with available pods: 2
Nov 14 23:55:28.283: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9490, will wait for the garbage collector to delete the pods
Nov 14 23:55:28.365: INFO: Deleting DaemonSet.extensions daemon-set took: 16.593664ms
Nov 14 23:55:29.065: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.4772ms
Nov 14 23:55:32.273: INFO: Number of nodes with available pods: 0
Nov 14 23:55:32.274: INFO: Number of running nodes: 0, number of available pods: 0
Nov 14 23:55:32.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9490/daemonsets","resourceVersion":"14650"},"items":null}

Nov 14 23:55:32.280: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9490/pods","resourceVersion":"14650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:55:32.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9490" for this suite.
Nov 14 23:55:38.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:55:38.538: INFO: namespace daemonsets-9490 deletion completed in 6.194193345s

• [SLOW TEST:19.609 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:55:38.539: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-915b5bba-b5ac-43fc-9899-8b9e6aa269da
STEP: Creating a pod to test consume configMaps
Nov 14 23:55:38.712: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf" in namespace "projected-6725" to be "success or failure"
Nov 14 23:55:38.718: INFO: Pod "pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660616ms
Nov 14 23:55:40.722: INFO: Pod "pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008727987s
Nov 14 23:55:42.727: INFO: Pod "pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013808104s
STEP: Saw pod success
Nov 14 23:55:42.727: INFO: Pod "pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf" satisfied condition "success or failure"
Nov 14 23:55:42.730: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:55:42.764: INFO: Waiting for pod pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf to disappear
Nov 14 23:55:42.769: INFO: Pod pod-projected-configmaps-f7bd354e-7127-4c01-8cb1-b6ca3ef094bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:55:42.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6725" for this suite.
Nov 14 23:55:48.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:55:48.950: INFO: namespace projected-6725 deletion completed in 6.176536489s

• [SLOW TEST:10.411 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:55:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Nov 14 23:55:51.275: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:55:51.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1114 23:55:51.275565      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2819" for this suite.
Nov 14 23:55:57.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:55:57.440: INFO: namespace gc-2819 deletion completed in 6.160373595s

• [SLOW TEST:8.488 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:55:57.443: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 14 23:56:02.179: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5943 pod-service-account-15bf8a28-9b31-43e4-8d4b-4cd1ae4fddf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 14 23:56:02.596: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5943 pod-service-account-15bf8a28-9b31-43e4-8d4b-4cd1ae4fddf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 14 23:56:02.947: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5943 pod-service-account-15bf8a28-9b31-43e4-8d4b-4cd1ae4fddf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:56:03.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5943" for this suite.
Nov 14 23:56:09.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:56:09.544: INFO: namespace svcaccounts-5943 deletion completed in 6.237900855s

• [SLOW TEST:12.102 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:56:09.548: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8834
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 14 23:56:09.772: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 14 23:56:31.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.135:8080/dial?request=hostName&protocol=http&host=192.168.2.134&port=8080&tries=1'] Namespace:pod-network-test-8834 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:56:31.887: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:56:32.094: INFO: Waiting for endpoints: map[]
Nov 14 23:56:32.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.135:8080/dial?request=hostName&protocol=http&host=192.168.1.35&port=8080&tries=1'] Namespace:pod-network-test-8834 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 14 23:56:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 14 23:56:32.281: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:56:32.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8834" for this suite.
Nov 14 23:56:54.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:56:54.523: INFO: namespace pod-network-test-8834 deletion completed in 22.237237777s

• [SLOW TEST:44.976 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:56:54.524: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 14 23:56:54.701: INFO: Waiting up to 5m0s for pod "pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24" in namespace "emptydir-7614" to be "success or failure"
Nov 14 23:56:54.718: INFO: Pod "pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24": Phase="Pending", Reason="", readiness=false. Elapsed: 17.610792ms
Nov 14 23:56:56.726: INFO: Pod "pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024675605s
Nov 14 23:56:58.730: INFO: Pod "pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028888019s
STEP: Saw pod success
Nov 14 23:56:58.730: INFO: Pod "pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24" satisfied condition "success or failure"
Nov 14 23:56:58.735: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24 container test-container: <nil>
STEP: delete the pod
Nov 14 23:56:58.771: INFO: Waiting for pod pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24 to disappear
Nov 14 23:56:58.774: INFO: Pod pod-f28ef538-dae1-4a47-bdf4-5fdd3cffda24 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:56:58.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7614" for this suite.
Nov 14 23:57:04.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:57:04.947: INFO: namespace emptydir-7614 deletion completed in 6.167862935s

• [SLOW TEST:10.423 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:57:04.950: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 14 23:57:05.121: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 14 23:57:14.163: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:57:14.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9377" for this suite.
Nov 14 23:57:20.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:57:20.377: INFO: namespace pods-9377 deletion completed in 6.206463047s

• [SLOW TEST:15.427 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:57:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:57:20.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4025" for this suite.
Nov 14 23:57:26.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:57:26.867: INFO: namespace services-4025 deletion completed in 6.270041616s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.488 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:57:26.872: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-7e902253-748d-4d52-aa01-3149116d87b9
STEP: Creating a pod to test consume configMaps
Nov 14 23:57:27.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2" in namespace "configmap-9297" to be "success or failure"
Nov 14 23:57:27.060: INFO: Pod "pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.961007ms
Nov 14 23:57:29.068: INFO: Pod "pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017082678s
Nov 14 23:57:31.075: INFO: Pod "pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024350851s
STEP: Saw pod success
Nov 14 23:57:31.075: INFO: Pod "pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2" satisfied condition "success or failure"
Nov 14 23:57:31.082: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:57:31.130: INFO: Waiting for pod pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2 to disappear
Nov 14 23:57:31.160: INFO: Pod pod-configmaps-d2a5fc5d-b721-4431-84fd-e3f3b328a1a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:57:31.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9297" for this suite.
Nov 14 23:57:37.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:57:37.343: INFO: namespace configmap-9297 deletion completed in 6.176138771s

• [SLOW TEST:10.471 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:57:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:57:41.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1815" for this suite.
Nov 14 23:58:23.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:58:23.787: INFO: namespace kubelet-test-1815 deletion completed in 42.239891085s

• [SLOW TEST:46.438 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:58:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 14 23:58:28.027: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 14 23:58:28.039: INFO: Pod pod-with-prestop-http-hook still exists
Nov 14 23:58:30.040: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 14 23:58:30.045: INFO: Pod pod-with-prestop-http-hook still exists
Nov 14 23:58:32.040: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 14 23:58:32.045: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:58:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9366" for this suite.
Nov 14 23:58:54.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:58:54.305: INFO: namespace container-lifecycle-hook-9366 deletion completed in 22.235597074s

• [SLOW TEST:30.515 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:58:54.311: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:58:58.577: INFO: DNS probes using dns-test-5e8a3e14-9998-4b95-bcc2-45860d4731a3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:59:04.685: INFO: DNS probes using dns-test-6c1b4678-f984-4158-ac29-47aca9a0883c succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2977.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2977.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 14 23:59:08.820: INFO: DNS probes using dns-test-10724350-9a2c-4ef4-8c9d-6b6643dc6d6b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:59:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2977" for this suite.
Nov 14 23:59:14.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:59:15.077: INFO: namespace dns-2977 deletion completed in 6.218577826s

• [SLOW TEST:20.767 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:59:15.079: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-32b8d89f-a5a8-4f44-830f-abe418760a02
STEP: Creating a pod to test consume configMaps
Nov 14 23:59:15.278: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db" in namespace "projected-6824" to be "success or failure"
Nov 14 23:59:15.290: INFO: Pod "pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.526952ms
Nov 14 23:59:17.296: INFO: Pod "pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018670411s
Nov 14 23:59:19.301: INFO: Pod "pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023407814s
STEP: Saw pod success
Nov 14 23:59:19.301: INFO: Pod "pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db" satisfied condition "success or failure"
Nov 14 23:59:19.304: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 14 23:59:19.346: INFO: Waiting for pod pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db to disappear
Nov 14 23:59:19.351: INFO: Pod pod-projected-configmaps-7307a258-5e24-4294-a5b3-1d37e727c4db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:59:19.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6824" for this suite.
Nov 14 23:59:25.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:59:25.627: INFO: namespace projected-6824 deletion completed in 6.266541538s

• [SLOW TEST:10.548 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:59:25.634: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 14 23:59:25.855: INFO: Waiting up to 5m0s for pod "pod-45aa6078-2f19-4e1a-9f59-92737915efc5" in namespace "emptydir-3576" to be "success or failure"
Nov 14 23:59:25.867: INFO: Pod "pod-45aa6078-2f19-4e1a-9f59-92737915efc5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.317431ms
Nov 14 23:59:27.874: INFO: Pod "pod-45aa6078-2f19-4e1a-9f59-92737915efc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018791716s
Nov 14 23:59:29.878: INFO: Pod "pod-45aa6078-2f19-4e1a-9f59-92737915efc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02352835s
STEP: Saw pod success
Nov 14 23:59:29.879: INFO: Pod "pod-45aa6078-2f19-4e1a-9f59-92737915efc5" satisfied condition "success or failure"
Nov 14 23:59:29.882: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-45aa6078-2f19-4e1a-9f59-92737915efc5 container test-container: <nil>
STEP: delete the pod
Nov 14 23:59:29.914: INFO: Waiting for pod pod-45aa6078-2f19-4e1a-9f59-92737915efc5 to disappear
Nov 14 23:59:29.926: INFO: Pod pod-45aa6078-2f19-4e1a-9f59-92737915efc5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:59:29.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3576" for this suite.
Nov 14 23:59:35.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:59:36.170: INFO: namespace emptydir-3576 deletion completed in 6.235676338s

• [SLOW TEST:10.537 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:59:36.177: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 14 23:59:36.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170" in namespace "projected-3439" to be "success or failure"
Nov 14 23:59:36.439: INFO: Pod "downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170": Phase="Pending", Reason="", readiness=false. Elapsed: 15.296616ms
Nov 14 23:59:38.447: INFO: Pod "downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023109903s
Nov 14 23:59:40.455: INFO: Pod "downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030648295s
STEP: Saw pod success
Nov 14 23:59:40.455: INFO: Pod "downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170" satisfied condition "success or failure"
Nov 14 23:59:40.459: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170 container client-container: <nil>
STEP: delete the pod
Nov 14 23:59:40.493: INFO: Waiting for pod downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170 to disappear
Nov 14 23:59:40.500: INFO: Pod downwardapi-volume-5db9b6a5-6f01-40b3-b8ca-41ba8703a170 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:59:40.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3439" for this suite.
Nov 14 23:59:46.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:59:46.675: INFO: namespace projected-3439 deletion completed in 6.169299499s

• [SLOW TEST:10.499 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:59:46.678: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8221
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 14 23:59:46.950: INFO: Waiting up to 5m0s for pod "pod-c534c39d-81d7-4e9b-807d-b95f5bae9623" in namespace "emptydir-8221" to be "success or failure"
Nov 14 23:59:46.961: INFO: Pod "pod-c534c39d-81d7-4e9b-807d-b95f5bae9623": Phase="Pending", Reason="", readiness=false. Elapsed: 11.043022ms
Nov 14 23:59:48.966: INFO: Pod "pod-c534c39d-81d7-4e9b-807d-b95f5bae9623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015879355s
Nov 14 23:59:50.970: INFO: Pod "pod-c534c39d-81d7-4e9b-807d-b95f5bae9623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020206608s
STEP: Saw pod success
Nov 14 23:59:50.970: INFO: Pod "pod-c534c39d-81d7-4e9b-807d-b95f5bae9623" satisfied condition "success or failure"
Nov 14 23:59:50.972: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-c534c39d-81d7-4e9b-807d-b95f5bae9623 container test-container: <nil>
STEP: delete the pod
Nov 14 23:59:51.002: INFO: Waiting for pod pod-c534c39d-81d7-4e9b-807d-b95f5bae9623 to disappear
Nov 14 23:59:51.009: INFO: Pod pod-c534c39d-81d7-4e9b-807d-b95f5bae9623 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 14 23:59:51.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8221" for this suite.
Nov 14 23:59:57.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 14 23:59:57.188: INFO: namespace emptydir-8221 deletion completed in 6.173666717s

• [SLOW TEST:10.510 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 14 23:59:57.194: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-cf07aaab-4290-4f11-aed2-bc6b1b3932b4
STEP: Creating a pod to test consume configMaps
Nov 14 23:59:57.368: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b" in namespace "projected-4842" to be "success or failure"
Nov 14 23:59:57.421: INFO: Pod "pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b": Phase="Pending", Reason="", readiness=false. Elapsed: 53.045026ms
Nov 14 23:59:59.429: INFO: Pod "pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061131306s
Nov 15 00:00:01.434: INFO: Pod "pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065617148s
STEP: Saw pod success
Nov 15 00:00:01.435: INFO: Pod "pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b" satisfied condition "success or failure"
Nov 15 00:00:01.439: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 15 00:00:01.471: INFO: Waiting for pod pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b to disappear
Nov 15 00:00:01.487: INFO: Pod pod-projected-configmaps-d08fec97-9a6d-4846-8433-79b6eec3a83b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:00:01.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4842" for this suite.
Nov 15 00:00:07.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:00:07.698: INFO: namespace projected-4842 deletion completed in 6.205616831s

• [SLOW TEST:10.504 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:00:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Nov 15 00:00:07.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-6168'
Nov 15 00:00:08.685: INFO: stderr: ""
Nov 15 00:00:08.686: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 15 00:00:09.693: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:09.693: INFO: Found 0 / 1
Nov 15 00:00:10.689: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:10.689: INFO: Found 0 / 1
Nov 15 00:00:11.690: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:11.690: INFO: Found 0 / 1
Nov 15 00:00:12.690: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:12.690: INFO: Found 1 / 1
Nov 15 00:00:12.690: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 15 00:00:12.693: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:12.693: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 00:00:12.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 patch pod redis-master-fxvlv --namespace=kubectl-6168 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 15 00:00:12.855: INFO: stderr: ""
Nov 15 00:00:12.855: INFO: stdout: "pod/redis-master-fxvlv patched\n"
STEP: checking annotations
Nov 15 00:00:12.860: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:00:12.860: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:00:12.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6168" for this suite.
Nov 15 00:00:34.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:00:35.089: INFO: namespace kubectl-6168 deletion completed in 22.222814985s

• [SLOW TEST:27.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:00:35.091: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-da8bd4a2-ad29-469c-85b3-0add4d925729
STEP: Creating a pod to test consume secrets
Nov 15 00:00:35.258: INFO: Waiting up to 5m0s for pod "pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f" in namespace "secrets-4995" to be "success or failure"
Nov 15 00:00:35.264: INFO: Pod "pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606232ms
Nov 15 00:00:37.270: INFO: Pod "pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01154479s
Nov 15 00:00:39.277: INFO: Pod "pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01805088s
STEP: Saw pod success
Nov 15 00:00:39.277: INFO: Pod "pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f" satisfied condition "success or failure"
Nov 15 00:00:39.280: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f container secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:00:39.316: INFO: Waiting for pod pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f to disappear
Nov 15 00:00:39.335: INFO: Pod pod-secrets-dcf9c9c5-39c8-4c55-9391-609e3d32de0f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:00:39.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4995" for this suite.
Nov 15 00:00:45.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:00:45.545: INFO: namespace secrets-4995 deletion completed in 6.183850679s

• [SLOW TEST:10.454 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:00:45.548: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-407079b9-6a6c-4170-b000-e502066db4ee
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:00:45.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8649" for this suite.
Nov 15 00:00:53.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:00:54.025: INFO: namespace secrets-8649 deletion completed in 8.249690346s

• [SLOW TEST:8.478 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:00:54.031: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:00:58.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2890" for this suite.
Nov 15 00:01:04.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:01:04.411: INFO: namespace kubelet-test-2890 deletion completed in 6.191257691s

• [SLOW TEST:10.380 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:01:04.418: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:01:04.641: INFO: Creating deployment "test-recreate-deployment"
Nov 15 00:01:04.648: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 15 00:01:04.677: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 15 00:01:06.685: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 15 00:01:06.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372864, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372864, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372864, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709372864, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:01:08.694: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 15 00:01:08.704: INFO: Updating deployment test-recreate-deployment
Nov 15 00:01:08.704: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 15 00:01:08.808: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7682,SelfLink:/apis/apps/v1/namespaces/deployment-7682/deployments/test-recreate-deployment,UID:9e76b572-a8d5-4a2e-aa48-79b9e353c61c,ResourceVersion:16140,Generation:2,CreationTimestamp:2019-11-15 00:01:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-11-15 00:01:08 +0000 UTC 2019-11-15 00:01:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-11-15 00:01:08 +0000 UTC 2019-11-15 00:01:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Nov 15 00:01:08.812: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-7682,SelfLink:/apis/apps/v1/namespaces/deployment-7682/replicasets/test-recreate-deployment-5c8c9cc69d,UID:384d717f-5c2b-4e12-b485-cd7dceb3b5c1,ResourceVersion:16136,Generation:1,CreationTimestamp:2019-11-15 00:01:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9e76b572-a8d5-4a2e-aa48-79b9e353c61c 0xc002b53a67 0xc002b53a68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 15 00:01:08.813: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 15 00:01:08.813: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-7682,SelfLink:/apis/apps/v1/namespaces/deployment-7682/replicasets/test-recreate-deployment-6df85df6b9,UID:1df741ec-1c7e-4509-8b74-a6d0544129fe,ResourceVersion:16129,Generation:2,CreationTimestamp:2019-11-15 00:01:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9e76b572-a8d5-4a2e-aa48-79b9e353c61c 0xc0006960f7 0xc0006960f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 15 00:01:08.821: INFO: Pod "test-recreate-deployment-5c8c9cc69d-6hrd2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-6hrd2,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-7682,SelfLink:/api/v1/namespaces/deployment-7682/pods/test-recreate-deployment-5c8c9cc69d-6hrd2,UID:8ed17ee6-0f9d-426f-8c84-cc7edec2736e,ResourceVersion:16141,Generation:0,CreationTimestamp:2019-11-15 00:01:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 384d717f-5c2b-4e12-b485-cd7dceb3b5c1 0xc0006977b7 0xc0006977b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sttz6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sttz6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sttz6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000697c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000697c20}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:01:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:01:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:01:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:01:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:,StartTime:2019-11-15 00:01:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:01:08.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7682" for this suite.
Nov 15 00:01:14.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:01:15.018: INFO: namespace deployment-7682 deletion completed in 6.190721866s

• [SLOW TEST:10.601 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:01:15.034: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7839/configmap-test-68766be3-e28e-4ed9-a9c0-0ac7f23b8229
STEP: Creating a pod to test consume configMaps
Nov 15 00:01:15.224: INFO: Waiting up to 5m0s for pod "pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97" in namespace "configmap-7839" to be "success or failure"
Nov 15 00:01:15.232: INFO: Pod "pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.730301ms
Nov 15 00:01:17.237: INFO: Pod "pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013272431s
Nov 15 00:01:19.241: INFO: Pod "pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017519809s
STEP: Saw pod success
Nov 15 00:01:19.241: INFO: Pod "pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97" satisfied condition "success or failure"
Nov 15 00:01:19.244: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97 container env-test: <nil>
STEP: delete the pod
Nov 15 00:01:19.279: INFO: Waiting for pod pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97 to disappear
Nov 15 00:01:19.288: INFO: Pod pod-configmaps-06a864fd-1eb8-4fe1-a348-e750e2547b97 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:01:19.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7839" for this suite.
Nov 15 00:01:25.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:01:25.458: INFO: namespace configmap-7839 deletion completed in 6.164315701s

• [SLOW TEST:10.425 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:01:25.460: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 15 00:01:25.623: INFO: Waiting up to 5m0s for pod "pod-70545df1-d30a-4d4e-b4a6-8666603051dc" in namespace "emptydir-2289" to be "success or failure"
Nov 15 00:01:25.630: INFO: Pod "pod-70545df1-d30a-4d4e-b4a6-8666603051dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.938292ms
Nov 15 00:01:27.635: INFO: Pod "pod-70545df1-d30a-4d4e-b4a6-8666603051dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012353666s
Nov 15 00:01:29.640: INFO: Pod "pod-70545df1-d30a-4d4e-b4a6-8666603051dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016573774s
STEP: Saw pod success
Nov 15 00:01:29.640: INFO: Pod "pod-70545df1-d30a-4d4e-b4a6-8666603051dc" satisfied condition "success or failure"
Nov 15 00:01:29.642: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-70545df1-d30a-4d4e-b4a6-8666603051dc container test-container: <nil>
STEP: delete the pod
Nov 15 00:01:29.690: INFO: Waiting for pod pod-70545df1-d30a-4d4e-b4a6-8666603051dc to disappear
Nov 15 00:01:29.694: INFO: Pod pod-70545df1-d30a-4d4e-b4a6-8666603051dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:01:29.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2289" for this suite.
Nov 15 00:01:35.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:01:35.847: INFO: namespace emptydir-2289 deletion completed in 6.148293938s

• [SLOW TEST:10.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:01:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 15 00:01:36.021: INFO: Waiting up to 5m0s for pod "downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb" in namespace "downward-api-5102" to be "success or failure"
Nov 15 00:01:36.034: INFO: Pod "downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.580821ms
Nov 15 00:01:38.040: INFO: Pod "downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018390711s
Nov 15 00:01:40.044: INFO: Pod "downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02257106s
STEP: Saw pod success
Nov 15 00:01:40.044: INFO: Pod "downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb" satisfied condition "success or failure"
Nov 15 00:01:40.047: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb container dapi-container: <nil>
STEP: delete the pod
Nov 15 00:01:40.082: INFO: Waiting for pod downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb to disappear
Nov 15 00:01:40.086: INFO: Pod downward-api-a91b3d81-5eef-4c99-bf48-d83740db0fcb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:01:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5102" for this suite.
Nov 15 00:01:46.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:01:46.267: INFO: namespace downward-api-5102 deletion completed in 6.176437464s

• [SLOW TEST:10.417 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:01:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1818
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-c237eefd-c120-4f1b-882b-44dd5d826ecc
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:01:50.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1818" for this suite.
Nov 15 00:02:12.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:02:12.703: INFO: namespace configmap-1818 deletion completed in 22.178851461s

• [SLOW TEST:26.431 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:02:12.703: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2441
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-57f848c7-4ee8-405b-b5ef-b0ed718d1885
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-57f848c7-4ee8-405b-b5ef-b0ed718d1885
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:03:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2441" for this suite.
Nov 15 00:03:47.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:03:48.067: INFO: namespace configmap-2441 deletion completed in 22.30842146s

• [SLOW TEST:95.364 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:03:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 15 00:03:50.813: INFO: Successfully updated pod "labelsupdatee9641204-3b8a-4833-8e54-386557bac9fc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:03:52.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5243" for this suite.
Nov 15 00:04:14.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:04:15.066: INFO: namespace downward-api-5243 deletion completed in 22.224381097s

• [SLOW TEST:26.992 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:04:15.069: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-113bc645-eaba-4b6f-9ecb-f94724b68a3f
STEP: Creating a pod to test consume configMaps
Nov 15 00:04:15.261: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5" in namespace "projected-3606" to be "success or failure"
Nov 15 00:04:15.274: INFO: Pod "pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.904536ms
Nov 15 00:04:17.278: INFO: Pod "pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016507736s
Nov 15 00:04:19.284: INFO: Pod "pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022512023s
STEP: Saw pod success
Nov 15 00:04:19.284: INFO: Pod "pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5" satisfied condition "success or failure"
Nov 15 00:04:19.291: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 15 00:04:19.336: INFO: Waiting for pod pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5 to disappear
Nov 15 00:04:19.345: INFO: Pod pod-projected-configmaps-2e7b4a41-568e-462d-8b29-1fc237e731a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:04:19.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3606" for this suite.
Nov 15 00:04:25.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:04:25.583: INFO: namespace projected-3606 deletion completed in 6.228146905s

• [SLOW TEST:10.515 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:04:25.585: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 15 00:04:25.750: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:04:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7216" for this suite.
Nov 15 00:04:52.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:04:52.888: INFO: namespace init-container-7216 deletion completed in 22.18891091s

• [SLOW TEST:27.304 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:04:52.895: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5808
STEP: Creating secret with name secret-test-eb0a68d9-0886-44f1-a4e4-64770e0ed1ab
STEP: Creating a pod to test consume secrets
Nov 15 00:04:53.512: INFO: Waiting up to 5m0s for pod "pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8" in namespace "secrets-7409" to be "success or failure"
Nov 15 00:04:53.518: INFO: Pod "pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.573591ms
Nov 15 00:04:55.523: INFO: Pod "pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010810038s
Nov 15 00:04:57.528: INFO: Pod "pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015917402s
STEP: Saw pod success
Nov 15 00:04:57.528: INFO: Pod "pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8" satisfied condition "success or failure"
Nov 15 00:04:57.531: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8 container secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:04:57.567: INFO: Waiting for pod pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8 to disappear
Nov 15 00:04:57.571: INFO: Pod pod-secrets-9a04230a-9ae5-4aac-88d6-45531b9eafb8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:04:57.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7409" for this suite.
Nov 15 00:05:03.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:05:03.837: INFO: namespace secrets-7409 deletion completed in 6.259430117s
STEP: Destroying namespace "secret-namespace-5808" for this suite.
Nov 15 00:05:09.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:05:10.057: INFO: namespace secret-namespace-5808 deletion completed in 6.219616786s

• [SLOW TEST:17.163 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:05:10.063: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c4653cb5-8485-439b-82cc-e0b165249090
STEP: Creating a pod to test consume configMaps
Nov 15 00:05:10.345: INFO: Waiting up to 5m0s for pod "pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f" in namespace "configmap-9016" to be "success or failure"
Nov 15 00:05:10.350: INFO: Pod "pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.318985ms
Nov 15 00:05:12.359: INFO: Pod "pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013467889s
Nov 15 00:05:14.372: INFO: Pod "pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026657808s
STEP: Saw pod success
Nov 15 00:05:14.372: INFO: Pod "pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f" satisfied condition "success or failure"
Nov 15 00:05:14.378: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f container configmap-volume-test: <nil>
STEP: delete the pod
Nov 15 00:05:14.416: INFO: Waiting for pod pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f to disappear
Nov 15 00:05:14.431: INFO: Pod pod-configmaps-008e2d0f-5a39-47f2-9d1b-bb1fb1cb445f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:05:14.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9016" for this suite.
Nov 15 00:05:20.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:05:20.626: INFO: namespace configmap-9016 deletion completed in 6.186491406s

• [SLOW TEST:10.562 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:05:20.630: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:05:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-969" for this suite.
Nov 15 00:06:05.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:06:05.903: INFO: namespace container-runtime-969 deletion completed in 6.200447455s

• [SLOW TEST:45.273 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:06:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1115 00:06:46.143999      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 15 00:06:46.144: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:06:46.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4214" for this suite.
Nov 15 00:06:52.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:06:52.442: INFO: namespace gc-4214 deletion completed in 6.294001221s

• [SLOW TEST:46.537 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:06:52.444: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6668
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6668
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6668
Nov 15 00:06:52.756: INFO: Found 0 stateful pods, waiting for 1
Nov 15 00:07:02.763: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 15 00:07:02.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:07:03.835: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:07:03.835: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:07:03.835: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:07:03.842: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 00:07:13.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:07:13.851: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:07:13.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999419s
Nov 15 00:07:14.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.967754341s
Nov 15 00:07:15.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.959181645s
Nov 15 00:07:16.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953995481s
Nov 15 00:07:17.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948282224s
Nov 15 00:07:19.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.943335586s
Nov 15 00:07:20.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.862446383s
Nov 15 00:07:21.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.856564603s
Nov 15 00:07:22.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.848738605s
Nov 15 00:07:23.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 842.733859ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6668
Nov 15 00:07:24.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:07:24.489: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:07:24.489: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:07:24.489: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:07:24.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:07:24.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 00:07:24.916: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:07:24.916: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:07:24.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:07:25.341: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 00:07:25.341: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:07:25.341: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:07:25.347: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:07:25.347: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:07:25.347: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 15 00:07:25.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:07:25.736: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:07:25.736: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:07:25.736: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:07:25.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:07:26.103: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:07:26.103: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:07:26.103: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:07:26.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:07:26.732: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:07:26.732: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:07:26.732: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:07:26.732: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:07:26.735: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 15 00:07:36.933: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:07:36.934: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:07:36.934: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:07:37.789: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:37.789: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:37.789: INFO: ss-1  k8s-100-3imo44lif6er-minion-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:37.789: INFO: ss-2  k8s-100-3imo44lif6er-minion-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:37.789: INFO: 
Nov 15 00:07:37.789: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 00:07:38.794: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:38.794: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:38.794: INFO: ss-1  k8s-100-3imo44lif6er-minion-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:38.794: INFO: ss-2  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:38.794: INFO: 
Nov 15 00:07:38.794: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 00:07:39.802: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:39.802: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:39.802: INFO: ss-1  k8s-100-3imo44lif6er-minion-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:39.802: INFO: ss-2  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:39.802: INFO: 
Nov 15 00:07:39.802: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 00:07:40.808: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:40.808: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:40.808: INFO: ss-1  k8s-100-3imo44lif6er-minion-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:40.809: INFO: ss-2  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:40.809: INFO: 
Nov 15 00:07:40.809: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 00:07:41.816: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:41.817: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:41.817: INFO: ss-2  k8s-100-3imo44lif6er-minion-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:13 +0000 UTC  }]
Nov 15 00:07:41.817: INFO: 
Nov 15 00:07:41.817: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 15 00:07:42.824: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:42.824: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:42.824: INFO: 
Nov 15 00:07:42.824: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 15 00:07:43.830: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:43.830: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:43.830: INFO: 
Nov 15 00:07:43.830: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 15 00:07:44.836: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:44.836: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:44.836: INFO: 
Nov 15 00:07:44.836: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 15 00:07:45.840: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:45.840: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:45.841: INFO: 
Nov 15 00:07:45.841: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 15 00:07:46.846: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Nov 15 00:07:46.847: INFO: ss-0  k8s-100-3imo44lif6er-minion-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:07:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:06:52 +0000 UTC  }]
Nov 15 00:07:46.847: INFO: 
Nov 15 00:07:46.847: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6668
Nov 15 00:07:47.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:07:48.227: INFO: rc: 1
Nov 15 00:07:48.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001db6210 exit status 1 <nil> <nil> true [0xc002d25690 0xc002d256a8 0xc002d256c0] [0xc002d25690 0xc002d256a8 0xc002d256c0] [0xc002d256a0 0xc002d256b8] [0x9d17b0 0x9d17b0] 0xc003411440 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Nov 15 00:07:58.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:07:58.408: INFO: rc: 1
Nov 15 00:07:58.409: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001db65d0 exit status 1 <nil> <nil> true [0xc002d256c8 0xc002d256e0 0xc002d256f8] [0xc002d256c8 0xc002d256e0 0xc002d256f8] [0xc002d256d8 0xc002d256f0] [0x9d17b0 0x9d17b0] 0xc003411980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:08.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:08.582: INFO: rc: 1
Nov 15 00:08:08.582: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a46240 exit status 1 <nil> <nil> true [0xc003ff2700 0xc003ff2718 0xc003ff2730] [0xc003ff2700 0xc003ff2718 0xc003ff2730] [0xc003ff2710 0xc003ff2728] [0x9d17b0 0x9d17b0] 0xc002ceca20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:18.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:18.735: INFO: rc: 1
Nov 15 00:08:18.735: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001db6ba0 exit status 1 <nil> <nil> true [0xc002d25700 0xc002d25718 0xc002d25730] [0xc002d25700 0xc002d25718 0xc002d25730] [0xc002d25710 0xc002d25728] [0x9d17b0 0x9d17b0] 0xc003411ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:28.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:28.903: INFO: rc: 1
Nov 15 00:08:28.903: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76510 exit status 1 <nil> <nil> true [0xc002d24020 0xc002d24060 0xc002d240a8] [0xc002d24020 0xc002d24060 0xc002d240a8] [0xc002d24048 0xc002d24090] [0x9d17b0 0x9d17b0] 0xc0036462a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:38.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:39.074: INFO: rc: 1
Nov 15 00:08:39.074: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76b40 exit status 1 <nil> <nil> true [0xc002d240d0 0xc002d240f8 0xc002d24110] [0xc002d240d0 0xc002d240f8 0xc002d24110] [0xc002d240f0 0xc002d24108] [0x9d17b0 0x9d17b0] 0xc003646780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:49.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:49.232: INFO: rc: 1
Nov 15 00:08:49.233: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76fc0 exit status 1 <nil> <nil> true [0xc002d24128 0xc002d24168 0xc002d24180] [0xc002d24128 0xc002d24168 0xc002d24180] [0xc002d24160 0xc002d24178] [0x9d17b0 0x9d17b0] 0xc003646cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:08:59.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:08:59.393: INFO: rc: 1
Nov 15 00:08:59.393: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002dd0390 exit status 1 <nil> <nil> true [0xc003ff2000 0xc003ff2018 0xc003ff2030] [0xc003ff2000 0xc003ff2018 0xc003ff2030] [0xc003ff2010 0xc003ff2028] [0x9d17b0 0x9d17b0] 0xc003120360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:09:09.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:09:09.573: INFO: rc: 1
Nov 15 00:09:09.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002dd0750 exit status 1 <nil> <nil> true [0xc003ff2038 0xc003ff2050 0xc003ff2068] [0xc003ff2038 0xc003ff2050 0xc003ff2068] [0xc003ff2048 0xc003ff2060] [0x9d17b0 0x9d17b0] 0xc0031206c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:09:19.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:09:19.743: INFO: rc: 1
Nov 15 00:09:19.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77380 exit status 1 <nil> <nil> true [0xc002d24198 0xc002d241c0 0xc002d241d8] [0xc002d24198 0xc002d241c0 0xc002d241d8] [0xc002d241b8 0xc002d241d0] [0x9d17b0 0x9d17b0] 0xc003647020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:09:29.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:09:29.919: INFO: rc: 1
Nov 15 00:09:29.920: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e777a0 exit status 1 <nil> <nil> true [0xc002d241f0 0xc002d24238 0xc002d24260] [0xc002d241f0 0xc002d24238 0xc002d24260] [0xc002d24228 0xc002d24258] [0x9d17b0 0x9d17b0] 0xc003647380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:09:39.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:09:40.069: INFO: rc: 1
Nov 15 00:09:40.069: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77b60 exit status 1 <nil> <nil> true [0xc002d24278 0xc002d242b8 0xc002d242e8] [0xc002d24278 0xc002d242b8 0xc002d242e8] [0xc002d242b0 0xc002d242c8] [0x9d17b0 0x9d17b0] 0xc0036476e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:09:50.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:09:50.221: INFO: rc: 1
Nov 15 00:09:50.221: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77f80 exit status 1 <nil> <nil> true [0xc002d24300 0xc002d24340 0xc002d24368] [0xc002d24300 0xc002d24340 0xc002d24368] [0xc002d24320 0xc002d24360] [0x9d17b0 0x9d17b0] 0xc003647a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:00.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:00.403: INFO: rc: 1
Nov 15 00:10:00.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003056570 exit status 1 <nil> <nil> true [0xc002d24370 0xc002d24398 0xc002d243f0] [0xc002d24370 0xc002d24398 0xc002d243f0] [0xc002d24380 0xc002d243d8] [0x9d17b0 0x9d17b0] 0xc003647f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:10.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:10.534: INFO: rc: 1
Nov 15 00:10:10.535: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003056930 exit status 1 <nil> <nil> true [0xc002d243f8 0xc002d24410 0xc002d24450] [0xc002d243f8 0xc002d24410 0xc002d24450] [0xc002d24408 0xc002d24438] [0x9d17b0 0x9d17b0] 0xc00321a840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:20.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:20.674: INFO: rc: 1
Nov 15 00:10:20.674: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003056cc0 exit status 1 <nil> <nil> true [0xc002d24458 0xc002d24470 0xc002d244a0] [0xc002d24458 0xc002d24470 0xc002d244a0] [0xc002d24468 0xc002d24488] [0x9d17b0 0x9d17b0] 0xc00321afc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:30.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:30.825: INFO: rc: 1
Nov 15 00:10:30.826: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76540 exit status 1 <nil> <nil> true [0xc003ff2008 0xc003ff2020 0xc003ff2038] [0xc003ff2008 0xc003ff2020 0xc003ff2038] [0xc003ff2018 0xc003ff2030] [0x9d17b0 0x9d17b0] 0xc0036462a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:40.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:40.981: INFO: rc: 1
Nov 15 00:10:40.981: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002dd0360 exit status 1 <nil> <nil> true [0xc002d24008 0xc002d24048 0xc002d24090] [0xc002d24008 0xc002d24048 0xc002d24090] [0xc002d24028 0xc002d24070] [0x9d17b0 0x9d17b0] 0xc003120360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:10:50.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:10:51.142: INFO: rc: 1
Nov 15 00:10:51.143: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002dd0780 exit status 1 <nil> <nil> true [0xc002d240a8 0xc002d240f0 0xc002d24108] [0xc002d240a8 0xc002d240f0 0xc002d24108] [0xc002d240e8 0xc002d24100] [0x9d17b0 0x9d17b0] 0xc0031206c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:01.289: INFO: rc: 1
Nov 15 00:11:01.289: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76c00 exit status 1 <nil> <nil> true [0xc003ff2040 0xc003ff2058 0xc003ff2070] [0xc003ff2040 0xc003ff2058 0xc003ff2070] [0xc003ff2050 0xc003ff2068] [0x9d17b0 0x9d17b0] 0xc003646780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:11.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:11.428: INFO: rc: 1
Nov 15 00:11:11.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77080 exit status 1 <nil> <nil> true [0xc003ff2078 0xc003ff2090 0xc003ff20a8] [0xc003ff2078 0xc003ff2090 0xc003ff20a8] [0xc003ff2088 0xc003ff20a0] [0x9d17b0 0x9d17b0] 0xc003646cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:21.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:21.583: INFO: rc: 1
Nov 15 00:11:21.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77470 exit status 1 <nil> <nil> true [0xc003ff20b0 0xc003ff20c8 0xc003ff20e0] [0xc003ff20b0 0xc003ff20c8 0xc003ff20e0] [0xc003ff20c0 0xc003ff20d8] [0x9d17b0 0x9d17b0] 0xc003647020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:31.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:31.742: INFO: rc: 1
Nov 15 00:11:31.742: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002dd1440 exit status 1 <nil> <nil> true [0xc002d24110 0xc002d24160 0xc002d24178] [0xc002d24110 0xc002d24160 0xc002d24178] [0xc002d24148 0xc002d24170] [0x9d17b0 0x9d17b0] 0xc003120a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:41.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:41.892: INFO: rc: 1
Nov 15 00:11:41.892: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77890 exit status 1 <nil> <nil> true [0xc003ff20e8 0xc003ff2108 0xc003ff2120] [0xc003ff20e8 0xc003ff2108 0xc003ff2120] [0xc003ff2100 0xc003ff2118] [0x9d17b0 0x9d17b0] 0xc003647380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:11:51.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:11:52.030: INFO: rc: 1
Nov 15 00:11:52.031: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e77c80 exit status 1 <nil> <nil> true [0xc003ff2128 0xc003ff2140 0xc003ff2158] [0xc003ff2128 0xc003ff2140 0xc003ff2158] [0xc003ff2138 0xc003ff2150] [0x9d17b0 0x9d17b0] 0xc0036476e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:02.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:02.192: INFO: rc: 1
Nov 15 00:12:02.193: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030560f0 exit status 1 <nil> <nil> true [0xc003ff2160 0xc003ff2178 0xc003ff2190] [0xc003ff2160 0xc003ff2178 0xc003ff2190] [0xc003ff2170 0xc003ff2188] [0x9d17b0 0x9d17b0] 0xc003647a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:12.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:12.332: INFO: rc: 1
Nov 15 00:12:12.332: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003056690 exit status 1 <nil> <nil> true [0xc003ff2198 0xc003ff21b0 0xc003ff21c8] [0xc003ff2198 0xc003ff21b0 0xc003ff21c8] [0xc003ff21a8 0xc003ff21c0] [0x9d17b0 0x9d17b0] 0xc003647f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:22.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:22.480: INFO: rc: 1
Nov 15 00:12:22.480: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003056a80 exit status 1 <nil> <nil> true [0xc003ff21d0 0xc003ff21e8 0xc003ff2200] [0xc003ff21d0 0xc003ff21e8 0xc003ff2200] [0xc003ff21e0 0xc003ff21f8] [0x9d17b0 0x9d17b0] 0xc00321a840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:32.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:32.650: INFO: rc: 1
Nov 15 00:12:32.651: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76510 exit status 1 <nil> <nil> true [0xc003ff2008 0xc003ff2020 0xc003ff2038] [0xc003ff2008 0xc003ff2020 0xc003ff2038] [0xc003ff2018 0xc003ff2030] [0x9d17b0 0x9d17b0] 0xc0036462a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:42.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:42.780: INFO: rc: 1
Nov 15 00:12:42.780: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e76b70 exit status 1 <nil> <nil> true [0xc003ff2040 0xc003ff2058 0xc003ff2070] [0xc003ff2040 0xc003ff2058 0xc003ff2070] [0xc003ff2050 0xc003ff2068] [0x9d17b0 0x9d17b0] 0xc003646780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 15 00:12:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-6668 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:12:52.998: INFO: rc: 1
Nov 15 00:12:52.999: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Nov 15 00:12:52.999: INFO: Scaling statefulset ss to 0
Nov 15 00:12:53.030: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 15 00:12:53.035: INFO: Deleting all statefulset in ns statefulset-6668
Nov 15 00:12:53.051: INFO: Scaling statefulset ss to 0
Nov 15 00:12:53.074: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:12:53.079: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:12:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6668" for this suite.
Nov 15 00:12:59.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:12:59.421: INFO: namespace statefulset-6668 deletion completed in 6.292393521s

• [SLOW TEST:366.978 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:12:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 15 00:12:59.614: INFO: Waiting up to 5m0s for pod "pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0" in namespace "emptydir-5988" to be "success or failure"
Nov 15 00:12:59.623: INFO: Pod "pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.356281ms
Nov 15 00:13:01.630: INFO: Pod "pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016192253s
Nov 15 00:13:03.636: INFO: Pod "pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022308503s
STEP: Saw pod success
Nov 15 00:13:03.636: INFO: Pod "pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0" satisfied condition "success or failure"
Nov 15 00:13:03.640: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0 container test-container: <nil>
STEP: delete the pod
Nov 15 00:13:03.692: INFO: Waiting for pod pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0 to disappear
Nov 15 00:13:03.703: INFO: Pod pod-64ba276f-3c1a-4913-bfd8-23897d5af9b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:13:03.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5988" for this suite.
Nov 15 00:13:09.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:13:09.953: INFO: namespace emptydir-5988 deletion completed in 6.24483381s

• [SLOW TEST:10.526 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:13:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 15 00:13:10.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b" in namespace "downward-api-1121" to be "success or failure"
Nov 15 00:13:10.226: INFO: Pod "downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.040668ms
Nov 15 00:13:12.404: INFO: Pod "downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191436405s
Nov 15 00:13:14.410: INFO: Pod "downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.197429284s
STEP: Saw pod success
Nov 15 00:13:14.410: INFO: Pod "downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b" satisfied condition "success or failure"
Nov 15 00:13:14.415: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b container client-container: <nil>
STEP: delete the pod
Nov 15 00:13:14.475: INFO: Waiting for pod downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b to disappear
Nov 15 00:13:14.482: INFO: Pod downwardapi-volume-f47b5c00-678c-4d83-8d80-3386348eb60b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:13:14.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1121" for this suite.
Nov 15 00:13:20.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:13:20.681: INFO: namespace downward-api-1121 deletion completed in 6.192262481s

• [SLOW TEST:10.726 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:13:20.682: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:13:20.851: INFO: Creating ReplicaSet my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c
Nov 15 00:13:20.885: INFO: Pod name my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c: Found 1 pods out of 1
Nov 15 00:13:20.885: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c" is running
Nov 15 00:13:24.915: INFO: Pod "my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c-psqq5" is running (conditions: [])
Nov 15 00:13:24.920: INFO: Trying to dial the pod
Nov 15 00:13:29.938: INFO: Controller my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c: Got expected result from replica 1 [my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c-psqq5]: "my-hostname-basic-d0f42428-e502-44c1-8085-c1f90357ea9c-psqq5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:13:29.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3546" for this suite.
Nov 15 00:13:35.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:13:36.241: INFO: namespace replicaset-3546 deletion completed in 6.293440225s

• [SLOW TEST:15.559 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:13:36.242: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:13:36.427: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 15 00:13:41.435: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 15 00:13:41.436: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 15 00:13:43.444: INFO: Creating deployment "test-rollover-deployment"
Nov 15 00:13:43.465: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 15 00:13:45.479: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 15 00:13:45.488: INFO: Ensure that both replica sets have 1 created replica
Nov 15 00:13:45.495: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 15 00:13:45.507: INFO: Updating deployment test-rollover-deployment
Nov 15 00:13:45.508: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 15 00:13:47.529: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 15 00:13:47.543: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 15 00:13:47.550: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:47.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373625, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:49.567: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:49.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373628, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:51.564: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:51.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373628, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:53.562: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:53.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373628, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:55.560: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:55.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373628, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:57.558: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 00:13:57.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373628, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63709373623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 00:13:59.567: INFO: 
Nov 15 00:13:59.579: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 15 00:13:59.595: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5779,SelfLink:/apis/apps/v1/namespaces/deployment-5779/deployments/test-rollover-deployment,UID:0456f58e-95e6-4de4-83fa-d6b26388c4ce,ResourceVersion:18788,Generation:2,CreationTimestamp:2019-11-15 00:13:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-15 00:13:43 +0000 UTC 2019-11-15 00:13:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-15 00:13:58 +0000 UTC 2019-11-15 00:13:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 00:13:59.601: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-5779,SelfLink:/apis/apps/v1/namespaces/deployment-5779/replicasets/test-rollover-deployment-854595fc44,UID:6710dffb-3451-4fde-8b1f-36883681d0c5,ResourceVersion:18777,Generation:2,CreationTimestamp:2019-11-15 00:13:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0456f58e-95e6-4de4-83fa-d6b26388c4ce 0xc003042e77 0xc003042e78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 15 00:13:59.601: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 15 00:13:59.601: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5779,SelfLink:/apis/apps/v1/namespaces/deployment-5779/replicasets/test-rollover-controller,UID:40d3febb-028f-42fb-9df2-b87575014898,ResourceVersion:18787,Generation:2,CreationTimestamp:2019-11-15 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0456f58e-95e6-4de4-83fa-d6b26388c4ce 0xc003042da7 0xc003042da8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 15 00:13:59.601: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-5779,SelfLink:/apis/apps/v1/namespaces/deployment-5779/replicasets/test-rollover-deployment-9b8b997cf,UID:da4649b1-5f10-4c96-b95c-606a4198cd2c,ResourceVersion:18733,Generation:2,CreationTimestamp:2019-11-15 00:13:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0456f58e-95e6-4de4-83fa-d6b26388c4ce 0xc003042f40 0xc003042f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 15 00:13:59.605: INFO: Pod "test-rollover-deployment-854595fc44-9z9m7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-9z9m7,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-5779,SelfLink:/api/v1/namespaces/deployment-5779/pods/test-rollover-deployment-854595fc44-9z9m7,UID:d60850d7-7030-4f23-af97-a743d9928271,ResourceVersion:18753,Generation:0,CreationTimestamp:2019-11-15 00:13:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.180/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 6710dffb-3451-4fde-8b1f-36883681d0c5 0xc003043b57 0xc003043b58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bzc7p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bzc7p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bzc7p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003043bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003043be0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:13:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:13:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:13:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:13:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.180,StartTime:2019-11-15 00:13:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-15 00:13:47 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3c4d9f43b09a9c2902af6944bc79631a9c9e0374333ee11a2508090983668b26}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:13:59.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5779" for this suite.
Nov 15 00:14:05.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:14:05.823: INFO: namespace deployment-5779 deletion completed in 6.212635373s

• [SLOW TEST:29.582 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:14:05.847: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-ccd61bbc-76ee-4ac4-8370-5838bef62390
STEP: Creating a pod to test consume configMaps
Nov 15 00:14:06.081: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7" in namespace "projected-9660" to be "success or failure"
Nov 15 00:14:06.096: INFO: Pod "pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.183519ms
Nov 15 00:14:08.103: INFO: Pod "pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021380235s
Nov 15 00:14:10.133: INFO: Pod "pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051717267s
STEP: Saw pod success
Nov 15 00:14:10.133: INFO: Pod "pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7" satisfied condition "success or failure"
Nov 15 00:14:10.171: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 15 00:14:10.227: INFO: Waiting for pod pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7 to disappear
Nov 15 00:14:10.235: INFO: Pod pod-projected-configmaps-ab48ff90-4e6b-4813-8ae2-ca5f526fa4e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:14:10.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9660" for this suite.
Nov 15 00:14:16.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:14:16.501: INFO: namespace projected-9660 deletion completed in 6.258741347s

• [SLOW TEST:10.655 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:14:16.505: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 15 00:14:21.321: INFO: Successfully updated pod "labelsupdate4e2663f0-30f1-4093-9b6f-ae019c023b45"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:14:23.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7115" for this suite.
Nov 15 00:14:45.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:14:45.537: INFO: namespace projected-7115 deletion completed in 22.170692973s

• [SLOW TEST:29.032 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:14:45.539: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 15 00:14:45.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16" in namespace "downward-api-1883" to be "success or failure"
Nov 15 00:14:45.725: INFO: Pod "downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.213298ms
Nov 15 00:14:47.732: INFO: Pod "downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011497257s
Nov 15 00:14:49.736: INFO: Pod "downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016313333s
STEP: Saw pod success
Nov 15 00:14:49.737: INFO: Pod "downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16" satisfied condition "success or failure"
Nov 15 00:14:49.744: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16 container client-container: <nil>
STEP: delete the pod
Nov 15 00:14:49.799: INFO: Waiting for pod downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16 to disappear
Nov 15 00:14:49.803: INFO: Pod downwardapi-volume-6a4388a2-9fbb-4744-a584-bb42539e9f16 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:14:49.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1883" for this suite.
Nov 15 00:14:55.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:14:55.992: INFO: namespace downward-api-1883 deletion completed in 6.183482873s

• [SLOW TEST:10.453 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:14:55.996: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5625
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5625
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5625
Nov 15 00:14:56.216: INFO: Found 0 stateful pods, waiting for 1
Nov 15 00:15:06.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 15 00:15:06.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:15:06.666: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:15:06.667: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:15:06.667: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:15:06.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 00:15:16.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:15:16.678: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:15:16.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999699s
Nov 15 00:15:17.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993469462s
Nov 15 00:15:18.729: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98742692s
Nov 15 00:15:19.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981586799s
Nov 15 00:15:21.233: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976966015s
Nov 15 00:15:22.237: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.478421605s
Nov 15 00:15:23.241: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.473552726s
Nov 15 00:15:24.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.469865353s
Nov 15 00:15:25.251: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.463470352s
Nov 15 00:15:26.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 459.749528ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5625
Nov 15 00:15:27.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:15:27.689: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:15:27.689: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:15:27.689: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:15:27.692: INFO: Found 1 stateful pods, waiting for 3
Nov 15 00:15:37.704: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:15:37.704: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:15:37.704: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 15 00:15:37.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:15:38.127: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:15:38.127: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:15:38.127: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:15:38.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:15:38.537: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:15:38.537: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:15:38.537: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:15:38.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:15:38.959: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:15:38.959: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:15:38.959: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:15:38.959: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:15:38.962: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 15 00:15:49.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:15:49.033: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:15:49.033: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 00:15:49.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999425s
Nov 15 00:15:50.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990650502s
Nov 15 00:15:51.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985157618s
Nov 15 00:15:52.145: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979333333s
Nov 15 00:15:53.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.901844225s
Nov 15 00:15:54.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.895234589s
Nov 15 00:15:55.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.890094544s
Nov 15 00:15:56.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.883472633s
Nov 15 00:15:57.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.878201573s
Nov 15 00:15:58.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 869.522772ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5625
Nov 15 00:15:59.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:15:59.820: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:15:59.820: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:15:59.820: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:15:59.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:16:00.242: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:16:00.242: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:16:00.242: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:16:00.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5625 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:16:00.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:16:00.586: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:16:00.586: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:16:00.586: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 15 00:16:20.775: INFO: Deleting all statefulset in ns statefulset-5625
Nov 15 00:16:20.907: INFO: Scaling statefulset ss to 0
Nov 15 00:16:20.985: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:16:20.988: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:16:21.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5625" for this suite.
Nov 15 00:16:28.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:16:28.408: INFO: namespace statefulset-5625 deletion completed in 7.179132774s

• [SLOW TEST:92.413 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:16:28.421: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 15 00:16:28.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2196'
Nov 15 00:16:28.765: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 15 00:16:28.765: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Nov 15 00:16:28.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete jobs e2e-test-nginx-job --namespace=kubectl-2196'
Nov 15 00:16:28.942: INFO: stderr: ""
Nov 15 00:16:28.942: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:16:28.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2196" for this suite.
Nov 15 00:16:34.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:16:35.151: INFO: namespace kubectl-2196 deletion completed in 6.193997782s

• [SLOW TEST:6.730 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:16:35.152: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:16:36.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7342'
Nov 15 00:16:36.648: INFO: stderr: ""
Nov 15 00:16:36.648: INFO: stdout: "replicationcontroller/redis-master created\n"
Nov 15 00:16:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7342'
Nov 15 00:16:37.019: INFO: stderr: ""
Nov 15 00:16:37.019: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 15 00:16:38.034: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:16:38.034: INFO: Found 0 / 1
Nov 15 00:16:39.025: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:16:39.025: INFO: Found 0 / 1
Nov 15 00:16:40.024: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:16:40.024: INFO: Found 1 / 1
Nov 15 00:16:40.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 15 00:16:40.027: INFO: Selector matched 1 pods for map[app:redis]
Nov 15 00:16:40.027: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 00:16:40.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 describe pod redis-master-kh92p --namespace=kubectl-7342'
Nov 15 00:16:40.209: INFO: stderr: ""
Nov 15 00:16:40.209: INFO: stdout: "Name:           redis-master-kh92p\nNamespace:      kubectl-7342\nNode:           k8s-100-3imo44lif6er-minion-0/10.0.0.6\nStart Time:     Fri, 15 Nov 2019 00:16:36 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 192.168.2.186/32\nStatus:         Running\nIP:             192.168.2.186\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://fc40d2e34550417782f157760c75a0063e58f4268f3b511bfad168df6315cf5e\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 15 Nov 2019 00:16:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qrj4k (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qrj4k:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qrj4k\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  4s    default-scheduler                       Successfully assigned kubectl-7342/redis-master-kh92p to k8s-100-3imo44lif6er-minion-0\n  Normal  Pulled     2s    kubelet, k8s-100-3imo44lif6er-minion-0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, k8s-100-3imo44lif6er-minion-0  Created container redis-master\n  Normal  Started    2s    kubelet, k8s-100-3imo44lif6er-minion-0  Started container redis-master\n"
Nov 15 00:16:40.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 describe rc redis-master --namespace=kubectl-7342'
Nov 15 00:16:40.390: INFO: stderr: ""
Nov 15 00:16:40.390: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7342\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-kh92p\n"
Nov 15 00:16:40.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 describe service redis-master --namespace=kubectl-7342'
Nov 15 00:16:40.544: INFO: stderr: ""
Nov 15 00:16:40.545: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7342\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.254.183.29\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.2.186:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 15 00:16:40.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 describe node k8s-100-3imo44lif6er-master-0'
Nov 15 00:16:40.728: INFO: stderr: ""
Nov 15 00:16:40.728: INFO: stdout: "Name:               k8s-100-3imo44lif6er-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=5ff0b09b-684c-4212-8edc-826f26f9ab78\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz_wlg_2\n                    failure-domain.beta.kubernetes.io/zone=NZ-WLG-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-100-3imo44lif6er-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.4/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 14 Nov 2019 22:49:34 +0000\nTaints:             CriticalAddonsOnly=True:NoSchedule\n                    dedicated=master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 15 Nov 2019 00:16:13 +0000   Thu, 14 Nov 2019 22:49:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 15 Nov 2019 00:16:13 +0000   Thu, 14 Nov 2019 22:49:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 15 Nov 2019 00:16:13 +0000   Thu, 14 Nov 2019 22:49:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 15 Nov 2019 00:16:13 +0000   Thu, 14 Nov 2019 22:49:34 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.4\n  ExternalIP:  103.254.157.121\nCapacity:\n cpu:                2\n ephemeral-storage:  9202Mi\n hugepages-2Mi:      0\n memory:             4036956Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  8684096703\n hugepages-2Mi:      0\n memory:             3934556Ki\n pods:               110\nSystem Info:\n Machine ID:                 57907da983fb43fa93bbeb2031d1d5ef\n System UUID:                57907da9-83fb-43fa-93bb-eb2031d1d5ef\n Boot ID:                    b2918cb1-e2d6-45d3-b5b7-afbb5c968581\n Kernel Version:             5.1.20-200.fc29.x86_64\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.15.2\n Kube-Proxy Version:         v1.15.2\nPodCIDR:                     192.168.0.0/24\nProviderID:                  openstack:///57907da9-83fb-43fa-93bb-eb2031d1d5ef\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-x4tbc                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                k8s-keystone-auth-6wtcv                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                magnum-auto-healer-4cf97                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                octavia-ingress-controller-0                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                openstack-cloud-controller-manager-6jdd7                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  prometheus-monitoring      node-exporter-fmvv9                                        10m (0%)      10m (0%)    50Mi (1%)        50Mi (1%)      86m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-524408101ba247a5-mgl7z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                460m (23%)  10m (0%)\n  memory             50Mi (1%)   50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 15 00:16:40.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 describe namespace kubectl-7342'
Nov 15 00:16:40.928: INFO: stderr: ""
Nov 15 00:16:40.928: INFO: stdout: "Name:         kubectl-7342\nLabels:       e2e-framework=kubectl\n              e2e-run=b98ff6b4-fcf2-438e-83e1-d7aabb79e69e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:16:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7342" for this suite.
Nov 15 00:17:02.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:17:03.145: INFO: namespace kubectl-7342 deletion completed in 22.212167713s

• [SLOW TEST:27.993 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:17:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-acabcfa5-ad5b-4fd2-a8ca-8d7e9e595ec8
STEP: Creating a pod to test consume secrets
Nov 15 00:17:03.389: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb" in namespace "projected-5557" to be "success or failure"
Nov 15 00:17:03.413: INFO: Pod "pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.103739ms
Nov 15 00:17:05.422: INFO: Pod "pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033274928s
Nov 15 00:17:07.427: INFO: Pod "pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038444564s
STEP: Saw pod success
Nov 15 00:17:07.427: INFO: Pod "pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb" satisfied condition "success or failure"
Nov 15 00:17:07.430: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:17:07.489: INFO: Waiting for pod pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb to disappear
Nov 15 00:17:07.500: INFO: Pod pod-projected-secrets-fe080d52-e36c-416a-826a-b993ce703ecb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:17:07.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5557" for this suite.
Nov 15 00:17:13.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:17:13.699: INFO: namespace projected-5557 deletion completed in 6.183269215s

• [SLOW TEST:10.549 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:17:13.704: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 15 00:17:13.877: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:17:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5431" for this suite.
Nov 15 00:17:28.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:17:29.100: INFO: namespace init-container-5431 deletion completed in 6.317663949s

• [SLOW TEST:15.396 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:17:29.104: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 15 00:17:29.280: INFO: Waiting up to 5m0s for pod "pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87" in namespace "emptydir-5793" to be "success or failure"
Nov 15 00:17:29.293: INFO: Pod "pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.508982ms
Nov 15 00:17:31.301: INFO: Pod "pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021067899s
Nov 15 00:17:33.309: INFO: Pod "pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029050644s
STEP: Saw pod success
Nov 15 00:17:33.309: INFO: Pod "pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87" satisfied condition "success or failure"
Nov 15 00:17:33.312: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87 container test-container: <nil>
STEP: delete the pod
Nov 15 00:17:33.347: INFO: Waiting for pod pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87 to disappear
Nov 15 00:17:33.358: INFO: Pod pod-6296b098-dacc-4a71-9e2c-36e6e62cdf87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:17:33.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5793" for this suite.
Nov 15 00:17:39.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:17:39.598: INFO: namespace emptydir-5793 deletion completed in 6.233475206s

• [SLOW TEST:10.495 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:17:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-f46a47c0-94b6-43f8-a1a4-bd6dcb63d4bd in namespace container-probe-5405
Nov 15 00:17:43.792: INFO: Started pod test-webserver-f46a47c0-94b6-43f8-a1a4-bd6dcb63d4bd in namespace container-probe-5405
STEP: checking the pod's current state and verifying that restartCount is present
Nov 15 00:17:43.795: INFO: Initial restart count of pod test-webserver-f46a47c0-94b6-43f8-a1a4-bd6dcb63d4bd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:21:44.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5405" for this suite.
Nov 15 00:21:50.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:21:51.235: INFO: namespace container-probe-5405 deletion completed in 6.274056862s

• [SLOW TEST:251.636 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:21:51.246: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:21:51.449: INFO: (0) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 21.252319ms)
Nov 15 00:21:51.453: INFO: (1) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.482623ms)
Nov 15 00:21:51.458: INFO: (2) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.012091ms)
Nov 15 00:21:51.462: INFO: (3) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.000993ms)
Nov 15 00:21:51.476: INFO: (4) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 14.740432ms)
Nov 15 00:21:51.481: INFO: (5) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.610867ms)
Nov 15 00:21:51.486: INFO: (6) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.663859ms)
Nov 15 00:21:51.490: INFO: (7) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 4.221607ms)
Nov 15 00:21:51.495: INFO: (8) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.077892ms)
Nov 15 00:21:51.501: INFO: (9) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.637201ms)
Nov 15 00:21:51.507: INFO: (10) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.37469ms)
Nov 15 00:21:51.520: INFO: (11) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 12.652765ms)
Nov 15 00:21:51.555: INFO: (12) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 34.7282ms)
Nov 15 00:21:51.563: INFO: (13) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 8.443307ms)
Nov 15 00:21:51.571: INFO: (14) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.288746ms)
Nov 15 00:21:51.576: INFO: (15) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.225013ms)
Nov 15 00:21:51.585: INFO: (16) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.679879ms)
Nov 15 00:21:51.592: INFO: (17) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 7.077142ms)
Nov 15 00:21:51.598: INFO: (18) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 6.438463ms)
Nov 15 00:21:51.604: INFO: (19) /api/v1/nodes/k8s-100-3imo44lif6er-minion-0:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 5.832074ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:21:51.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2013" for this suite.
Nov 15 00:21:57.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:21:57.902: INFO: namespace proxy-2013 deletion completed in 6.293661586s

• [SLOW TEST:6.657 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:21:57.903: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Nov 15 00:21:58.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-9873'
Nov 15 00:21:59.177: INFO: stderr: ""
Nov 15 00:21:59.178: INFO: stdout: "pod/pause created\n"
Nov 15 00:21:59.178: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 15 00:21:59.179: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9873" to be "running and ready"
Nov 15 00:21:59.200: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.431185ms
Nov 15 00:22:01.207: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027883631s
Nov 15 00:22:03.212: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033254031s
Nov 15 00:22:05.219: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039961563s
Nov 15 00:22:07.224: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045201556s
Nov 15 00:22:09.229: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05034523s
Nov 15 00:22:11.236: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.057220571s
Nov 15 00:22:13.241: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.062471271s
Nov 15 00:22:15.247: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 16.0678454s
Nov 15 00:22:15.247: INFO: Pod "pause" satisfied condition "running and ready"
Nov 15 00:22:15.247: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 15 00:22:15.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 label pods pause testing-label=testing-label-value --namespace=kubectl-9873'
Nov 15 00:22:15.413: INFO: stderr: ""
Nov 15 00:22:15.413: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 15 00:22:15.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pod pause -L testing-label --namespace=kubectl-9873'
Nov 15 00:22:15.548: INFO: stderr: ""
Nov 15 00:22:15.548: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          16s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 15 00:22:15.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 label pods pause testing-label- --namespace=kubectl-9873'
Nov 15 00:22:15.711: INFO: stderr: ""
Nov 15 00:22:15.711: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 15 00:22:15.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pod pause -L testing-label --namespace=kubectl-9873'
Nov 15 00:22:15.853: INFO: stderr: ""
Nov 15 00:22:15.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          16s   \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Nov 15 00:22:15.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-9873'
Nov 15 00:22:16.073: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:22:16.073: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 15 00:22:16.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get rc,svc -l name=pause --no-headers --namespace=kubectl-9873'
Nov 15 00:22:16.240: INFO: stderr: "No resources found.\n"
Nov 15 00:22:16.241: INFO: stdout: ""
Nov 15 00:22:16.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -l name=pause --namespace=kubectl-9873 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 00:22:16.401: INFO: stderr: ""
Nov 15 00:22:16.401: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:22:16.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9873" for this suite.
Nov 15 00:22:22.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:22:22.648: INFO: namespace kubectl-9873 deletion completed in 6.23831631s

• [SLOW TEST:24.746 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:22:22.654: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-55c012fb-7275-4ca8-ad53-59f9726288a8 in namespace container-probe-110
Nov 15 00:22:28.895: INFO: Started pod busybox-55c012fb-7275-4ca8-ad53-59f9726288a8 in namespace container-probe-110
STEP: checking the pod's current state and verifying that restartCount is present
Nov 15 00:22:28.899: INFO: Initial restart count of pod busybox-55c012fb-7275-4ca8-ad53-59f9726288a8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:26:29.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-110" for this suite.
Nov 15 00:26:35.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:26:35.919: INFO: namespace container-probe-110 deletion completed in 6.321015812s

• [SLOW TEST:253.265 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:26:35.921: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-be629afe-b315-4ee4-a562-0ec8e5fc7dab
STEP: Creating secret with name secret-projected-all-test-volume-a7b09438-7653-4689-9524-c98f92fa0b9e
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 15 00:26:36.136: INFO: Waiting up to 5m0s for pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104" in namespace "projected-3866" to be "success or failure"
Nov 15 00:26:36.158: INFO: Pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104": Phase="Pending", Reason="", readiness=false. Elapsed: 21.692244ms
Nov 15 00:26:38.173: INFO: Pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036557774s
Nov 15 00:26:40.193: INFO: Pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056318682s
Nov 15 00:26:42.198: INFO: Pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062013674s
STEP: Saw pod success
Nov 15 00:26:42.198: INFO: Pod "projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104" satisfied condition "success or failure"
Nov 15 00:26:42.203: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 15 00:26:42.261: INFO: Waiting for pod projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104 to disappear
Nov 15 00:26:42.270: INFO: Pod projected-volume-76e14d34-72fb-4ff3-be7d-d2e9365e8104 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:26:42.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3866" for this suite.
Nov 15 00:26:48.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:26:48.509: INFO: namespace projected-3866 deletion completed in 6.233428049s

• [SLOW TEST:12.589 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:26:48.518: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Nov 15 00:26:48.761: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-417" to be "success or failure"
Nov 15 00:26:48.775: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.067488ms
Nov 15 00:26:50.782: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020391143s
Nov 15 00:26:52.787: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025831443s
Nov 15 00:26:54.792: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03061579s
Nov 15 00:26:56.796: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034459329s
Nov 15 00:26:58.801: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039582798s
Nov 15 00:27:00.810: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049062148s
Nov 15 00:27:02.817: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.055341483s
Nov 15 00:27:04.821: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.059714135s
STEP: Saw pod success
Nov 15 00:27:04.821: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov 15 00:27:04.825: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 15 00:27:04.867: INFO: Waiting for pod pod-host-path-test to disappear
Nov 15 00:27:04.872: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:27:04.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-417" for this suite.
Nov 15 00:27:10.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:27:11.173: INFO: namespace hostpath-417 deletion completed in 6.290888206s

• [SLOW TEST:22.655 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:27:11.178: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4654
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-eabea45e-9e00-4839-9cdc-d6cec0f3d46f
STEP: Creating configMap with name cm-test-opt-upd-3ff18a7b-4859-454c-a6f1-573aa8828a23
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-eabea45e-9e00-4839-9cdc-d6cec0f3d46f
STEP: Updating configmap cm-test-opt-upd-3ff18a7b-4859-454c-a6f1-573aa8828a23
STEP: Creating configMap with name cm-test-opt-create-ec50423e-171d-4b9a-8a3f-dd29a896a7f2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:27:19.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4654" for this suite.
Nov 15 00:27:41.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:27:42.106: INFO: namespace configmap-4654 deletion completed in 22.319491829s

• [SLOW TEST:30.929 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:27:42.108: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:27:48.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6463" for this suite.
Nov 15 00:27:54.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:27:54.663: INFO: namespace emptydir-wrapper-6463 deletion completed in 6.234767083s

• [SLOW TEST:12.555 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:27:54.663: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 15 00:27:54.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078" in namespace "downward-api-8767" to be "success or failure"
Nov 15 00:27:54.931: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078": Phase="Pending", Reason="", readiness=false. Elapsed: 18.596528ms
Nov 15 00:27:56.935: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023062784s
Nov 15 00:27:58.942: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078": Phase="Running", Reason="", readiness=true. Elapsed: 4.029555131s
Nov 15 00:28:00.948: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078": Phase="Running", Reason="", readiness=true. Elapsed: 6.035825121s
Nov 15 00:28:02.953: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04047282s
STEP: Saw pod success
Nov 15 00:28:02.953: INFO: Pod "downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078" satisfied condition "success or failure"
Nov 15 00:28:02.956: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078 container client-container: <nil>
STEP: delete the pod
Nov 15 00:28:03.011: INFO: Waiting for pod downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078 to disappear
Nov 15 00:28:03.046: INFO: Pod downwardapi-volume-32ab72e1-2528-4241-8b13-72d1c464a078 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:28:03.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8767" for this suite.
Nov 15 00:28:09.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:28:09.333: INFO: namespace downward-api-8767 deletion completed in 6.261741914s

• [SLOW TEST:14.670 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:28:09.336: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 15 00:28:17.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:17.666: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:19.667: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:19.673: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:21.668: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:21.673: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:23.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:23.672: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:25.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:25.673: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:27.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:27.688: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:29.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:29.676: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:31.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:31.678: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:33.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:33.671: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:35.667: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:35.680: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 00:28:37.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 00:28:37.673: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:28:37.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9259" for this suite.
Nov 15 00:28:59.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:28:59.963: INFO: namespace container-lifecycle-hook-9259 deletion completed in 22.280521082s

• [SLOW TEST:50.628 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:28:59.967: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-316a879d-3f4b-426b-804e-f62a3811b4b3 in namespace container-probe-4033
Nov 15 00:29:04.176: INFO: Started pod busybox-316a879d-3f4b-426b-804e-f62a3811b4b3 in namespace container-probe-4033
STEP: checking the pod's current state and verifying that restartCount is present
Nov 15 00:29:04.179: INFO: Initial restart count of pod busybox-316a879d-3f4b-426b-804e-f62a3811b4b3 is 0
Nov 15 00:29:52.439: INFO: Restart count of pod container-probe-4033/busybox-316a879d-3f4b-426b-804e-f62a3811b4b3 is now 1 (48.259031327s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:29:52.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4033" for this suite.
Nov 15 00:29:58.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:29:58.768: INFO: namespace container-probe-4033 deletion completed in 6.278089254s

• [SLOW TEST:58.802 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:29:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-1ab33451-6511-46df-ab1a-edc3a99f3d14
STEP: Creating a pod to test consume secrets
Nov 15 00:29:58.985: INFO: Waiting up to 5m0s for pod "pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36" in namespace "secrets-5566" to be "success or failure"
Nov 15 00:29:58.998: INFO: Pod "pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36": Phase="Pending", Reason="", readiness=false. Elapsed: 13.797214ms
Nov 15 00:30:01.007: INFO: Pod "pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022390974s
Nov 15 00:30:03.011: INFO: Pod "pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026795241s
STEP: Saw pod success
Nov 15 00:30:03.011: INFO: Pod "pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36" satisfied condition "success or failure"
Nov 15 00:30:03.014: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36 container secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:30:03.051: INFO: Waiting for pod pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36 to disappear
Nov 15 00:30:03.056: INFO: Pod pod-secrets-b32fdada-ad41-4017-8378-6c3a20d77d36 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:30:03.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5566" for this suite.
Nov 15 00:30:09.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:30:09.274: INFO: namespace secrets-5566 deletion completed in 6.212613601s

• [SLOW TEST:10.501 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:30:09.276: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-knph
STEP: Creating a pod to test atomic-volume-subpath
Nov 15 00:30:09.469: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-knph" in namespace "subpath-6208" to be "success or failure"
Nov 15 00:30:09.480: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550191ms
Nov 15 00:30:11.485: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015960194s
Nov 15 00:30:13.490: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021302489s
Nov 15 00:30:15.498: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029043289s
Nov 15 00:30:17.502: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033025551s
Nov 15 00:30:19.508: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039235578s
Nov 15 00:30:21.515: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 12.045524591s
Nov 15 00:30:23.524: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 14.055136896s
Nov 15 00:30:25.533: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 16.063735291s
Nov 15 00:30:27.539: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 18.06997171s
Nov 15 00:30:29.546: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 20.076623783s
Nov 15 00:30:31.551: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 22.082191173s
Nov 15 00:30:33.560: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 24.091230199s
Nov 15 00:30:35.565: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 26.096469758s
Nov 15 00:30:37.613: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 28.143553395s
Nov 15 00:30:39.620: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Running", Reason="", readiness=true. Elapsed: 30.150721107s
Nov 15 00:30:41.632: INFO: Pod "pod-subpath-test-configmap-knph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.162866647s
STEP: Saw pod success
Nov 15 00:30:41.632: INFO: Pod "pod-subpath-test-configmap-knph" satisfied condition "success or failure"
Nov 15 00:30:41.636: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-subpath-test-configmap-knph container test-container-subpath-configmap-knph: <nil>
STEP: delete the pod
Nov 15 00:30:41.699: INFO: Waiting for pod pod-subpath-test-configmap-knph to disappear
Nov 15 00:30:41.705: INFO: Pod pod-subpath-test-configmap-knph no longer exists
STEP: Deleting pod pod-subpath-test-configmap-knph
Nov 15 00:30:41.705: INFO: Deleting pod "pod-subpath-test-configmap-knph" in namespace "subpath-6208"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:30:41.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6208" for this suite.
Nov 15 00:30:47.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:30:48.023: INFO: namespace subpath-6208 deletion completed in 6.302374225s

• [SLOW TEST:38.747 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:30:48.024: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 15 00:30:51.260: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:30:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7141" for this suite.
Nov 15 00:30:57.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:30:57.496: INFO: namespace container-runtime-7141 deletion completed in 6.203048441s

• [SLOW TEST:9.472 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:30:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-7992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7992 to expose endpoints map[]
Nov 15 00:30:57.740: INFO: successfully validated that service endpoint-test2 in namespace services-7992 exposes endpoints map[] (26.576537ms elapsed)
STEP: Creating pod pod1 in namespace services-7992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7992 to expose endpoints map[pod1:[80]]
Nov 15 00:31:02.197: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.443855169s elapsed, will retry)
Nov 15 00:31:03.210: INFO: successfully validated that service endpoint-test2 in namespace services-7992 exposes endpoints map[pod1:[80]] (5.457693093s elapsed)
STEP: Creating pod pod2 in namespace services-7992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7992 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 15 00:31:06.305: INFO: successfully validated that service endpoint-test2 in namespace services-7992 exposes endpoints map[pod1:[80] pod2:[80]] (3.084229419s elapsed)
STEP: Deleting pod pod1 in namespace services-7992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7992 to expose endpoints map[pod2:[80]]
Nov 15 00:31:06.351: INFO: successfully validated that service endpoint-test2 in namespace services-7992 exposes endpoints map[pod2:[80]] (18.477174ms elapsed)
STEP: Deleting pod pod2 in namespace services-7992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7992 to expose endpoints map[]
Nov 15 00:31:06.399: INFO: successfully validated that service endpoint-test2 in namespace services-7992 exposes endpoints map[] (2.507401ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:31:06.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7992" for this suite.
Nov 15 00:31:12.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:31:12.664: INFO: namespace services-7992 deletion completed in 6.215697364s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:15.166 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:31:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 15 00:31:19.566: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7bf38190-977d-4c84-b53c-b6149a7ba1db"
Nov 15 00:31:19.567: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7bf38190-977d-4c84-b53c-b6149a7ba1db" in namespace "pods-8150" to be "terminated due to deadline exceeded"
Nov 15 00:31:19.580: INFO: Pod "pod-update-activedeadlineseconds-7bf38190-977d-4c84-b53c-b6149a7ba1db": Phase="Running", Reason="", readiness=true. Elapsed: 13.409676ms
Nov 15 00:31:21.591: INFO: Pod "pod-update-activedeadlineseconds-7bf38190-977d-4c84-b53c-b6149a7ba1db": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.024315491s
Nov 15 00:31:21.592: INFO: Pod "pod-update-activedeadlineseconds-7bf38190-977d-4c84-b53c-b6149a7ba1db" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:31:21.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8150" for this suite.
Nov 15 00:31:27.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:31:27.960: INFO: namespace pods-8150 deletion completed in 6.357288602s

• [SLOW TEST:15.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:31:27.963: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-5525/secret-test-5d1017cb-9c18-4825-b935-2d621aedfe42
STEP: Creating a pod to test consume secrets
Nov 15 00:31:28.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c" in namespace "secrets-5525" to be "success or failure"
Nov 15 00:31:28.401: INFO: Pod "pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c": Phase="Pending", Reason="", readiness=false. Elapsed: 116.258776ms
Nov 15 00:31:30.409: INFO: Pod "pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124911386s
Nov 15 00:31:32.415: INFO: Pod "pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.131063676s
STEP: Saw pod success
Nov 15 00:31:32.416: INFO: Pod "pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c" satisfied condition "success or failure"
Nov 15 00:31:32.419: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c container env-test: <nil>
STEP: delete the pod
Nov 15 00:31:32.460: INFO: Waiting for pod pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c to disappear
Nov 15 00:31:32.480: INFO: Pod pod-configmaps-0e9b368c-3aea-4ed4-b8f9-0510a3cd437c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:31:32.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5525" for this suite.
Nov 15 00:31:38.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:31:38.704: INFO: namespace secrets-5525 deletion completed in 6.213706236s

• [SLOW TEST:10.741 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:31:38.705: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 15 00:31:38.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5989'
Nov 15 00:31:39.053: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 15 00:31:39.053: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Nov 15 00:31:39.073: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-lnl98]
Nov 15 00:31:39.073: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-lnl98" in namespace "kubectl-5989" to be "running and ready"
Nov 15 00:31:39.080: INFO: Pod "e2e-test-nginx-rc-lnl98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.408321ms
Nov 15 00:31:41.086: INFO: Pod "e2e-test-nginx-rc-lnl98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012531366s
Nov 15 00:31:43.092: INFO: Pod "e2e-test-nginx-rc-lnl98": Phase="Running", Reason="", readiness=true. Elapsed: 4.018371615s
Nov 15 00:31:43.092: INFO: Pod "e2e-test-nginx-rc-lnl98" satisfied condition "running and ready"
Nov 15 00:31:43.092: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-lnl98]
Nov 15 00:31:43.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 logs rc/e2e-test-nginx-rc --namespace=kubectl-5989'
Nov 15 00:31:43.507: INFO: stderr: ""
Nov 15 00:31:43.507: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Nov 15 00:31:43.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete rc e2e-test-nginx-rc --namespace=kubectl-5989'
Nov 15 00:31:43.695: INFO: stderr: ""
Nov 15 00:31:43.695: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:31:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5989" for this suite.
Nov 15 00:32:05.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:32:05.944: INFO: namespace kubectl-5989 deletion completed in 22.242331341s

• [SLOW TEST:27.239 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:32:05.952: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 15 00:32:06.262: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4" in namespace "projected-6342" to be "success or failure"
Nov 15 00:32:06.280: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.100791ms
Nov 15 00:32:08.286: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023045027s
Nov 15 00:32:10.291: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028851154s
Nov 15 00:32:12.296: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033437767s
Nov 15 00:32:14.301: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039039374s
Nov 15 00:32:16.306: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.043709378s
STEP: Saw pod success
Nov 15 00:32:16.306: INFO: Pod "downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4" satisfied condition "success or failure"
Nov 15 00:32:16.309: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4 container client-container: <nil>
STEP: delete the pod
Nov 15 00:32:16.343: INFO: Waiting for pod downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4 to disappear
Nov 15 00:32:16.353: INFO: Pod downwardapi-volume-1613e60a-fd8f-46ac-a9ea-9a574ecab4a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:32:16.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6342" for this suite.
Nov 15 00:32:22.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:32:22.600: INFO: namespace projected-6342 deletion completed in 6.240228358s

• [SLOW TEST:16.649 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:32:22.605: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 15 00:32:56.929: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:32:56.949: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:32:58.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:32:58.954: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:00.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:00.960: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:02.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:02.955: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:04.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:04.957: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:06.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:06.955: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:08.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:08.954: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:10.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:10.955: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:12.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:13.096: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:14.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:14.955: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:16.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:16.957: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:18.950: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:18.954: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:20.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:20.954: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:22.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:23.027: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 00:33:24.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 00:33:24.955: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:33:24.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7654" for this suite.
Nov 15 00:33:47.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:33:47.283: INFO: namespace container-lifecycle-hook-7654 deletion completed in 22.288442763s

• [SLOW TEST:84.679 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:33:47.295: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7912
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 15 00:33:47.508: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 15 00:34:24.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.43:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7912 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 15 00:34:24.240: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 15 00:34:24.491: INFO: Found all expected endpoints: [netserver-0]
Nov 15 00:34:24.495: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.211:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7912 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 15 00:34:24.496: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
Nov 15 00:34:24.692: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:34:24.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7912" for this suite.
Nov 15 00:34:50.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:34:51.020: INFO: namespace pod-network-test-7912 deletion completed in 26.320980486s

• [SLOW TEST:63.726 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:34:51.021: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-3542a0d5-3443-4195-a852-219a5664fd92 in namespace container-probe-3475
Nov 15 00:35:23.426: INFO: Started pod liveness-3542a0d5-3443-4195-a852-219a5664fd92 in namespace container-probe-3475
STEP: checking the pod's current state and verifying that restartCount is present
Nov 15 00:35:23.430: INFO: Initial restart count of pod liveness-3542a0d5-3443-4195-a852-219a5664fd92 is 0
Nov 15 00:35:35.461: INFO: Restart count of pod container-probe-3475/liveness-3542a0d5-3443-4195-a852-219a5664fd92 is now 1 (12.030723893s elapsed)
Nov 15 00:36:01.540: INFO: Restart count of pod container-probe-3475/liveness-3542a0d5-3443-4195-a852-219a5664fd92 is now 2 (38.110289774s elapsed)
Nov 15 00:36:15.583: INFO: Restart count of pod container-probe-3475/liveness-3542a0d5-3443-4195-a852-219a5664fd92 is now 3 (52.153217347s elapsed)
Nov 15 00:36:35.895: INFO: Restart count of pod container-probe-3475/liveness-3542a0d5-3443-4195-a852-219a5664fd92 is now 4 (1m12.46503738s elapsed)
Nov 15 00:37:40.549: INFO: Restart count of pod container-probe-3475/liveness-3542a0d5-3443-4195-a852-219a5664fd92 is now 5 (2m17.119142928s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:37:40.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3475" for this suite.
Nov 15 00:37:46.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:37:46.838: INFO: namespace container-probe-3475 deletion completed in 6.244369308s

• [SLOW TEST:175.817 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:37:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:37:47.107: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 15 00:37:51.119: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 15 00:37:55.176: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-408,SelfLink:/apis/apps/v1/namespaces/deployment-408/deployments/test-cleanup-deployment,UID:f502380d-d1f4-4da8-9859-d20f0efcd26b,ResourceVersion:23211,Generation:1,CreationTimestamp:2019-11-15 00:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-15 00:37:51 +0000 UTC 2019-11-15 00:37:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-15 00:37:54 +0000 UTC 2019-11-15 00:37:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 00:37:55.187: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-408,SelfLink:/apis/apps/v1/namespaces/deployment-408/replicasets/test-cleanup-deployment-55bbcbc84c,UID:11cf3f0b-2751-4d75-a2fd-69ef81ef048b,ResourceVersion:23199,Generation:1,CreationTimestamp:2019-11-15 00:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment f502380d-d1f4-4da8-9859-d20f0efcd26b 0xc0031f54c7 0xc0031f54c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 15 00:37:55.193: INFO: Pod "test-cleanup-deployment-55bbcbc84c-d8s94" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-d8s94,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-408,SelfLink:/api/v1/namespaces/deployment-408/pods/test-cleanup-deployment-55bbcbc84c-d8s94,UID:74e8d18a-8484-4adb-a2b7-2cdde76a2e64,ResourceVersion:23198,Generation:0,CreationTimestamp:2019-11-15 00:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.215/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 11cf3f0b-2751-4d75-a2fd-69ef81ef048b 0xc0031f5b07 0xc0031f5b08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d7gfj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d7gfj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-d7gfj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031f5b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031f5b90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:37:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:37:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:37:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:37:51 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.215,StartTime:2019-11-15 00:37:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-15 00:37:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://22df1d7d8fe642692e231825740a475b9f11cea59d72a46852ecc5c7e6ef184d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:37:55.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-408" for this suite.
Nov 15 00:38:01.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:38:01.373: INFO: namespace deployment-408 deletion completed in 6.173885982s

• [SLOW TEST:14.534 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:38:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 15 00:38:01.605: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 15 00:38:06.610: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:38:06.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3246" for this suite.
Nov 15 00:38:16.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:38:17.183: INFO: namespace replication-controller-3246 deletion completed in 10.51209161s

• [SLOW TEST:15.800 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:38:17.184: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-rkt8
STEP: Creating a pod to test atomic-volume-subpath
Nov 15 00:38:17.435: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rkt8" in namespace "subpath-6192" to be "success or failure"
Nov 15 00:38:17.441: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.335511ms
Nov 15 00:38:19.445: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009709723s
Nov 15 00:38:21.453: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 4.017296674s
Nov 15 00:38:23.459: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 6.023400269s
Nov 15 00:38:25.478: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 8.042585522s
Nov 15 00:38:28.507: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 11.070857664s
Nov 15 00:38:30.512: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 13.07594241s
Nov 15 00:38:32.516: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 15.080234209s
Nov 15 00:38:34.522: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 17.085817721s
Nov 15 00:38:36.526: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 19.090291007s
Nov 15 00:38:38.533: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Running", Reason="", readiness=true. Elapsed: 21.097559549s
Nov 15 00:38:40.537: INFO: Pod "pod-subpath-test-secret-rkt8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.101527283s
STEP: Saw pod success
Nov 15 00:38:40.537: INFO: Pod "pod-subpath-test-secret-rkt8" satisfied condition "success or failure"
Nov 15 00:38:40.541: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-subpath-test-secret-rkt8 container test-container-subpath-secret-rkt8: <nil>
STEP: delete the pod
Nov 15 00:38:40.620: INFO: Waiting for pod pod-subpath-test-secret-rkt8 to disappear
Nov 15 00:38:40.626: INFO: Pod pod-subpath-test-secret-rkt8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-rkt8
Nov 15 00:38:40.626: INFO: Deleting pod "pod-subpath-test-secret-rkt8" in namespace "subpath-6192"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:38:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6192" for this suite.
Nov 15 00:38:46.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:38:46.874: INFO: namespace subpath-6192 deletion completed in 6.231652044s

• [SLOW TEST:29.691 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:38:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-400f6dfd-8921-450f-9bae-7e6b4fc404c6
STEP: Creating a pod to test consume secrets
Nov 15 00:38:47.133: INFO: Waiting up to 5m0s for pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35" in namespace "secrets-3061" to be "success or failure"
Nov 15 00:38:47.187: INFO: Pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35": Phase="Pending", Reason="", readiness=false. Elapsed: 53.559607ms
Nov 15 00:38:49.194: INFO: Pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060623564s
Nov 15 00:38:51.201: INFO: Pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067545382s
Nov 15 00:38:53.210: INFO: Pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076798208s
STEP: Saw pod success
Nov 15 00:38:53.210: INFO: Pod "pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35" satisfied condition "success or failure"
Nov 15 00:38:53.220: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:38:53.268: INFO: Waiting for pod pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35 to disappear
Nov 15 00:38:53.275: INFO: Pod pod-secrets-e90610b4-3152-4243-b1d4-3e7d05abbd35 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:38:53.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3061" for this suite.
Nov 15 00:39:01.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:39:02.161: INFO: namespace secrets-3061 deletion completed in 8.880276018s

• [SLOW TEST:15.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:39:02.164: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:39:02.719: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:39:11.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3647" for this suite.
Nov 15 00:40:15.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:40:16.980: INFO: namespace pods-3647 deletion completed in 1m5.65296998s

• [SLOW TEST:74.817 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:40:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:40:32.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8585" for this suite.
Nov 15 00:41:12.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:41:12.717: INFO: namespace kubelet-test-8585 deletion completed in 40.302368137s

• [SLOW TEST:55.730 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:41:12.718: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 15 00:41:17.661: INFO: Successfully updated pod "pod-update-e6d5a499-4746-4607-a0ce-2bed2507dbe6"
STEP: verifying the updated pod is in kubernetes
Nov 15 00:41:17.681: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:41:17.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4122" for this suite.
Nov 15 00:41:39.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:41:39.903: INFO: namespace pods-4122 deletion completed in 22.212740639s

• [SLOW TEST:27.186 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:41:39.908: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 15 00:41:40.077: INFO: Waiting up to 5m0s for pod "pod-5a882f06-8f91-4364-80b9-916ebe8f92c4" in namespace "emptydir-8792" to be "success or failure"
Nov 15 00:41:40.102: INFO: Pod "pod-5a882f06-8f91-4364-80b9-916ebe8f92c4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.374386ms
Nov 15 00:41:42.107: INFO: Pod "pod-5a882f06-8f91-4364-80b9-916ebe8f92c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029423585s
Nov 15 00:41:44.112: INFO: Pod "pod-5a882f06-8f91-4364-80b9-916ebe8f92c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034799841s
STEP: Saw pod success
Nov 15 00:41:44.112: INFO: Pod "pod-5a882f06-8f91-4364-80b9-916ebe8f92c4" satisfied condition "success or failure"
Nov 15 00:41:44.115: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-5a882f06-8f91-4364-80b9-916ebe8f92c4 container test-container: <nil>
STEP: delete the pod
Nov 15 00:41:44.150: INFO: Waiting for pod pod-5a882f06-8f91-4364-80b9-916ebe8f92c4 to disappear
Nov 15 00:41:44.159: INFO: Pod pod-5a882f06-8f91-4364-80b9-916ebe8f92c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:41:44.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8792" for this suite.
Nov 15 00:41:50.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:41:50.430: INFO: namespace emptydir-8792 deletion completed in 6.266400992s

• [SLOW TEST:10.523 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:41:50.433: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 15 00:41:50.629: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 00:41:50.639: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 00:41:50.643: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-0 before test
Nov 15 00:41:50.653: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-hqkrc from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 15 00:41:50.653: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 15 00:41:50.653: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 00:41:50.653: INFO: npd-wvz4s from kube-system started at 2019-11-14 22:58:24 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.654: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 15 00:41:50.654: INFO: sonobuoy from sonobuoy started at 2019-11-14 23:02:29 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.654: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 00:41:50.654: INFO: sonobuoy-e2e-job-65028defe7584f98 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 15 00:41:50.654: INFO: 	Container e2e ready: true, restart count 0
Nov 15 00:41:50.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 00:41:50.654: INFO: calico-node-ntzpq from kube-system started at 2019-11-14 22:58:21 +0000 UTC (2 container statuses recorded)
Nov 15 00:41:50.654: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 00:41:50.654: INFO: 	Container install-cni ready: true, restart count 0
Nov 15 00:41:50.654: INFO: node-exporter-w9wqv from prometheus-monitoring started at 2019-11-14 22:58:23 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.654: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 15 00:41:50.654: INFO: 
Logging pods the kubelet thinks is on node k8s-100-3imo44lif6er-minion-1 before test
Nov 15 00:41:50.674: INFO: kubernetes-dashboard-f456bc54b-nl4gf from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.674: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 15 00:41:50.674: INFO: coredns-646fbb9987-c6bz4 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.674: INFO: 	Container coredns ready: true, restart count 0
Nov 15 00:41:50.674: INFO: prometheus-ff4dc5bfd-27sxj from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.674: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 00:41:50.674: INFO: heapster-bbf96fd9d-fnrqj from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.674: INFO: 	Container heapster ready: true, restart count 0
Nov 15 00:41:50.675: INFO: npd-g7tcr from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 15 00:41:50.675: INFO: node-exporter-hmjtl from prometheus-monitoring started at 2019-11-14 22:56:57 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Nov 15 00:41:50.675: INFO: calico-node-2mxq7 from kube-system started at 2019-11-14 22:56:57 +0000 UTC (2 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 00:41:50.675: INFO: 	Container install-cni ready: true, restart count 0
Nov 15 00:41:50.675: INFO: grafana-85c5975df9-7hxpg from prometheus-monitoring started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container grafana ready: true, restart count 0
Nov 15 00:41:50.675: INFO: coredns-646fbb9987-g28vb from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container coredns ready: true, restart count 0
Nov 15 00:41:50.675: INFO: kube-dns-autoscaler-97b76b9b4-grhf6 from kube-system started at 2019-11-14 22:57:02 +0000 UTC (1 container statuses recorded)
Nov 15 00:41:50.675: INFO: 	Container autoscaler ready: true, restart count 0
Nov 15 00:41:50.675: INFO: sonobuoy-systemd-logs-daemon-set-524408101ba247a5-6wwn9 from sonobuoy started at 2019-11-14 23:02:48 +0000 UTC (2 container statuses recorded)
Nov 15 00:41:50.676: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 15 00:41:50.676: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d90a5509-d244-4095-98e9-1480163385f2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d90a5509-d244-4095-98e9-1480163385f2 off the node k8s-100-3imo44lif6er-minion-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d90a5509-d244-4095-98e9-1480163385f2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:42:06.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1647" for this suite.
Nov 15 00:42:24.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:42:25.222: INFO: namespace sched-pred-1647 deletion completed in 18.246434502s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:34.790 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:42:25.232: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-15
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:42:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:42:26.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-15" for this suite.
Nov 15 00:42:32.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:42:32.732: INFO: namespace custom-resource-definition-15 deletion completed in 6.190463256s

• [SLOW TEST:7.501 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:42:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2605
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2605
STEP: Creating statefulset with conflicting port in namespace statefulset-2605
STEP: Waiting until pod test-pod will start running in namespace statefulset-2605
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2605
Nov 15 00:42:37.172: INFO: Observed stateful pod in namespace: statefulset-2605, name: ss-0, uid: 2ebb97f5-0e4e-44af-ac28-c3505da9fd0a, status phase: Pending. Waiting for statefulset controller to delete.
Nov 15 00:42:41.569: INFO: Observed stateful pod in namespace: statefulset-2605, name: ss-0, uid: 2ebb97f5-0e4e-44af-ac28-c3505da9fd0a, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 00:42:41.591: INFO: Observed stateful pod in namespace: statefulset-2605, name: ss-0, uid: 2ebb97f5-0e4e-44af-ac28-c3505da9fd0a, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 00:42:41.603: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2605
STEP: Removing pod with conflicting port in namespace statefulset-2605
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2605 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 15 00:42:45.721: INFO: Deleting all statefulset in ns statefulset-2605
Nov 15 00:42:45.731: INFO: Scaling statefulset ss to 0
Nov 15 00:42:55.762: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:42:55.765: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:42:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2605" for this suite.
Nov 15 00:43:01.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:43:02.053: INFO: namespace statefulset-2605 deletion completed in 6.234681547s

• [SLOW TEST:29.314 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:43:02.060: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bdfe8bd1-1c54-4bb8-8a39-c8e4163bf133
STEP: Creating a pod to test consume secrets
Nov 15 00:43:02.231: INFO: Waiting up to 5m0s for pod "pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697" in namespace "secrets-1903" to be "success or failure"
Nov 15 00:43:02.244: INFO: Pod "pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697": Phase="Pending", Reason="", readiness=false. Elapsed: 11.941085ms
Nov 15 00:43:04.251: INFO: Pod "pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019825226s
Nov 15 00:43:06.562: INFO: Pod "pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.330242054s
STEP: Saw pod success
Nov 15 00:43:06.562: INFO: Pod "pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697" satisfied condition "success or failure"
Nov 15 00:43:06.566: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697 container secret-env-test: <nil>
STEP: delete the pod
Nov 15 00:43:06.617: INFO: Waiting for pod pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697 to disappear
Nov 15 00:43:06.625: INFO: Pod pod-secrets-c8aa5b15-189b-4fe8-9b64-8154037ca697 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:43:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1903" for this suite.
Nov 15 00:43:12.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:43:12.979: INFO: namespace secrets-1903 deletion completed in 6.344702897s

• [SLOW TEST:10.920 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:43:12.986: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-75e9e86d-b016-486c-af91-6a6033b5ef0f
STEP: Creating a pod to test consume configMaps
Nov 15 00:43:13.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4" in namespace "configmap-8905" to be "success or failure"
Nov 15 00:43:13.244: INFO: Pod "pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.833733ms
Nov 15 00:43:15.252: INFO: Pod "pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032787165s
Nov 15 00:43:17.259: INFO: Pod "pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03999356s
STEP: Saw pod success
Nov 15 00:43:17.260: INFO: Pod "pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4" satisfied condition "success or failure"
Nov 15 00:43:17.262: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 15 00:43:17.307: INFO: Waiting for pod pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4 to disappear
Nov 15 00:43:17.312: INFO: Pod pod-configmaps-2e0eb634-02db-44c3-80e0-30600de519c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:43:17.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8905" for this suite.
Nov 15 00:43:23.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:43:23.566: INFO: namespace configmap-8905 deletion completed in 6.249144956s

• [SLOW TEST:10.580 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:43:23.569: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 15 00:43:28.446: INFO: Successfully updated pod "annotationupdateeed66ab3-f0ee-4de3-ba04-2d8356bd64b9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:43:30.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9794" for this suite.
Nov 15 00:43:52.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:43:52.764: INFO: namespace projected-9794 deletion completed in 22.270325521s

• [SLOW TEST:29.197 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:43:52.774: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9678
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9678 to expose endpoints map[]
Nov 15 00:43:53.082: INFO: successfully validated that service multi-endpoint-test in namespace services-9678 exposes endpoints map[] (22.962369ms elapsed)
STEP: Creating pod pod1 in namespace services-9678
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9678 to expose endpoints map[pod1:[100]]
Nov 15 00:43:56.152: INFO: successfully validated that service multi-endpoint-test in namespace services-9678 exposes endpoints map[pod1:[100]] (3.053482653s elapsed)
STEP: Creating pod pod2 in namespace services-9678
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9678 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 15 00:43:58.261: INFO: successfully validated that service multi-endpoint-test in namespace services-9678 exposes endpoints map[pod1:[100] pod2:[101]] (2.09573759s elapsed)
STEP: Deleting pod pod1 in namespace services-9678
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9678 to expose endpoints map[pod2:[101]]
Nov 15 00:43:58.321: INFO: successfully validated that service multi-endpoint-test in namespace services-9678 exposes endpoints map[pod2:[101]] (45.535552ms elapsed)
STEP: Deleting pod pod2 in namespace services-9678
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9678 to expose endpoints map[]
Nov 15 00:43:58.381: INFO: successfully validated that service multi-endpoint-test in namespace services-9678 exposes endpoints map[] (21.924961ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:43:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9678" for this suite.
Nov 15 00:44:04.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:44:04.640: INFO: namespace services-9678 deletion completed in 6.223532392s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:11.868 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:44:04.646: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8899
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6249
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:44:11.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5764" for this suite.
Nov 15 00:44:17.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:44:17.501: INFO: namespace namespaces-5764 deletion completed in 6.229738684s
STEP: Destroying namespace "nsdeletetest-8899" for this suite.
Nov 15 00:44:17.505: INFO: Namespace nsdeletetest-8899 was already deleted
STEP: Destroying namespace "nsdeletetest-6249" for this suite.
Nov 15 00:44:23.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:44:23.776: INFO: namespace nsdeletetest-6249 deletion completed in 6.271057732s

• [SLOW TEST:19.131 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:44:23.785: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 15 00:44:28.014: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-59c35888-2091-4763-9dc4-d55097a2c0ca,GenerateName:,Namespace:events-9976,SelfLink:/api/v1/namespaces/events-9976/pods/send-events-59c35888-2091-4763-9dc4-d55097a2c0ca,UID:379c9708-412f-4cf8-b5a2-c5afcadeede8,ResourceVersion:24621,Generation:0,CreationTimestamp:2019-11-15 00:44:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 981482061,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.231/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6f75t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6f75t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-6f75t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a89d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a89d60}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:44:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:44:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:44:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:44:24 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.231,StartTime:2019-11-15 00:44:24 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-11-15 00:44:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://1da5c330b858911107dc71a37d66a663a17e820b0b5c82217a469d0a4f9aec05}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Nov 15 00:44:30.022: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 15 00:44:32.031: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:44:32.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9976" for this suite.
Nov 15 00:45:12.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:45:12.350: INFO: namespace events-9976 deletion completed in 40.27572618s

• [SLOW TEST:48.566 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:45:12.351: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 15 00:45:12.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3005'
Nov 15 00:45:14.012: INFO: stderr: ""
Nov 15 00:45:14.012: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Nov 15 00:45:14.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete pods e2e-test-nginx-pod --namespace=kubectl-3005'
Nov 15 00:45:15.116: INFO: stderr: ""
Nov 15 00:45:15.116: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:45:15.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3005" for this suite.
Nov 15 00:45:21.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:45:21.324: INFO: namespace kubectl-3005 deletion completed in 6.201222703s

• [SLOW TEST:8.973 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:45:21.332: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:45:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2825" for this suite.
Nov 15 00:45:27.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:45:27.879: INFO: namespace kubelet-test-2825 deletion completed in 6.284333794s

• [SLOW TEST:6.548 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:45:27.884: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Nov 15 00:45:28.107: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov 15 00:45:28.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:28.807: INFO: stderr: ""
Nov 15 00:45:28.808: INFO: stdout: "service/redis-slave created\n"
Nov 15 00:45:28.808: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov 15 00:45:28.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:29.170: INFO: stderr: ""
Nov 15 00:45:29.170: INFO: stdout: "service/redis-master created\n"
Nov 15 00:45:29.171: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 15 00:45:29.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:29.627: INFO: stderr: ""
Nov 15 00:45:29.627: INFO: stdout: "service/frontend created\n"
Nov 15 00:45:29.629: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov 15 00:45:29.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:29.994: INFO: stderr: ""
Nov 15 00:45:29.994: INFO: stdout: "deployment.apps/frontend created\n"
Nov 15 00:45:29.995: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 15 00:45:29.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:30.344: INFO: stderr: ""
Nov 15 00:45:30.344: INFO: stdout: "deployment.apps/redis-master created\n"
Nov 15 00:45:30.345: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov 15 00:45:30.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-7479'
Nov 15 00:45:30.701: INFO: stderr: ""
Nov 15 00:45:30.701: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Nov 15 00:45:30.701: INFO: Waiting for all frontend pods to be Running.
Nov 15 00:46:30.758: INFO: Waiting for frontend to serve content.
Nov 15 00:46:30.793: INFO: Trying to add a new entry to the guestbook.
Nov 15 00:46:30.831: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 15 00:46:30.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.050: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.050: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 15 00:46:31.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.234: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.234: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 15 00:46:31.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.410: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 15 00:46:31.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.594: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.594: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 15 00:46:31.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.733: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.733: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 15 00:46:31.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-7479'
Nov 15 00:46:31.888: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:46:31.888: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:46:31.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7479" for this suite.
Nov 15 00:47:09.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:47:10.155: INFO: namespace kubectl-7479 deletion completed in 38.260152929s

• [SLOW TEST:102.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:47:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9931
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Nov 15 00:47:10.359: INFO: Found 0 stateful pods, waiting for 3
Nov 15 00:47:20.382: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:47:20.382: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:47:20.383: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 15 00:47:30.369: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:47:30.370: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:47:30.370: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov 15 00:47:30.403: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 15 00:47:40.464: INFO: Updating stateful set ss2
Nov 15 00:47:40.503: INFO: Waiting for Pod statefulset-9931/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Nov 15 00:47:50.667: INFO: Found 2 stateful pods, waiting for 3
Nov 15 00:48:00.676: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:48:00.676: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:48:00.676: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 15 00:48:00.707: INFO: Updating stateful set ss2
Nov 15 00:48:00.730: INFO: Waiting for Pod statefulset-9931/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov 15 00:48:10.775: INFO: Updating stateful set ss2
Nov 15 00:48:10.785: INFO: Waiting for StatefulSet statefulset-9931/ss2 to complete update
Nov 15 00:48:10.785: INFO: Waiting for Pod statefulset-9931/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov 15 00:48:21.177: INFO: Waiting for StatefulSet statefulset-9931/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 15 00:48:30.799: INFO: Deleting all statefulset in ns statefulset-9931
Nov 15 00:48:30.816: INFO: Scaling statefulset ss2 to 0
Nov 15 00:48:40.858: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:48:40.863: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:48:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9931" for this suite.
Nov 15 00:48:46.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:48:47.103: INFO: namespace statefulset-9931 deletion completed in 6.208533798s

• [SLOW TEST:96.945 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:48:47.105: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d4282774-e74f-408e-8c7c-8fa97c896e3b
STEP: Creating a pod to test consume secrets
Nov 15 00:48:47.279: INFO: Waiting up to 5m0s for pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1" in namespace "secrets-590" to be "success or failure"
Nov 15 00:48:47.294: INFO: Pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.457254ms
Nov 15 00:48:49.298: INFO: Pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019176974s
Nov 15 00:48:51.303: INFO: Pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023722453s
Nov 15 00:48:53.306: INFO: Pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02743178s
STEP: Saw pod success
Nov 15 00:48:53.306: INFO: Pod "pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1" satisfied condition "success or failure"
Nov 15 00:48:53.315: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1 container secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:48:53.352: INFO: Waiting for pod pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1 to disappear
Nov 15 00:48:53.356: INFO: Pod pod-secrets-9b311830-c0a7-4a2f-bee6-4b1a7aa921b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:48:53.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-590" for this suite.
Nov 15 00:48:59.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:48:59.599: INFO: namespace secrets-590 deletion completed in 6.239152807s

• [SLOW TEST:12.494 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:48:59.601: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Nov 15 00:48:59.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 create -f - --namespace=kubectl-8319'
Nov 15 00:49:00.567: INFO: stderr: ""
Nov 15 00:49:00.567: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 15 00:49:00.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8319'
Nov 15 00:49:00.891: INFO: stderr: ""
Nov 15 00:49:00.895: INFO: stdout: "update-demo-nautilus-r9rwr update-demo-nautilus-w8c55 "
Nov 15 00:49:00.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-r9rwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8319'
Nov 15 00:49:01.083: INFO: stderr: ""
Nov 15 00:49:01.084: INFO: stdout: ""
Nov 15 00:49:01.084: INFO: update-demo-nautilus-r9rwr is created but not running
Nov 15 00:49:06.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8319'
Nov 15 00:49:06.253: INFO: stderr: ""
Nov 15 00:49:06.253: INFO: stdout: "update-demo-nautilus-r9rwr update-demo-nautilus-w8c55 "
Nov 15 00:49:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-r9rwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8319'
Nov 15 00:49:06.390: INFO: stderr: ""
Nov 15 00:49:06.390: INFO: stdout: "true"
Nov 15 00:49:06.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-r9rwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8319'
Nov 15 00:49:06.530: INFO: stderr: ""
Nov 15 00:49:06.530: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 15 00:49:06.530: INFO: validating pod update-demo-nautilus-r9rwr
Nov 15 00:49:06.543: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 00:49:06.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 00:49:06.543: INFO: update-demo-nautilus-r9rwr is verified up and running
Nov 15 00:49:06.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-w8c55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8319'
Nov 15 00:49:06.707: INFO: stderr: ""
Nov 15 00:49:06.707: INFO: stdout: "true"
Nov 15 00:49:06.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods update-demo-nautilus-w8c55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8319'
Nov 15 00:49:06.856: INFO: stderr: ""
Nov 15 00:49:06.856: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 15 00:49:06.856: INFO: validating pod update-demo-nautilus-w8c55
Nov 15 00:49:06.866: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 00:49:06.866: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 00:49:06.866: INFO: update-demo-nautilus-w8c55 is verified up and running
STEP: using delete to clean up resources
Nov 15 00:49:06.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 delete --grace-period=0 --force -f - --namespace=kubectl-8319'
Nov 15 00:49:07.010: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 00:49:07.010: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 15 00:49:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8319'
Nov 15 00:49:07.212: INFO: stderr: "No resources found.\n"
Nov 15 00:49:07.212: INFO: stdout: ""
Nov 15 00:49:07.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 get pods -l name=update-demo --namespace=kubectl-8319 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 00:49:07.355: INFO: stderr: ""
Nov 15 00:49:07.355: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:49:07.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8319" for this suite.
Nov 15 00:49:29.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:49:29.628: INFO: namespace kubectl-8319 deletion completed in 22.267435618s

• [SLOW TEST:30.027 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:49:29.629: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-520
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0556a79e-6c0a-4365-91aa-95f23802ee62
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-0556a79e-6c0a-4365-91aa-95f23802ee62
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:50:47.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-520" for this suite.
Nov 15 00:51:09.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:51:09.315: INFO: namespace projected-520 deletion completed in 22.256154148s

• [SLOW TEST:99.686 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:51:09.323: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5155
I1115 00:51:09.644184      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5155, replica count: 1
I1115 00:51:10.695246      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 00:51:11.695592      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 00:51:12.695839      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 00:51:13.061: INFO: Created: latency-svc-hlh22
Nov 15 00:51:13.061: INFO: Got endpoints: latency-svc-hlh22 [265.334772ms]
Nov 15 00:51:13.099: INFO: Created: latency-svc-7r62w
Nov 15 00:51:13.110: INFO: Got endpoints: latency-svc-7r62w [48.234141ms]
Nov 15 00:51:13.131: INFO: Created: latency-svc-sjc9x
Nov 15 00:51:13.140: INFO: Created: latency-svc-jxtbt
Nov 15 00:51:13.146: INFO: Got endpoints: latency-svc-sjc9x [82.812266ms]
Nov 15 00:51:13.151: INFO: Got endpoints: latency-svc-jxtbt [88.401944ms]
Nov 15 00:51:13.166: INFO: Created: latency-svc-5499b
Nov 15 00:51:13.176: INFO: Created: latency-svc-q4frj
Nov 15 00:51:13.179: INFO: Got endpoints: latency-svc-5499b [115.764728ms]
Nov 15 00:51:13.188: INFO: Got endpoints: latency-svc-q4frj [124.362855ms]
Nov 15 00:51:13.196: INFO: Created: latency-svc-8l7nd
Nov 15 00:51:13.200: INFO: Got endpoints: latency-svc-8l7nd [136.260053ms]
Nov 15 00:51:13.211: INFO: Created: latency-svc-gbxm2
Nov 15 00:51:13.223: INFO: Got endpoints: latency-svc-gbxm2 [158.685681ms]
Nov 15 00:51:13.225: INFO: Created: latency-svc-4696s
Nov 15 00:51:13.236: INFO: Got endpoints: latency-svc-4696s [173.648369ms]
Nov 15 00:51:13.241: INFO: Created: latency-svc-564mn
Nov 15 00:51:13.247: INFO: Got endpoints: latency-svc-564mn [182.952687ms]
Nov 15 00:51:13.255: INFO: Created: latency-svc-mkxvj
Nov 15 00:51:13.272: INFO: Created: latency-svc-4wkmn
Nov 15 00:51:13.283: INFO: Got endpoints: latency-svc-mkxvj [218.516553ms]
Nov 15 00:51:13.299: INFO: Got endpoints: latency-svc-4wkmn [62.548526ms]
Nov 15 00:51:13.319: INFO: Created: latency-svc-n9pdl
Nov 15 00:51:13.346: INFO: Got endpoints: latency-svc-n9pdl [284.214051ms]
Nov 15 00:51:13.353: INFO: Created: latency-svc-vd7mm
Nov 15 00:51:13.377: INFO: Created: latency-svc-462lr
Nov 15 00:51:13.383: INFO: Got endpoints: latency-svc-vd7mm [318.528072ms]
Nov 15 00:51:13.412: INFO: Created: latency-svc-p272w
Nov 15 00:51:13.412: INFO: Got endpoints: latency-svc-462lr [348.233341ms]
Nov 15 00:51:13.425: INFO: Got endpoints: latency-svc-p272w [360.489542ms]
Nov 15 00:51:13.446: INFO: Created: latency-svc-q5d2n
Nov 15 00:51:13.457: INFO: Got endpoints: latency-svc-q5d2n [392.540908ms]
Nov 15 00:51:13.464: INFO: Created: latency-svc-22k54
Nov 15 00:51:13.470: INFO: Got endpoints: latency-svc-22k54 [360.146907ms]
Nov 15 00:51:13.482: INFO: Created: latency-svc-q4s9w
Nov 15 00:51:13.485: INFO: Got endpoints: latency-svc-q4s9w [338.881291ms]
Nov 15 00:51:13.488: INFO: Created: latency-svc-62gvg
Nov 15 00:51:13.494: INFO: Got endpoints: latency-svc-62gvg [342.139569ms]
Nov 15 00:51:13.498: INFO: Created: latency-svc-26ccv
Nov 15 00:51:13.505: INFO: Created: latency-svc-2wjzg
Nov 15 00:51:13.509: INFO: Got endpoints: latency-svc-26ccv [329.241056ms]
Nov 15 00:51:13.511: INFO: Created: latency-svc-vnmgj
Nov 15 00:51:13.515: INFO: Got endpoints: latency-svc-2wjzg [326.883442ms]
Nov 15 00:51:13.525: INFO: Got endpoints: latency-svc-vnmgj [324.455646ms]
Nov 15 00:51:13.527: INFO: Created: latency-svc-klpnx
Nov 15 00:51:13.542: INFO: Got endpoints: latency-svc-klpnx [318.864356ms]
Nov 15 00:51:13.545: INFO: Created: latency-svc-z295w
Nov 15 00:51:13.552: INFO: Got endpoints: latency-svc-z295w [304.399244ms]
Nov 15 00:51:13.571: INFO: Created: latency-svc-d5ndj
Nov 15 00:51:13.577: INFO: Created: latency-svc-brsts
Nov 15 00:51:13.595: INFO: Created: latency-svc-ncsh5
Nov 15 00:51:13.611: INFO: Created: latency-svc-vsz9h
Nov 15 00:51:13.618: INFO: Got endpoints: latency-svc-ncsh5 [272.220542ms]
Nov 15 00:51:13.619: INFO: Got endpoints: latency-svc-d5ndj [335.857204ms]
Nov 15 00:51:13.618: INFO: Got endpoints: latency-svc-brsts [318.671635ms]
Nov 15 00:51:13.634: INFO: Created: latency-svc-qhwlm
Nov 15 00:51:13.645: INFO: Got endpoints: latency-svc-vsz9h [261.159444ms]
Nov 15 00:51:13.664: INFO: Created: latency-svc-n9pzz
Nov 15 00:51:13.667: INFO: Got endpoints: latency-svc-qhwlm [253.472761ms]
Nov 15 00:51:13.683: INFO: Got endpoints: latency-svc-n9pzz [258.129722ms]
Nov 15 00:51:13.690: INFO: Created: latency-svc-8rnl8
Nov 15 00:51:13.698: INFO: Created: latency-svc-zj2xd
Nov 15 00:51:13.700: INFO: Got endpoints: latency-svc-8rnl8 [242.932463ms]
Nov 15 00:51:13.715: INFO: Got endpoints: latency-svc-zj2xd [244.643962ms]
Nov 15 00:51:13.719: INFO: Created: latency-svc-grc6f
Nov 15 00:51:13.730: INFO: Created: latency-svc-7snqm
Nov 15 00:51:13.732: INFO: Got endpoints: latency-svc-grc6f [247.043889ms]
Nov 15 00:51:13.739: INFO: Created: latency-svc-klkwh
Nov 15 00:51:13.739: INFO: Got endpoints: latency-svc-7snqm [245.454221ms]
Nov 15 00:51:13.753: INFO: Created: latency-svc-99x49
Nov 15 00:51:13.759: INFO: Got endpoints: latency-svc-klkwh [250.703549ms]
Nov 15 00:51:13.770: INFO: Created: latency-svc-b9p7k
Nov 15 00:51:13.775: INFO: Got endpoints: latency-svc-99x49 [259.511567ms]
Nov 15 00:51:13.786: INFO: Got endpoints: latency-svc-b9p7k [261.222448ms]
Nov 15 00:51:13.800: INFO: Created: latency-svc-phwt4
Nov 15 00:51:13.804: INFO: Got endpoints: latency-svc-phwt4 [262.735988ms]
Nov 15 00:51:13.816: INFO: Created: latency-svc-xxvtf
Nov 15 00:51:13.828: INFO: Created: latency-svc-m7jww
Nov 15 00:51:13.835: INFO: Got endpoints: latency-svc-xxvtf [283.205874ms]
Nov 15 00:51:13.842: INFO: Created: latency-svc-d6vvp
Nov 15 00:51:13.844: INFO: Got endpoints: latency-svc-m7jww [225.674586ms]
Nov 15 00:51:13.868: INFO: Created: latency-svc-wwlnl
Nov 15 00:51:13.875: INFO: Got endpoints: latency-svc-wwlnl [256.19592ms]
Nov 15 00:51:13.875: INFO: Got endpoints: latency-svc-d6vvp [255.955411ms]
Nov 15 00:51:13.879: INFO: Created: latency-svc-qx8lz
Nov 15 00:51:13.890: INFO: Got endpoints: latency-svc-qx8lz [245.406874ms]
Nov 15 00:51:13.892: INFO: Created: latency-svc-csxk7
Nov 15 00:51:13.917: INFO: Got endpoints: latency-svc-csxk7 [249.646965ms]
Nov 15 00:51:13.929: INFO: Created: latency-svc-tfbq4
Nov 15 00:51:13.938: INFO: Got endpoints: latency-svc-tfbq4 [254.843057ms]
Nov 15 00:51:13.953: INFO: Created: latency-svc-hwfs8
Nov 15 00:51:13.955: INFO: Created: latency-svc-bgkkj
Nov 15 00:51:13.961: INFO: Created: latency-svc-f825r
Nov 15 00:51:13.970: INFO: Got endpoints: latency-svc-hwfs8 [269.871426ms]
Nov 15 00:51:13.970: INFO: Got endpoints: latency-svc-bgkkj [254.933511ms]
Nov 15 00:51:13.977: INFO: Created: latency-svc-phzrt
Nov 15 00:51:13.989: INFO: Created: latency-svc-rlhrf
Nov 15 00:51:13.993: INFO: Created: latency-svc-76kml
Nov 15 00:51:13.994: INFO: Got endpoints: latency-svc-f825r [261.599875ms]
Nov 15 00:51:14.008: INFO: Created: latency-svc-w7wsk
Nov 15 00:51:14.008: INFO: Created: latency-svc-cmhfp
Nov 15 00:51:14.043: INFO: Created: latency-svc-zbslc
Nov 15 00:51:14.045: INFO: Got endpoints: latency-svc-phzrt [305.714417ms]
Nov 15 00:51:14.057: INFO: Created: latency-svc-njwml
Nov 15 00:51:14.065: INFO: Created: latency-svc-cjlvp
Nov 15 00:51:14.076: INFO: Created: latency-svc-s9zkr
Nov 15 00:51:14.080: INFO: Created: latency-svc-t2wsv
Nov 15 00:51:14.096: INFO: Created: latency-svc-r2759
Nov 15 00:51:14.109: INFO: Created: latency-svc-7nfn9
Nov 15 00:51:14.113: INFO: Got endpoints: latency-svc-rlhrf [353.16907ms]
Nov 15 00:51:14.125: INFO: Created: latency-svc-d447q
Nov 15 00:51:14.135: INFO: Created: latency-svc-jvcqb
Nov 15 00:51:14.142: INFO: Created: latency-svc-4pgk7
Nov 15 00:51:14.145: INFO: Got endpoints: latency-svc-76kml [370.635497ms]
Nov 15 00:51:14.182: INFO: Created: latency-svc-phvwz
Nov 15 00:51:14.230: INFO: Got endpoints: latency-svc-cmhfp [444.435432ms]
Nov 15 00:51:14.238: INFO: Created: latency-svc-zwd6v
Nov 15 00:51:14.246: INFO: Created: latency-svc-nt4f6
Nov 15 00:51:14.248: INFO: Got endpoints: latency-svc-w7wsk [443.629297ms]
Nov 15 00:51:14.255: INFO: Created: latency-svc-dmbsn
Nov 15 00:51:14.270: INFO: Created: latency-svc-2vnzc
Nov 15 00:51:14.294: INFO: Got endpoints: latency-svc-zbslc [458.384886ms]
Nov 15 00:51:14.304: INFO: Created: latency-svc-bc8bm
Nov 15 00:51:14.341: INFO: Got endpoints: latency-svc-njwml [497.158806ms]
Nov 15 00:51:14.380: INFO: Created: latency-svc-qf8hw
Nov 15 00:51:14.392: INFO: Got endpoints: latency-svc-cjlvp [516.63204ms]
Nov 15 00:51:14.411: INFO: Created: latency-svc-gkdjm
Nov 15 00:51:14.450: INFO: Got endpoints: latency-svc-s9zkr [575.066748ms]
Nov 15 00:51:14.467: INFO: Created: latency-svc-ghw6s
Nov 15 00:51:14.508: INFO: Got endpoints: latency-svc-t2wsv [617.747626ms]
Nov 15 00:51:14.526: INFO: Created: latency-svc-mhwqm
Nov 15 00:51:14.542: INFO: Got endpoints: latency-svc-r2759 [624.371777ms]
Nov 15 00:51:14.560: INFO: Created: latency-svc-r886t
Nov 15 00:51:14.594: INFO: Got endpoints: latency-svc-7nfn9 [655.539915ms]
Nov 15 00:51:14.604: INFO: Created: latency-svc-x99rj
Nov 15 00:51:14.643: INFO: Got endpoints: latency-svc-d447q [672.407952ms]
Nov 15 00:51:14.660: INFO: Created: latency-svc-hggch
Nov 15 00:51:14.692: INFO: Got endpoints: latency-svc-jvcqb [721.944087ms]
Nov 15 00:51:14.708: INFO: Created: latency-svc-86gvf
Nov 15 00:51:14.743: INFO: Got endpoints: latency-svc-4pgk7 [749.205255ms]
Nov 15 00:51:14.758: INFO: Created: latency-svc-w4plh
Nov 15 00:51:14.798: INFO: Got endpoints: latency-svc-phvwz [752.295099ms]
Nov 15 00:51:14.824: INFO: Created: latency-svc-ngp6f
Nov 15 00:51:14.846: INFO: Got endpoints: latency-svc-zwd6v [733.388599ms]
Nov 15 00:51:14.861: INFO: Created: latency-svc-bjdv4
Nov 15 00:51:14.895: INFO: Got endpoints: latency-svc-nt4f6 [749.516072ms]
Nov 15 00:51:14.907: INFO: Created: latency-svc-2tt4d
Nov 15 00:51:14.946: INFO: Got endpoints: latency-svc-dmbsn [715.10765ms]
Nov 15 00:51:14.977: INFO: Created: latency-svc-gkx7r
Nov 15 00:51:14.994: INFO: Got endpoints: latency-svc-2vnzc [745.340468ms]
Nov 15 00:51:15.008: INFO: Created: latency-svc-k79s2
Nov 15 00:51:15.044: INFO: Got endpoints: latency-svc-bc8bm [749.725625ms]
Nov 15 00:51:15.059: INFO: Created: latency-svc-vwljw
Nov 15 00:51:15.090: INFO: Got endpoints: latency-svc-qf8hw [748.89388ms]
Nov 15 00:51:15.104: INFO: Created: latency-svc-6m67j
Nov 15 00:51:15.142: INFO: Got endpoints: latency-svc-gkdjm [750.297776ms]
Nov 15 00:51:15.160: INFO: Created: latency-svc-gwvbb
Nov 15 00:51:15.194: INFO: Got endpoints: latency-svc-ghw6s [743.456768ms]
Nov 15 00:51:15.211: INFO: Created: latency-svc-br22b
Nov 15 00:51:15.242: INFO: Got endpoints: latency-svc-mhwqm [733.393045ms]
Nov 15 00:51:15.256: INFO: Created: latency-svc-rdbxb
Nov 15 00:51:15.305: INFO: Got endpoints: latency-svc-r886t [762.641004ms]
Nov 15 00:51:15.321: INFO: Created: latency-svc-qtc8m
Nov 15 00:51:15.347: INFO: Got endpoints: latency-svc-x99rj [752.414842ms]
Nov 15 00:51:15.364: INFO: Created: latency-svc-5sz46
Nov 15 00:51:15.392: INFO: Got endpoints: latency-svc-hggch [749.723751ms]
Nov 15 00:51:15.420: INFO: Created: latency-svc-jd66x
Nov 15 00:51:15.445: INFO: Got endpoints: latency-svc-86gvf [752.502368ms]
Nov 15 00:51:15.478: INFO: Created: latency-svc-r76t8
Nov 15 00:51:15.494: INFO: Got endpoints: latency-svc-w4plh [750.225668ms]
Nov 15 00:51:15.514: INFO: Created: latency-svc-7pckn
Nov 15 00:51:15.543: INFO: Got endpoints: latency-svc-ngp6f [744.754997ms]
Nov 15 00:51:15.563: INFO: Created: latency-svc-ksp76
Nov 15 00:51:15.594: INFO: Got endpoints: latency-svc-bjdv4 [747.174337ms]
Nov 15 00:51:15.606: INFO: Created: latency-svc-6t8p5
Nov 15 00:51:15.640: INFO: Got endpoints: latency-svc-2tt4d [744.628938ms]
Nov 15 00:51:15.658: INFO: Created: latency-svc-rllp2
Nov 15 00:51:15.694: INFO: Got endpoints: latency-svc-gkx7r [748.327645ms]
Nov 15 00:51:15.711: INFO: Created: latency-svc-nqnpk
Nov 15 00:51:15.766: INFO: Got endpoints: latency-svc-k79s2 [772.217177ms]
Nov 15 00:51:15.777: INFO: Created: latency-svc-2r5vh
Nov 15 00:51:15.790: INFO: Got endpoints: latency-svc-vwljw [746.509648ms]
Nov 15 00:51:15.801: INFO: Created: latency-svc-r8n9g
Nov 15 00:51:15.841: INFO: Got endpoints: latency-svc-6m67j [750.730455ms]
Nov 15 00:51:15.853: INFO: Created: latency-svc-7b45h
Nov 15 00:51:15.891: INFO: Got endpoints: latency-svc-gwvbb [748.20818ms]
Nov 15 00:51:15.904: INFO: Created: latency-svc-n2lhl
Nov 15 00:51:15.941: INFO: Got endpoints: latency-svc-br22b [747.171965ms]
Nov 15 00:51:15.958: INFO: Created: latency-svc-rt78p
Nov 15 00:51:15.993: INFO: Got endpoints: latency-svc-rdbxb [750.849563ms]
Nov 15 00:51:16.004: INFO: Created: latency-svc-8f2rg
Nov 15 00:51:16.042: INFO: Got endpoints: latency-svc-qtc8m [737.628809ms]
Nov 15 00:51:16.063: INFO: Created: latency-svc-nt5hx
Nov 15 00:51:16.111: INFO: Got endpoints: latency-svc-5sz46 [763.783454ms]
Nov 15 00:51:16.123: INFO: Created: latency-svc-klqmt
Nov 15 00:51:16.146: INFO: Got endpoints: latency-svc-jd66x [753.323262ms]
Nov 15 00:51:16.163: INFO: Created: latency-svc-hb49z
Nov 15 00:51:16.191: INFO: Got endpoints: latency-svc-r76t8 [745.73297ms]
Nov 15 00:51:16.211: INFO: Created: latency-svc-fmg76
Nov 15 00:51:16.243: INFO: Got endpoints: latency-svc-7pckn [749.576197ms]
Nov 15 00:51:16.256: INFO: Created: latency-svc-8pb6z
Nov 15 00:51:16.292: INFO: Got endpoints: latency-svc-ksp76 [749.13796ms]
Nov 15 00:51:16.305: INFO: Created: latency-svc-7zghd
Nov 15 00:51:16.341: INFO: Got endpoints: latency-svc-6t8p5 [747.65374ms]
Nov 15 00:51:16.354: INFO: Created: latency-svc-s6jng
Nov 15 00:51:16.391: INFO: Got endpoints: latency-svc-rllp2 [750.56455ms]
Nov 15 00:51:16.412: INFO: Created: latency-svc-sl8hp
Nov 15 00:51:16.444: INFO: Got endpoints: latency-svc-nqnpk [749.193802ms]
Nov 15 00:51:16.456: INFO: Created: latency-svc-sbrpn
Nov 15 00:51:16.493: INFO: Got endpoints: latency-svc-2r5vh [727.15164ms]
Nov 15 00:51:16.507: INFO: Created: latency-svc-l8rqm
Nov 15 00:51:16.569: INFO: Got endpoints: latency-svc-r8n9g [778.918316ms]
Nov 15 00:51:16.586: INFO: Created: latency-svc-m9llh
Nov 15 00:51:16.597: INFO: Got endpoints: latency-svc-7b45h [756.168376ms]
Nov 15 00:51:16.619: INFO: Created: latency-svc-lcsvz
Nov 15 00:51:16.658: INFO: Got endpoints: latency-svc-n2lhl [767.275068ms]
Nov 15 00:51:16.691: INFO: Created: latency-svc-fr2g2
Nov 15 00:51:16.694: INFO: Got endpoints: latency-svc-rt78p [753.217077ms]
Nov 15 00:51:16.715: INFO: Created: latency-svc-255rr
Nov 15 00:51:16.741: INFO: Got endpoints: latency-svc-8f2rg [748.400215ms]
Nov 15 00:51:16.755: INFO: Created: latency-svc-mvlgd
Nov 15 00:51:16.795: INFO: Got endpoints: latency-svc-nt5hx [752.174945ms]
Nov 15 00:51:16.809: INFO: Created: latency-svc-7smzr
Nov 15 00:51:16.844: INFO: Got endpoints: latency-svc-klqmt [733.376277ms]
Nov 15 00:51:16.860: INFO: Created: latency-svc-wmnf9
Nov 15 00:51:16.907: INFO: Got endpoints: latency-svc-hb49z [760.70176ms]
Nov 15 00:51:16.933: INFO: Created: latency-svc-4fwbw
Nov 15 00:51:16.945: INFO: Got endpoints: latency-svc-fmg76 [754.130331ms]
Nov 15 00:51:16.978: INFO: Created: latency-svc-zc5mn
Nov 15 00:51:16.991: INFO: Got endpoints: latency-svc-8pb6z [747.830858ms]
Nov 15 00:51:17.025: INFO: Created: latency-svc-jcjt8
Nov 15 00:51:17.042: INFO: Got endpoints: latency-svc-7zghd [750.267343ms]
Nov 15 00:51:17.058: INFO: Created: latency-svc-22vww
Nov 15 00:51:17.096: INFO: Got endpoints: latency-svc-s6jng [754.396256ms]
Nov 15 00:51:17.119: INFO: Created: latency-svc-wqljl
Nov 15 00:51:17.142: INFO: Got endpoints: latency-svc-sl8hp [750.917764ms]
Nov 15 00:51:17.153: INFO: Created: latency-svc-55ns9
Nov 15 00:51:17.194: INFO: Got endpoints: latency-svc-sbrpn [749.890683ms]
Nov 15 00:51:17.212: INFO: Created: latency-svc-8t72l
Nov 15 00:51:17.248: INFO: Got endpoints: latency-svc-l8rqm [754.201956ms]
Nov 15 00:51:17.260: INFO: Created: latency-svc-xgsxd
Nov 15 00:51:17.292: INFO: Got endpoints: latency-svc-m9llh [722.046514ms]
Nov 15 00:51:17.312: INFO: Created: latency-svc-7rsdx
Nov 15 00:51:17.353: INFO: Got endpoints: latency-svc-lcsvz [755.720307ms]
Nov 15 00:51:17.508: INFO: Got endpoints: latency-svc-255rr [813.213516ms]
Nov 15 00:51:17.508: INFO: Got endpoints: latency-svc-fr2g2 [849.455092ms]
Nov 15 00:51:17.517: INFO: Created: latency-svc-875bv
Nov 15 00:51:17.555: INFO: Got endpoints: latency-svc-mvlgd [813.47166ms]
Nov 15 00:51:17.557: INFO: Got endpoints: latency-svc-7smzr [762.326988ms]
Nov 15 00:51:17.624: INFO: Got endpoints: latency-svc-wmnf9 [780.056582ms]
Nov 15 00:51:17.628: INFO: Created: latency-svc-mkhq5
Nov 15 00:51:17.631: INFO: Created: latency-svc-hh5kc
Nov 15 00:51:17.645: INFO: Created: latency-svc-dhmlp
Nov 15 00:51:17.649: INFO: Got endpoints: latency-svc-4fwbw [742.217717ms]
Nov 15 00:51:17.650: INFO: Created: latency-svc-hkxmp
Nov 15 00:51:17.694: INFO: Created: latency-svc-4s8qx
Nov 15 00:51:17.710: INFO: Got endpoints: latency-svc-zc5mn [764.374707ms]
Nov 15 00:51:17.718: INFO: Created: latency-svc-c2xdp
Nov 15 00:51:17.745: INFO: Created: latency-svc-xxnsr
Nov 15 00:51:17.753: INFO: Got endpoints: latency-svc-jcjt8 [761.076016ms]
Nov 15 00:51:17.815: INFO: Created: latency-svc-q4gnv
Nov 15 00:51:17.845: INFO: Got endpoints: latency-svc-22vww [802.608133ms]
Nov 15 00:51:17.865: INFO: Got endpoints: latency-svc-wqljl [768.89515ms]
Nov 15 00:51:17.884: INFO: Created: latency-svc-qnz6r
Nov 15 00:51:17.895: INFO: Got endpoints: latency-svc-55ns9 [752.813263ms]
Nov 15 00:51:17.899: INFO: Created: latency-svc-8pbs5
Nov 15 00:51:17.936: INFO: Created: latency-svc-5clkh
Nov 15 00:51:17.952: INFO: Got endpoints: latency-svc-8t72l [758.458883ms]
Nov 15 00:51:17.964: INFO: Created: latency-svc-7fgfp
Nov 15 00:51:18.005: INFO: Got endpoints: latency-svc-xgsxd [756.714058ms]
Nov 15 00:51:18.019: INFO: Created: latency-svc-nj9gs
Nov 15 00:51:18.058: INFO: Got endpoints: latency-svc-7rsdx [766.631507ms]
Nov 15 00:51:18.075: INFO: Created: latency-svc-t8x7m
Nov 15 00:51:18.092: INFO: Got endpoints: latency-svc-875bv [738.978294ms]
Nov 15 00:51:18.117: INFO: Created: latency-svc-nlbs6
Nov 15 00:51:18.154: INFO: Got endpoints: latency-svc-mkhq5 [646.227217ms]
Nov 15 00:51:18.170: INFO: Created: latency-svc-97jtc
Nov 15 00:51:18.193: INFO: Got endpoints: latency-svc-hh5kc [631.847219ms]
Nov 15 00:51:18.220: INFO: Created: latency-svc-sps42
Nov 15 00:51:18.242: INFO: Got endpoints: latency-svc-dhmlp [733.235748ms]
Nov 15 00:51:18.263: INFO: Created: latency-svc-fdb9r
Nov 15 00:51:18.293: INFO: Got endpoints: latency-svc-hkxmp [738.574026ms]
Nov 15 00:51:18.315: INFO: Created: latency-svc-v4skz
Nov 15 00:51:18.343: INFO: Got endpoints: latency-svc-4s8qx [718.365815ms]
Nov 15 00:51:18.355: INFO: Created: latency-svc-mmz9m
Nov 15 00:51:18.393: INFO: Got endpoints: latency-svc-c2xdp [743.271857ms]
Nov 15 00:51:18.402: INFO: Created: latency-svc-7lq5v
Nov 15 00:51:18.442: INFO: Got endpoints: latency-svc-xxnsr [731.683315ms]
Nov 15 00:51:18.458: INFO: Created: latency-svc-kfwl9
Nov 15 00:51:18.492: INFO: Got endpoints: latency-svc-q4gnv [738.90376ms]
Nov 15 00:51:18.509: INFO: Created: latency-svc-6hd7c
Nov 15 00:51:18.543: INFO: Got endpoints: latency-svc-qnz6r [697.232031ms]
Nov 15 00:51:18.554: INFO: Created: latency-svc-md6mc
Nov 15 00:51:18.595: INFO: Got endpoints: latency-svc-8pbs5 [729.792739ms]
Nov 15 00:51:18.610: INFO: Created: latency-svc-r6n9r
Nov 15 00:51:18.642: INFO: Got endpoints: latency-svc-5clkh [747.106351ms]
Nov 15 00:51:18.656: INFO: Created: latency-svc-pnjbc
Nov 15 00:51:18.698: INFO: Got endpoints: latency-svc-7fgfp [745.959871ms]
Nov 15 00:51:18.713: INFO: Created: latency-svc-45vn8
Nov 15 00:51:18.744: INFO: Got endpoints: latency-svc-nj9gs [739.261058ms]
Nov 15 00:51:18.756: INFO: Created: latency-svc-k7c7j
Nov 15 00:51:18.791: INFO: Got endpoints: latency-svc-t8x7m [732.30785ms]
Nov 15 00:51:18.816: INFO: Created: latency-svc-vtzlm
Nov 15 00:51:18.850: INFO: Got endpoints: latency-svc-nlbs6 [757.284098ms]
Nov 15 00:51:18.865: INFO: Created: latency-svc-hpf6k
Nov 15 00:51:18.893: INFO: Got endpoints: latency-svc-97jtc [738.957633ms]
Nov 15 00:51:18.904: INFO: Created: latency-svc-ddgv4
Nov 15 00:51:18.944: INFO: Got endpoints: latency-svc-sps42 [750.754997ms]
Nov 15 00:51:18.964: INFO: Created: latency-svc-6tljz
Nov 15 00:51:18.991: INFO: Got endpoints: latency-svc-fdb9r [748.982099ms]
Nov 15 00:51:19.006: INFO: Created: latency-svc-225h7
Nov 15 00:51:19.041: INFO: Got endpoints: latency-svc-v4skz [747.489753ms]
Nov 15 00:51:19.052: INFO: Created: latency-svc-r9dn2
Nov 15 00:51:19.092: INFO: Got endpoints: latency-svc-mmz9m [748.840755ms]
Nov 15 00:51:19.104: INFO: Created: latency-svc-sjhlr
Nov 15 00:51:19.142: INFO: Got endpoints: latency-svc-7lq5v [749.527598ms]
Nov 15 00:51:19.154: INFO: Created: latency-svc-8lnkf
Nov 15 00:51:19.196: INFO: Got endpoints: latency-svc-kfwl9 [753.924007ms]
Nov 15 00:51:19.206: INFO: Created: latency-svc-znd48
Nov 15 00:51:19.242: INFO: Got endpoints: latency-svc-6hd7c [749.828566ms]
Nov 15 00:51:19.252: INFO: Created: latency-svc-dhf7h
Nov 15 00:51:19.293: INFO: Got endpoints: latency-svc-md6mc [750.350653ms]
Nov 15 00:51:19.310: INFO: Created: latency-svc-bmmbp
Nov 15 00:51:19.343: INFO: Got endpoints: latency-svc-r6n9r [747.395433ms]
Nov 15 00:51:19.359: INFO: Created: latency-svc-cmmh8
Nov 15 00:51:19.391: INFO: Got endpoints: latency-svc-pnjbc [749.02203ms]
Nov 15 00:51:19.402: INFO: Created: latency-svc-wbkln
Nov 15 00:51:19.447: INFO: Got endpoints: latency-svc-45vn8 [747.953919ms]
Nov 15 00:51:19.464: INFO: Created: latency-svc-9rsx4
Nov 15 00:51:19.494: INFO: Got endpoints: latency-svc-k7c7j [750.039312ms]
Nov 15 00:51:19.512: INFO: Created: latency-svc-hr4md
Nov 15 00:51:19.540: INFO: Got endpoints: latency-svc-vtzlm [741.839789ms]
Nov 15 00:51:19.574: INFO: Created: latency-svc-h57s4
Nov 15 00:51:19.650: INFO: Got endpoints: latency-svc-hpf6k [800.177601ms]
Nov 15 00:51:19.650: INFO: Got endpoints: latency-svc-ddgv4 [756.660373ms]
Nov 15 00:51:19.666: INFO: Created: latency-svc-twpvx
Nov 15 00:51:19.672: INFO: Created: latency-svc-pgxxk
Nov 15 00:51:19.691: INFO: Got endpoints: latency-svc-6tljz [747.071597ms]
Nov 15 00:51:19.704: INFO: Created: latency-svc-6hmk7
Nov 15 00:51:19.741: INFO: Got endpoints: latency-svc-225h7 [749.627139ms]
Nov 15 00:51:19.758: INFO: Created: latency-svc-mzr5z
Nov 15 00:51:19.790: INFO: Got endpoints: latency-svc-r9dn2 [748.996766ms]
Nov 15 00:51:19.801: INFO: Created: latency-svc-4fhdl
Nov 15 00:51:19.841: INFO: Got endpoints: latency-svc-sjhlr [748.716691ms]
Nov 15 00:51:19.855: INFO: Created: latency-svc-mzdd7
Nov 15 00:51:19.891: INFO: Got endpoints: latency-svc-8lnkf [748.622046ms]
Nov 15 00:51:19.902: INFO: Created: latency-svc-7g7q6
Nov 15 00:51:19.942: INFO: Got endpoints: latency-svc-znd48 [746.372278ms]
Nov 15 00:51:19.998: INFO: Created: latency-svc-cw69k
Nov 15 00:51:20.004: INFO: Got endpoints: latency-svc-dhf7h [762.00287ms]
Nov 15 00:51:20.029: INFO: Created: latency-svc-n6kg6
Nov 15 00:51:20.045: INFO: Got endpoints: latency-svc-bmmbp [752.055793ms]
Nov 15 00:51:20.058: INFO: Created: latency-svc-tkkdx
Nov 15 00:51:20.094: INFO: Got endpoints: latency-svc-cmmh8 [751.484848ms]
Nov 15 00:51:20.230: INFO: Created: latency-svc-69sj9
Nov 15 00:51:20.231: INFO: Got endpoints: latency-svc-9rsx4 [784.5937ms]
Nov 15 00:51:20.232: INFO: Got endpoints: latency-svc-wbkln [840.4719ms]
Nov 15 00:51:20.242: INFO: Got endpoints: latency-svc-hr4md [747.752527ms]
Nov 15 00:51:20.248: INFO: Created: latency-svc-4wrxm
Nov 15 00:51:20.257: INFO: Created: latency-svc-nh7lz
Nov 15 00:51:20.264: INFO: Created: latency-svc-z7nlm
Nov 15 00:51:20.291: INFO: Got endpoints: latency-svc-h57s4 [750.8963ms]
Nov 15 00:51:20.303: INFO: Created: latency-svc-6lvv5
Nov 15 00:51:20.341: INFO: Got endpoints: latency-svc-twpvx [690.501264ms]
Nov 15 00:51:20.357: INFO: Created: latency-svc-4fvfq
Nov 15 00:51:20.391: INFO: Got endpoints: latency-svc-pgxxk [739.968781ms]
Nov 15 00:51:20.403: INFO: Created: latency-svc-s7dqm
Nov 15 00:51:20.442: INFO: Got endpoints: latency-svc-6hmk7 [750.247912ms]
Nov 15 00:51:20.453: INFO: Created: latency-svc-v7mwn
Nov 15 00:51:20.492: INFO: Got endpoints: latency-svc-mzr5z [751.087859ms]
Nov 15 00:51:20.504: INFO: Created: latency-svc-7x826
Nov 15 00:51:20.541: INFO: Got endpoints: latency-svc-4fhdl [750.739524ms]
Nov 15 00:51:20.561: INFO: Created: latency-svc-7z68b
Nov 15 00:51:20.593: INFO: Got endpoints: latency-svc-mzdd7 [751.845206ms]
Nov 15 00:51:20.603: INFO: Created: latency-svc-dqbbh
Nov 15 00:51:20.641: INFO: Got endpoints: latency-svc-7g7q6 [749.404056ms]
Nov 15 00:51:20.653: INFO: Created: latency-svc-nmm4z
Nov 15 00:51:20.691: INFO: Got endpoints: latency-svc-cw69k [748.572804ms]
Nov 15 00:51:20.704: INFO: Created: latency-svc-hkxxt
Nov 15 00:51:20.741: INFO: Got endpoints: latency-svc-n6kg6 [736.987226ms]
Nov 15 00:51:20.751: INFO: Created: latency-svc-rg59b
Nov 15 00:51:20.791: INFO: Got endpoints: latency-svc-tkkdx [745.644509ms]
Nov 15 00:51:20.802: INFO: Created: latency-svc-mvnjb
Nov 15 00:51:20.841: INFO: Got endpoints: latency-svc-69sj9 [746.42469ms]
Nov 15 00:51:20.854: INFO: Created: latency-svc-djcrc
Nov 15 00:51:20.895: INFO: Got endpoints: latency-svc-4wrxm [663.21726ms]
Nov 15 00:51:20.941: INFO: Got endpoints: latency-svc-nh7lz [708.828975ms]
Nov 15 00:51:20.996: INFO: Got endpoints: latency-svc-z7nlm [754.215334ms]
Nov 15 00:51:21.041: INFO: Got endpoints: latency-svc-6lvv5 [750.048982ms]
Nov 15 00:51:21.094: INFO: Got endpoints: latency-svc-4fvfq [752.898629ms]
Nov 15 00:51:21.142: INFO: Got endpoints: latency-svc-s7dqm [751.698547ms]
Nov 15 00:51:21.193: INFO: Got endpoints: latency-svc-v7mwn [751.057022ms]
Nov 15 00:51:21.241: INFO: Got endpoints: latency-svc-7x826 [748.877655ms]
Nov 15 00:51:21.292: INFO: Got endpoints: latency-svc-7z68b [751.00746ms]
Nov 15 00:51:21.341: INFO: Got endpoints: latency-svc-dqbbh [748.367301ms]
Nov 15 00:51:21.391: INFO: Got endpoints: latency-svc-nmm4z [749.918434ms]
Nov 15 00:51:21.441: INFO: Got endpoints: latency-svc-hkxxt [749.724574ms]
Nov 15 00:51:21.491: INFO: Got endpoints: latency-svc-rg59b [749.752586ms]
Nov 15 00:51:21.541: INFO: Got endpoints: latency-svc-mvnjb [749.532209ms]
Nov 15 00:51:21.592: INFO: Got endpoints: latency-svc-djcrc [751.033871ms]
Nov 15 00:51:21.593: INFO: Latencies: [48.234141ms 62.548526ms 82.812266ms 88.401944ms 115.764728ms 124.362855ms 136.260053ms 158.685681ms 173.648369ms 182.952687ms 218.516553ms 225.674586ms 242.932463ms 244.643962ms 245.406874ms 245.454221ms 247.043889ms 249.646965ms 250.703549ms 253.472761ms 254.843057ms 254.933511ms 255.955411ms 256.19592ms 258.129722ms 259.511567ms 261.159444ms 261.222448ms 261.599875ms 262.735988ms 269.871426ms 272.220542ms 283.205874ms 284.214051ms 304.399244ms 305.714417ms 318.528072ms 318.671635ms 318.864356ms 324.455646ms 326.883442ms 329.241056ms 335.857204ms 338.881291ms 342.139569ms 348.233341ms 353.16907ms 360.146907ms 360.489542ms 370.635497ms 392.540908ms 443.629297ms 444.435432ms 458.384886ms 497.158806ms 516.63204ms 575.066748ms 617.747626ms 624.371777ms 631.847219ms 646.227217ms 655.539915ms 663.21726ms 672.407952ms 690.501264ms 697.232031ms 708.828975ms 715.10765ms 718.365815ms 721.944087ms 722.046514ms 727.15164ms 729.792739ms 731.683315ms 732.30785ms 733.235748ms 733.376277ms 733.388599ms 733.393045ms 736.987226ms 737.628809ms 738.574026ms 738.90376ms 738.957633ms 738.978294ms 739.261058ms 739.968781ms 741.839789ms 742.217717ms 743.271857ms 743.456768ms 744.628938ms 744.754997ms 745.340468ms 745.644509ms 745.73297ms 745.959871ms 746.372278ms 746.42469ms 746.509648ms 747.071597ms 747.106351ms 747.171965ms 747.174337ms 747.395433ms 747.489753ms 747.65374ms 747.752527ms 747.830858ms 747.953919ms 748.20818ms 748.327645ms 748.367301ms 748.400215ms 748.572804ms 748.622046ms 748.716691ms 748.840755ms 748.877655ms 748.89388ms 748.982099ms 748.996766ms 749.02203ms 749.13796ms 749.193802ms 749.205255ms 749.404056ms 749.516072ms 749.527598ms 749.532209ms 749.576197ms 749.627139ms 749.723751ms 749.724574ms 749.725625ms 749.752586ms 749.828566ms 749.890683ms 749.918434ms 750.039312ms 750.048982ms 750.225668ms 750.247912ms 750.267343ms 750.297776ms 750.350653ms 750.56455ms 750.730455ms 750.739524ms 750.754997ms 750.849563ms 750.8963ms 750.917764ms 751.00746ms 751.033871ms 751.057022ms 751.087859ms 751.484848ms 751.698547ms 751.845206ms 752.055793ms 752.174945ms 752.295099ms 752.414842ms 752.502368ms 752.813263ms 752.898629ms 753.217077ms 753.323262ms 753.924007ms 754.130331ms 754.201956ms 754.215334ms 754.396256ms 755.720307ms 756.168376ms 756.660373ms 756.714058ms 757.284098ms 758.458883ms 760.70176ms 761.076016ms 762.00287ms 762.326988ms 762.641004ms 763.783454ms 764.374707ms 766.631507ms 767.275068ms 768.89515ms 772.217177ms 778.918316ms 780.056582ms 784.5937ms 800.177601ms 802.608133ms 813.213516ms 813.47166ms 840.4719ms 849.455092ms]
Nov 15 00:51:21.593: INFO: 50 %ile: 747.071597ms
Nov 15 00:51:21.593: INFO: 90 %ile: 760.70176ms
Nov 15 00:51:21.593: INFO: 99 %ile: 840.4719ms
Nov 15 00:51:21.593: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:51:21.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5155" for this suite.
Nov 15 00:51:37.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:51:37.900: INFO: namespace svc-latency-5155 deletion completed in 16.297377613s

• [SLOW TEST:28.577 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:51:37.902: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8830, will wait for the garbage collector to delete the pods
Nov 15 00:51:44.151: INFO: Deleting Job.batch foo took: 7.394141ms
Nov 15 00:51:44.852: INFO: Terminating Job.batch foo pods took: 700.440845ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:52:21.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8830" for this suite.
Nov 15 00:52:27.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:52:27.935: INFO: namespace job-8830 deletion completed in 6.259339753s

• [SLOW TEST:50.033 seconds]
[sig-apps] Job
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:52:27.939: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Nov 15 00:52:28.715: INFO: created pod pod-service-account-defaultsa
Nov 15 00:52:28.715: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 15 00:52:28.732: INFO: created pod pod-service-account-mountsa
Nov 15 00:52:28.732: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 15 00:52:28.769: INFO: created pod pod-service-account-nomountsa
Nov 15 00:52:28.769: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 15 00:52:28.792: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 15 00:52:28.792: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 15 00:52:28.826: INFO: created pod pod-service-account-mountsa-mountspec
Nov 15 00:52:28.826: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 15 00:52:28.929: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 15 00:52:28.930: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 15 00:52:28.953: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 15 00:52:28.953: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 15 00:52:28.974: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 15 00:52:28.974: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 15 00:52:28.985: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 15 00:52:28.985: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:52:28.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9974" for this suite.
Nov 15 00:52:51.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:52:51.216: INFO: namespace svcaccounts-9974 deletion completed in 22.215447369s

• [SLOW TEST:23.278 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:52:51.222: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Nov 15 00:52:51.450: INFO: Waiting up to 5m0s for pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b" in namespace "var-expansion-6920" to be "success or failure"
Nov 15 00:52:51.462: INFO: Pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.528716ms
Nov 15 00:52:53.466: INFO: Pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01574221s
Nov 15 00:52:56.047: INFO: Pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.596312088s
Nov 15 00:52:58.051: INFO: Pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.601196933s
STEP: Saw pod success
Nov 15 00:52:58.051: INFO: Pod "var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b" satisfied condition "success or failure"
Nov 15 00:52:58.065: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b container dapi-container: <nil>
STEP: delete the pod
Nov 15 00:52:58.113: INFO: Waiting for pod var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b to disappear
Nov 15 00:52:58.123: INFO: Pod var-expansion-97fbb3ad-3016-4a2e-af26-80bd3157769b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:52:58.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6920" for this suite.
Nov 15 00:53:04.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:53:04.397: INFO: namespace var-expansion-6920 deletion completed in 6.268698852s

• [SLOW TEST:13.177 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:53:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Nov 15 00:53:04.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 api-versions'
Nov 15 00:53:05.772: INFO: stderr: ""
Nov 15 00:53:05.772: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:53:05.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2955" for this suite.
Nov 15 00:53:11.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:53:11.982: INFO: namespace kubectl-2955 deletion completed in 6.204403821s

• [SLOW TEST:7.581 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:53:11.986: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 15 00:53:17.289: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:53:17.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9531" for this suite.
Nov 15 00:53:23.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:53:23.579: INFO: namespace container-runtime-9531 deletion completed in 6.254707375s

• [SLOW TEST:11.593 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:53:23.580: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 15 00:53:23.810: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27909,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 15 00:53:23.811: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27909,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 15 00:53:33.825: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27932,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 15 00:53:33.826: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27932,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 15 00:53:43.840: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27954,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 15 00:53:43.842: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27954,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 15 00:53:53.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27976,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 15 00:53:53.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-a,UID:b71744ee-dfac-4a4a-9cb7-b0071995e515,ResourceVersion:27976,Generation:0,CreationTimestamp:2019-11-15 00:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 15 00:54:03.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-b,UID:ed9be334-13ab-4f83-ab78-bc4630da19b8,ResourceVersion:27999,Generation:0,CreationTimestamp:2019-11-15 00:54:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 15 00:54:03.869: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-b,UID:ed9be334-13ab-4f83-ab78-bc4630da19b8,ResourceVersion:27999,Generation:0,CreationTimestamp:2019-11-15 00:54:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 15 00:54:13.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-b,UID:ed9be334-13ab-4f83-ab78-bc4630da19b8,ResourceVersion:28022,Generation:0,CreationTimestamp:2019-11-15 00:54:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 15 00:54:13.883: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5522,SelfLink:/api/v1/namespaces/watch-5522/configmaps/e2e-watch-test-configmap-b,UID:ed9be334-13ab-4f83-ab78-bc4630da19b8,ResourceVersion:28022,Generation:0,CreationTimestamp:2019-11-15 00:54:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:54:23.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5522" for this suite.
Nov 15 00:54:30.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:54:30.661: INFO: namespace watch-5522 deletion completed in 6.770229418s

• [SLOW TEST:67.082 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:54:30.667: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 15 00:54:30.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d" in namespace "projected-271" to be "success or failure"
Nov 15 00:54:30.855: INFO: Pod "downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.668964ms
Nov 15 00:54:32.860: INFO: Pod "downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015335234s
Nov 15 00:54:34.866: INFO: Pod "downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02153005s
STEP: Saw pod success
Nov 15 00:54:34.866: INFO: Pod "downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d" satisfied condition "success or failure"
Nov 15 00:54:34.870: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d container client-container: <nil>
STEP: delete the pod
Nov 15 00:54:34.906: INFO: Waiting for pod downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d to disappear
Nov 15 00:54:34.909: INFO: Pod downwardapi-volume-e3755155-85ab-4811-af2a-c9770dec177d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:54:34.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-271" for this suite.
Nov 15 00:54:40.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:54:41.102: INFO: namespace projected-271 deletion completed in 6.186387068s

• [SLOW TEST:10.435 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:54:41.105: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 15 00:54:41.333: INFO: Waiting up to 5m0s for pod "downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df" in namespace "downward-api-933" to be "success or failure"
Nov 15 00:54:41.344: INFO: Pod "downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.751579ms
Nov 15 00:54:43.349: INFO: Pod "downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015324611s
Nov 15 00:54:45.353: INFO: Pod "downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019770521s
STEP: Saw pod success
Nov 15 00:54:45.353: INFO: Pod "downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df" satisfied condition "success or failure"
Nov 15 00:54:45.356: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df container dapi-container: <nil>
STEP: delete the pod
Nov 15 00:54:45.390: INFO: Waiting for pod downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df to disappear
Nov 15 00:54:45.395: INFO: Pod downward-api-9800ae61-8b3e-4b3e-8714-d74100f991df no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:54:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-933" for this suite.
Nov 15 00:54:51.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:54:51.621: INFO: namespace downward-api-933 deletion completed in 6.21992952s

• [SLOW TEST:10.516 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:54:51.632: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5837
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Nov 15 00:54:51.884: INFO: Found 1 stateful pods, waiting for 3
Nov 15 00:55:01.894: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:55:01.895: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:55:01.895: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 00:55:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5837 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:55:02.294: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:55:02.294: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:55:02.294: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov 15 00:55:12.336: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 15 00:55:22.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5837 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:55:23.524: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:55:23.524: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:55:23.524: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 15 00:55:33.550: INFO: Waiting for StatefulSet statefulset-5837/ss2 to complete update
Nov 15 00:55:33.551: INFO: Waiting for Pod statefulset-5837/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov 15 00:55:33.551: INFO: Waiting for Pod statefulset-5837/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov 15 00:55:43.562: INFO: Waiting for StatefulSet statefulset-5837/ss2 to complete update
Nov 15 00:55:43.562: INFO: Waiting for Pod statefulset-5837/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Nov 15 00:55:53.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5837 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 15 00:55:53.969: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 15 00:55:53.969: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 15 00:55:53.969: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 15 00:56:04.013: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 15 00:56:14.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-687135821 exec --namespace=statefulset-5837 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 15 00:56:14.447: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 15 00:56:14.447: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 15 00:56:14.447: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 15 00:56:34.473: INFO: Deleting all statefulset in ns statefulset-5837
Nov 15 00:56:34.477: INFO: Scaling statefulset ss2 to 0
Nov 15 00:56:44.498: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 00:56:44.501: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:56:44.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5837" for this suite.
Nov 15 00:56:50.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:56:50.739: INFO: namespace statefulset-5837 deletion completed in 6.212652353s

• [SLOW TEST:119.108 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:56:50.741: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-a1adbeb4-38bf-4c71-b156-5603d6e4942b in namespace container-probe-8368
Nov 15 00:56:54.995: INFO: Started pod liveness-a1adbeb4-38bf-4c71-b156-5603d6e4942b in namespace container-probe-8368
STEP: checking the pod's current state and verifying that restartCount is present
Nov 15 00:56:54.998: INFO: Initial restart count of pod liveness-a1adbeb4-38bf-4c71-b156-5603d6e4942b is 0
Nov 15 00:57:13.211: INFO: Restart count of pod container-probe-8368/liveness-a1adbeb4-38bf-4c71-b156-5603d6e4942b is now 1 (18.213294272s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:57:13.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8368" for this suite.
Nov 15 00:57:19.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:57:19.867: INFO: namespace container-probe-8368 deletion completed in 6.198463369s

• [SLOW TEST:29.126 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:57:19.870: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 15 00:57:20.114: INFO: Waiting up to 5m0s for pod "pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6" in namespace "emptydir-3584" to be "success or failure"
Nov 15 00:57:20.117: INFO: Pod "pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614608ms
Nov 15 00:57:22.125: INFO: Pod "pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010821797s
Nov 15 00:57:24.131: INFO: Pod "pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01747571s
STEP: Saw pod success
Nov 15 00:57:24.131: INFO: Pod "pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6" satisfied condition "success or failure"
Nov 15 00:57:24.135: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6 container test-container: <nil>
STEP: delete the pod
Nov 15 00:57:24.172: INFO: Waiting for pod pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6 to disappear
Nov 15 00:57:24.180: INFO: Pod pod-9dac7f7c-5ed0-4064-ac13-820d13dda3f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:57:24.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3584" for this suite.
Nov 15 00:57:30.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:57:30.412: INFO: namespace emptydir-3584 deletion completed in 6.221975956s

• [SLOW TEST:10.542 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:57:30.414: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 15 00:57:33.621: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:57:33.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6297" for this suite.
Nov 15 00:57:39.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:57:39.853: INFO: namespace container-runtime-6297 deletion completed in 6.206077869s

• [SLOW TEST:9.439 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:57:39.857: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-74810711-2847-490d-9e16-dbd4ceddec7e
STEP: Creating a pod to test consume secrets
Nov 15 00:57:40.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6" in namespace "projected-5109" to be "success or failure"
Nov 15 00:57:40.049: INFO: Pod "pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.481951ms
Nov 15 00:57:42.054: INFO: Pod "pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022010462s
Nov 15 00:57:44.065: INFO: Pod "pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033526741s
STEP: Saw pod success
Nov 15 00:57:44.065: INFO: Pod "pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6" satisfied condition "success or failure"
Nov 15 00:57:44.067: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 15 00:57:44.098: INFO: Waiting for pod pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6 to disappear
Nov 15 00:57:44.103: INFO: Pod pod-projected-secrets-2819d41b-381b-4d27-962c-43f3e3fc63d6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:57:44.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5109" for this suite.
Nov 15 00:57:50.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:57:50.273: INFO: namespace projected-5109 deletion completed in 6.156409924s

• [SLOW TEST:10.417 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:57:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:57:54.475: INFO: Waiting up to 5m0s for pod "client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93" in namespace "pods-1118" to be "success or failure"
Nov 15 00:57:54.492: INFO: Pod "client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673781ms
Nov 15 00:57:56.497: INFO: Pod "client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016147317s
Nov 15 00:57:58.503: INFO: Pod "client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021839474s
STEP: Saw pod success
Nov 15 00:57:58.503: INFO: Pod "client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93" satisfied condition "success or failure"
Nov 15 00:57:58.506: INFO: Trying to get logs from node k8s-100-3imo44lif6er-minion-0 pod client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93 container env3cont: <nil>
STEP: delete the pod
Nov 15 00:57:58.544: INFO: Waiting for pod client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93 to disappear
Nov 15 00:57:58.550: INFO: Pod client-envvars-b9da778e-8da2-44e2-8a53-c8e23468fd93 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:57:58.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1118" for this suite.
Nov 15 00:58:38.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:58:38.779: INFO: namespace pods-1118 deletion completed in 40.221827821s

• [SLOW TEST:48.505 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:58:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Nov 15 00:58:39.001: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-687135821 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:58:39.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4424" for this suite.
Nov 15 00:58:45.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:58:45.437: INFO: namespace kubectl-4424 deletion completed in 6.289286277s

• [SLOW TEST:6.655 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:58:45.440: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7397
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 15 00:58:45.613: INFO: Creating deployment "nginx-deployment"
Nov 15 00:58:45.621: INFO: Waiting for observed generation 1
Nov 15 00:58:47.633: INFO: Waiting for all required pods to come up
Nov 15 00:58:47.639: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 15 00:58:55.672: INFO: Waiting for deployment "nginx-deployment" to complete
Nov 15 00:58:55.680: INFO: Updating deployment "nginx-deployment" with a non-existent image
Nov 15 00:58:55.689: INFO: Updating deployment nginx-deployment
Nov 15 00:58:55.689: INFO: Waiting for observed generation 2
Nov 15 00:58:57.707: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 15 00:58:57.778: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 15 00:58:57.782: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov 15 00:58:57.793: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 15 00:58:57.793: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 15 00:58:57.797: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov 15 00:58:57.800: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Nov 15 00:58:57.801: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Nov 15 00:58:57.811: INFO: Updating deployment nginx-deployment
Nov 15 00:58:57.811: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Nov 15 00:58:57.826: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 15 00:58:57.865: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 15 00:58:57.948: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7397,SelfLink:/apis/apps/v1/namespaces/deployment-7397/deployments/nginx-deployment,UID:a800e924-472e-4eac-a311-5304fcd9d7e9,ResourceVersion:29436,Generation:3,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-11-15 00:58:55 +0000 UTC 2019-11-15 00:58:45 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-11-15 00:58:57 +0000 UTC 2019-11-15 00:58:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Nov 15 00:58:57.975: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7397,SelfLink:/apis/apps/v1/namespaces/deployment-7397/replicasets/nginx-deployment-55fb7cb77f,UID:a5434dd4-c36c-4806-8ed9-0e47f6b50f3d,ResourceVersion:29469,Generation:3,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a800e924-472e-4eac-a311-5304fcd9d7e9 0xc0036e7b97 0xc0036e7b98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 15 00:58:57.975: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Nov 15 00:58:57.978: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7397,SelfLink:/apis/apps/v1/namespaces/deployment-7397/replicasets/nginx-deployment-7b8c6f4498,UID:c56a9f20-33b4-4741-ac57-403134c8755c,ResourceVersion:29471,Generation:3,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a800e924-472e-4eac-a311-5304fcd9d7e9 0xc0036e7c67 0xc0036e7c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Nov 15 00:58:58.045: INFO: Pod "nginx-deployment-55fb7cb77f-4x8c8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4x8c8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-4x8c8,UID:5c02246c-aaee-4b00-bc16-a9dcb1b2a857,ResourceVersion:29433,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f45d7 0xc0019f45d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4660}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.045: INFO: Pod "nginx-deployment-55fb7cb77f-6ztwd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6ztwd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-6ztwd,UID:60999049-bd5a-4091-9045-98648b43f560,ResourceVersion:29445,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f46d0 0xc0019f46d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4760}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.045: INFO: Pod "nginx-deployment-55fb7cb77f-98zb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-98zb5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-98zb5,UID:20679344-ee1b-4108-b47c-2940fcfc3d48,ResourceVersion:29452,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f47d0 0xc0019f47d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4860}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.046: INFO: Pod "nginx-deployment-55fb7cb77f-fgwp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fgwp2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-fgwp2,UID:b3c183bd-88e1-4bc5-9636-f2a98f71cc79,ResourceVersion:29403,Generation:0,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f48d0 0xc0019f48d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4960}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:,StartTime:2019-11-15 00:58:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.047: INFO: Pod "nginx-deployment-55fb7cb77f-hw5sq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hw5sq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-hw5sq,UID:ba49fb21-c0ad-4c2e-8e2d-cd7741cc0807,ResourceVersion:29457,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4a20 0xc0019f4a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4ab0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.047: INFO: Pod "nginx-deployment-55fb7cb77f-ktkx4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ktkx4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-ktkx4,UID:139d481a-bb80-46ee-a4ea-ef03146d634f,ResourceVersion:29463,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4b20 0xc0019f4b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4bb0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.048: INFO: Pod "nginx-deployment-55fb7cb77f-lcsrw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lcsrw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-lcsrw,UID:2b9b7dfb-ea57-4870-88cf-68dd465709f3,ResourceVersion:29413,Generation:0,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4c20 0xc0019f4c21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4cb0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:,StartTime:2019-11-15 00:58:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.048: INFO: Pod "nginx-deployment-55fb7cb77f-njbjj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-njbjj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-njbjj,UID:d65635df-353e-407b-b7a3-f5f70109fc90,ResourceVersion:29383,Generation:0,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4d70 0xc0019f4d71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4e00}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:,StartTime:2019-11-15 00:58:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.048: INFO: Pod "nginx-deployment-55fb7cb77f-pnhbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pnhbd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-pnhbd,UID:8042f417-61f3-4434-ac78-5ba3be2aba39,ResourceVersion:29479,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4ec0 0xc0019f4ec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f4f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f4f50}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.050: INFO: Pod "nginx-deployment-55fb7cb77f-qg6fm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qg6fm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-qg6fm,UID:f2d47655-60e5-477b-a3b1-5695cc225d43,ResourceVersion:29418,Generation:0,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.60/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f4fd0 0xc0019f4fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5060}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2019-11-15 00:58:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.050: INFO: Pod "nginx-deployment-55fb7cb77f-rr64c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rr64c,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-rr64c,UID:96210990-eb04-4221-a1e0-3d7adac1e799,ResourceVersion:29450,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f5120 0xc0019f5121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f51b0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.051: INFO: Pod "nginx-deployment-55fb7cb77f-tpk9p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tpk9p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-tpk9p,UID:dab4789d-1cb5-43b5-a912-220d262cb737,ResourceVersion:29467,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f5220 0xc0019f5221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f52b0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.051: INFO: Pod "nginx-deployment-55fb7cb77f-xf7ms" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xf7ms,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-55fb7cb77f-xf7ms,UID:19e8cc47-8fbf-41a4-89d8-9d4d2959aa87,ResourceVersion:29414,Generation:0,CreationTimestamp:2019-11-15 00:58:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.59/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a5434dd4-c36c-4806-8ed9-0e47f6b50f3d 0xc0019f5330 0xc0019f5331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f53a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f53c0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2019-11-15 00:58:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.051: INFO: Pod "nginx-deployment-7b8c6f4498-2c8bd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2c8bd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-2c8bd,UID:6ce2fb8f-1a17-4829-851a-5fdb5ac1d46f,ResourceVersion:29311,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f54a0 0xc0019f54a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5520}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:192.168.1.55,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:51 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://eccb7348e012485d4ac5246c095853529443a77f3382da36623fef4c07073960}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.052: INFO: Pod "nginx-deployment-7b8c6f4498-68jd6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-68jd6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-68jd6,UID:6289758c-c659-4b7c-beb4-cda4629d28c6,ResourceVersion:29318,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f55f0 0xc0019f55f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5670}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:192.168.1.57,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:51 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1c86aaaa2844975aae7f941ee991be171d85a6debf06361bce8bab8e0cbe2d12}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.052: INFO: Pod "nginx-deployment-7b8c6f4498-7pm6f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7pm6f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-7pm6f,UID:db9c2629-3157-4aef-bdf2-f1b343a5190a,ResourceVersion:29350,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5740 0xc0019f5741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f57a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f57c0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.26,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:53 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://01c3875884d5bb232e3fb48f44cc64748004d656ebd0d97db4731acd38c5792b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.052: INFO: Pod "nginx-deployment-7b8c6f4498-9hskj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9hskj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-9hskj,UID:19c74185-3c42-4642-80eb-aa0a029c1ddb,ResourceVersion:29447,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5880 0xc0019f5881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f58e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5900}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.053: INFO: Pod "nginx-deployment-7b8c6f4498-9nqdk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9nqdk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-9nqdk,UID:1bab355b-85a7-4ab0-a193-473cbef0543c,ResourceVersion:29448,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5970 0xc0019f5971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f59d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f59f0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.054: INFO: Pod "nginx-deployment-7b8c6f4498-b9j5n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-b9j5n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-b9j5n,UID:39be0a57-0b22-483d-ad0c-cd924d58e116,ResourceVersion:29474,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5a60 0xc0019f5a61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5ae0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.054: INFO: Pod "nginx-deployment-7b8c6f4498-c7dsx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c7dsx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-c7dsx,UID:fed5cd5a-f753-4ab2-929e-cffe2ecb5a82,ResourceVersion:29347,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5b60 0xc0019f5b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5be0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.24,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:53 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b47430e795857f1e8f87e753710424700609f2c6e335822b8ff27fe5dd4f494d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.054: INFO: Pod "nginx-deployment-7b8c6f4498-css7x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-css7x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-css7x,UID:1d6df45f-754f-4bde-8c97-6d09dc53d3f9,ResourceVersion:29428,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5ca0 0xc0019f5ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5d20}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.055: INFO: Pod "nginx-deployment-7b8c6f4498-jmqpf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jmqpf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-jmqpf,UID:b48779df-bc3d-4b4b-83fa-e9befefeae9d,ResourceVersion:29454,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5d90 0xc0019f5d91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5e20}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.055: INFO: Pod "nginx-deployment-7b8c6f4498-jz5cc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jz5cc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-jz5cc,UID:eab55126-44f7-445e-a40e-0c471c55f6e5,ResourceVersion:29478,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5e90 0xc0019f5e91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019f5f10}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.056: INFO: Pod "nginx-deployment-7b8c6f4498-kjndx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kjndx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-kjndx,UID:30759824-5971-4633-aa7d-f3b1ba7a7f90,ResourceVersion:29468,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0019f5f80 0xc0019f5f81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019f5fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be000}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.056: INFO: Pod "nginx-deployment-7b8c6f4498-l9lct" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l9lct,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-l9lct,UID:075fb88d-48d5-4b9d-916f-355e67dbb56d,ResourceVersion:29477,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be070 0xc0032be071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032be0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be0f0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.057: INFO: Pod "nginx-deployment-7b8c6f4498-lx9bf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lx9bf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-lx9bf,UID:fcfa3302-5ba2-460f-ad32-91e2439605e5,ResourceVersion:29475,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be160 0xc0032be161}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032be230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be250}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.057: INFO: Pod "nginx-deployment-7b8c6f4498-rb9rx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rb9rx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-rb9rx,UID:bfd1f878-000a-40ad-b3bb-3a2d7a0a1e10,ResourceVersion:29476,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be340 0xc0032be341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032be450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be470}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.057: INFO: Pod "nginx-deployment-7b8c6f4498-rwgkq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rwgkq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-rwgkq,UID:9322cdcf-9c35-4136-8e1e-373c035d34a8,ResourceVersion:29466,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be530 0xc0032be531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032be650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be670}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.058: INFO: Pod "nginx-deployment-7b8c6f4498-trjlc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-trjlc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-trjlc,UID:77136b10-f049-4056-985c-8a3f3658f705,ResourceVersion:29356,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.22/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be6f0 0xc0032be6f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032be7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032be840}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.22,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:53 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://38e88dcc12c70d76907dc283153a2311aa5d5332c5f0bb417534d38a36a74b2f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.058: INFO: Pod "nginx-deployment-7b8c6f4498-vkvc9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vkvc9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-vkvc9,UID:94e6d9af-5c6c-4b4b-9a27-e2e96d56347a,ResourceVersion:29321,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032be960 0xc0032be961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032bea80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032beaa0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:192.168.1.56,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:52 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://93c75071bec4c807c00fe048e5603d22c88c47bdb4b6aa74546dfea9951be272}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.058: INFO: Pod "nginx-deployment-7b8c6f4498-w9kmd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w9kmd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-w9kmd,UID:7ef32b08-bf26-4550-a159-f81a048a8dab,ResourceVersion:29451,Generation:0,CreationTimestamp:2019-11-15 00:58:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032bebf0 0xc0032bebf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032bec70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032bec90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.059: INFO: Pod "nginx-deployment-7b8c6f4498-xktxm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xktxm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-xktxm,UID:8263ecfd-b7f4-428d-9cb7-3ff45f935a54,ResourceVersion:29332,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.21/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032bed90 0xc0032bed91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032bee40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032bee60}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.6,PodIP:192.168.2.21,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:52 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a4218ac3edcf4056293dfc3d76eb136c79ce6495543c6d246d1ec9f869db036b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 15 00:58:58.059: INFO: Pod "nginx-deployment-7b8c6f4498-xn4jd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xn4jd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7397,SelfLink:/api/v1/namespaces/deployment-7397/pods/nginx-deployment-7b8c6f4498-xn4jd,UID:58c3e3bd-744c-41c9-ac9e-12d478d0bc87,ResourceVersion:29315,Generation:0,CreationTimestamp:2019-11-15 00:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c56a9f20-33b4-4741-ac57-403134c8755c 0xc0032befd0 0xc0032befd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx9tx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx9tx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx9tx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-100-3imo44lif6er-minion-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032bf090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032bf0b0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-15 00:58:45 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.5,PodIP:192.168.1.58,StartTime:2019-11-15 00:58:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-15 00:58:52 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d75edba12b659d127e4ab2711fc83077d674e85469b44f7086163bd69a41de1b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:58:58.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7397" for this suite.
Nov 15 00:59:06.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:59:06.310: INFO: namespace deployment-7397 deletion completed in 8.243810608s

• [SLOW TEST:20.871 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 15 00:59:06.316: INFO: >>> kubeConfig: /tmp/kubeconfig-687135821
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 15 00:59:28.755: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 00:59:28.766: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 00:59:30.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 00:59:30.770: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 00:59:32.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 00:59:32.770: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 15 00:59:32.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7328" for this suite.
Nov 15 00:59:54.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 15 00:59:55.026: INFO: namespace container-lifecycle-hook-7328 deletion completed in 22.247719702s

• [SLOW TEST:48.710 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSNov 15 00:59:55.030: INFO: Running AfterSuite actions on all nodes
Nov 15 00:59:55.031: INFO: Running AfterSuite actions on node 1
Nov 15 00:59:55.031: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6932.616 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h55m35.893125554s
Test Suite Passed
