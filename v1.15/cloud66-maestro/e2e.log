I0130 17:19:57.477372      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-482490662
I0130 17:19:57.477511      16 e2e.go:243] Starting e2e run "38c88da7-0ead-4a74-8028-9d064bc24495" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1580404796 - Will randomize all specs
Will run 215 of 4412 specs

Jan 30 17:19:57.626: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:19:57.630: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 30 17:19:57.663: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 30 17:19:57.707: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 30 17:19:57.708: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 30 17:19:57.708: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 30 17:19:57.723: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 30 17:19:57.723: INFO: e2e test version: v1.15.9-beta.0
Jan 30 17:19:57.725: INFO: kube-apiserver version: v1.15.9-beta.0
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:19:57.725: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
Jan 30 17:19:57.789: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jan 30 17:19:57.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-394'
Jan 30 17:19:58.397: INFO: stderr: ""
Jan 30 17:19:58.397: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 17:19:58.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jan 30 17:19:58.542: INFO: stderr: ""
Jan 30 17:19:58.542: INFO: stdout: "update-demo-nautilus-jd6ss update-demo-nautilus-t9grq "
Jan 30 17:19:58.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-jd6ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:19:58.677: INFO: stderr: ""
Jan 30 17:19:58.677: INFO: stdout: ""
Jan 30 17:19:58.677: INFO: update-demo-nautilus-jd6ss is created but not running
Jan 30 17:20:03.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jan 30 17:20:03.788: INFO: stderr: ""
Jan 30 17:20:03.788: INFO: stdout: "update-demo-nautilus-jd6ss update-demo-nautilus-t9grq "
Jan 30 17:20:03.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-jd6ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:03.897: INFO: stderr: ""
Jan 30 17:20:03.897: INFO: stdout: "true"
Jan 30 17:20:03.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-jd6ss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:03.991: INFO: stderr: ""
Jan 30 17:20:03.991: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 17:20:03.991: INFO: validating pod update-demo-nautilus-jd6ss
Jan 30 17:20:04.003: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 17:20:04.003: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 17:20:04.003: INFO: update-demo-nautilus-jd6ss is verified up and running
Jan 30 17:20:04.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-t9grq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:04.107: INFO: stderr: ""
Jan 30 17:20:04.107: INFO: stdout: "true"
Jan 30 17:20:04.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-t9grq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:04.207: INFO: stderr: ""
Jan 30 17:20:04.207: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 17:20:04.208: INFO: validating pod update-demo-nautilus-t9grq
Jan 30 17:20:04.218: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 17:20:04.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 17:20:04.218: INFO: update-demo-nautilus-t9grq is verified up and running
STEP: rolling-update to new replication controller
Jan 30 17:20:04.220: INFO: scanned /root for discovery docs: <nil>
Jan 30 17:20:04.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-394'
Jan 30 17:20:26.843: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 30 17:20:26.843: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 17:20:26.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jan 30 17:20:26.954: INFO: stderr: ""
Jan 30 17:20:26.954: INFO: stdout: "update-demo-kitten-68blv update-demo-kitten-7zx4j "
Jan 30 17:20:26.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-kitten-68blv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:27.091: INFO: stderr: ""
Jan 30 17:20:27.091: INFO: stdout: "true"
Jan 30 17:20:27.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-kitten-68blv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:27.196: INFO: stderr: ""
Jan 30 17:20:27.196: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 30 17:20:27.196: INFO: validating pod update-demo-kitten-68blv
Jan 30 17:20:27.207: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 30 17:20:27.207: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 30 17:20:27.207: INFO: update-demo-kitten-68blv is verified up and running
Jan 30 17:20:27.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-kitten-7zx4j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:27.312: INFO: stderr: ""
Jan 30 17:20:27.312: INFO: stdout: "true"
Jan 30 17:20:27.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-kitten-7zx4j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jan 30 17:20:27.413: INFO: stderr: ""
Jan 30 17:20:27.413: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 30 17:20:27.413: INFO: validating pod update-demo-kitten-7zx4j
Jan 30 17:20:27.423: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 30 17:20:27.423: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 30 17:20:27.423: INFO: update-demo-kitten-7zx4j is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:20:27.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-394" for this suite.
Jan 30 17:20:49.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:20:49.610: INFO: namespace kubectl-394 deletion completed in 22.180822054s

• [SLOW TEST:51.885 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:20:49.611: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:20:49.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0" in namespace "projected-7040" to be "success or failure"
Jan 30 17:20:49.687: INFO: Pod "downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304987ms
Jan 30 17:20:51.694: INFO: Pod "downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012065993s
Jan 30 17:20:53.700: INFO: Pod "downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018779755s
STEP: Saw pod success
Jan 30 17:20:53.700: INFO: Pod "downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0" satisfied condition "success or failure"
Jan 30 17:20:53.705: INFO: Trying to get logs from node aardvark pod downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0 container client-container: <nil>
STEP: delete the pod
Jan 30 17:20:53.761: INFO: Waiting for pod downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0 to disappear
Jan 30 17:20:53.774: INFO: Pod downwardapi-volume-99730343-5efa-4198-b469-75740a48b6f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:20:53.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7040" for this suite.
Jan 30 17:20:59.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:20:59.959: INFO: namespace projected-7040 deletion completed in 6.177496202s

• [SLOW TEST:10.348 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:20:59.962: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 30 17:21:05.078: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:21:05.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7547" for this suite.
Jan 30 17:21:11.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:21:11.289: INFO: namespace container-runtime-7547 deletion completed in 6.17999749s

• [SLOW TEST:11.328 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:21:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 30 17:21:21.403: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:21.605: INFO: Exec stderr: ""
Jan 30 17:21:21.605: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:21.803: INFO: Exec stderr: ""
Jan 30 17:21:21.803: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:21.803: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:21.991: INFO: Exec stderr: ""
Jan 30 17:21:21.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:21.991: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:22.181: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 30 17:21:22.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:22.181: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:22.375: INFO: Exec stderr: ""
Jan 30 17:21:22.375: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:22.375: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:22.577: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 30 17:21:22.577: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:22.577: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:22.741: INFO: Exec stderr: ""
Jan 30 17:21:22.741: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:22.741: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:22.906: INFO: Exec stderr: ""
Jan 30 17:21:22.906: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:23.064: INFO: Exec stderr: ""
Jan 30 17:21:23.064: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:21:23.064: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:21:23.255: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:21:23.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1914" for this suite.
Jan 30 17:22:07.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:22:07.428: INFO: namespace e2e-kubelet-etc-hosts-1914 deletion completed in 44.16410516s

• [SLOW TEST:56.139 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:22:07.429: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 30 17:22:07.490: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:22:25.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6032" for this suite.
Jan 30 17:22:31.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:22:31.377: INFO: namespace pods-6032 deletion completed in 6.174199013s

• [SLOW TEST:23.949 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:22:31.379: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1d2f9a16-c4e4-4e53-8178-3d7609871085
STEP: Creating a pod to test consume configMaps
Jan 30 17:22:31.462: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7" in namespace "projected-560" to be "success or failure"
Jan 30 17:22:31.469: INFO: Pod "pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.96423ms
Jan 30 17:22:33.478: INFO: Pod "pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015669456s
Jan 30 17:22:35.484: INFO: Pod "pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021842597s
STEP: Saw pod success
Jan 30 17:22:35.484: INFO: Pod "pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7" satisfied condition "success or failure"
Jan 30 17:22:35.489: INFO: Trying to get logs from node gazelle pod pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:22:35.538: INFO: Waiting for pod pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7 to disappear
Jan 30 17:22:35.543: INFO: Pod pod-projected-configmaps-536b8a18-63f1-4c88-9bb1-aee2bb88afb7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:22:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-560" for this suite.
Jan 30 17:22:41.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:22:41.723: INFO: namespace projected-560 deletion completed in 6.172584526s

• [SLOW TEST:10.344 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:22:41.724: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6231
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 30 17:22:41.775: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 30 17:23:13.900: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.9:8080/dial?request=hostName&protocol=http&host=25.0.2.7&port=8080&tries=1'] Namespace:pod-network-test-6231 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:23:13.900: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:23:14.090: INFO: Waiting for endpoints: map[]
Jan 30 17:23:14.095: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.9:8080/dial?request=hostName&protocol=http&host=25.0.1.8&port=8080&tries=1'] Namespace:pod-network-test-6231 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:23:14.096: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:23:14.267: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:23:14.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6231" for this suite.
Jan 30 17:23:36.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:23:36.444: INFO: namespace pod-network-test-6231 deletion completed in 22.169579152s

• [SLOW TEST:54.720 seconds]
[sig-network] Networking
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:23:36.444: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jan 30 17:23:40.523: INFO: Pod pod-hostip-3a4667e1-3b26-4882-a67a-56a207b6b806 has hostIP: 167.172.59.233
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:23:40.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6267" for this suite.
Jan 30 17:24:02.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:24:02.706: INFO: namespace pods-6267 deletion completed in 22.176302082s

• [SLOW TEST:26.261 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:24:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-8lmq
STEP: Creating a pod to test atomic-volume-subpath
Jan 30 17:24:02.792: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8lmq" in namespace "subpath-3292" to be "success or failure"
Jan 30 17:24:02.802: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.242014ms
Jan 30 17:24:04.809: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01630437s
Jan 30 17:24:06.815: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 4.022227232s
Jan 30 17:24:08.823: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 6.030668372s
Jan 30 17:24:10.830: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 8.037476205s
Jan 30 17:24:12.837: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 10.044130776s
Jan 30 17:24:14.843: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 12.050473541s
Jan 30 17:24:16.848: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 14.055749105s
Jan 30 17:24:18.854: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 16.06212639s
Jan 30 17:24:20.861: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 18.068312903s
Jan 30 17:24:22.867: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 20.07437557s
Jan 30 17:24:24.873: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Running", Reason="", readiness=true. Elapsed: 22.080860409s
Jan 30 17:24:26.879: INFO: Pod "pod-subpath-test-configmap-8lmq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086886486s
STEP: Saw pod success
Jan 30 17:24:26.879: INFO: Pod "pod-subpath-test-configmap-8lmq" satisfied condition "success or failure"
Jan 30 17:24:26.885: INFO: Trying to get logs from node gazelle pod pod-subpath-test-configmap-8lmq container test-container-subpath-configmap-8lmq: <nil>
STEP: delete the pod
Jan 30 17:24:26.917: INFO: Waiting for pod pod-subpath-test-configmap-8lmq to disappear
Jan 30 17:24:26.923: INFO: Pod pod-subpath-test-configmap-8lmq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8lmq
Jan 30 17:24:26.923: INFO: Deleting pod "pod-subpath-test-configmap-8lmq" in namespace "subpath-3292"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:24:26.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3292" for this suite.
Jan 30 17:24:32.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:24:33.112: INFO: namespace subpath-3292 deletion completed in 6.180822986s

• [SLOW TEST:30.405 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:24:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jan 30 17:24:33.168: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 17:24:33.179: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 17:24:33.184: INFO: 
Logging pods the kubelet thinks is on node aardvark before test
Jan 30 17:24:33.195: INFO: kube-flannel-ds-amd64-mlnjj from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 17:24:33.195: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 17:24:33.195: INFO: kube-proxy-rlk8c from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 17:24:33.195: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 17:24:33.195: INFO: sonobuoy from sonobuoy started at 2020-01-30 17:19:28 +0000 UTC (1 container statuses recorded)
Jan 30 17:24:33.195: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 17:24:33.195: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-22zg6 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 17:24:33.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 17:24:33.195: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 17:24:33.195: INFO: 
Logging pods the kubelet thinks is on node gazelle before test
Jan 30 17:24:33.207: INFO: kube-flannel-ds-amd64-dbdl8 from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 17:24:33.207: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 17:24:33.207: INFO: kube-proxy-l62gs from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 17:24:33.207: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 17:24:33.207: INFO: sonobuoy-e2e-job-70068240cc4e41b9 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 17:24:33.207: INFO: 	Container e2e ready: true, restart count 0
Jan 30 17:24:33.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 17:24:33.207: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-5w56c from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 17:24:33.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 17:24:33.207: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node aardvark
STEP: verifying the node has the label node gazelle
Jan 30 17:24:33.263: INFO: Pod kube-flannel-ds-amd64-dbdl8 requesting resource cpu=100m on Node gazelle
Jan 30 17:24:33.263: INFO: Pod kube-flannel-ds-amd64-mlnjj requesting resource cpu=100m on Node aardvark
Jan 30 17:24:33.263: INFO: Pod kube-proxy-l62gs requesting resource cpu=0m on Node gazelle
Jan 30 17:24:33.263: INFO: Pod kube-proxy-rlk8c requesting resource cpu=0m on Node aardvark
Jan 30 17:24:33.263: INFO: Pod sonobuoy requesting resource cpu=0m on Node aardvark
Jan 30 17:24:33.263: INFO: Pod sonobuoy-e2e-job-70068240cc4e41b9 requesting resource cpu=0m on Node gazelle
Jan 30 17:24:33.263: INFO: Pod sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-22zg6 requesting resource cpu=0m on Node aardvark
Jan 30 17:24:33.263: INFO: Pod sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-5w56c requesting resource cpu=0m on Node gazelle
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd.15eeba0a72581fbb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7437/filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd to aardvark]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd.15eeba0ab5950e0d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd.15eeba0abac68a68], Reason = [Created], Message = [Created container filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd.15eeba0acae6a20a], Reason = [Started], Message = [Started container filler-pod-3aa79baa-734d-44b4-838b-00edbdb230cd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875.15eeba0a73004e84], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7437/filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875 to gazelle]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875.15eeba0aba2dc4f7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875.15eeba0ac1fd6841], Reason = [Created], Message = [Created container filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875.15eeba0acf6a3db4], Reason = [Started], Message = [Started container filler-pod-ff5cd639-a29d-4bd1-8f8b-f261bb9a7875]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15eeba0b6314a3a7], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node gazelle
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node aardvark
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:24:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7437" for this suite.
Jan 30 17:24:44.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:24:44.548: INFO: namespace sched-pred-7437 deletion completed in 6.167857719s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.436 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:24:44.549: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jan 30 17:24:44.598: INFO: PodSpec: initContainers in spec.initContainers
Jan 30 17:25:33.777: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0bff8606-eae4-462b-a10e-d47b6bf19e2f", GenerateName:"", Namespace:"init-container-1171", SelfLink:"/api/v1/namespaces/init-container-1171/pods/pod-init-0bff8606-eae4-462b-a10e-d47b6bf19e2f", UID:"b5333cf2-0840-43b8-8234-43d657801ac8", ResourceVersion:"1184611", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63716001884, loc:(*time.Location)(0x7ed0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"598256026"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-t54k9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020ce000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t54k9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t54k9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t54k9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027d8088), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aardvark", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028c6060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027d8110)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027d8130)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027d8138), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027d813c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716001884, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716001884, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716001884, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716001884, loc:(*time.Location)(0x7ed0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"167.172.59.233", PodIP:"25.0.1.12", StartTime:(*v1.Time)(0xc001cc8060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00252e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00252e150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://bb6016d21bedcbd151a4dd47d572c4e8039a57066825aa0ba85dea313c4f8d11"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cc80a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cc8080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:25:33.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1171" for this suite.
Jan 30 17:25:55.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:25:55.964: INFO: namespace init-container-1171 deletion completed in 22.179967847s

• [SLOW TEST:71.415 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:25:55.964: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jan 30 17:25:56.033: INFO: Waiting up to 5m0s for pod "client-containers-cdd79277-07dd-42fd-b947-03a23e22c107" in namespace "containers-8310" to be "success or failure"
Jan 30 17:25:56.041: INFO: Pod "client-containers-cdd79277-07dd-42fd-b947-03a23e22c107": Phase="Pending", Reason="", readiness=false. Elapsed: 7.892449ms
Jan 30 17:25:58.048: INFO: Pod "client-containers-cdd79277-07dd-42fd-b947-03a23e22c107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014356516s
Jan 30 17:26:00.053: INFO: Pod "client-containers-cdd79277-07dd-42fd-b947-03a23e22c107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019573183s
STEP: Saw pod success
Jan 30 17:26:00.053: INFO: Pod "client-containers-cdd79277-07dd-42fd-b947-03a23e22c107" satisfied condition "success or failure"
Jan 30 17:26:00.057: INFO: Trying to get logs from node gazelle pod client-containers-cdd79277-07dd-42fd-b947-03a23e22c107 container test-container: <nil>
STEP: delete the pod
Jan 30 17:26:00.089: INFO: Waiting for pod client-containers-cdd79277-07dd-42fd-b947-03a23e22c107 to disappear
Jan 30 17:26:00.092: INFO: Pod client-containers-cdd79277-07dd-42fd-b947-03a23e22c107 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:26:00.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8310" for this suite.
Jan 30 17:26:06.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:26:06.270: INFO: namespace containers-8310 deletion completed in 6.172699676s

• [SLOW TEST:10.305 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:26:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-75e19ba3-6535-448f-9815-35064022033d
STEP: Creating configMap with name cm-test-opt-upd-f7bcd813-8208-4b1f-9b96-69bb8f31c4fc
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-75e19ba3-6535-448f-9815-35064022033d
STEP: Updating configmap cm-test-opt-upd-f7bcd813-8208-4b1f-9b96-69bb8f31c4fc
STEP: Creating configMap with name cm-test-opt-create-64f69ad4-63e5-4ab9-8ebe-153fe9d70808
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:26:14.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5457" for this suite.
Jan 30 17:26:36.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:26:36.698: INFO: namespace configmap-5457 deletion completed in 22.171582233s

• [SLOW TEST:30.428 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:26:36.698: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:26:40.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4310" for this suite.
Jan 30 17:26:46.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:26:46.934: INFO: namespace kubelet-test-4310 deletion completed in 6.155080574s

• [SLOW TEST:10.236 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:26:46.935: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2792, will wait for the garbage collector to delete the pods
Jan 30 17:26:51.065: INFO: Deleting Job.batch foo took: 13.439049ms
Jan 30 17:26:51.365: INFO: Terminating Job.batch foo pods took: 300.298643ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:27:34.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2792" for this suite.
Jan 30 17:27:40.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:27:40.762: INFO: namespace job-2792 deletion completed in 6.184453377s

• [SLOW TEST:53.827 seconds]
[sig-apps] Job
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:27:40.763: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6cddb2b4-db80-4629-afb3-3c6dbd975efc
STEP: Creating a pod to test consume configMaps
Jan 30 17:27:40.835: INFO: Waiting up to 5m0s for pod "pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80" in namespace "configmap-6426" to be "success or failure"
Jan 30 17:27:40.842: INFO: Pod "pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.868128ms
Jan 30 17:27:42.847: INFO: Pod "pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012076661s
Jan 30 17:27:44.854: INFO: Pod "pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018558741s
STEP: Saw pod success
Jan 30 17:27:44.854: INFO: Pod "pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80" satisfied condition "success or failure"
Jan 30 17:27:44.859: INFO: Trying to get logs from node aardvark pod pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:27:44.890: INFO: Waiting for pod pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80 to disappear
Jan 30 17:27:44.895: INFO: Pod pod-configmaps-7989449c-8f9f-4d68-a7a1-050a43da3b80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:27:44.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6426" for this suite.
Jan 30 17:27:50.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:27:51.051: INFO: namespace configmap-6426 deletion completed in 6.150564937s

• [SLOW TEST:10.289 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:27:51.056: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jan 30 17:27:51.103: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:27:55.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9417" for this suite.
Jan 30 17:28:01.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:01.510: INFO: namespace init-container-9417 deletion completed in 6.154582999s

• [SLOW TEST:10.455 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:28:01.510: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 17:28:01.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3437'
Jan 30 17:28:01.676: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 30 17:28:01.676: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Jan 30 17:28:01.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3437'
Jan 30 17:28:01.804: INFO: stderr: ""
Jan 30 17:28:01.804: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:28:01.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3437" for this suite.
Jan 30 17:28:07.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:08.000: INFO: namespace kubectl-3437 deletion completed in 6.189837608s

• [SLOW TEST:6.490 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:28:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 30 17:28:12.603: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8076 pod-service-account-1115f770-1ce7-489d-97eb-97a4da72f3c1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 30 17:28:12.882: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8076 pod-service-account-1115f770-1ce7-489d-97eb-97a4da72f3c1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 30 17:28:13.187: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8076 pod-service-account-1115f770-1ce7-489d-97eb-97a4da72f3c1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:28:13.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8076" for this suite.
Jan 30 17:28:19.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:19.676: INFO: namespace svcaccounts-8076 deletion completed in 6.179589188s

• [SLOW TEST:11.675 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:28:19.677: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:28:19.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a" in namespace "downward-api-7249" to be "success or failure"
Jan 30 17:28:19.753: INFO: Pod "downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33299ms
Jan 30 17:28:21.759: INFO: Pod "downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015128332s
Jan 30 17:28:23.766: INFO: Pod "downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022371079s
STEP: Saw pod success
Jan 30 17:28:23.766: INFO: Pod "downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a" satisfied condition "success or failure"
Jan 30 17:28:23.771: INFO: Trying to get logs from node aardvark pod downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a container client-container: <nil>
STEP: delete the pod
Jan 30 17:28:23.803: INFO: Waiting for pod downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a to disappear
Jan 30 17:28:23.807: INFO: Pod downwardapi-volume-a1429d49-ed95-48ae-9011-8689af2f9d7a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:28:23.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7249" for this suite.
Jan 30 17:28:29.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:30.009: INFO: namespace downward-api-7249 deletion completed in 6.19572534s

• [SLOW TEST:10.332 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:28:30.011: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:28:36.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6664" for this suite.
Jan 30 17:28:42.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:42.387: INFO: namespace namespaces-6664 deletion completed in 6.173867808s
STEP: Destroying namespace "nsdeletetest-9371" for this suite.
Jan 30 17:28:42.391: INFO: Namespace nsdeletetest-9371 was already deleted
STEP: Destroying namespace "nsdeletetest-1216" for this suite.
Jan 30 17:28:48.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:28:48.571: INFO: namespace nsdeletetest-1216 deletion completed in 6.180228289s

• [SLOW TEST:18.561 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:28:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4cgvw in namespace proxy-633
I0130 17:28:48.652728      16 runners.go:180] Created replication controller with name: proxy-service-4cgvw, namespace: proxy-633, replica count: 1
I0130 17:28:49.703268      16 runners.go:180] proxy-service-4cgvw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 17:28:50.703674      16 runners.go:180] proxy-service-4cgvw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 17:28:51.703985      16 runners.go:180] proxy-service-4cgvw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 17:28:52.704436      16 runners.go:180] proxy-service-4cgvw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 17:28:52.722: INFO: setup took 4.099390941s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 30 17:28:52.732: INFO: (0) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.723962ms)
Jan 30 17:28:52.737: INFO: (0) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 14.815883ms)
Jan 30 17:28:52.738: INFO: (0) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 15.249285ms)
Jan 30 17:28:52.743: INFO: (0) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 20.932783ms)
Jan 30 17:28:52.744: INFO: (0) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 21.784696ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 22.610904ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 22.448202ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 22.946996ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 23.033773ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 23.032576ms)
Jan 30 17:28:52.745: INFO: (0) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 22.936806ms)
Jan 30 17:28:52.747: INFO: (0) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 24.43188ms)
Jan 30 17:28:52.750: INFO: (0) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 27.331827ms)
Jan 30 17:28:52.752: INFO: (0) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 30.065844ms)
Jan 30 17:28:52.756: INFO: (0) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 33.57693ms)
Jan 30 17:28:52.760: INFO: (0) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 37.356945ms)
Jan 30 17:28:52.769: INFO: (1) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 9.591115ms)
Jan 30 17:28:52.769: INFO: (1) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 9.426744ms)
Jan 30 17:28:52.769: INFO: (1) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 9.481824ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 9.34984ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.436518ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 9.696496ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 9.860498ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 10.482624ms)
Jan 30 17:28:52.770: INFO: (1) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 10.467551ms)
Jan 30 17:28:52.771: INFO: (1) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 11.046929ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 11.887808ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 11.67725ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 11.869647ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 12.006705ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 12.132491ms)
Jan 30 17:28:52.772: INFO: (1) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 12.355744ms)
Jan 30 17:28:52.779: INFO: (2) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.397307ms)
Jan 30 17:28:52.779: INFO: (2) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 6.324643ms)
Jan 30 17:28:52.779: INFO: (2) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.312104ms)
Jan 30 17:28:52.779: INFO: (2) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 6.242077ms)
Jan 30 17:28:52.779: INFO: (2) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.348793ms)
Jan 30 17:28:52.781: INFO: (2) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 8.245875ms)
Jan 30 17:28:52.782: INFO: (2) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 9.72958ms)
Jan 30 17:28:52.782: INFO: (2) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 9.89975ms)
Jan 30 17:28:52.783: INFO: (2) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 10.083751ms)
Jan 30 17:28:52.783: INFO: (2) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 10.067975ms)
Jan 30 17:28:52.783: INFO: (2) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 10.179188ms)
Jan 30 17:28:52.783: INFO: (2) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 10.159149ms)
Jan 30 17:28:52.784: INFO: (2) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 11.569638ms)
Jan 30 17:28:52.784: INFO: (2) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 11.65991ms)
Jan 30 17:28:52.784: INFO: (2) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 11.661702ms)
Jan 30 17:28:52.784: INFO: (2) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 11.549956ms)
Jan 30 17:28:52.789: INFO: (3) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 5.096965ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 7.030457ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 7.107912ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.299382ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.373873ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.348984ms)
Jan 30 17:28:52.792: INFO: (3) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 7.303221ms)
Jan 30 17:28:52.793: INFO: (3) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 7.966888ms)
Jan 30 17:28:52.793: INFO: (3) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 8.028417ms)
Jan 30 17:28:52.793: INFO: (3) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.711838ms)
Jan 30 17:28:52.794: INFO: (3) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 9.590377ms)
Jan 30 17:28:52.795: INFO: (3) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 10.499921ms)
Jan 30 17:28:52.796: INFO: (3) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 11.790605ms)
Jan 30 17:28:52.797: INFO: (3) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 12.191992ms)
Jan 30 17:28:52.797: INFO: (3) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 12.126409ms)
Jan 30 17:28:52.797: INFO: (3) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 12.51148ms)
Jan 30 17:28:52.801: INFO: (4) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 4.307404ms)
Jan 30 17:28:52.803: INFO: (4) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 6.423421ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.24818ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 6.306317ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.464111ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 7.080996ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 7.172174ms)
Jan 30 17:28:52.804: INFO: (4) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.116971ms)
Jan 30 17:28:52.805: INFO: (4) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 7.51034ms)
Jan 30 17:28:52.805: INFO: (4) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.829308ms)
Jan 30 17:28:52.805: INFO: (4) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 7.962105ms)
Jan 30 17:28:52.806: INFO: (4) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 9.169962ms)
Jan 30 17:28:52.807: INFO: (4) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 9.731618ms)
Jan 30 17:28:52.807: INFO: (4) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 9.68274ms)
Jan 30 17:28:52.807: INFO: (4) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 9.498741ms)
Jan 30 17:28:52.807: INFO: (4) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 9.766445ms)
Jan 30 17:28:52.814: INFO: (5) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.674042ms)
Jan 30 17:28:52.814: INFO: (5) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 6.646758ms)
Jan 30 17:28:52.814: INFO: (5) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.226278ms)
Jan 30 17:28:52.815: INFO: (5) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 8.021123ms)
Jan 30 17:28:52.815: INFO: (5) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 8.160516ms)
Jan 30 17:28:52.815: INFO: (5) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.353305ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 8.243891ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 8.114973ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.044038ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 8.307852ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 8.910874ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 8.707523ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 8.771307ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 8.765382ms)
Jan 30 17:28:52.816: INFO: (5) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 8.874436ms)
Jan 30 17:28:52.818: INFO: (5) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 10.553816ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 5.996654ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 5.875415ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 5.923112ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 6.086128ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 5.996382ms)
Jan 30 17:28:52.824: INFO: (6) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.293618ms)
Jan 30 17:28:52.825: INFO: (6) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.671798ms)
Jan 30 17:28:52.825: INFO: (6) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 7.292884ms)
Jan 30 17:28:52.825: INFO: (6) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.86577ms)
Jan 30 17:28:52.825: INFO: (6) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.048499ms)
Jan 30 17:28:52.826: INFO: (6) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 7.786698ms)
Jan 30 17:28:52.827: INFO: (6) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 8.912845ms)
Jan 30 17:28:52.827: INFO: (6) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 9.075413ms)
Jan 30 17:28:52.828: INFO: (6) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 9.213702ms)
Jan 30 17:28:52.829: INFO: (6) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 10.182839ms)
Jan 30 17:28:52.829: INFO: (6) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 10.457352ms)
Jan 30 17:28:52.834: INFO: (7) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 4.500848ms)
Jan 30 17:28:52.834: INFO: (7) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 4.578557ms)
Jan 30 17:28:52.835: INFO: (7) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 5.541398ms)
Jan 30 17:28:52.835: INFO: (7) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 5.859916ms)
Jan 30 17:28:52.835: INFO: (7) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 6.044105ms)
Jan 30 17:28:52.835: INFO: (7) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 6.294216ms)
Jan 30 17:28:52.835: INFO: (7) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 6.106622ms)
Jan 30 17:28:52.837: INFO: (7) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 7.746048ms)
Jan 30 17:28:52.837: INFO: (7) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.998636ms)
Jan 30 17:28:52.837: INFO: (7) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.942898ms)
Jan 30 17:28:52.838: INFO: (7) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 9.246563ms)
Jan 30 17:28:52.839: INFO: (7) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 9.605848ms)
Jan 30 17:28:52.840: INFO: (7) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 10.051171ms)
Jan 30 17:28:52.840: INFO: (7) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 10.795093ms)
Jan 30 17:28:52.840: INFO: (7) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 11.215433ms)
Jan 30 17:28:52.840: INFO: (7) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 11.092645ms)
Jan 30 17:28:52.847: INFO: (8) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 5.917049ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 7.568487ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.531927ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 7.533429ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 7.454588ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.610597ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.664679ms)
Jan 30 17:28:52.848: INFO: (8) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 7.938675ms)
Jan 30 17:28:52.849: INFO: (8) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.84089ms)
Jan 30 17:28:52.849: INFO: (8) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 8.203664ms)
Jan 30 17:28:52.850: INFO: (8) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 9.171814ms)
Jan 30 17:28:52.851: INFO: (8) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 10.533094ms)
Jan 30 17:28:52.851: INFO: (8) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 10.585359ms)
Jan 30 17:28:52.853: INFO: (8) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 11.966742ms)
Jan 30 17:28:52.853: INFO: (8) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 11.755769ms)
Jan 30 17:28:52.853: INFO: (8) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 12.396394ms)
Jan 30 17:28:52.866: INFO: (9) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 12.761529ms)
Jan 30 17:28:52.867: INFO: (9) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 13.343538ms)
Jan 30 17:28:52.867: INFO: (9) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 13.29152ms)
Jan 30 17:28:52.867: INFO: (9) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 14.058239ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 15.245358ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 15.246635ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 15.266877ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 15.319865ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 15.856693ms)
Jan 30 17:28:52.869: INFO: (9) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 15.804686ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 16.331536ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 16.515661ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 16.350612ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 16.336235ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 16.377572ms)
Jan 30 17:28:52.870: INFO: (9) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 16.744712ms)
Jan 30 17:28:52.876: INFO: (10) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 5.373659ms)
Jan 30 17:28:52.876: INFO: (10) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 5.887838ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.993268ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 6.495124ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 6.857942ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 6.707646ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 7.640598ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 7.087294ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 7.244274ms)
Jan 30 17:28:52.878: INFO: (10) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 7.001114ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 8.472752ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 7.628391ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 8.089071ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 8.295843ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 7.493816ms)
Jan 30 17:28:52.879: INFO: (10) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 8.259616ms)
Jan 30 17:28:52.885: INFO: (11) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 5.408689ms)
Jan 30 17:28:52.885: INFO: (11) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 5.923531ms)
Jan 30 17:28:52.885: INFO: (11) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 5.736627ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 6.407141ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 6.669381ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 6.407511ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 6.37988ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 6.465318ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.908764ms)
Jan 30 17:28:52.886: INFO: (11) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 6.862813ms)
Jan 30 17:28:52.888: INFO: (11) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 8.451018ms)
Jan 30 17:28:52.889: INFO: (11) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 9.675892ms)
Jan 30 17:28:52.889: INFO: (11) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 9.860715ms)
Jan 30 17:28:52.889: INFO: (11) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 9.813991ms)
Jan 30 17:28:52.889: INFO: (11) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 10.128948ms)
Jan 30 17:28:52.889: INFO: (11) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 9.880163ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 8.945854ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.465972ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 8.645964ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 9.760072ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 8.653756ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.982474ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 8.930781ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 9.122664ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 9.201601ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.53594ms)
Jan 30 17:28:52.899: INFO: (12) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 9.279275ms)
Jan 30 17:28:52.902: INFO: (12) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 12.561509ms)
Jan 30 17:28:52.903: INFO: (12) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 13.281153ms)
Jan 30 17:28:52.903: INFO: (12) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 13.249733ms)
Jan 30 17:28:52.903: INFO: (12) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 12.967376ms)
Jan 30 17:28:52.907: INFO: (12) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 16.7175ms)
Jan 30 17:28:52.914: INFO: (13) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 7.103751ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 10.465808ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 10.924585ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 11.186847ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 11.235324ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 10.962389ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 11.186635ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 11.114835ms)
Jan 30 17:28:52.918: INFO: (13) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 11.095877ms)
Jan 30 17:28:52.919: INFO: (13) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 11.320715ms)
Jan 30 17:28:52.920: INFO: (13) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 12.972828ms)
Jan 30 17:28:52.922: INFO: (13) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 14.288257ms)
Jan 30 17:28:52.922: INFO: (13) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 14.44723ms)
Jan 30 17:28:52.922: INFO: (13) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 14.516843ms)
Jan 30 17:28:52.922: INFO: (13) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 14.871262ms)
Jan 30 17:28:52.923: INFO: (13) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 15.536484ms)
Jan 30 17:28:52.932: INFO: (14) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 8.730664ms)
Jan 30 17:28:52.932: INFO: (14) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.229907ms)
Jan 30 17:28:52.932: INFO: (14) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 9.258797ms)
Jan 30 17:28:52.932: INFO: (14) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 9.098615ms)
Jan 30 17:28:52.932: INFO: (14) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 9.004461ms)
Jan 30 17:28:52.933: INFO: (14) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.36581ms)
Jan 30 17:28:52.933: INFO: (14) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 9.340666ms)
Jan 30 17:28:52.933: INFO: (14) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 9.569721ms)
Jan 30 17:28:52.933: INFO: (14) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 9.846722ms)
Jan 30 17:28:52.933: INFO: (14) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 10.200142ms)
Jan 30 17:28:52.934: INFO: (14) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 11.061708ms)
Jan 30 17:28:52.935: INFO: (14) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 11.345543ms)
Jan 30 17:28:52.936: INFO: (14) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 12.988521ms)
Jan 30 17:28:52.937: INFO: (14) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 13.35591ms)
Jan 30 17:28:52.937: INFO: (14) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 14.133961ms)
Jan 30 17:28:52.937: INFO: (14) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 14.217555ms)
Jan 30 17:28:52.946: INFO: (15) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 8.453579ms)
Jan 30 17:28:52.946: INFO: (15) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 8.591543ms)
Jan 30 17:28:52.946: INFO: (15) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 8.775854ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 8.921455ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 9.014952ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 9.017689ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 8.961209ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 9.227445ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 9.455486ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 9.579557ms)
Jan 30 17:28:52.947: INFO: (15) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 9.47237ms)
Jan 30 17:28:52.952: INFO: (15) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 14.823869ms)
Jan 30 17:28:52.953: INFO: (15) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 15.218843ms)
Jan 30 17:28:52.953: INFO: (15) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 15.664359ms)
Jan 30 17:28:52.954: INFO: (15) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 16.229709ms)
Jan 30 17:28:52.954: INFO: (15) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 16.423209ms)
Jan 30 17:28:52.963: INFO: (16) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 9.09799ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 8.130257ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.342304ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 8.889809ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 8.047785ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 7.802982ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 9.624893ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 8.438142ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 9.375005ms)
Jan 30 17:28:52.964: INFO: (16) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 8.694738ms)
Jan 30 17:28:52.966: INFO: (16) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 11.197428ms)
Jan 30 17:28:52.967: INFO: (16) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 11.389826ms)
Jan 30 17:28:52.967: INFO: (16) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 11.36952ms)
Jan 30 17:28:52.967: INFO: (16) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 11.744466ms)
Jan 30 17:28:52.967: INFO: (16) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 11.287327ms)
Jan 30 17:28:52.967: INFO: (16) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 10.802592ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.931522ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 9.896361ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 9.861682ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 10.026839ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 10.137734ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 9.975815ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 10.091706ms)
Jan 30 17:28:52.977: INFO: (17) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 10.187938ms)
Jan 30 17:28:52.978: INFO: (17) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 10.033527ms)
Jan 30 17:28:52.978: INFO: (17) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 10.390642ms)
Jan 30 17:28:52.978: INFO: (17) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 10.347727ms)
Jan 30 17:28:52.978: INFO: (17) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 10.613416ms)
Jan 30 17:28:52.979: INFO: (17) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 11.463311ms)
Jan 30 17:28:52.979: INFO: (17) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 11.230783ms)
Jan 30 17:28:52.979: INFO: (17) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 11.746573ms)
Jan 30 17:28:52.979: INFO: (17) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 11.907058ms)
Jan 30 17:28:52.988: INFO: (18) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 8.599608ms)
Jan 30 17:28:52.989: INFO: (18) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 9.74564ms)
Jan 30 17:28:52.990: INFO: (18) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 10.244974ms)
Jan 30 17:28:52.991: INFO: (18) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 11.736415ms)
Jan 30 17:28:52.991: INFO: (18) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 11.782174ms)
Jan 30 17:28:52.991: INFO: (18) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 11.977752ms)
Jan 30 17:28:52.992: INFO: (18) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 11.978559ms)
Jan 30 17:28:52.992: INFO: (18) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 12.038412ms)
Jan 30 17:28:52.992: INFO: (18) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 12.528159ms)
Jan 30 17:28:52.992: INFO: (18) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 13.018832ms)
Jan 30 17:28:52.993: INFO: (18) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 13.280909ms)
Jan 30 17:28:52.993: INFO: (18) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 13.599752ms)
Jan 30 17:28:52.998: INFO: (18) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 18.869761ms)
Jan 30 17:28:52.999: INFO: (18) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 19.448394ms)
Jan 30 17:28:53.003: INFO: (18) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 23.270808ms)
Jan 30 17:28:53.007: INFO: (18) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 27.911464ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:460/proxy/: tls baz (200; 31.181131ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 31.699408ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:162/proxy/: bar (200; 31.842076ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:443/proxy/tlsrewriteme... (200; 31.762623ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc/proxy/rewriteme">test</a> (200; 31.82128ms)
Jan 30 17:28:53.039: INFO: (19) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 31.874659ms)
Jan 30 17:28:53.040: INFO: (19) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:160/proxy/: foo (200; 31.840614ms)
Jan 30 17:28:53.040: INFO: (19) /api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/http:proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">t... (200; 32.436229ms)
Jan 30 17:28:53.040: INFO: (19) /api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/: <a href="/api/v1/namespaces/proxy-633/pods/proxy-service-4cgvw-kqttc:1080/proxy/rewriteme">test</... (200; 32.658418ms)
Jan 30 17:28:53.040: INFO: (19) /api/v1/namespaces/proxy-633/pods/https:proxy-service-4cgvw-kqttc:462/proxy/: tls qux (200; 32.797818ms)
Jan 30 17:28:53.040: INFO: (19) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname2/proxy/: tls qux (200; 32.864028ms)
Jan 30 17:28:53.041: INFO: (19) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname2/proxy/: bar (200; 33.24721ms)
Jan 30 17:28:53.041: INFO: (19) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname2/proxy/: bar (200; 33.526722ms)
Jan 30 17:28:53.041: INFO: (19) /api/v1/namespaces/proxy-633/services/http:proxy-service-4cgvw:portname1/proxy/: foo (200; 33.484481ms)
Jan 30 17:28:53.041: INFO: (19) /api/v1/namespaces/proxy-633/services/proxy-service-4cgvw:portname1/proxy/: foo (200; 33.752399ms)
Jan 30 17:28:53.041: INFO: (19) /api/v1/namespaces/proxy-633/services/https:proxy-service-4cgvw:tlsportname1/proxy/: tls baz (200; 33.828458ms)
STEP: deleting ReplicationController proxy-service-4cgvw in namespace proxy-633, will wait for the garbage collector to delete the pods
Jan 30 17:28:53.107: INFO: Deleting ReplicationController proxy-service-4cgvw took: 12.151364ms
Jan 30 17:28:53.407: INFO: Terminating ReplicationController proxy-service-4cgvw pods took: 300.281105ms
[AfterEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:29:04.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-633" for this suite.
Jan 30 17:29:10.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:10.687: INFO: namespace proxy-633 deletion completed in 6.170524475s

• [SLOW TEST:22.115 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:29:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:29:10.747: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 30 17:29:12.795: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:29:13.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6929" for this suite.
Jan 30 17:29:19.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:19.994: INFO: namespace replication-controller-6929 deletion completed in 6.179610506s

• [SLOW TEST:9.305 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:29:19.995: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e27dc5aa-c024-4adb-be2e-c8916a6fbef3
STEP: Creating a pod to test consume secrets
Jan 30 17:29:20.068: INFO: Waiting up to 5m0s for pod "pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631" in namespace "secrets-5013" to be "success or failure"
Jan 30 17:29:20.082: INFO: Pod "pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631": Phase="Pending", Reason="", readiness=false. Elapsed: 13.695468ms
Jan 30 17:29:22.088: INFO: Pod "pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019608494s
Jan 30 17:29:24.094: INFO: Pod "pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026126143s
STEP: Saw pod success
Jan 30 17:29:24.094: INFO: Pod "pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631" satisfied condition "success or failure"
Jan 30 17:29:24.099: INFO: Trying to get logs from node aardvark pod pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631 container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:29:24.132: INFO: Waiting for pod pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631 to disappear
Jan 30 17:29:24.137: INFO: Pod pod-secrets-fabf446d-2f23-4506-90d8-f1d813be4631 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:29:24.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5013" for this suite.
Jan 30 17:29:30.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:30.319: INFO: namespace secrets-5013 deletion completed in 6.176442872s

• [SLOW TEST:10.325 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:29:30.320: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 30 17:29:34.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec pod-sharedvolume-74108cda-55ed-4098-9453-fb766d5f8887 -c busybox-main-container --namespace=emptydir-3532 -- cat /usr/share/volumeshare/shareddata.txt'
Jan 30 17:29:34.723: INFO: stderr: ""
Jan 30 17:29:34.723: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:29:34.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3532" for this suite.
Jan 30 17:29:40.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:40.911: INFO: namespace emptydir-3532 deletion completed in 6.18176377s

• [SLOW TEST:10.591 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:29:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4169975a-5b45-439b-8a51-ad36b9027d70
STEP: Creating a pod to test consume secrets
Jan 30 17:29:41.058: INFO: Waiting up to 5m0s for pod "pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe" in namespace "secrets-4865" to be "success or failure"
Jan 30 17:29:41.065: INFO: Pod "pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.64849ms
Jan 30 17:29:43.071: INFO: Pod "pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607066s
Jan 30 17:29:45.077: INFO: Pod "pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019658377s
STEP: Saw pod success
Jan 30 17:29:45.077: INFO: Pod "pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe" satisfied condition "success or failure"
Jan 30 17:29:45.083: INFO: Trying to get logs from node aardvark pod pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:29:45.119: INFO: Waiting for pod pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe to disappear
Jan 30 17:29:45.124: INFO: Pod pod-secrets-29270700-ea64-4d07-bdb4-764a764428fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:29:45.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4865" for this suite.
Jan 30 17:29:51.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:51.302: INFO: namespace secrets-4865 deletion completed in 6.171123858s
STEP: Destroying namespace "secret-namespace-289" for this suite.
Jan 30 17:29:57.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:29:57.468: INFO: namespace secret-namespace-289 deletion completed in 6.166024226s

• [SLOW TEST:16.556 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:29:57.468: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:30:21.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8904" for this suite.
Jan 30 17:30:27.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:30:28.040: INFO: namespace container-runtime-8904 deletion completed in 6.180187797s

• [SLOW TEST:30.572 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:30:28.041: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jan 30 17:30:28.097: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-482490662 proxy --unix-socket=/tmp/kubectl-proxy-unix056925261/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:30:28.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8649" for this suite.
Jan 30 17:30:34.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:30:34.360: INFO: namespace kubectl-8649 deletion completed in 6.191735974s

• [SLOW TEST:6.319 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:30:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:30:38.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7393" for this suite.
Jan 30 17:31:18.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:31:18.633: INFO: namespace kubelet-test-7393 deletion completed in 40.16626848s

• [SLOW TEST:44.273 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:31:18.633: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jan 30 17:31:18.684: INFO: namespace kubectl-6846
Jan 30 17:31:18.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-6846'
Jan 30 17:31:19.251: INFO: stderr: ""
Jan 30 17:31:19.251: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 30 17:31:20.259: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:31:20.259: INFO: Found 0 / 1
Jan 30 17:31:21.259: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:31:21.259: INFO: Found 0 / 1
Jan 30 17:31:22.258: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:31:22.259: INFO: Found 0 / 1
Jan 30 17:31:23.260: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:31:23.260: INFO: Found 1 / 1
Jan 30 17:31:23.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 17:31:23.267: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:31:23.268: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 17:31:23.268: INFO: wait on redis-master startup in kubectl-6846 
Jan 30 17:31:23.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-tv5g2 redis-master --namespace=kubectl-6846'
Jan 30 17:31:23.416: INFO: stderr: ""
Jan 30 17:31:23.416: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 30 Jan 17:31:21.923 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 30 Jan 17:31:21.924 # Server started, Redis version 3.2.12\n1:M 30 Jan 17:31:21.924 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 30 Jan 17:31:21.924 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 30 17:31:23.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6846'
Jan 30 17:31:23.616: INFO: stderr: ""
Jan 30 17:31:23.616: INFO: stdout: "service/rm2 exposed\n"
Jan 30 17:31:23.623: INFO: Service rm2 in namespace kubectl-6846 found.
STEP: exposing service
Jan 30 17:31:25.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6846'
Jan 30 17:31:25.781: INFO: stderr: ""
Jan 30 17:31:25.781: INFO: stdout: "service/rm3 exposed\n"
Jan 30 17:31:25.788: INFO: Service rm3 in namespace kubectl-6846 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:31:27.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6846" for this suite.
Jan 30 17:31:49.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:31:49.961: INFO: namespace kubectl-6846 deletion completed in 22.156007625s

• [SLOW TEST:31.328 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:31:49.962: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-baf5322f-ad0f-4cca-9efb-ba60c0762436
STEP: Creating a pod to test consume configMaps
Jan 30 17:31:50.046: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec" in namespace "projected-714" to be "success or failure"
Jan 30 17:31:50.053: INFO: Pod "pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.416136ms
Jan 30 17:31:52.060: INFO: Pod "pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013663866s
Jan 30 17:31:54.066: INFO: Pod "pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019914473s
STEP: Saw pod success
Jan 30 17:31:54.066: INFO: Pod "pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec" satisfied condition "success or failure"
Jan 30 17:31:54.071: INFO: Trying to get logs from node aardvark pod pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:31:54.109: INFO: Waiting for pod pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec to disappear
Jan 30 17:31:54.119: INFO: Pod pod-projected-configmaps-5f442571-cb85-4c6a-a9aa-66a1db14d0ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:31:54.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-714" for this suite.
Jan 30 17:32:00.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:00.303: INFO: namespace projected-714 deletion completed in 6.17885961s

• [SLOW TEST:10.341 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:00.303: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:32:00.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d" in namespace "projected-4814" to be "success or failure"
Jan 30 17:32:00.382: INFO: Pod "downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031988ms
Jan 30 17:32:02.388: INFO: Pod "downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479217s
Jan 30 17:32:04.394: INFO: Pod "downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020612207s
STEP: Saw pod success
Jan 30 17:32:04.395: INFO: Pod "downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d" satisfied condition "success or failure"
Jan 30 17:32:04.400: INFO: Trying to get logs from node gazelle pod downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d container client-container: <nil>
STEP: delete the pod
Jan 30 17:32:04.433: INFO: Waiting for pod downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d to disappear
Jan 30 17:32:04.438: INFO: Pod downwardapi-volume-443d3a15-6be3-4131-885e-19a589074f2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:32:04.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4814" for this suite.
Jan 30 17:32:10.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:10.617: INFO: namespace projected-4814 deletion completed in 6.171229843s

• [SLOW TEST:10.314 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:10.617: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-920b2081-0834-4db7-ac66-dcb9eeb63c55
STEP: Creating a pod to test consume secrets
Jan 30 17:32:10.687: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e" in namespace "projected-7776" to be "success or failure"
Jan 30 17:32:10.692: INFO: Pod "pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223087ms
Jan 30 17:32:12.700: INFO: Pod "pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012638982s
Jan 30 17:32:14.706: INFO: Pod "pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019408289s
STEP: Saw pod success
Jan 30 17:32:14.706: INFO: Pod "pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e" satisfied condition "success or failure"
Jan 30 17:32:14.713: INFO: Trying to get logs from node aardvark pod pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:32:14.749: INFO: Waiting for pod pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e to disappear
Jan 30 17:32:14.754: INFO: Pod pod-projected-secrets-cbc72415-3f11-4883-8ae6-366e90e6e91e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:32:14.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7776" for this suite.
Jan 30 17:32:20.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:20.933: INFO: namespace projected-7776 deletion completed in 6.173596927s

• [SLOW TEST:10.316 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 30 17:32:21.001: INFO: Waiting up to 5m0s for pod "pod-ad6357ed-3969-46f4-88b7-5609bf849813" in namespace "emptydir-3130" to be "success or failure"
Jan 30 17:32:21.006: INFO: Pod "pod-ad6357ed-3969-46f4-88b7-5609bf849813": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375211ms
Jan 30 17:32:23.013: INFO: Pod "pod-ad6357ed-3969-46f4-88b7-5609bf849813": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012013033s
Jan 30 17:32:25.019: INFO: Pod "pod-ad6357ed-3969-46f4-88b7-5609bf849813": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018350737s
STEP: Saw pod success
Jan 30 17:32:25.020: INFO: Pod "pod-ad6357ed-3969-46f4-88b7-5609bf849813" satisfied condition "success or failure"
Jan 30 17:32:25.025: INFO: Trying to get logs from node gazelle pod pod-ad6357ed-3969-46f4-88b7-5609bf849813 container test-container: <nil>
STEP: delete the pod
Jan 30 17:32:25.057: INFO: Waiting for pod pod-ad6357ed-3969-46f4-88b7-5609bf849813 to disappear
Jan 30 17:32:25.062: INFO: Pod pod-ad6357ed-3969-46f4-88b7-5609bf849813 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:32:25.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3130" for this suite.
Jan 30 17:32:31.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:31.252: INFO: namespace emptydir-3130 deletion completed in 6.183172584s

• [SLOW TEST:10.318 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 17:32:31.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8154'
Jan 30 17:32:31.435: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 30 17:32:31.435: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Jan 30 17:32:31.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete jobs e2e-test-nginx-job --namespace=kubectl-8154'
Jan 30 17:32:31.573: INFO: stderr: ""
Jan 30 17:32:31.573: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:32:31.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8154" for this suite.
Jan 30 17:32:37.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:37.760: INFO: namespace kubectl-8154 deletion completed in 6.180596526s

• [SLOW TEST:6.507 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a878b019-bdd8-429f-9a53-f0716442b616
STEP: Creating a pod to test consume secrets
Jan 30 17:32:37.836: INFO: Waiting up to 5m0s for pod "pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3" in namespace "secrets-61" to be "success or failure"
Jan 30 17:32:37.841: INFO: Pod "pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838385ms
Jan 30 17:32:39.847: INFO: Pod "pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010990764s
Jan 30 17:32:41.854: INFO: Pod "pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017546536s
STEP: Saw pod success
Jan 30 17:32:41.854: INFO: Pod "pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3" satisfied condition "success or failure"
Jan 30 17:32:41.859: INFO: Trying to get logs from node aardvark pod pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3 container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:32:41.894: INFO: Waiting for pod pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3 to disappear
Jan 30 17:32:41.898: INFO: Pod pod-secrets-550fe381-3ee5-4ebb-b022-d7aa5bc27ec3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:32:41.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-61" for this suite.
Jan 30 17:32:47.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:32:48.080: INFO: namespace secrets-61 deletion completed in 6.175324776s

• [SLOW TEST:10.319 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:32:48.081: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-ebba16d9-c6b5-44b2-b2d0-2fe2095ceb53 in namespace container-probe-3532
Jan 30 17:32:52.157: INFO: Started pod liveness-ebba16d9-c6b5-44b2-b2d0-2fe2095ceb53 in namespace container-probe-3532
STEP: checking the pod's current state and verifying that restartCount is present
Jan 30 17:32:52.163: INFO: Initial restart count of pod liveness-ebba16d9-c6b5-44b2-b2d0-2fe2095ceb53 is 0
Jan 30 17:33:08.223: INFO: Restart count of pod container-probe-3532/liveness-ebba16d9-c6b5-44b2-b2d0-2fe2095ceb53 is now 1 (16.059848006s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:33:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3532" for this suite.
Jan 30 17:33:14.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:33:14.447: INFO: namespace container-probe-3532 deletion completed in 6.170535123s

• [SLOW TEST:26.366 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:33:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 30 17:33:14.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186382,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 30 17:33:14.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186383,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 30 17:33:14.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186384,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 30 17:33:24.576: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186405,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 30 17:33:24.576: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186406,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 30 17:33:24.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7318,SelfLink:/api/v1/namespaces/watch-7318/configmaps/e2e-watch-test-label-changed,UID:bb747a43-519e-4ebd-978a-56867decc770,ResourceVersion:1186407,Generation:0,CreationTimestamp:2020-01-30 17:33:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:33:24.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7318" for this suite.
Jan 30 17:33:30.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:33:30.755: INFO: namespace watch-7318 deletion completed in 6.173124824s

• [SLOW TEST:16.308 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:33:30.756: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 30 17:33:30.827: INFO: Waiting up to 5m0s for pod "pod-c6f5ea54-48c5-499f-a081-293658e063e9" in namespace "emptydir-5431" to be "success or failure"
Jan 30 17:33:30.836: INFO: Pod "pod-c6f5ea54-48c5-499f-a081-293658e063e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651882ms
Jan 30 17:33:32.843: INFO: Pod "pod-c6f5ea54-48c5-499f-a081-293658e063e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015483138s
Jan 30 17:33:34.850: INFO: Pod "pod-c6f5ea54-48c5-499f-a081-293658e063e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022351542s
STEP: Saw pod success
Jan 30 17:33:34.850: INFO: Pod "pod-c6f5ea54-48c5-499f-a081-293658e063e9" satisfied condition "success or failure"
Jan 30 17:33:34.855: INFO: Trying to get logs from node gazelle pod pod-c6f5ea54-48c5-499f-a081-293658e063e9 container test-container: <nil>
STEP: delete the pod
Jan 30 17:33:34.894: INFO: Waiting for pod pod-c6f5ea54-48c5-499f-a081-293658e063e9 to disappear
Jan 30 17:33:34.898: INFO: Pod pod-c6f5ea54-48c5-499f-a081-293658e063e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:33:34.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5431" for this suite.
Jan 30 17:33:40.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:33:41.070: INFO: namespace emptydir-5431 deletion completed in 6.166719669s

• [SLOW TEST:10.314 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:33:41.070: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-dcddc4b8-f9b6-47c8-8f53-10667feff7df
STEP: Creating a pod to test consume secrets
Jan 30 17:33:41.140: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa" in namespace "projected-1500" to be "success or failure"
Jan 30 17:33:41.147: INFO: Pod "pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.314235ms
Jan 30 17:33:43.153: INFO: Pod "pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01294466s
Jan 30 17:33:45.160: INFO: Pod "pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019875093s
STEP: Saw pod success
Jan 30 17:33:45.160: INFO: Pod "pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa" satisfied condition "success or failure"
Jan 30 17:33:45.166: INFO: Trying to get logs from node aardvark pod pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:33:45.198: INFO: Waiting for pod pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa to disappear
Jan 30 17:33:45.203: INFO: Pod pod-projected-secrets-729a8cb0-c33b-4d13-ad0e-0075d84024fa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:33:45.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1500" for this suite.
Jan 30 17:33:51.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:33:51.386: INFO: namespace projected-1500 deletion completed in 6.176075771s

• [SLOW TEST:10.316 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:33:51.387: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 30 17:33:57.508: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:33:57.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8909" for this suite.
Jan 30 17:34:03.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:34:03.693: INFO: namespace gc-8909 deletion completed in 6.178552126s

• [SLOW TEST:12.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:34:03.694: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jan 30 17:34:08.309: INFO: Successfully updated pod "annotationupdate7bea6df0-ceb7-48b4-8d5e-71f9b181087f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:34:10.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9218" for this suite.
Jan 30 17:34:32.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:34:32.515: INFO: namespace downward-api-9218 deletion completed in 22.169982451s

• [SLOW TEST:28.821 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:34:32.517: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-95a7db2a-4f55-4608-b680-689ab9e471a9
STEP: Creating a pod to test consume configMaps
Jan 30 17:34:32.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df" in namespace "configmap-5956" to be "success or failure"
Jan 30 17:34:32.592: INFO: Pod "pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66051ms
Jan 30 17:34:34.599: INFO: Pod "pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012761808s
Jan 30 17:34:36.605: INFO: Pod "pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019040955s
STEP: Saw pod success
Jan 30 17:34:36.606: INFO: Pod "pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df" satisfied condition "success or failure"
Jan 30 17:34:36.611: INFO: Trying to get logs from node aardvark pod pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:34:36.645: INFO: Waiting for pod pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df to disappear
Jan 30 17:34:36.649: INFO: Pod pod-configmaps-df5943c9-56e1-4ede-b064-c550890d25df no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:34:36.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5956" for this suite.
Jan 30 17:34:42.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:34:42.829: INFO: namespace configmap-5956 deletion completed in 6.173084161s

• [SLOW TEST:10.312 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:34:42.830: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:34:42.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407" in namespace "downward-api-4331" to be "success or failure"
Jan 30 17:34:42.901: INFO: Pod "downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103916ms
Jan 30 17:34:44.907: INFO: Pod "downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010637019s
Jan 30 17:34:46.913: INFO: Pod "downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017040332s
STEP: Saw pod success
Jan 30 17:34:46.913: INFO: Pod "downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407" satisfied condition "success or failure"
Jan 30 17:34:46.919: INFO: Trying to get logs from node aardvark pod downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407 container client-container: <nil>
STEP: delete the pod
Jan 30 17:34:46.948: INFO: Waiting for pod downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407 to disappear
Jan 30 17:34:46.953: INFO: Pod downwardapi-volume-5d6fb3cd-91a3-4953-a992-0294f8b9e407 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:34:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4331" for this suite.
Jan 30 17:34:52.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:34:53.133: INFO: namespace downward-api-4331 deletion completed in 6.173885896s

• [SLOW TEST:10.304 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:34:53.134: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-vp65
STEP: Creating a pod to test atomic-volume-subpath
Jan 30 17:34:53.243: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vp65" in namespace "subpath-4226" to be "success or failure"
Jan 30 17:34:53.246: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373618ms
Jan 30 17:34:55.252: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0094712s
Jan 30 17:34:57.258: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 4.015643362s
Jan 30 17:34:59.265: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 6.022672811s
Jan 30 17:35:01.272: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 8.029753742s
Jan 30 17:35:03.279: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 10.036200873s
Jan 30 17:35:05.286: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 12.043588821s
Jan 30 17:35:07.292: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 14.04927798s
Jan 30 17:35:09.298: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 16.055206117s
Jan 30 17:35:11.305: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 18.062731569s
Jan 30 17:35:13.312: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 20.069180734s
Jan 30 17:35:15.318: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Running", Reason="", readiness=true. Elapsed: 22.075712251s
Jan 30 17:35:17.325: INFO: Pod "pod-subpath-test-downwardapi-vp65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.082197968s
STEP: Saw pod success
Jan 30 17:35:17.325: INFO: Pod "pod-subpath-test-downwardapi-vp65" satisfied condition "success or failure"
Jan 30 17:35:17.330: INFO: Trying to get logs from node aardvark pod pod-subpath-test-downwardapi-vp65 container test-container-subpath-downwardapi-vp65: <nil>
STEP: delete the pod
Jan 30 17:35:17.364: INFO: Waiting for pod pod-subpath-test-downwardapi-vp65 to disappear
Jan 30 17:35:17.368: INFO: Pod pod-subpath-test-downwardapi-vp65 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vp65
Jan 30 17:35:17.368: INFO: Deleting pod "pod-subpath-test-downwardapi-vp65" in namespace "subpath-4226"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:35:17.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4226" for this suite.
Jan 30 17:35:23.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:35:23.559: INFO: namespace subpath-4226 deletion completed in 6.180355404s

• [SLOW TEST:30.425 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:35:23.559: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-60ef39f0-ecbb-4b5d-a1f7-9710f39bd75d
STEP: Creating a pod to test consume secrets
Jan 30 17:35:23.627: INFO: Waiting up to 5m0s for pod "pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d" in namespace "secrets-8692" to be "success or failure"
Jan 30 17:35:23.636: INFO: Pod "pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.413666ms
Jan 30 17:35:25.642: INFO: Pod "pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015807745s
Jan 30 17:35:27.649: INFO: Pod "pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022547999s
STEP: Saw pod success
Jan 30 17:35:27.649: INFO: Pod "pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d" satisfied condition "success or failure"
Jan 30 17:35:27.654: INFO: Trying to get logs from node gazelle pod pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 17:35:27.685: INFO: Waiting for pod pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d to disappear
Jan 30 17:35:27.690: INFO: Pod pod-secrets-448d31e8-62e2-4697-9d6f-b40873b9776d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:35:27.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8692" for this suite.
Jan 30 17:35:33.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:35:33.871: INFO: namespace secrets-8692 deletion completed in 6.174574106s

• [SLOW TEST:10.311 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:35:33.871: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-900454bb-d2b0-4f34-a282-1a3a636c80c9
STEP: Creating a pod to test consume configMaps
Jan 30 17:35:33.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38" in namespace "projected-3916" to be "success or failure"
Jan 30 17:35:33.954: INFO: Pod "pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38": Phase="Pending", Reason="", readiness=false. Elapsed: 8.440084ms
Jan 30 17:35:35.961: INFO: Pod "pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015220831s
Jan 30 17:35:37.968: INFO: Pod "pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022285317s
STEP: Saw pod success
Jan 30 17:35:37.968: INFO: Pod "pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38" satisfied condition "success or failure"
Jan 30 17:35:37.974: INFO: Trying to get logs from node aardvark pod pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:35:38.009: INFO: Waiting for pod pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38 to disappear
Jan 30 17:35:38.014: INFO: Pod pod-projected-configmaps-2b8d19eb-8b5b-44c1-81c3-63920e249d38 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:35:38.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3916" for this suite.
Jan 30 17:35:44.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:35:44.179: INFO: namespace projected-3916 deletion completed in 6.157403741s

• [SLOW TEST:10.307 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:35:44.180: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9932.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9932.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 17:35:58.324: INFO: DNS probes using dns-9932/dns-test-a3247b71-9a23-4899-8916-08b55f72d0bb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:35:58.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9932" for this suite.
Jan 30 17:36:04.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:36:04.524: INFO: namespace dns-9932 deletion completed in 6.17497302s

• [SLOW TEST:20.345 seconds]
[sig-network] DNS
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:36:04.525: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-4d4c74be-02d8-4bae-9ffe-4430397684ef in namespace container-probe-7020
Jan 30 17:36:08.604: INFO: Started pod busybox-4d4c74be-02d8-4bae-9ffe-4430397684ef in namespace container-probe-7020
STEP: checking the pod's current state and verifying that restartCount is present
Jan 30 17:36:08.609: INFO: Initial restart count of pod busybox-4d4c74be-02d8-4bae-9ffe-4430397684ef is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:40:09.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7020" for this suite.
Jan 30 17:40:15.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:40:15.580: INFO: namespace container-probe-7020 deletion completed in 6.170004971s

• [SLOW TEST:251.055 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:40:15.581: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 30 17:40:16.708: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:40:16.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4185" for this suite.
Jan 30 17:40:22.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:40:22.885: INFO: namespace gc-4185 deletion completed in 6.170303431s

• [SLOW TEST:7.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:40:22.888: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-28b0c163-ad69-4294-a7ba-cb8704598590 in namespace container-probe-5410
Jan 30 17:40:26.971: INFO: Started pod test-webserver-28b0c163-ad69-4294-a7ba-cb8704598590 in namespace container-probe-5410
STEP: checking the pod's current state and verifying that restartCount is present
Jan 30 17:40:26.976: INFO: Initial restart count of pod test-webserver-28b0c163-ad69-4294-a7ba-cb8704598590 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:44:27.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5410" for this suite.
Jan 30 17:44:33.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:44:33.955: INFO: namespace container-probe-5410 deletion completed in 6.175040353s

• [SLOW TEST:251.068 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:44:33.956: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:44:34.008: INFO: Creating deployment "test-recreate-deployment"
Jan 30 17:44:34.015: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 30 17:44:34.026: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 30 17:44:36.039: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 30 17:44:36.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716003074, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716003074, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716003074, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716003074, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 17:44:38.052: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 30 17:44:38.064: INFO: Updating deployment test-recreate-deployment
Jan 30 17:44:38.064: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jan 30 17:44:38.170: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1597,SelfLink:/apis/apps/v1/namespaces/deployment-1597/deployments/test-recreate-deployment,UID:74001a49-3988-47b0-be37-accef8bce59d,ResourceVersion:1188285,Generation:2,CreationTimestamp:2020-01-30 17:44:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-01-30 17:44:38 +0000 UTC 2020-01-30 17:44:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-01-30 17:44:38 +0000 UTC 2020-01-30 17:44:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 17:44:38.176: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-1597,SelfLink:/apis/apps/v1/namespaces/deployment-1597/replicasets/test-recreate-deployment-5c8c9cc69d,UID:1aae5a30-ed55-40c7-9615-65801637fa58,ResourceVersion:1188284,Generation:1,CreationTimestamp:2020-01-30 17:44:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74001a49-3988-47b0-be37-accef8bce59d 0xc0036cd037 0xc0036cd038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 17:44:38.176: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 30 17:44:38.176: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-1597,SelfLink:/apis/apps/v1/namespaces/deployment-1597/replicasets/test-recreate-deployment-6df85df6b9,UID:002303cf-b583-4938-b045-9075e6cb03a0,ResourceVersion:1188273,Generation:2,CreationTimestamp:2020-01-30 17:44:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74001a49-3988-47b0-be37-accef8bce59d 0xc0036cd117 0xc0036cd118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 17:44:38.182: INFO: Pod "test-recreate-deployment-5c8c9cc69d-njcq7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-njcq7,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-1597,SelfLink:/api/v1/namespaces/deployment-1597/pods/test-recreate-deployment-5c8c9cc69d-njcq7,UID:56f42329-2fbf-4227-922c-f8749b280c80,ResourceVersion:1188286,Generation:0,CreationTimestamp:2020-01-30 17:44:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 1aae5a30-ed55-40c7-9615-65801637fa58 0xc0036cda17 0xc0036cda18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-x79zr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-x79zr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-x79zr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036cda90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036cdab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:44:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:44:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:44:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:44:38 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:,StartTime:2020-01-30 17:44:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:44:38.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1597" for this suite.
Jan 30 17:44:44.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:44:44.360: INFO: namespace deployment-1597 deletion completed in 6.171288277s

• [SLOW TEST:10.405 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:44:44.362: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:44:44.436: INFO: (0) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 13.564575ms)
Jan 30 17:44:44.446: INFO: (1) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 9.378198ms)
Jan 30 17:44:44.452: INFO: (2) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.98893ms)
Jan 30 17:44:44.460: INFO: (3) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.485088ms)
Jan 30 17:44:44.468: INFO: (4) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.19553ms)
Jan 30 17:44:44.475: INFO: (5) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.979258ms)
Jan 30 17:44:44.481: INFO: (6) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.047069ms)
Jan 30 17:44:44.487: INFO: (7) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.24002ms)
Jan 30 17:44:44.495: INFO: (8) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.294114ms)
Jan 30 17:44:44.501: INFO: (9) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.520163ms)
Jan 30 17:44:44.507: INFO: (10) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.308759ms)
Jan 30 17:44:44.515: INFO: (11) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.947544ms)
Jan 30 17:44:44.521: INFO: (12) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.924338ms)
Jan 30 17:44:44.528: INFO: (13) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.089771ms)
Jan 30 17:44:44.535: INFO: (14) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.802571ms)
Jan 30 17:44:44.547: INFO: (15) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 12.568167ms)
Jan 30 17:44:44.553: INFO: (16) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.209358ms)
Jan 30 17:44:44.559: INFO: (17) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.898659ms)
Jan 30 17:44:44.566: INFO: (18) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.163922ms)
Jan 30 17:44:44.573: INFO: (19) /api/v1/nodes/aardvark:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.033966ms)
[AfterEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:44:44.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3806" for this suite.
Jan 30 17:44:50.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:44:50.761: INFO: namespace proxy-3806 deletion completed in 6.182932947s

• [SLOW TEST:6.399 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:44:50.761: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 30 17:45:30.868: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:45:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1913" for this suite.
Jan 30 17:45:36.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:45:37.031: INFO: namespace gc-1913 deletion completed in 6.155257108s

• [SLOW TEST:46.270 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:45:37.032: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-af202d5f-017b-4f54-b8f3-fd61032fef27
STEP: Creating secret with name s-test-opt-upd-6cb0c789-5a8f-4b3e-9456-e224faa813a4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-af202d5f-017b-4f54-b8f3-fd61032fef27
STEP: Updating secret s-test-opt-upd-6cb0c789-5a8f-4b3e-9456-e224faa813a4
STEP: Creating secret with name s-test-opt-create-eeff7b24-5929-4e00-9491-1a40902737a2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:46:53.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-221" for this suite.
Jan 30 17:47:15.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:47:16.003: INFO: namespace projected-221 deletion completed in 22.165524487s

• [SLOW TEST:98.971 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:47:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jan 30 17:47:16.070: INFO: Waiting up to 5m0s for pod "var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff" in namespace "var-expansion-7237" to be "success or failure"
Jan 30 17:47:16.075: INFO: Pod "var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023331ms
Jan 30 17:47:18.082: INFO: Pod "var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011793864s
Jan 30 17:47:20.088: INFO: Pod "var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018308985s
STEP: Saw pod success
Jan 30 17:47:20.088: INFO: Pod "var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff" satisfied condition "success or failure"
Jan 30 17:47:20.094: INFO: Trying to get logs from node gazelle pod var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff container dapi-container: <nil>
STEP: delete the pod
Jan 30 17:47:20.129: INFO: Waiting for pod var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff to disappear
Jan 30 17:47:20.134: INFO: Pod var-expansion-7372a460-7ba8-4461-8493-11bf0e4ff4ff no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:47:20.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7237" for this suite.
Jan 30 17:47:26.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:47:26.319: INFO: namespace var-expansion-7237 deletion completed in 6.176101011s

• [SLOW TEST:10.312 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:47:26.331: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:47:26.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1263" for this suite.
Jan 30 17:47:32.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:47:32.575: INFO: namespace services-1263 deletion completed in 6.17742225s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.244 seconds]
[sig-network] Services
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:47:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jan 30 17:47:32.638: INFO: Waiting up to 5m0s for pod "client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54" in namespace "containers-9430" to be "success or failure"
Jan 30 17:47:32.642: INFO: Pod "client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69515ms
Jan 30 17:47:34.649: INFO: Pod "client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010849478s
Jan 30 17:47:36.655: INFO: Pod "client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017275668s
STEP: Saw pod success
Jan 30 17:47:36.655: INFO: Pod "client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54" satisfied condition "success or failure"
Jan 30 17:47:36.660: INFO: Trying to get logs from node aardvark pod client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54 container test-container: <nil>
STEP: delete the pod
Jan 30 17:47:36.695: INFO: Waiting for pod client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54 to disappear
Jan 30 17:47:36.701: INFO: Pod client-containers-18c1b6f7-4166-4b39-ad5d-fd7211646f54 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:47:36.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9430" for this suite.
Jan 30 17:47:42.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:47:42.889: INFO: namespace containers-9430 deletion completed in 6.176508247s

• [SLOW TEST:10.314 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:47:42.890: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 17:47:46.996: INFO: DNS probes using dns-test-75b363d9-3b64-460f-97b7-ed693698a725 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 17:47:51.062: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:47:51.068: INFO: File jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:47:51.069: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:47:56.078: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:47:56.086: INFO: File jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:47:56.086: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:48:01.132: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:01.138: INFO: File jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:01.138: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:48:06.077: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:06.085: INFO: File jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:06.085: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:48:11.078: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:11.084: INFO: File jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:11.084: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:48:16.076: INFO: File wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local from pod  dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 17:48:16.083: INFO: Lookups using dns-6140/dns-test-080d920b-056b-42d2-8629-d5fbe384e688 failed for: [wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local]

Jan 30 17:48:21.085: INFO: DNS probes using dns-test-080d920b-056b-42d2-8629-d5fbe384e688 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6140.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6140.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 17:48:25.198: INFO: DNS probes using dns-test-9c5f0dae-f614-4820-bbf9-18e6805dddae succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:48:25.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6140" for this suite.
Jan 30 17:48:31.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:48:31.436: INFO: namespace dns-6140 deletion completed in 6.170255943s

• [SLOW TEST:48.546 seconds]
[sig-network] DNS
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:48:31.437: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5717/configmap-test-be65fc42-1b4d-49fc-8472-b5ef4e9757be
STEP: Creating a pod to test consume configMaps
Jan 30 17:48:31.507: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec" in namespace "configmap-5717" to be "success or failure"
Jan 30 17:48:31.516: INFO: Pod "pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353415ms
Jan 30 17:48:33.522: INFO: Pod "pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015271498s
Jan 30 17:48:35.529: INFO: Pod "pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02178844s
STEP: Saw pod success
Jan 30 17:48:35.529: INFO: Pod "pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec" satisfied condition "success or failure"
Jan 30 17:48:35.534: INFO: Trying to get logs from node aardvark pod pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec container env-test: <nil>
STEP: delete the pod
Jan 30 17:48:35.569: INFO: Waiting for pod pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec to disappear
Jan 30 17:48:35.573: INFO: Pod pod-configmaps-ca530e4d-e49a-4594-8a39-594371e0f1ec no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:48:35.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5717" for this suite.
Jan 30 17:48:41.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:48:41.760: INFO: namespace configmap-5717 deletion completed in 6.178944198s

• [SLOW TEST:10.324 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:48:41.761: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:48:41.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 version'
Jan 30 17:48:41.928: INFO: stderr: ""
Jan 30 17:48:41.928: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.9-beta.0\", GitCommit:\"14ede42c4fe699a7078b566d89abc160f26857a2\", GitTreeState:\"clean\", BuildDate:\"2020-01-15T11:36:51Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.9-beta.0\", GitCommit:\"14ede42c4fe699a7078b566d89abc160f26857a2\", GitTreeState:\"clean\", BuildDate:\"2020-01-15T11:07:11Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:48:41.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4315" for this suite.
Jan 30 17:48:47.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:48:48.123: INFO: namespace kubectl-4315 deletion completed in 6.187418448s

• [SLOW TEST:6.362 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:48:48.123: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b
Jan 30 17:48:48.191: INFO: Pod name my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b: Found 0 pods out of 1
Jan 30 17:48:53.200: INFO: Pod name my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b: Found 1 pods out of 1
Jan 30 17:48:53.200: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b" are running
Jan 30 17:48:53.206: INFO: Pod "my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b-hm2rj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 17:48:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 17:48:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 17:48:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 17:48:48 +0000 UTC Reason: Message:}])
Jan 30 17:48:53.206: INFO: Trying to dial the pod
Jan 30 17:48:58.227: INFO: Controller my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b: Got expected result from replica 1 [my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b-hm2rj]: "my-hostname-basic-a87460c5-17f0-400b-a1d0-c66b42a67e7b-hm2rj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:48:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7573" for this suite.
Jan 30 17:49:04.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:04.417: INFO: namespace replication-controller-7573 deletion completed in 6.182122678s

• [SLOW TEST:16.293 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:04.418: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jan 30 17:49:04.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-2372'
Jan 30 17:49:05.073: INFO: stderr: ""
Jan 30 17:49:05.073: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 17:49:05.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2372'
Jan 30 17:49:05.193: INFO: stderr: ""
Jan 30 17:49:05.193: INFO: stdout: "update-demo-nautilus-9w565 update-demo-nautilus-r2vh2 "
Jan 30 17:49:05.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-9w565 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2372'
Jan 30 17:49:05.286: INFO: stderr: ""
Jan 30 17:49:05.286: INFO: stdout: ""
Jan 30 17:49:05.286: INFO: update-demo-nautilus-9w565 is created but not running
Jan 30 17:49:10.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2372'
Jan 30 17:49:10.411: INFO: stderr: ""
Jan 30 17:49:10.411: INFO: stdout: "update-demo-nautilus-9w565 update-demo-nautilus-r2vh2 "
Jan 30 17:49:10.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-9w565 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2372'
Jan 30 17:49:10.534: INFO: stderr: ""
Jan 30 17:49:10.534: INFO: stdout: "true"
Jan 30 17:49:10.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-9w565 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2372'
Jan 30 17:49:10.643: INFO: stderr: ""
Jan 30 17:49:10.643: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 17:49:10.643: INFO: validating pod update-demo-nautilus-9w565
Jan 30 17:49:10.652: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 17:49:10.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 17:49:10.652: INFO: update-demo-nautilus-9w565 is verified up and running
Jan 30 17:49:10.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-r2vh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2372'
Jan 30 17:49:10.768: INFO: stderr: ""
Jan 30 17:49:10.768: INFO: stdout: "true"
Jan 30 17:49:10.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-r2vh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2372'
Jan 30 17:49:10.875: INFO: stderr: ""
Jan 30 17:49:10.875: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 17:49:10.875: INFO: validating pod update-demo-nautilus-r2vh2
Jan 30 17:49:10.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 17:49:10.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 17:49:10.883: INFO: update-demo-nautilus-r2vh2 is verified up and running
STEP: using delete to clean up resources
Jan 30 17:49:10.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-2372'
Jan 30 17:49:11.012: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 17:49:11.012: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 17:49:11.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2372'
Jan 30 17:49:11.121: INFO: stderr: "No resources found.\n"
Jan 30 17:49:11.121: INFO: stdout: ""
Jan 30 17:49:11.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -l name=update-demo --namespace=kubectl-2372 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 17:49:11.237: INFO: stderr: ""
Jan 30 17:49:11.237: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:49:11.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2372" for this suite.
Jan 30 17:49:17.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:17.406: INFO: namespace kubectl-2372 deletion completed in 6.161196855s

• [SLOW TEST:12.988 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:17.406: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5d297e8d-6c1f-404c-a701-e157f5a5ec7c
STEP: Creating a pod to test consume configMaps
Jan 30 17:49:17.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829" in namespace "configmap-4137" to be "success or failure"
Jan 30 17:49:17.478: INFO: Pod "pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829": Phase="Pending", Reason="", readiness=false. Elapsed: 5.089327ms
Jan 30 17:49:19.485: INFO: Pod "pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012345472s
Jan 30 17:49:21.491: INFO: Pod "pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018347379s
STEP: Saw pod success
Jan 30 17:49:21.491: INFO: Pod "pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829" satisfied condition "success or failure"
Jan 30 17:49:21.496: INFO: Trying to get logs from node aardvark pod pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:49:21.523: INFO: Waiting for pod pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829 to disappear
Jan 30 17:49:21.528: INFO: Pod pod-configmaps-ec712512-e2bb-4533-bc22-a7746fd11829 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:49:21.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4137" for this suite.
Jan 30 17:49:27.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:27.728: INFO: namespace configmap-4137 deletion completed in 6.193901168s

• [SLOW TEST:10.322 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1876/configmap-test-8e2263ee-891a-4b28-818f-d87ab74bb845
STEP: Creating a pod to test consume configMaps
Jan 30 17:49:27.809: INFO: Waiting up to 5m0s for pod "pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948" in namespace "configmap-1876" to be "success or failure"
Jan 30 17:49:27.818: INFO: Pod "pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948": Phase="Pending", Reason="", readiness=false. Elapsed: 9.479895ms
Jan 30 17:49:29.824: INFO: Pod "pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014979789s
Jan 30 17:49:31.830: INFO: Pod "pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021498084s
STEP: Saw pod success
Jan 30 17:49:31.830: INFO: Pod "pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948" satisfied condition "success or failure"
Jan 30 17:49:31.836: INFO: Trying to get logs from node gazelle pod pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948 container env-test: <nil>
STEP: delete the pod
Jan 30 17:49:31.866: INFO: Waiting for pod pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948 to disappear
Jan 30 17:49:31.871: INFO: Pod pod-configmaps-8eae2091-e4d4-4bf5-961e-93d76b036948 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:49:31.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1876" for this suite.
Jan 30 17:49:37.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:38.048: INFO: namespace configmap-1876 deletion completed in 6.171481075s

• [SLOW TEST:10.319 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 30 17:49:38.115: INFO: Waiting up to 5m0s for pod "pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f" in namespace "emptydir-4931" to be "success or failure"
Jan 30 17:49:38.120: INFO: Pod "pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.534597ms
Jan 30 17:49:40.125: INFO: Pod "pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010461291s
Jan 30 17:49:42.132: INFO: Pod "pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016688616s
STEP: Saw pod success
Jan 30 17:49:42.132: INFO: Pod "pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f" satisfied condition "success or failure"
Jan 30 17:49:42.137: INFO: Trying to get logs from node aardvark pod pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f container test-container: <nil>
STEP: delete the pod
Jan 30 17:49:42.170: INFO: Waiting for pod pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f to disappear
Jan 30 17:49:42.175: INFO: Pod pod-f865a55a-4ab8-45a9-9816-572e8bba9f0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:49:42.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4931" for this suite.
Jan 30 17:49:48.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:48.341: INFO: namespace emptydir-4931 deletion completed in 6.159155849s

• [SLOW TEST:10.292 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:48.341: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 30 17:49:48.409: INFO: Waiting up to 5m0s for pod "pod-ad0292b6-47f8-4008-8078-ae4d13769371" in namespace "emptydir-5941" to be "success or failure"
Jan 30 17:49:48.418: INFO: Pod "pod-ad0292b6-47f8-4008-8078-ae4d13769371": Phase="Pending", Reason="", readiness=false. Elapsed: 8.901841ms
Jan 30 17:49:50.430: INFO: Pod "pod-ad0292b6-47f8-4008-8078-ae4d13769371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020925048s
Jan 30 17:49:52.436: INFO: Pod "pod-ad0292b6-47f8-4008-8078-ae4d13769371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027057645s
STEP: Saw pod success
Jan 30 17:49:52.436: INFO: Pod "pod-ad0292b6-47f8-4008-8078-ae4d13769371" satisfied condition "success or failure"
Jan 30 17:49:52.441: INFO: Trying to get logs from node gazelle pod pod-ad0292b6-47f8-4008-8078-ae4d13769371 container test-container: <nil>
STEP: delete the pod
Jan 30 17:49:52.470: INFO: Waiting for pod pod-ad0292b6-47f8-4008-8078-ae4d13769371 to disappear
Jan 30 17:49:52.474: INFO: Pod pod-ad0292b6-47f8-4008-8078-ae4d13769371 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:49:52.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5941" for this suite.
Jan 30 17:49:58.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:49:58.657: INFO: namespace emptydir-5941 deletion completed in 6.177845804s

• [SLOW TEST:10.316 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:49:58.660: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:50:58.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2247" for this suite.
Jan 30 17:51:20.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:51:20.922: INFO: namespace container-probe-2247 deletion completed in 22.16846793s

• [SLOW TEST:82.262 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:51:20.923: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 30 17:51:20.984: INFO: Waiting up to 5m0s for pod "pod-df587598-5dbf-4d4d-b264-a7b3e177c105" in namespace "emptydir-4990" to be "success or failure"
Jan 30 17:51:20.988: INFO: Pod "pod-df587598-5dbf-4d4d-b264-a7b3e177c105": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797409ms
Jan 30 17:51:22.994: INFO: Pod "pod-df587598-5dbf-4d4d-b264-a7b3e177c105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009745478s
Jan 30 17:51:25.001: INFO: Pod "pod-df587598-5dbf-4d4d-b264-a7b3e177c105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017077265s
STEP: Saw pod success
Jan 30 17:51:25.001: INFO: Pod "pod-df587598-5dbf-4d4d-b264-a7b3e177c105" satisfied condition "success or failure"
Jan 30 17:51:25.007: INFO: Trying to get logs from node gazelle pod pod-df587598-5dbf-4d4d-b264-a7b3e177c105 container test-container: <nil>
STEP: delete the pod
Jan 30 17:51:25.039: INFO: Waiting for pod pod-df587598-5dbf-4d4d-b264-a7b3e177c105 to disappear
Jan 30 17:51:25.043: INFO: Pod pod-df587598-5dbf-4d4d-b264-a7b3e177c105 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:51:25.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4990" for this suite.
Jan 30 17:51:31.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:51:31.218: INFO: namespace emptydir-4990 deletion completed in 6.168398621s

• [SLOW TEST:10.295 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:51:31.218: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 30 17:51:39.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 17:51:39.359: INFO: Pod pod-with-poststart-http-hook still exists
Jan 30 17:51:41.359: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 17:51:41.366: INFO: Pod pod-with-poststart-http-hook still exists
Jan 30 17:51:43.359: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 17:51:43.364: INFO: Pod pod-with-poststart-http-hook still exists
Jan 30 17:51:45.359: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 17:51:45.365: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:51:45.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4151" for this suite.
Jan 30 17:52:07.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:52:07.552: INFO: namespace container-lifecycle-hook-4151 deletion completed in 22.178996107s

• [SLOW TEST:36.333 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:52:07.553: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jan 30 17:52:07.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-9420'
Jan 30 17:52:07.904: INFO: stderr: ""
Jan 30 17:52:07.904: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 30 17:52:08.911: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:52:08.911: INFO: Found 0 / 1
Jan 30 17:52:09.910: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:52:09.910: INFO: Found 0 / 1
Jan 30 17:52:10.910: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:52:10.910: INFO: Found 1 / 1
Jan 30 17:52:10.911: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 30 17:52:10.916: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:52:10.916: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 17:52:10.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 patch pod redis-master-fv98r --namespace=kubectl-9420 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 30 17:52:11.038: INFO: stderr: ""
Jan 30 17:52:11.038: INFO: stdout: "pod/redis-master-fv98r patched\n"
STEP: checking annotations
Jan 30 17:52:11.043: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 17:52:11.043: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:52:11.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9420" for this suite.
Jan 30 17:52:33.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:52:33.219: INFO: namespace kubectl-9420 deletion completed in 22.169616643s

• [SLOW TEST:25.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:52:33.220: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-27b6f90a-1ef2-4d72-ad45-a3e78f6b8bce
STEP: Creating a pod to test consume configMaps
Jan 30 17:52:33.295: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5" in namespace "projected-2239" to be "success or failure"
Jan 30 17:52:33.301: INFO: Pod "pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.241842ms
Jan 30 17:52:35.307: INFO: Pod "pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012502482s
Jan 30 17:52:37.314: INFO: Pod "pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01968397s
STEP: Saw pod success
Jan 30 17:52:37.314: INFO: Pod "pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5" satisfied condition "success or failure"
Jan 30 17:52:37.320: INFO: Trying to get logs from node gazelle pod pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:52:37.356: INFO: Waiting for pod pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5 to disappear
Jan 30 17:52:37.360: INFO: Pod pod-projected-configmaps-ccd49418-8b5c-461b-bfcc-7438d65af1a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:52:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2239" for this suite.
Jan 30 17:52:43.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:52:43.532: INFO: namespace projected-2239 deletion completed in 6.166243737s

• [SLOW TEST:10.313 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:52:43.533: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 30 17:52:43.618: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5742,SelfLink:/api/v1/namespaces/watch-5742/configmaps/e2e-watch-test-watch-closed,UID:7247865f-aeaa-404d-bb6c-aa7ccae0d20f,ResourceVersion:1190038,Generation:0,CreationTimestamp:2020-01-30 17:52:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 30 17:52:43.618: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5742,SelfLink:/api/v1/namespaces/watch-5742/configmaps/e2e-watch-test-watch-closed,UID:7247865f-aeaa-404d-bb6c-aa7ccae0d20f,ResourceVersion:1190039,Generation:0,CreationTimestamp:2020-01-30 17:52:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 30 17:52:43.641: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5742,SelfLink:/api/v1/namespaces/watch-5742/configmaps/e2e-watch-test-watch-closed,UID:7247865f-aeaa-404d-bb6c-aa7ccae0d20f,ResourceVersion:1190040,Generation:0,CreationTimestamp:2020-01-30 17:52:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 30 17:52:43.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5742,SelfLink:/api/v1/namespaces/watch-5742/configmaps/e2e-watch-test-watch-closed,UID:7247865f-aeaa-404d-bb6c-aa7ccae0d20f,ResourceVersion:1190041,Generation:0,CreationTimestamp:2020-01-30 17:52:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:52:43.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5742" for this suite.
Jan 30 17:52:49.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:52:49.828: INFO: namespace watch-5742 deletion completed in 6.177895064s

• [SLOW TEST:6.295 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:52:49.831: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6526
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6526
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6526
Jan 30 17:52:49.915: INFO: Found 0 stateful pods, waiting for 1
Jan 30 17:52:59.924: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 30 17:52:59.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 17:53:00.264: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 17:53:00.264: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 17:53:00.264: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 17:53:00.271: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 17:53:10.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 17:53:10.278: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 17:53:10.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999429s
Jan 30 17:53:11.313: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991695857s
Jan 30 17:53:12.319: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985093625s
Jan 30 17:53:13.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978503101s
Jan 30 17:53:14.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971635479s
Jan 30 17:53:15.345: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.959783306s
Jan 30 17:53:16.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.952388036s
Jan 30 17:53:17.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946741896s
Jan 30 17:53:18.364: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.939986563s
Jan 30 17:53:19.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 933.888218ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6526
Jan 30 17:53:20.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 17:53:20.986: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 17:53:20.986: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 17:53:20.986: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 17:53:20.992: INFO: Found 1 stateful pods, waiting for 3
Jan 30 17:53:30.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 17:53:30.999: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 17:53:30.999: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 30 17:53:31.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 17:53:31.313: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 17:53:31.313: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 17:53:31.313: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 17:53:31.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 17:53:31.639: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 17:53:31.639: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 17:53:31.639: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 17:53:31.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 17:53:31.950: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 17:53:31.950: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 17:53:31.950: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 17:53:31.950: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 17:53:31.956: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 30 17:53:41.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 17:53:41.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 17:53:41.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 17:53:41.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999274s
Jan 30 17:53:43.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991495612s
Jan 30 17:53:44.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982834874s
Jan 30 17:53:45.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9755431s
Jan 30 17:53:46.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968074624s
Jan 30 17:53:47.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961599736s
Jan 30 17:53:48.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954270999s
Jan 30 17:53:49.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947500604s
Jan 30 17:53:50.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939484238s
Jan 30 17:53:51.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.057648ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6526
Jan 30 17:53:52.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 17:53:52.348: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 17:53:52.348: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 17:53:52.348: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 17:53:52.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 17:53:52.652: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 17:53:52.652: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 17:53:52.652: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 17:53:52.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-6526 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 17:53:52.964: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 17:53:52.964: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 17:53:52.964: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 17:53:52.964: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jan 30 17:54:22.989: INFO: Deleting all statefulset in ns statefulset-6526
Jan 30 17:54:22.995: INFO: Scaling statefulset ss to 0
Jan 30 17:54:23.011: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 17:54:23.016: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:54:23.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6526" for this suite.
Jan 30 17:54:29.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:54:29.215: INFO: namespace statefulset-6526 deletion completed in 6.167473284s

• [SLOW TEST:99.384 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:54:29.216: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:54:29.283: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d" in namespace "downward-api-8774" to be "success or failure"
Jan 30 17:54:29.290: INFO: Pod "downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315944ms
Jan 30 17:54:31.297: INFO: Pod "downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01386091s
Jan 30 17:54:33.304: INFO: Pod "downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020845958s
STEP: Saw pod success
Jan 30 17:54:33.304: INFO: Pod "downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d" satisfied condition "success or failure"
Jan 30 17:54:33.309: INFO: Trying to get logs from node aardvark pod downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d container client-container: <nil>
STEP: delete the pod
Jan 30 17:54:33.344: INFO: Waiting for pod downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d to disappear
Jan 30 17:54:33.349: INFO: Pod downwardapi-volume-6e94dd34-0afa-43ef-8382-436ae0cdfe7d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:54:33.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8774" for this suite.
Jan 30 17:54:39.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:54:39.519: INFO: namespace downward-api-8774 deletion completed in 6.164167775s

• [SLOW TEST:10.303 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:54:39.519: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:54:39.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911" in namespace "projected-4541" to be "success or failure"
Jan 30 17:54:39.607: INFO: Pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911": Phase="Pending", Reason="", readiness=false. Elapsed: 15.867544ms
Jan 30 17:54:41.614: INFO: Pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022470432s
Jan 30 17:54:43.620: INFO: Pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028757753s
Jan 30 17:54:45.627: INFO: Pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035596297s
STEP: Saw pod success
Jan 30 17:54:45.627: INFO: Pod "downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911" satisfied condition "success or failure"
Jan 30 17:54:45.633: INFO: Trying to get logs from node aardvark pod downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911 container client-container: <nil>
STEP: delete the pod
Jan 30 17:54:45.668: INFO: Waiting for pod downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911 to disappear
Jan 30 17:54:45.673: INFO: Pod downwardapi-volume-aaf8f254-d71f-409d-9244-7123e669c911 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:54:45.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4541" for this suite.
Jan 30 17:54:51.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:54:51.860: INFO: namespace projected-4541 deletion completed in 6.179724401s

• [SLOW TEST:12.341 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:54:51.863: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jan 30 17:54:51.926: INFO: Waiting up to 5m0s for pod "client-containers-49518651-101e-4764-bc1f-34fcae4c56bc" in namespace "containers-6342" to be "success or failure"
Jan 30 17:54:51.932: INFO: Pod "client-containers-49518651-101e-4764-bc1f-34fcae4c56bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.317745ms
Jan 30 17:54:53.940: INFO: Pod "client-containers-49518651-101e-4764-bc1f-34fcae4c56bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013865648s
Jan 30 17:54:55.946: INFO: Pod "client-containers-49518651-101e-4764-bc1f-34fcae4c56bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019929745s
STEP: Saw pod success
Jan 30 17:54:55.946: INFO: Pod "client-containers-49518651-101e-4764-bc1f-34fcae4c56bc" satisfied condition "success or failure"
Jan 30 17:54:55.950: INFO: Trying to get logs from node gazelle pod client-containers-49518651-101e-4764-bc1f-34fcae4c56bc container test-container: <nil>
STEP: delete the pod
Jan 30 17:54:55.981: INFO: Waiting for pod client-containers-49518651-101e-4764-bc1f-34fcae4c56bc to disappear
Jan 30 17:54:55.986: INFO: Pod client-containers-49518651-101e-4764-bc1f-34fcae4c56bc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:54:55.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6342" for this suite.
Jan 30 17:55:02.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:55:02.162: INFO: namespace containers-6342 deletion completed in 6.170658795s

• [SLOW TEST:10.300 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:55:02.163: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:55:02.235: INFO: (0) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 11.033561ms)
Jan 30 17:55:02.242: INFO: (1) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.325171ms)
Jan 30 17:55:02.250: INFO: (2) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.405379ms)
Jan 30 17:55:02.260: INFO: (3) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 9.932021ms)
Jan 30 17:55:02.269: INFO: (4) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.725372ms)
Jan 30 17:55:02.277: INFO: (5) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.208239ms)
Jan 30 17:55:02.283: INFO: (6) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.996072ms)
Jan 30 17:55:02.290: INFO: (7) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.663505ms)
Jan 30 17:55:02.297: INFO: (8) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.369508ms)
Jan 30 17:55:02.305: INFO: (9) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.405342ms)
Jan 30 17:55:02.312: INFO: (10) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.658079ms)
Jan 30 17:55:02.318: INFO: (11) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.240224ms)
Jan 30 17:55:02.326: INFO: (12) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.584818ms)
Jan 30 17:55:02.334: INFO: (13) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.177982ms)
Jan 30 17:55:02.341: INFO: (14) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.559162ms)
Jan 30 17:55:02.349: INFO: (15) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.996268ms)
Jan 30 17:55:02.357: INFO: (16) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.219825ms)
Jan 30 17:55:02.365: INFO: (17) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 8.23144ms)
Jan 30 17:55:02.372: INFO: (18) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.70618ms)
Jan 30 17:55:02.379: INFO: (19) /api/v1/nodes/aardvark/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 7.255394ms)
[AfterEach] version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:55:02.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7874" for this suite.
Jan 30 17:55:08.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:55:08.557: INFO: namespace proxy-7874 deletion completed in 6.170868752s

• [SLOW TEST:6.394 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:55:08.557: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jan 30 17:55:08.619: INFO: Waiting up to 5m0s for pod "var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e" in namespace "var-expansion-390" to be "success or failure"
Jan 30 17:55:08.627: INFO: Pod "var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.181434ms
Jan 30 17:55:10.633: INFO: Pod "var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013973499s
Jan 30 17:55:12.640: INFO: Pod "var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021171506s
STEP: Saw pod success
Jan 30 17:55:12.640: INFO: Pod "var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e" satisfied condition "success or failure"
Jan 30 17:55:12.647: INFO: Trying to get logs from node aardvark pod var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e container dapi-container: <nil>
STEP: delete the pod
Jan 30 17:55:12.678: INFO: Waiting for pod var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e to disappear
Jan 30 17:55:12.683: INFO: Pod var-expansion-3bed2240-6c64-44e7-a9f5-1daff0d8127e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:55:12.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-390" for this suite.
Jan 30 17:55:18.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:55:18.856: INFO: namespace var-expansion-390 deletion completed in 6.167871051s

• [SLOW TEST:10.299 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:55:18.859: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3619
I0130 17:55:18.921498      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3619, replica count: 1
I0130 17:55:19.972038      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 17:55:20.972360      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 17:55:21.972599      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 17:55:22.091: INFO: Created: latency-svc-5dq5t
Jan 30 17:55:22.099: INFO: Got endpoints: latency-svc-5dq5t [26.288543ms]
Jan 30 17:55:22.118: INFO: Created: latency-svc-bpkv4
Jan 30 17:55:22.129: INFO: Got endpoints: latency-svc-bpkv4 [30.735179ms]
Jan 30 17:55:22.134: INFO: Created: latency-svc-xhmjz
Jan 30 17:55:22.143: INFO: Got endpoints: latency-svc-xhmjz [43.874864ms]
Jan 30 17:55:22.149: INFO: Created: latency-svc-9gvlr
Jan 30 17:55:22.156: INFO: Got endpoints: latency-svc-9gvlr [57.091562ms]
Jan 30 17:55:22.163: INFO: Created: latency-svc-nbgmm
Jan 30 17:55:22.170: INFO: Got endpoints: latency-svc-nbgmm [70.818548ms]
Jan 30 17:55:22.188: INFO: Created: latency-svc-vm86l
Jan 30 17:55:22.195: INFO: Got endpoints: latency-svc-vm86l [95.849273ms]
Jan 30 17:55:22.212: INFO: Created: latency-svc-zrvxl
Jan 30 17:55:22.219: INFO: Got endpoints: latency-svc-zrvxl [120.257902ms]
Jan 30 17:55:22.239: INFO: Created: latency-svc-rchcl
Jan 30 17:55:22.247: INFO: Got endpoints: latency-svc-rchcl [147.859631ms]
Jan 30 17:55:22.259: INFO: Created: latency-svc-hqtqc
Jan 30 17:55:22.263: INFO: Got endpoints: latency-svc-hqtqc [163.637453ms]
Jan 30 17:55:22.275: INFO: Created: latency-svc-rgz7q
Jan 30 17:55:22.280: INFO: Got endpoints: latency-svc-rgz7q [181.404762ms]
Jan 30 17:55:22.301: INFO: Created: latency-svc-xhxx4
Jan 30 17:55:22.307: INFO: Got endpoints: latency-svc-xhxx4 [208.267785ms]
Jan 30 17:55:22.321: INFO: Created: latency-svc-542hm
Jan 30 17:55:22.327: INFO: Got endpoints: latency-svc-542hm [227.945798ms]
Jan 30 17:55:22.343: INFO: Created: latency-svc-k2t26
Jan 30 17:55:22.349: INFO: Got endpoints: latency-svc-k2t26 [249.36066ms]
Jan 30 17:55:22.363: INFO: Created: latency-svc-48j6h
Jan 30 17:55:22.368: INFO: Got endpoints: latency-svc-48j6h [268.87355ms]
Jan 30 17:55:22.383: INFO: Created: latency-svc-5sffz
Jan 30 17:55:22.388: INFO: Got endpoints: latency-svc-5sffz [289.180493ms]
Jan 30 17:55:22.409: INFO: Created: latency-svc-c22kd
Jan 30 17:55:22.415: INFO: Got endpoints: latency-svc-c22kd [315.532813ms]
Jan 30 17:55:22.427: INFO: Created: latency-svc-bldxv
Jan 30 17:55:22.432: INFO: Got endpoints: latency-svc-bldxv [302.408626ms]
Jan 30 17:55:22.450: INFO: Created: latency-svc-w4q8p
Jan 30 17:55:22.453: INFO: Got endpoints: latency-svc-w4q8p [310.55977ms]
Jan 30 17:55:22.470: INFO: Created: latency-svc-jksd5
Jan 30 17:55:22.479: INFO: Got endpoints: latency-svc-jksd5 [323.263044ms]
Jan 30 17:55:22.487: INFO: Created: latency-svc-54vxv
Jan 30 17:55:22.493: INFO: Got endpoints: latency-svc-54vxv [323.185572ms]
Jan 30 17:55:22.514: INFO: Created: latency-svc-rr7g4
Jan 30 17:55:22.522: INFO: Got endpoints: latency-svc-rr7g4 [327.621717ms]
Jan 30 17:55:22.536: INFO: Created: latency-svc-xbmsq
Jan 30 17:55:22.542: INFO: Got endpoints: latency-svc-xbmsq [322.916316ms]
Jan 30 17:55:22.556: INFO: Created: latency-svc-gf6pw
Jan 30 17:55:22.561: INFO: Got endpoints: latency-svc-gf6pw [314.270873ms]
Jan 30 17:55:22.579: INFO: Created: latency-svc-sw7r8
Jan 30 17:55:22.585: INFO: Got endpoints: latency-svc-sw7r8 [322.593791ms]
Jan 30 17:55:22.596: INFO: Created: latency-svc-hwb89
Jan 30 17:55:22.606: INFO: Got endpoints: latency-svc-hwb89 [325.492338ms]
Jan 30 17:55:22.613: INFO: Created: latency-svc-4g79f
Jan 30 17:55:22.630: INFO: Got endpoints: latency-svc-4g79f [322.575713ms]
Jan 30 17:55:22.639: INFO: Created: latency-svc-qzwzq
Jan 30 17:55:22.647: INFO: Got endpoints: latency-svc-qzwzq [41.141017ms]
Jan 30 17:55:22.663: INFO: Created: latency-svc-p2kks
Jan 30 17:55:22.670: INFO: Got endpoints: latency-svc-p2kks [342.914395ms]
Jan 30 17:55:22.680: INFO: Created: latency-svc-c4j59
Jan 30 17:55:22.686: INFO: Got endpoints: latency-svc-c4j59 [337.073865ms]
Jan 30 17:55:22.696: INFO: Created: latency-svc-tj5t2
Jan 30 17:55:22.713: INFO: Got endpoints: latency-svc-tj5t2 [344.859752ms]
Jan 30 17:55:22.722: INFO: Created: latency-svc-msc5f
Jan 30 17:55:22.727: INFO: Got endpoints: latency-svc-msc5f [338.505184ms]
Jan 30 17:55:22.747: INFO: Created: latency-svc-r7g4v
Jan 30 17:55:22.751: INFO: Got endpoints: latency-svc-r7g4v [336.58237ms]
Jan 30 17:55:22.763: INFO: Created: latency-svc-bs8w4
Jan 30 17:55:22.772: INFO: Got endpoints: latency-svc-bs8w4 [339.961798ms]
Jan 30 17:55:22.778: INFO: Created: latency-svc-qjhq5
Jan 30 17:55:22.790: INFO: Got endpoints: latency-svc-qjhq5 [336.296482ms]
Jan 30 17:55:22.796: INFO: Created: latency-svc-5pjwt
Jan 30 17:55:22.805: INFO: Got endpoints: latency-svc-5pjwt [325.781728ms]
Jan 30 17:55:22.812: INFO: Created: latency-svc-7mct8
Jan 30 17:55:22.832: INFO: Got endpoints: latency-svc-7mct8 [339.610622ms]
Jan 30 17:55:22.853: INFO: Created: latency-svc-z4dbb
Jan 30 17:55:22.858: INFO: Created: latency-svc-dbw69
Jan 30 17:55:22.859: INFO: Got endpoints: latency-svc-z4dbb [336.454797ms]
Jan 30 17:55:22.864: INFO: Got endpoints: latency-svc-dbw69 [321.224005ms]
Jan 30 17:55:22.877: INFO: Created: latency-svc-9dgbr
Jan 30 17:55:22.882: INFO: Got endpoints: latency-svc-9dgbr [320.897682ms]
Jan 30 17:55:22.893: INFO: Created: latency-svc-t2zbv
Jan 30 17:55:22.898: INFO: Got endpoints: latency-svc-t2zbv [312.331538ms]
Jan 30 17:55:22.910: INFO: Created: latency-svc-lr8q9
Jan 30 17:55:22.920: INFO: Got endpoints: latency-svc-lr8q9 [289.996241ms]
Jan 30 17:55:22.930: INFO: Created: latency-svc-g89kf
Jan 30 17:55:22.935: INFO: Got endpoints: latency-svc-g89kf [287.965442ms]
Jan 30 17:55:22.952: INFO: Created: latency-svc-tdlzg
Jan 30 17:55:22.965: INFO: Got endpoints: latency-svc-tdlzg [295.308974ms]
Jan 30 17:55:22.974: INFO: Created: latency-svc-pqtsj
Jan 30 17:55:22.977: INFO: Got endpoints: latency-svc-pqtsj [291.70732ms]
Jan 30 17:55:22.990: INFO: Created: latency-svc-kbcmn
Jan 30 17:55:22.995: INFO: Got endpoints: latency-svc-kbcmn [282.038674ms]
Jan 30 17:55:23.011: INFO: Created: latency-svc-xccr9
Jan 30 17:55:23.016: INFO: Got endpoints: latency-svc-xccr9 [289.527627ms]
Jan 30 17:55:23.028: INFO: Created: latency-svc-qrz7g
Jan 30 17:55:23.035: INFO: Got endpoints: latency-svc-qrz7g [283.666435ms]
Jan 30 17:55:23.051: INFO: Created: latency-svc-vlxpz
Jan 30 17:55:23.055: INFO: Got endpoints: latency-svc-vlxpz [283.376708ms]
Jan 30 17:55:23.079: INFO: Created: latency-svc-nr72g
Jan 30 17:55:23.083: INFO: Got endpoints: latency-svc-nr72g [293.479716ms]
Jan 30 17:55:23.105: INFO: Created: latency-svc-hfllw
Jan 30 17:55:23.114: INFO: Got endpoints: latency-svc-hfllw [308.951788ms]
Jan 30 17:55:23.140: INFO: Created: latency-svc-79qdz
Jan 30 17:55:23.153: INFO: Got endpoints: latency-svc-79qdz [320.875103ms]
Jan 30 17:55:23.158: INFO: Created: latency-svc-zl6qt
Jan 30 17:55:23.172: INFO: Created: latency-svc-4jc85
Jan 30 17:55:23.188: INFO: Created: latency-svc-4zpnt
Jan 30 17:55:23.202: INFO: Got endpoints: latency-svc-zl6qt [342.502509ms]
Jan 30 17:55:23.206: INFO: Created: latency-svc-d4nzj
Jan 30 17:55:23.220: INFO: Created: latency-svc-vk6lk
Jan 30 17:55:23.235: INFO: Created: latency-svc-p9zpt
Jan 30 17:55:23.248: INFO: Got endpoints: latency-svc-4jc85 [384.192055ms]
Jan 30 17:55:23.250: INFO: Created: latency-svc-jgv44
Jan 30 17:55:23.270: INFO: Created: latency-svc-qt54l
Jan 30 17:55:23.294: INFO: Created: latency-svc-2ht65
Jan 30 17:55:23.300: INFO: Got endpoints: latency-svc-4zpnt [417.552304ms]
Jan 30 17:55:23.311: INFO: Created: latency-svc-cqddk
Jan 30 17:55:23.326: INFO: Created: latency-svc-6ncsr
Jan 30 17:55:23.340: INFO: Created: latency-svc-fwn2f
Jan 30 17:55:23.352: INFO: Got endpoints: latency-svc-d4nzj [453.491688ms]
Jan 30 17:55:23.361: INFO: Created: latency-svc-9fc2j
Jan 30 17:55:23.378: INFO: Created: latency-svc-2xl69
Jan 30 17:55:23.408: INFO: Created: latency-svc-5vjkr
Jan 30 17:55:23.408: INFO: Got endpoints: latency-svc-vk6lk [487.701959ms]
Jan 30 17:55:23.428: INFO: Created: latency-svc-cqb9s
Jan 30 17:55:23.444: INFO: Created: latency-svc-rm62q
Jan 30 17:55:23.449: INFO: Got endpoints: latency-svc-p9zpt [513.088075ms]
Jan 30 17:55:23.462: INFO: Created: latency-svc-xtk9q
Jan 30 17:55:23.481: INFO: Created: latency-svc-mvj7l
Jan 30 17:55:23.502: INFO: Created: latency-svc-tfnw2
Jan 30 17:55:23.502: INFO: Got endpoints: latency-svc-jgv44 [536.630122ms]
Jan 30 17:55:23.536: INFO: Created: latency-svc-skvmd
Jan 30 17:55:23.547: INFO: Got endpoints: latency-svc-qt54l [569.349996ms]
Jan 30 17:55:23.549: INFO: Created: latency-svc-nrmd4
Jan 30 17:55:23.573: INFO: Created: latency-svc-hsvw2
Jan 30 17:55:23.599: INFO: Got endpoints: latency-svc-2ht65 [603.500161ms]
Jan 30 17:55:23.621: INFO: Created: latency-svc-qtt2p
Jan 30 17:55:23.648: INFO: Got endpoints: latency-svc-cqddk [631.968919ms]
Jan 30 17:55:23.675: INFO: Created: latency-svc-24wbf
Jan 30 17:55:23.698: INFO: Got endpoints: latency-svc-6ncsr [663.38716ms]
Jan 30 17:55:23.723: INFO: Created: latency-svc-rmmdz
Jan 30 17:55:23.748: INFO: Got endpoints: latency-svc-fwn2f [692.288823ms]
Jan 30 17:55:23.774: INFO: Created: latency-svc-rtrlj
Jan 30 17:55:23.801: INFO: Got endpoints: latency-svc-9fc2j [717.993093ms]
Jan 30 17:55:23.826: INFO: Created: latency-svc-tcjjj
Jan 30 17:55:23.852: INFO: Got endpoints: latency-svc-2xl69 [737.720402ms]
Jan 30 17:55:23.888: INFO: Created: latency-svc-jbdtr
Jan 30 17:55:23.898: INFO: Got endpoints: latency-svc-5vjkr [744.326977ms]
Jan 30 17:55:23.921: INFO: Created: latency-svc-5xzbs
Jan 30 17:55:23.948: INFO: Got endpoints: latency-svc-cqb9s [746.698254ms]
Jan 30 17:55:23.980: INFO: Created: latency-svc-bn2pd
Jan 30 17:55:23.998: INFO: Got endpoints: latency-svc-rm62q [750.079173ms]
Jan 30 17:55:24.024: INFO: Created: latency-svc-sdqwt
Jan 30 17:55:24.047: INFO: Got endpoints: latency-svc-xtk9q [747.263756ms]
Jan 30 17:55:24.073: INFO: Created: latency-svc-mlzl5
Jan 30 17:55:24.098: INFO: Got endpoints: latency-svc-mvj7l [746.761129ms]
Jan 30 17:55:24.122: INFO: Created: latency-svc-tckvw
Jan 30 17:55:24.150: INFO: Got endpoints: latency-svc-tfnw2 [742.353386ms]
Jan 30 17:55:24.182: INFO: Created: latency-svc-c7pck
Jan 30 17:55:24.198: INFO: Got endpoints: latency-svc-skvmd [749.34642ms]
Jan 30 17:55:24.223: INFO: Created: latency-svc-zwnsr
Jan 30 17:55:24.252: INFO: Got endpoints: latency-svc-nrmd4 [750.090715ms]
Jan 30 17:55:24.287: INFO: Created: latency-svc-kh6x2
Jan 30 17:55:24.298: INFO: Got endpoints: latency-svc-hsvw2 [750.983622ms]
Jan 30 17:55:24.320: INFO: Created: latency-svc-z8xk5
Jan 30 17:55:24.348: INFO: Got endpoints: latency-svc-qtt2p [748.943057ms]
Jan 30 17:55:24.369: INFO: Created: latency-svc-z5v8n
Jan 30 17:55:24.398: INFO: Got endpoints: latency-svc-24wbf [749.710001ms]
Jan 30 17:55:24.417: INFO: Created: latency-svc-57zvd
Jan 30 17:55:24.450: INFO: Got endpoints: latency-svc-rmmdz [751.659573ms]
Jan 30 17:55:24.472: INFO: Created: latency-svc-tlkcm
Jan 30 17:55:24.500: INFO: Got endpoints: latency-svc-rtrlj [752.32358ms]
Jan 30 17:55:24.525: INFO: Created: latency-svc-hd275
Jan 30 17:55:24.548: INFO: Got endpoints: latency-svc-tcjjj [746.456888ms]
Jan 30 17:55:24.572: INFO: Created: latency-svc-w78mp
Jan 30 17:55:24.598: INFO: Got endpoints: latency-svc-jbdtr [745.997104ms]
Jan 30 17:55:24.628: INFO: Created: latency-svc-5vd7g
Jan 30 17:55:24.649: INFO: Got endpoints: latency-svc-5xzbs [751.460538ms]
Jan 30 17:55:24.678: INFO: Created: latency-svc-j76kx
Jan 30 17:55:24.708: INFO: Got endpoints: latency-svc-bn2pd [759.574194ms]
Jan 30 17:55:24.736: INFO: Created: latency-svc-cbnk5
Jan 30 17:55:24.747: INFO: Got endpoints: latency-svc-sdqwt [749.232143ms]
Jan 30 17:55:24.775: INFO: Created: latency-svc-4hvnp
Jan 30 17:55:24.798: INFO: Got endpoints: latency-svc-mlzl5 [750.673724ms]
Jan 30 17:55:24.817: INFO: Created: latency-svc-4p965
Jan 30 17:55:24.849: INFO: Got endpoints: latency-svc-tckvw [750.410933ms]
Jan 30 17:55:24.871: INFO: Created: latency-svc-cg7wv
Jan 30 17:55:24.897: INFO: Got endpoints: latency-svc-c7pck [746.818055ms]
Jan 30 17:55:24.917: INFO: Created: latency-svc-kdp5r
Jan 30 17:55:24.948: INFO: Got endpoints: latency-svc-zwnsr [750.390824ms]
Jan 30 17:55:24.973: INFO: Created: latency-svc-66spc
Jan 30 17:55:24.998: INFO: Got endpoints: latency-svc-kh6x2 [745.492847ms]
Jan 30 17:55:25.019: INFO: Created: latency-svc-fgnx4
Jan 30 17:55:25.048: INFO: Got endpoints: latency-svc-z8xk5 [750.278499ms]
Jan 30 17:55:25.071: INFO: Created: latency-svc-5s6ks
Jan 30 17:55:25.100: INFO: Got endpoints: latency-svc-z5v8n [751.834935ms]
Jan 30 17:55:25.120: INFO: Created: latency-svc-fvwwk
Jan 30 17:55:25.153: INFO: Got endpoints: latency-svc-57zvd [754.254666ms]
Jan 30 17:55:25.175: INFO: Created: latency-svc-gs55r
Jan 30 17:55:25.198: INFO: Got endpoints: latency-svc-tlkcm [747.662324ms]
Jan 30 17:55:25.218: INFO: Created: latency-svc-xjcnh
Jan 30 17:55:25.247: INFO: Got endpoints: latency-svc-hd275 [746.366126ms]
Jan 30 17:55:25.274: INFO: Created: latency-svc-mjl8d
Jan 30 17:55:25.300: INFO: Got endpoints: latency-svc-w78mp [752.243043ms]
Jan 30 17:55:25.320: INFO: Created: latency-svc-thjvk
Jan 30 17:55:25.349: INFO: Got endpoints: latency-svc-5vd7g [750.771583ms]
Jan 30 17:55:25.377: INFO: Created: latency-svc-m4lcs
Jan 30 17:55:25.398: INFO: Got endpoints: latency-svc-j76kx [748.683318ms]
Jan 30 17:55:25.421: INFO: Created: latency-svc-q8rxd
Jan 30 17:55:25.449: INFO: Got endpoints: latency-svc-cbnk5 [740.74032ms]
Jan 30 17:55:25.471: INFO: Created: latency-svc-gggp4
Jan 30 17:55:25.498: INFO: Got endpoints: latency-svc-4hvnp [750.436942ms]
Jan 30 17:55:25.521: INFO: Created: latency-svc-wggcf
Jan 30 17:55:25.548: INFO: Got endpoints: latency-svc-4p965 [749.886259ms]
Jan 30 17:55:25.570: INFO: Created: latency-svc-6wkz4
Jan 30 17:55:25.598: INFO: Got endpoints: latency-svc-cg7wv [749.527479ms]
Jan 30 17:55:25.619: INFO: Created: latency-svc-nb2jk
Jan 30 17:55:25.652: INFO: Got endpoints: latency-svc-kdp5r [753.958047ms]
Jan 30 17:55:25.673: INFO: Created: latency-svc-frlxt
Jan 30 17:55:25.701: INFO: Got endpoints: latency-svc-66spc [751.835513ms]
Jan 30 17:55:25.723: INFO: Created: latency-svc-4zwvp
Jan 30 17:55:25.748: INFO: Got endpoints: latency-svc-fgnx4 [750.025032ms]
Jan 30 17:55:25.771: INFO: Created: latency-svc-58dds
Jan 30 17:55:25.799: INFO: Got endpoints: latency-svc-5s6ks [750.727454ms]
Jan 30 17:55:25.836: INFO: Created: latency-svc-j6mjl
Jan 30 17:55:25.847: INFO: Got endpoints: latency-svc-fvwwk [747.633507ms]
Jan 30 17:55:25.883: INFO: Created: latency-svc-bhhp9
Jan 30 17:55:25.897: INFO: Got endpoints: latency-svc-gs55r [744.290249ms]
Jan 30 17:55:25.925: INFO: Created: latency-svc-z7bhv
Jan 30 17:55:25.948: INFO: Got endpoints: latency-svc-xjcnh [750.06414ms]
Jan 30 17:55:25.969: INFO: Created: latency-svc-8hgl7
Jan 30 17:55:25.999: INFO: Got endpoints: latency-svc-mjl8d [751.335162ms]
Jan 30 17:55:26.029: INFO: Created: latency-svc-44tl2
Jan 30 17:55:26.048: INFO: Got endpoints: latency-svc-thjvk [747.393601ms]
Jan 30 17:55:26.068: INFO: Created: latency-svc-v9xjf
Jan 30 17:55:26.099: INFO: Got endpoints: latency-svc-m4lcs [750.116426ms]
Jan 30 17:55:26.119: INFO: Created: latency-svc-4wkhf
Jan 30 17:55:26.148: INFO: Got endpoints: latency-svc-q8rxd [749.517817ms]
Jan 30 17:55:26.167: INFO: Created: latency-svc-cmhtc
Jan 30 17:55:26.198: INFO: Got endpoints: latency-svc-gggp4 [749.255482ms]
Jan 30 17:55:26.220: INFO: Created: latency-svc-tm4f4
Jan 30 17:55:26.249: INFO: Got endpoints: latency-svc-wggcf [750.282648ms]
Jan 30 17:55:26.272: INFO: Created: latency-svc-2b4md
Jan 30 17:55:26.299: INFO: Got endpoints: latency-svc-6wkz4 [750.874383ms]
Jan 30 17:55:26.322: INFO: Created: latency-svc-lcvvj
Jan 30 17:55:26.350: INFO: Got endpoints: latency-svc-nb2jk [751.928772ms]
Jan 30 17:55:26.372: INFO: Created: latency-svc-fr229
Jan 30 17:55:26.398: INFO: Got endpoints: latency-svc-frlxt [746.54103ms]
Jan 30 17:55:26.420: INFO: Created: latency-svc-q6sbf
Jan 30 17:55:26.448: INFO: Got endpoints: latency-svc-4zwvp [747.826767ms]
Jan 30 17:55:26.473: INFO: Created: latency-svc-dthd5
Jan 30 17:55:26.498: INFO: Got endpoints: latency-svc-58dds [749.877106ms]
Jan 30 17:55:26.522: INFO: Created: latency-svc-c6hfq
Jan 30 17:55:26.547: INFO: Got endpoints: latency-svc-j6mjl [748.291056ms]
Jan 30 17:55:26.568: INFO: Created: latency-svc-sgnbn
Jan 30 17:55:26.598: INFO: Got endpoints: latency-svc-bhhp9 [750.174651ms]
Jan 30 17:55:26.621: INFO: Created: latency-svc-vqsw4
Jan 30 17:55:26.650: INFO: Got endpoints: latency-svc-z7bhv [753.059134ms]
Jan 30 17:55:26.675: INFO: Created: latency-svc-lwkc5
Jan 30 17:55:26.697: INFO: Got endpoints: latency-svc-8hgl7 [749.021998ms]
Jan 30 17:55:26.727: INFO: Created: latency-svc-qhjfq
Jan 30 17:55:26.748: INFO: Got endpoints: latency-svc-44tl2 [749.341582ms]
Jan 30 17:55:26.769: INFO: Created: latency-svc-lmtks
Jan 30 17:55:26.799: INFO: Got endpoints: latency-svc-v9xjf [751.119474ms]
Jan 30 17:55:26.821: INFO: Created: latency-svc-jxckx
Jan 30 17:55:26.847: INFO: Got endpoints: latency-svc-4wkhf [747.863515ms]
Jan 30 17:55:26.870: INFO: Created: latency-svc-kx9qn
Jan 30 17:55:26.900: INFO: Got endpoints: latency-svc-cmhtc [752.269214ms]
Jan 30 17:55:26.923: INFO: Created: latency-svc-cv7n5
Jan 30 17:55:26.947: INFO: Got endpoints: latency-svc-tm4f4 [749.069998ms]
Jan 30 17:55:26.969: INFO: Created: latency-svc-5cdk6
Jan 30 17:55:26.998: INFO: Got endpoints: latency-svc-2b4md [749.074358ms]
Jan 30 17:55:27.021: INFO: Created: latency-svc-tw4w7
Jan 30 17:55:27.048: INFO: Got endpoints: latency-svc-lcvvj [748.425929ms]
Jan 30 17:55:27.069: INFO: Created: latency-svc-wxhfx
Jan 30 17:55:27.102: INFO: Got endpoints: latency-svc-fr229 [751.802692ms]
Jan 30 17:55:27.127: INFO: Created: latency-svc-vckwn
Jan 30 17:55:27.148: INFO: Got endpoints: latency-svc-q6sbf [749.410156ms]
Jan 30 17:55:27.171: INFO: Created: latency-svc-t4qxx
Jan 30 17:55:27.198: INFO: Got endpoints: latency-svc-dthd5 [749.512769ms]
Jan 30 17:55:27.230: INFO: Created: latency-svc-w2cw2
Jan 30 17:55:27.248: INFO: Got endpoints: latency-svc-c6hfq [749.665033ms]
Jan 30 17:55:27.279: INFO: Created: latency-svc-7s5sn
Jan 30 17:55:27.298: INFO: Got endpoints: latency-svc-sgnbn [750.867364ms]
Jan 30 17:55:27.330: INFO: Created: latency-svc-bwsc9
Jan 30 17:55:27.353: INFO: Got endpoints: latency-svc-vqsw4 [755.394792ms]
Jan 30 17:55:27.376: INFO: Created: latency-svc-8h9sx
Jan 30 17:55:27.398: INFO: Got endpoints: latency-svc-lwkc5 [747.424699ms]
Jan 30 17:55:27.419: INFO: Created: latency-svc-x88wh
Jan 30 17:55:27.450: INFO: Got endpoints: latency-svc-qhjfq [752.485319ms]
Jan 30 17:55:27.473: INFO: Created: latency-svc-rbrsh
Jan 30 17:55:27.498: INFO: Got endpoints: latency-svc-lmtks [749.595174ms]
Jan 30 17:55:27.520: INFO: Created: latency-svc-kz4pw
Jan 30 17:55:27.548: INFO: Got endpoints: latency-svc-jxckx [749.138469ms]
Jan 30 17:55:27.570: INFO: Created: latency-svc-szllr
Jan 30 17:55:27.598: INFO: Got endpoints: latency-svc-kx9qn [750.268434ms]
Jan 30 17:55:27.618: INFO: Created: latency-svc-tmd8q
Jan 30 17:55:27.648: INFO: Got endpoints: latency-svc-cv7n5 [748.112226ms]
Jan 30 17:55:27.681: INFO: Created: latency-svc-cr7mr
Jan 30 17:55:27.698: INFO: Got endpoints: latency-svc-5cdk6 [750.631059ms]
Jan 30 17:55:27.722: INFO: Created: latency-svc-nnzv9
Jan 30 17:55:27.750: INFO: Got endpoints: latency-svc-tw4w7 [752.262739ms]
Jan 30 17:55:27.782: INFO: Created: latency-svc-t8dn9
Jan 30 17:55:27.797: INFO: Got endpoints: latency-svc-wxhfx [749.545841ms]
Jan 30 17:55:27.820: INFO: Created: latency-svc-vhbbl
Jan 30 17:55:27.848: INFO: Got endpoints: latency-svc-vckwn [745.320041ms]
Jan 30 17:55:27.877: INFO: Created: latency-svc-hfpx7
Jan 30 17:55:27.897: INFO: Got endpoints: latency-svc-t4qxx [749.367768ms]
Jan 30 17:55:27.921: INFO: Created: latency-svc-94gct
Jan 30 17:55:27.948: INFO: Got endpoints: latency-svc-w2cw2 [749.450626ms]
Jan 30 17:55:27.970: INFO: Created: latency-svc-mgxjk
Jan 30 17:55:27.997: INFO: Got endpoints: latency-svc-7s5sn [749.015389ms]
Jan 30 17:55:28.018: INFO: Created: latency-svc-c4fxq
Jan 30 17:55:28.047: INFO: Got endpoints: latency-svc-bwsc9 [748.218011ms]
Jan 30 17:55:28.077: INFO: Created: latency-svc-pwl8q
Jan 30 17:55:28.097: INFO: Got endpoints: latency-svc-8h9sx [743.511222ms]
Jan 30 17:55:28.116: INFO: Created: latency-svc-h2vwl
Jan 30 17:55:28.148: INFO: Got endpoints: latency-svc-x88wh [750.429717ms]
Jan 30 17:55:28.171: INFO: Created: latency-svc-rcrbs
Jan 30 17:55:28.199: INFO: Got endpoints: latency-svc-rbrsh [749.276762ms]
Jan 30 17:55:28.222: INFO: Created: latency-svc-xvppz
Jan 30 17:55:28.250: INFO: Got endpoints: latency-svc-kz4pw [751.901587ms]
Jan 30 17:55:28.275: INFO: Created: latency-svc-6wkkq
Jan 30 17:55:28.299: INFO: Got endpoints: latency-svc-szllr [750.679705ms]
Jan 30 17:55:28.322: INFO: Created: latency-svc-k4cv6
Jan 30 17:55:28.350: INFO: Got endpoints: latency-svc-tmd8q [752.203295ms]
Jan 30 17:55:28.370: INFO: Created: latency-svc-th4kv
Jan 30 17:55:28.397: INFO: Got endpoints: latency-svc-cr7mr [749.292478ms]
Jan 30 17:55:28.420: INFO: Created: latency-svc-zggk5
Jan 30 17:55:28.449: INFO: Got endpoints: latency-svc-nnzv9 [751.223153ms]
Jan 30 17:55:28.474: INFO: Created: latency-svc-jwv7j
Jan 30 17:55:28.500: INFO: Got endpoints: latency-svc-t8dn9 [749.512996ms]
Jan 30 17:55:28.524: INFO: Created: latency-svc-gr2rf
Jan 30 17:55:28.549: INFO: Got endpoints: latency-svc-vhbbl [751.224356ms]
Jan 30 17:55:28.574: INFO: Created: latency-svc-4hngp
Jan 30 17:55:28.598: INFO: Got endpoints: latency-svc-hfpx7 [749.910379ms]
Jan 30 17:55:28.626: INFO: Created: latency-svc-tmwvg
Jan 30 17:55:28.649: INFO: Got endpoints: latency-svc-94gct [751.867677ms]
Jan 30 17:55:28.670: INFO: Created: latency-svc-rzxpw
Jan 30 17:55:28.698: INFO: Got endpoints: latency-svc-mgxjk [750.200479ms]
Jan 30 17:55:28.734: INFO: Created: latency-svc-kstj2
Jan 30 17:55:28.747: INFO: Got endpoints: latency-svc-c4fxq [749.828044ms]
Jan 30 17:55:28.772: INFO: Created: latency-svc-lb95t
Jan 30 17:55:28.798: INFO: Got endpoints: latency-svc-pwl8q [751.540732ms]
Jan 30 17:55:28.832: INFO: Created: latency-svc-tggdt
Jan 30 17:55:28.847: INFO: Got endpoints: latency-svc-h2vwl [750.784119ms]
Jan 30 17:55:28.871: INFO: Created: latency-svc-gmqnp
Jan 30 17:55:28.898: INFO: Got endpoints: latency-svc-rcrbs [749.975253ms]
Jan 30 17:55:28.926: INFO: Created: latency-svc-pdmx6
Jan 30 17:55:28.947: INFO: Got endpoints: latency-svc-xvppz [747.76863ms]
Jan 30 17:55:28.971: INFO: Created: latency-svc-8z97z
Jan 30 17:55:28.998: INFO: Got endpoints: latency-svc-6wkkq [748.46007ms]
Jan 30 17:55:29.032: INFO: Created: latency-svc-v5xw4
Jan 30 17:55:29.051: INFO: Got endpoints: latency-svc-k4cv6 [752.662468ms]
Jan 30 17:55:29.076: INFO: Created: latency-svc-b4snh
Jan 30 17:55:29.100: INFO: Got endpoints: latency-svc-th4kv [749.686335ms]
Jan 30 17:55:29.122: INFO: Created: latency-svc-h5wnf
Jan 30 17:55:29.150: INFO: Got endpoints: latency-svc-zggk5 [752.056568ms]
Jan 30 17:55:29.174: INFO: Created: latency-svc-gnc2r
Jan 30 17:55:29.199: INFO: Got endpoints: latency-svc-jwv7j [749.728309ms]
Jan 30 17:55:29.249: INFO: Got endpoints: latency-svc-gr2rf [749.274156ms]
Jan 30 17:55:29.251: INFO: Created: latency-svc-lx5zz
Jan 30 17:55:29.265: INFO: Created: latency-svc-8kp22
Jan 30 17:55:29.298: INFO: Got endpoints: latency-svc-4hngp [748.886156ms]
Jan 30 17:55:29.319: INFO: Created: latency-svc-jch9l
Jan 30 17:55:29.349: INFO: Got endpoints: latency-svc-tmwvg [751.584508ms]
Jan 30 17:55:29.375: INFO: Created: latency-svc-mfxg8
Jan 30 17:55:29.397: INFO: Got endpoints: latency-svc-rzxpw [747.504931ms]
Jan 30 17:55:29.419: INFO: Created: latency-svc-lp56q
Jan 30 17:55:29.448: INFO: Got endpoints: latency-svc-kstj2 [749.695363ms]
Jan 30 17:55:29.467: INFO: Created: latency-svc-lkjm7
Jan 30 17:55:29.499: INFO: Got endpoints: latency-svc-lb95t [751.797792ms]
Jan 30 17:55:29.525: INFO: Created: latency-svc-6dx2b
Jan 30 17:55:29.549: INFO: Got endpoints: latency-svc-tggdt [750.030289ms]
Jan 30 17:55:29.576: INFO: Created: latency-svc-b6z4w
Jan 30 17:55:29.599: INFO: Got endpoints: latency-svc-gmqnp [751.064477ms]
Jan 30 17:55:29.618: INFO: Created: latency-svc-tmrjq
Jan 30 17:55:29.650: INFO: Got endpoints: latency-svc-pdmx6 [751.676026ms]
Jan 30 17:55:29.672: INFO: Created: latency-svc-j7z5f
Jan 30 17:55:29.700: INFO: Got endpoints: latency-svc-8z97z [752.348134ms]
Jan 30 17:55:29.720: INFO: Created: latency-svc-kgk4m
Jan 30 17:55:29.748: INFO: Got endpoints: latency-svc-v5xw4 [749.440922ms]
Jan 30 17:55:29.768: INFO: Created: latency-svc-ff75p
Jan 30 17:55:29.801: INFO: Got endpoints: latency-svc-b4snh [749.759979ms]
Jan 30 17:55:29.824: INFO: Created: latency-svc-cpxrq
Jan 30 17:55:29.848: INFO: Got endpoints: latency-svc-h5wnf [747.733913ms]
Jan 30 17:55:29.868: INFO: Created: latency-svc-8xt2p
Jan 30 17:55:29.898: INFO: Got endpoints: latency-svc-gnc2r [748.333078ms]
Jan 30 17:55:29.924: INFO: Created: latency-svc-7xlzt
Jan 30 17:55:29.949: INFO: Got endpoints: latency-svc-lx5zz [749.481642ms]
Jan 30 17:55:29.997: INFO: Got endpoints: latency-svc-8kp22 [748.453663ms]
Jan 30 17:55:30.050: INFO: Got endpoints: latency-svc-jch9l [752.099594ms]
Jan 30 17:55:30.099: INFO: Got endpoints: latency-svc-mfxg8 [749.186277ms]
Jan 30 17:55:30.148: INFO: Got endpoints: latency-svc-lp56q [751.218179ms]
Jan 30 17:55:30.199: INFO: Got endpoints: latency-svc-lkjm7 [751.186793ms]
Jan 30 17:55:30.249: INFO: Got endpoints: latency-svc-6dx2b [749.799176ms]
Jan 30 17:55:30.298: INFO: Got endpoints: latency-svc-b6z4w [749.495397ms]
Jan 30 17:55:30.348: INFO: Got endpoints: latency-svc-tmrjq [749.108509ms]
Jan 30 17:55:30.398: INFO: Got endpoints: latency-svc-j7z5f [748.592081ms]
Jan 30 17:55:30.449: INFO: Got endpoints: latency-svc-kgk4m [748.908315ms]
Jan 30 17:55:30.501: INFO: Got endpoints: latency-svc-ff75p [752.86437ms]
Jan 30 17:55:30.549: INFO: Got endpoints: latency-svc-cpxrq [747.659651ms]
Jan 30 17:55:30.598: INFO: Got endpoints: latency-svc-8xt2p [750.388299ms]
Jan 30 17:55:30.651: INFO: Got endpoints: latency-svc-7xlzt [753.014257ms]
Jan 30 17:55:30.651: INFO: Latencies: [30.735179ms 41.141017ms 43.874864ms 57.091562ms 70.818548ms 95.849273ms 120.257902ms 147.859631ms 163.637453ms 181.404762ms 208.267785ms 227.945798ms 249.36066ms 268.87355ms 282.038674ms 283.376708ms 283.666435ms 287.965442ms 289.180493ms 289.527627ms 289.996241ms 291.70732ms 293.479716ms 295.308974ms 302.408626ms 308.951788ms 310.55977ms 312.331538ms 314.270873ms 315.532813ms 320.875103ms 320.897682ms 321.224005ms 322.575713ms 322.593791ms 322.916316ms 323.185572ms 323.263044ms 325.492338ms 325.781728ms 327.621717ms 336.296482ms 336.454797ms 336.58237ms 337.073865ms 338.505184ms 339.610622ms 339.961798ms 342.502509ms 342.914395ms 344.859752ms 384.192055ms 417.552304ms 453.491688ms 487.701959ms 513.088075ms 536.630122ms 569.349996ms 603.500161ms 631.968919ms 663.38716ms 692.288823ms 717.993093ms 737.720402ms 740.74032ms 742.353386ms 743.511222ms 744.290249ms 744.326977ms 745.320041ms 745.492847ms 745.997104ms 746.366126ms 746.456888ms 746.54103ms 746.698254ms 746.761129ms 746.818055ms 747.263756ms 747.393601ms 747.424699ms 747.504931ms 747.633507ms 747.659651ms 747.662324ms 747.733913ms 747.76863ms 747.826767ms 747.863515ms 748.112226ms 748.218011ms 748.291056ms 748.333078ms 748.425929ms 748.453663ms 748.46007ms 748.592081ms 748.683318ms 748.886156ms 748.908315ms 748.943057ms 749.015389ms 749.021998ms 749.069998ms 749.074358ms 749.108509ms 749.138469ms 749.186277ms 749.232143ms 749.255482ms 749.274156ms 749.276762ms 749.292478ms 749.341582ms 749.34642ms 749.367768ms 749.410156ms 749.440922ms 749.450626ms 749.481642ms 749.495397ms 749.512769ms 749.512996ms 749.517817ms 749.527479ms 749.545841ms 749.595174ms 749.665033ms 749.686335ms 749.695363ms 749.710001ms 749.728309ms 749.759979ms 749.799176ms 749.828044ms 749.877106ms 749.886259ms 749.910379ms 749.975253ms 750.025032ms 750.030289ms 750.06414ms 750.079173ms 750.090715ms 750.116426ms 750.174651ms 750.200479ms 750.268434ms 750.278499ms 750.282648ms 750.388299ms 750.390824ms 750.410933ms 750.429717ms 750.436942ms 750.631059ms 750.673724ms 750.679705ms 750.727454ms 750.771583ms 750.784119ms 750.867364ms 750.874383ms 750.983622ms 751.064477ms 751.119474ms 751.186793ms 751.218179ms 751.223153ms 751.224356ms 751.335162ms 751.460538ms 751.540732ms 751.584508ms 751.659573ms 751.676026ms 751.797792ms 751.802692ms 751.834935ms 751.835513ms 751.867677ms 751.901587ms 751.928772ms 752.056568ms 752.099594ms 752.203295ms 752.243043ms 752.262739ms 752.269214ms 752.32358ms 752.348134ms 752.485319ms 752.662468ms 752.86437ms 753.014257ms 753.059134ms 753.958047ms 754.254666ms 755.394792ms 759.574194ms]
Jan 30 17:55:30.652: INFO: 50 %ile: 748.943057ms
Jan 30 17:55:30.652: INFO: 90 %ile: 751.867677ms
Jan 30 17:55:30.652: INFO: 99 %ile: 755.394792ms
Jan 30 17:55:30.652: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:55:30.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3619" for this suite.
Jan 30 17:55:44.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:55:44.838: INFO: namespace svc-latency-3619 deletion completed in 14.178945004s

• [SLOW TEST:25.980 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:55:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 30 17:55:44.893: INFO: Waiting up to 5m0s for pod "pod-418097d6-9282-47e6-8adc-466440e2d0ed" in namespace "emptydir-5112" to be "success or failure"
Jan 30 17:55:44.899: INFO: Pod "pod-418097d6-9282-47e6-8adc-466440e2d0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826323ms
Jan 30 17:55:46.905: INFO: Pod "pod-418097d6-9282-47e6-8adc-466440e2d0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012663662s
Jan 30 17:55:48.911: INFO: Pod "pod-418097d6-9282-47e6-8adc-466440e2d0ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018528351s
STEP: Saw pod success
Jan 30 17:55:48.911: INFO: Pod "pod-418097d6-9282-47e6-8adc-466440e2d0ed" satisfied condition "success or failure"
Jan 30 17:55:48.916: INFO: Trying to get logs from node aardvark pod pod-418097d6-9282-47e6-8adc-466440e2d0ed container test-container: <nil>
STEP: delete the pod
Jan 30 17:55:48.949: INFO: Waiting for pod pod-418097d6-9282-47e6-8adc-466440e2d0ed to disappear
Jan 30 17:55:48.954: INFO: Pod pod-418097d6-9282-47e6-8adc-466440e2d0ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:55:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5112" for this suite.
Jan 30 17:55:54.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:55:55.138: INFO: namespace emptydir-5112 deletion completed in 6.177002318s

• [SLOW TEST:10.298 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:55:55.139: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:56:21.225: INFO: Container started at 2020-01-30 17:55:56 +0000 UTC, pod became ready at 2020-01-30 17:56:20 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:56:21.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4748" for this suite.
Jan 30 17:56:43.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:56:43.403: INFO: namespace container-probe-4748 deletion completed in 22.17113317s

• [SLOW TEST:48.265 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:56:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 17:56:43.472: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 30 17:56:48.478: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 30 17:56:48.478: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jan 30 17:56:48.511: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8983,SelfLink:/apis/apps/v1/namespaces/deployment-8983/deployments/test-cleanup-deployment,UID:92671d80-673f-44cd-ad9b-197895862bc8,ResourceVersion:1192227,Generation:1,CreationTimestamp:2020-01-30 17:56:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 17:56:48.520: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8983,SelfLink:/apis/apps/v1/namespaces/deployment-8983/replicasets/test-cleanup-deployment-55bbcbc84c,UID:cb646651-bb2e-4b48-8c3c-3172275dafd7,ResourceVersion:1192229,Generation:1,CreationTimestamp:2020-01-30 17:56:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 92671d80-673f-44cd-ad9b-197895862bc8 0xc001b97b27 0xc001b97b28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 17:56:48.520: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 30 17:56:48.521: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8983,SelfLink:/apis/apps/v1/namespaces/deployment-8983/replicasets/test-cleanup-controller,UID:9108c866-6e54-48c1-873b-59c7afac4cb6,ResourceVersion:1192228,Generation:1,CreationTimestamp:2020-01-30 17:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 92671d80-673f-44cd-ad9b-197895862bc8 0xc001b97a57 0xc001b97a58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 30 17:56:48.529: INFO: Pod "test-cleanup-controller-5b7xh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-5b7xh,GenerateName:test-cleanup-controller-,Namespace:deployment-8983,SelfLink:/api/v1/namespaces/deployment-8983/pods/test-cleanup-controller-5b7xh,UID:710a60c2-0565-4ad3-ab01-eeb5ab8d347e,ResourceVersion:1192222,Generation:0,CreationTimestamp:2020-01-30 17:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 9108c866-6e54-48c1-873b-59c7afac4cb6 0xc0030e6d87 0xc0030e6d88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n48kl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n48kl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n48kl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030e6e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030e6e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:56:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:56:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:56:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 17:56:43 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.63,StartTime:2020-01-30 17:56:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 17:56:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://77b43d20e8145d55a0591604fa2d29a5f93af8d866bf52df4ec8aaffe9618a81}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:56:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8983" for this suite.
Jan 30 17:56:54.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:56:54.727: INFO: namespace deployment-8983 deletion completed in 6.182692891s

• [SLOW TEST:11.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:56:54.727: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jan 30 17:56:54.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 cluster-info'
Jan 30 17:56:54.898: INFO: stderr: ""
Jan 30 17:56:54.898: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:56:54.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1687" for this suite.
Jan 30 17:57:00.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:57:01.066: INFO: namespace kubectl-1687 deletion completed in 6.161687506s

• [SLOW TEST:6.339 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:57:01.066: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4323
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 30 17:57:01.117: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 30 17:57:21.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.65:8080/dial?request=hostName&protocol=udp&host=25.0.2.55&port=8081&tries=1'] Namespace:pod-network-test-4323 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:57:21.245: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:57:21.398: INFO: Waiting for endpoints: map[]
Jan 30 17:57:21.403: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.65:8080/dial?request=hostName&protocol=udp&host=25.0.1.64&port=8081&tries=1'] Namespace:pod-network-test-4323 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 17:57:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 17:57:21.566: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:57:21.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4323" for this suite.
Jan 30 17:57:43.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:57:43.750: INFO: namespace pod-network-test-4323 deletion completed in 22.175448339s

• [SLOW TEST:42.684 seconds]
[sig-network] Networking
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:57:43.751: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:57:43.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9" in namespace "downward-api-810" to be "success or failure"
Jan 30 17:57:43.830: INFO: Pod "downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.546905ms
Jan 30 17:57:45.836: INFO: Pod "downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010953857s
Jan 30 17:57:47.844: INFO: Pod "downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018486311s
STEP: Saw pod success
Jan 30 17:57:47.844: INFO: Pod "downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9" satisfied condition "success or failure"
Jan 30 17:57:47.849: INFO: Trying to get logs from node aardvark pod downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9 container client-container: <nil>
STEP: delete the pod
Jan 30 17:57:47.879: INFO: Waiting for pod downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9 to disappear
Jan 30 17:57:47.884: INFO: Pod downwardapi-volume-313812fa-5066-4839-87f6-0abf317434e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:57:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-810" for this suite.
Jan 30 17:57:53.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:57:54.060: INFO: namespace downward-api-810 deletion completed in 6.16834734s

• [SLOW TEST:10.310 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:57:54.062: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-d50d3ffc-3674-43d9-9c64-960ab3a67f6d
STEP: Creating a pod to test consume configMaps
Jan 30 17:57:54.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8" in namespace "configmap-9342" to be "success or failure"
Jan 30 17:57:54.144: INFO: Pod "pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.577149ms
Jan 30 17:57:56.151: INFO: Pod "pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013977549s
Jan 30 17:57:58.158: INFO: Pod "pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020844398s
STEP: Saw pod success
Jan 30 17:57:58.158: INFO: Pod "pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8" satisfied condition "success or failure"
Jan 30 17:57:58.162: INFO: Trying to get logs from node gazelle pod pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 17:57:58.196: INFO: Waiting for pod pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8 to disappear
Jan 30 17:57:58.201: INFO: Pod pod-configmaps-77ab63b5-3c26-45cc-a981-63132180e8a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:57:58.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9342" for this suite.
Jan 30 17:58:04.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:58:04.379: INFO: namespace configmap-9342 deletion completed in 6.172055634s

• [SLOW TEST:10.318 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:58:04.385: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5f5b8bf5-253f-46a8-ab01-2799fca7c070
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5f5b8bf5-253f-46a8-ab01-2799fca7c070
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:59:11.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5707" for this suite.
Jan 30 17:59:33.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:59:33.204: INFO: namespace projected-5707 deletion completed in 22.168600904s

• [SLOW TEST:88.819 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:59:33.207: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jan 30 17:59:33.274: INFO: Waiting up to 5m0s for pod "client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347" in namespace "containers-7666" to be "success or failure"
Jan 30 17:59:33.283: INFO: Pod "client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018229ms
Jan 30 17:59:35.290: INFO: Pod "client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015220682s
Jan 30 17:59:37.295: INFO: Pod "client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020810039s
STEP: Saw pod success
Jan 30 17:59:37.295: INFO: Pod "client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347" satisfied condition "success or failure"
Jan 30 17:59:37.301: INFO: Trying to get logs from node gazelle pod client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347 container test-container: <nil>
STEP: delete the pod
Jan 30 17:59:37.332: INFO: Waiting for pod client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347 to disappear
Jan 30 17:59:37.338: INFO: Pod client-containers-65f5fce2-ad13-498f-bc3a-85e3dca5b347 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:59:37.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7666" for this suite.
Jan 30 17:59:43.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:59:43.515: INFO: namespace containers-7666 deletion completed in 6.171218213s

• [SLOW TEST:10.309 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:59:43.516: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-9430/secret-test-933cc9f5-e1d6-442a-a9f7-5e1a3bea0e9f
STEP: Creating a pod to test consume secrets
Jan 30 17:59:43.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35" in namespace "secrets-9430" to be "success or failure"
Jan 30 17:59:43.596: INFO: Pod "pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283264ms
Jan 30 17:59:45.602: INFO: Pod "pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010454889s
Jan 30 17:59:47.608: INFO: Pod "pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016824535s
STEP: Saw pod success
Jan 30 17:59:47.609: INFO: Pod "pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35" satisfied condition "success or failure"
Jan 30 17:59:47.613: INFO: Trying to get logs from node aardvark pod pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35 container env-test: <nil>
STEP: delete the pod
Jan 30 17:59:47.648: INFO: Waiting for pod pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35 to disappear
Jan 30 17:59:47.653: INFO: Pod pod-configmaps-c170ff42-19fd-42ef-b7f2-b7c88a841d35 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 17:59:47.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9430" for this suite.
Jan 30 17:59:53.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 17:59:53.837: INFO: namespace secrets-9430 deletion completed in 6.175644018s

• [SLOW TEST:10.322 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 17:59:53.838: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 17:59:53.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca" in namespace "downward-api-1673" to be "success or failure"
Jan 30 17:59:53.911: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.71692ms
Jan 30 17:59:55.917: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013321635s
Jan 30 17:59:57.924: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019514852s
Jan 30 17:59:59.930: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025606724s
Jan 30 18:00:01.937: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032767559s
Jan 30 18:00:03.943: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038713069s
Jan 30 18:00:05.950: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045526636s
Jan 30 18:00:07.956: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051809676s
Jan 30 18:00:09.962: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.058183931s
STEP: Saw pod success
Jan 30 18:00:09.962: INFO: Pod "downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca" satisfied condition "success or failure"
Jan 30 18:00:09.967: INFO: Trying to get logs from node aardvark pod downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca container client-container: <nil>
STEP: delete the pod
Jan 30 18:00:10.000: INFO: Waiting for pod downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca to disappear
Jan 30 18:00:10.010: INFO: Pod downwardapi-volume-4ea11ab3-5601-4b3b-a8ad-5401b509cdca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:00:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1673" for this suite.
Jan 30 18:00:16.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:00:16.192: INFO: namespace downward-api-1673 deletion completed in 6.174966224s

• [SLOW TEST:22.355 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:00:16.200: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9e1e153a-cced-46bb-9414-5cb958ad5240
STEP: Creating a pod to test consume secrets
Jan 30 18:00:16.280: INFO: Waiting up to 5m0s for pod "pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104" in namespace "secrets-3659" to be "success or failure"
Jan 30 18:00:16.289: INFO: Pod "pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104": Phase="Pending", Reason="", readiness=false. Elapsed: 9.327581ms
Jan 30 18:00:18.295: INFO: Pod "pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014765359s
Jan 30 18:00:20.301: INFO: Pod "pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021446623s
STEP: Saw pod success
Jan 30 18:00:20.301: INFO: Pod "pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104" satisfied condition "success or failure"
Jan 30 18:00:20.307: INFO: Trying to get logs from node aardvark pod pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104 container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:00:20.345: INFO: Waiting for pod pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104 to disappear
Jan 30 18:00:20.350: INFO: Pod pod-secrets-ac8e55a6-5a5f-4b89-aa7a-99bdcc490104 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:00:20.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3659" for this suite.
Jan 30 18:00:26.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:00:26.510: INFO: namespace secrets-3659 deletion completed in 6.154663889s

• [SLOW TEST:10.310 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:00:26.511: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:00:26.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab" in namespace "projected-4116" to be "success or failure"
Jan 30 18:00:26.575: INFO: Pod "downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188254ms
Jan 30 18:00:28.582: INFO: Pod "downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009636052s
Jan 30 18:00:30.587: INFO: Pod "downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015257847s
STEP: Saw pod success
Jan 30 18:00:30.588: INFO: Pod "downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab" satisfied condition "success or failure"
Jan 30 18:00:30.592: INFO: Trying to get logs from node aardvark pod downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab container client-container: <nil>
STEP: delete the pod
Jan 30 18:00:30.623: INFO: Waiting for pod downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab to disappear
Jan 30 18:00:30.628: INFO: Pod downwardapi-volume-f618b2f4-e2a7-488e-bc75-9bc7aad471ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:00:30.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4116" for this suite.
Jan 30 18:00:36.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:00:36.809: INFO: namespace projected-4116 deletion completed in 6.174837449s

• [SLOW TEST:10.298 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:00:36.810: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:00:36.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9667" for this suite.
Jan 30 18:00:42.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:00:43.058: INFO: namespace kubelet-test-9667 deletion completed in 6.158197169s

• [SLOW TEST:6.248 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:00:43.058: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:00:43.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf" in namespace "projected-3641" to be "success or failure"
Jan 30 18:00:43.133: INFO: Pod "downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.845615ms
Jan 30 18:00:45.140: INFO: Pod "downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013112702s
Jan 30 18:00:47.146: INFO: Pod "downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019590366s
STEP: Saw pod success
Jan 30 18:00:47.146: INFO: Pod "downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf" satisfied condition "success or failure"
Jan 30 18:00:47.151: INFO: Trying to get logs from node aardvark pod downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf container client-container: <nil>
STEP: delete the pod
Jan 30 18:00:47.184: INFO: Waiting for pod downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf to disappear
Jan 30 18:00:47.189: INFO: Pod downwardapi-volume-68ce72bd-4fc7-4d5c-ab74-fad47a4fe8cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:00:47.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3641" for this suite.
Jan 30 18:00:53.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:00:53.367: INFO: namespace projected-3641 deletion completed in 6.172366278s

• [SLOW TEST:10.309 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:00:53.368: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:00:53.445: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 30 18:00:58.451: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 30 18:00:58.452: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 30 18:01:00.458: INFO: Creating deployment "test-rollover-deployment"
Jan 30 18:01:00.469: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 30 18:01:02.481: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 30 18:01:02.493: INFO: Ensure that both replica sets have 1 created replica
Jan 30 18:01:02.504: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 30 18:01:02.517: INFO: Updating deployment test-rollover-deployment
Jan 30 18:01:02.518: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 30 18:01:04.537: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 30 18:01:04.547: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 30 18:01:04.557: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:04.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004062, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:06.569: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:06.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:08.572: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:08.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:10.568: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:10.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:12.570: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:12.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:14.575: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 18:01:14.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716004060, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:01:16.570: INFO: 
Jan 30 18:01:16.570: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jan 30 18:01:16.588: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9268,SelfLink:/apis/apps/v1/namespaces/deployment-9268/deployments/test-rollover-deployment,UID:d810b284-b7c1-465c-9bb7-c29725d56e8d,ResourceVersion:1193158,Generation:2,CreationTimestamp:2020-01-30 18:01:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-01-30 18:01:00 +0000 UTC 2020-01-30 18:01:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-01-30 18:01:14 +0000 UTC 2020-01-30 18:01:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 18:01:16.594: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9268,SelfLink:/apis/apps/v1/namespaces/deployment-9268/replicasets/test-rollover-deployment-854595fc44,UID:b61ad84f-544e-404c-8f85-f12cc3d0948d,ResourceVersion:1193147,Generation:2,CreationTimestamp:2020-01-30 18:01:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d810b284-b7c1-465c-9bb7-c29725d56e8d 0xc003370f37 0xc003370f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 30 18:01:16.594: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 30 18:01:16.594: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9268,SelfLink:/apis/apps/v1/namespaces/deployment-9268/replicasets/test-rollover-controller,UID:eba20f04-188c-4036-a926-2e37c1c20378,ResourceVersion:1193156,Generation:2,CreationTimestamp:2020-01-30 18:00:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d810b284-b7c1-465c-9bb7-c29725d56e8d 0xc003370e4f 0xc003370e60}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 18:01:16.594: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9268,SelfLink:/apis/apps/v1/namespaces/deployment-9268/replicasets/test-rollover-deployment-9b8b997cf,UID:cbace7bc-5df4-4121-b409-fcd0c4579a76,ResourceVersion:1193114,Generation:2,CreationTimestamp:2020-01-30 18:01:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d810b284-b7c1-465c-9bb7-c29725d56e8d 0xc003371000 0xc003371001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 18:01:16.599: INFO: Pod "test-rollover-deployment-854595fc44-8bxx5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-8bxx5,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9268,SelfLink:/api/v1/namespaces/deployment-9268/pods/test-rollover-deployment-854595fc44-8bxx5,UID:201510b1-8e2f-4987-9291-2cd6bbc52336,ResourceVersion:1193124,Generation:0,CreationTimestamp:2020-01-30 18:01:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 b61ad84f-544e-404c-8f85-f12cc3d0948d 0xc003371c17 0xc003371c18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kffmt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kffmt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-kffmt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003371c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003371cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:01:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:01:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:01:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:01:02 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.74,StartTime:2020-01-30 18:01:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-01-30 18:01:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e794044bb1dd674cff1257766c09d9c5f2237770864afca08b7c682df9dfe22c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:01:16.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9268" for this suite.
Jan 30 18:01:22.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:01:22.776: INFO: namespace deployment-9268 deletion completed in 6.171030972s

• [SLOW TEST:29.409 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:01:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d2eb5bd6-f0de-4e8e-bb70-4ec5e4722f2f
STEP: Creating a pod to test consume configMaps
Jan 30 18:01:22.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea" in namespace "configmap-6612" to be "success or failure"
Jan 30 18:01:22.867: INFO: Pod "pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.928336ms
Jan 30 18:01:24.874: INFO: Pod "pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014932191s
Jan 30 18:01:26.880: INFO: Pod "pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021112753s
STEP: Saw pod success
Jan 30 18:01:26.881: INFO: Pod "pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea" satisfied condition "success or failure"
Jan 30 18:01:26.885: INFO: Trying to get logs from node gazelle pod pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:01:26.916: INFO: Waiting for pod pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea to disappear
Jan 30 18:01:26.921: INFO: Pod pod-configmaps-86c790b9-aaf8-4ff2-bcef-d690eacfadea no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:01:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6612" for this suite.
Jan 30 18:01:32.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:01:33.102: INFO: namespace configmap-6612 deletion completed in 6.174429216s

• [SLOW TEST:10.324 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:01:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4933.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4933.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4933.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4933.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4933.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4933.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 18:01:37.238: INFO: DNS probes using dns-4933/dns-test-6715ddd7-9983-4975-986f-6c9398453af2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:01:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4933" for this suite.
Jan 30 18:01:43.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:01:43.428: INFO: namespace dns-4933 deletion completed in 6.161819346s

• [SLOW TEST:10.325 seconds]
[sig-network] DNS
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:01:43.430: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 18:01:43.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4093'
Jan 30 18:01:44.971: INFO: stderr: ""
Jan 30 18:01:44.971: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Jan 30 18:01:44.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete pods e2e-test-nginx-pod --namespace=kubectl-4093'
Jan 30 18:01:47.415: INFO: stderr: ""
Jan 30 18:01:47.416: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:01:47.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4093" for this suite.
Jan 30 18:01:53.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:01:53.604: INFO: namespace kubectl-4093 deletion completed in 6.181180844s

• [SLOW TEST:10.174 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:01:53.605: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:01:57.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3060" for this suite.
Jan 30 18:02:03.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:02:03.937: INFO: namespace emptydir-wrapper-3060 deletion completed in 6.166280257s

• [SLOW TEST:10.332 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:02:03.937: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-7849
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7849
STEP: Deleting pre-stop pod
Jan 30 18:02:17.061: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:02:17.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7849" for this suite.
Jan 30 18:02:57.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:02:57.244: INFO: namespace prestop-7849 deletion completed in 40.165934313s

• [SLOW TEST:53.308 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:02:57.248: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:02:57.336: INFO: Create a RollingUpdate DaemonSet
Jan 30 18:02:57.345: INFO: Check that daemon pods launch on every node of the cluster
Jan 30 18:02:57.351: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:57.352: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:57.352: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:57.356: INFO: Number of nodes with available pods: 0
Jan 30 18:02:57.356: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:02:58.364: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:58.364: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:58.364: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:58.369: INFO: Number of nodes with available pods: 0
Jan 30 18:02:58.369: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:02:59.368: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:59.368: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:59.369: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:02:59.375: INFO: Number of nodes with available pods: 0
Jan 30 18:02:59.375: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:03:00.364: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:00.364: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:00.364: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:00.370: INFO: Number of nodes with available pods: 2
Jan 30 18:03:00.370: INFO: Number of running nodes: 2, number of available pods: 2
Jan 30 18:03:00.370: INFO: Update the DaemonSet to trigger a rollout
Jan 30 18:03:00.388: INFO: Updating DaemonSet daemon-set
Jan 30 18:03:04.415: INFO: Roll back the DaemonSet before rollout is complete
Jan 30 18:03:04.430: INFO: Updating DaemonSet daemon-set
Jan 30 18:03:04.430: INFO: Make sure DaemonSet rollback is complete
Jan 30 18:03:04.437: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:04.437: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:04.444: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:04.444: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:04.444: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:05.452: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:05.452: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:05.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:05.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:05.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:06.452: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:06.452: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:06.460: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:06.460: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:06.460: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:07.452: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:07.452: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:07.459: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:07.459: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:07.459: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:08.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:08.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:08.457: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:08.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:08.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:09.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:09.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:09.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:09.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:09.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:10.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:10.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:10.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:10.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:10.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:11.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:11.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:11.457: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:11.457: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:11.457: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:12.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:12.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:12.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:12.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:12.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:13.450: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:13.450: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:13.457: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:13.457: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:13.457: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:14.451: INFO: Wrong image for pod: daemon-set-hkdhj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 30 18:03:14.451: INFO: Pod daemon-set-hkdhj is not available
Jan 30 18:03:14.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:14.458: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:14.458: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:15.451: INFO: Pod daemon-set-dcxtt is not available
Jan 30 18:03:15.458: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:15.459: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:03:15.459: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8789, will wait for the garbage collector to delete the pods
Jan 30 18:03:15.537: INFO: Deleting DaemonSet.extensions daemon-set took: 13.123212ms
Jan 30 18:03:15.838: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.45154ms
Jan 30 18:03:24.543: INFO: Number of nodes with available pods: 0
Jan 30 18:03:24.543: INFO: Number of running nodes: 0, number of available pods: 0
Jan 30 18:03:24.552: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8789/daemonsets","resourceVersion":"1193675"},"items":null}

Jan 30 18:03:24.558: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8789/pods","resourceVersion":"1193675"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:03:24.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8789" for this suite.
Jan 30 18:03:30.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:03:30.751: INFO: namespace daemonsets-8789 deletion completed in 6.171155911s

• [SLOW TEST:33.504 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:03:30.752: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 30 18:03:31.181: INFO: Pod name wrapped-volume-race-3dac9c80-052a-4a25-8582-116370106a08: Found 0 pods out of 5
Jan 30 18:03:36.192: INFO: Pod name wrapped-volume-race-3dac9c80-052a-4a25-8582-116370106a08: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3dac9c80-052a-4a25-8582-116370106a08 in namespace emptydir-wrapper-1981, will wait for the garbage collector to delete the pods
Jan 30 18:03:48.307: INFO: Deleting ReplicationController wrapped-volume-race-3dac9c80-052a-4a25-8582-116370106a08 took: 16.114766ms
Jan 30 18:03:48.607: INFO: Terminating ReplicationController wrapped-volume-race-3dac9c80-052a-4a25-8582-116370106a08 pods took: 300.461226ms
STEP: Creating RC which spawns configmap-volume pods
Jan 30 18:04:26.838: INFO: Pod name wrapped-volume-race-50b18bf5-e17e-48ee-9a63-42f9e614a0f5: Found 0 pods out of 5
Jan 30 18:04:31.850: INFO: Pod name wrapped-volume-race-50b18bf5-e17e-48ee-9a63-42f9e614a0f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-50b18bf5-e17e-48ee-9a63-42f9e614a0f5 in namespace emptydir-wrapper-1981, will wait for the garbage collector to delete the pods
Jan 30 18:04:41.952: INFO: Deleting ReplicationController wrapped-volume-race-50b18bf5-e17e-48ee-9a63-42f9e614a0f5 took: 12.847309ms
Jan 30 18:04:42.252: INFO: Terminating ReplicationController wrapped-volume-race-50b18bf5-e17e-48ee-9a63-42f9e614a0f5 pods took: 300.476812ms
STEP: Creating RC which spawns configmap-volume pods
Jan 30 18:05:25.383: INFO: Pod name wrapped-volume-race-0c918f0a-b4b5-4f8a-83c8-4d6bd20035d9: Found 0 pods out of 5
Jan 30 18:05:30.394: INFO: Pod name wrapped-volume-race-0c918f0a-b4b5-4f8a-83c8-4d6bd20035d9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0c918f0a-b4b5-4f8a-83c8-4d6bd20035d9 in namespace emptydir-wrapper-1981, will wait for the garbage collector to delete the pods
Jan 30 18:05:40.501: INFO: Deleting ReplicationController wrapped-volume-race-0c918f0a-b4b5-4f8a-83c8-4d6bd20035d9 took: 15.604349ms
Jan 30 18:05:40.801: INFO: Terminating ReplicationController wrapped-volume-race-0c918f0a-b4b5-4f8a-83c8-4d6bd20035d9 pods took: 300.491654ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:06:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1981" for this suite.
Jan 30 18:06:34.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:06:34.348: INFO: namespace emptydir-wrapper-1981 deletion completed in 8.177231262s

• [SLOW TEST:183.597 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:06:34.349: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 30 18:06:34.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2608,SelfLink:/api/v1/namespaces/watch-2608/configmaps/e2e-watch-test-resource-version,UID:4aed57c3-9666-452b-9d26-0059a7ec8a15,ResourceVersion:1194918,Generation:0,CreationTimestamp:2020-01-30 18:06:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 30 18:06:34.444: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2608,SelfLink:/api/v1/namespaces/watch-2608/configmaps/e2e-watch-test-resource-version,UID:4aed57c3-9666-452b-9d26-0059a7ec8a15,ResourceVersion:1194919,Generation:0,CreationTimestamp:2020-01-30 18:06:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:06:34.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2608" for this suite.
Jan 30 18:06:40.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:06:40.623: INFO: namespace watch-2608 deletion completed in 6.172792117s

• [SLOW TEST:6.274 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:06:40.624: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:06:40.711: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 30 18:06:40.729: INFO: Number of nodes with available pods: 0
Jan 30 18:06:40.730: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 30 18:06:40.763: INFO: Number of nodes with available pods: 0
Jan 30 18:06:40.763: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:41.770: INFO: Number of nodes with available pods: 0
Jan 30 18:06:41.770: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:42.770: INFO: Number of nodes with available pods: 0
Jan 30 18:06:42.770: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:43.771: INFO: Number of nodes with available pods: 1
Jan 30 18:06:43.771: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 30 18:06:43.800: INFO: Number of nodes with available pods: 1
Jan 30 18:06:43.800: INFO: Number of running nodes: 0, number of available pods: 1
Jan 30 18:06:44.812: INFO: Number of nodes with available pods: 0
Jan 30 18:06:44.812: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 30 18:06:44.833: INFO: Number of nodes with available pods: 0
Jan 30 18:06:44.833: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:45.840: INFO: Number of nodes with available pods: 0
Jan 30 18:06:45.840: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:46.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:46.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:47.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:47.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:48.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:48.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:49.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:49.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:50.840: INFO: Number of nodes with available pods: 0
Jan 30 18:06:50.840: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:51.840: INFO: Number of nodes with available pods: 0
Jan 30 18:06:51.840: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:52.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:52.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:53.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:53.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:54.839: INFO: Number of nodes with available pods: 0
Jan 30 18:06:54.839: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:55.840: INFO: Number of nodes with available pods: 0
Jan 30 18:06:55.840: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:56.838: INFO: Number of nodes with available pods: 0
Jan 30 18:06:56.838: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:06:57.840: INFO: Number of nodes with available pods: 1
Jan 30 18:06:57.840: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6113, will wait for the garbage collector to delete the pods
Jan 30 18:06:57.921: INFO: Deleting DaemonSet.extensions daemon-set took: 13.652864ms
Jan 30 18:06:58.221: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.412383ms
Jan 30 18:07:01.528: INFO: Number of nodes with available pods: 0
Jan 30 18:07:01.528: INFO: Number of running nodes: 0, number of available pods: 0
Jan 30 18:07:01.533: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6113/daemonsets","resourceVersion":"1195029"},"items":null}

Jan 30 18:07:01.537: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6113/pods","resourceVersion":"1195029"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:07:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6113" for this suite.
Jan 30 18:07:07.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:07:07.739: INFO: namespace daemonsets-6113 deletion completed in 6.16387415s

• [SLOW TEST:27.115 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:07:07.739: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:07:07.794: INFO: Creating ReplicaSet my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3
Jan 30 18:07:07.809: INFO: Pod name my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3: Found 0 pods out of 1
Jan 30 18:07:12.817: INFO: Pod name my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3: Found 1 pods out of 1
Jan 30 18:07:12.817: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3" is running
Jan 30 18:07:12.822: INFO: Pod "my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3-hkd89" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 18:07:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 18:07:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 18:07:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-30 18:07:07 +0000 UTC Reason: Message:}])
Jan 30 18:07:12.822: INFO: Trying to dial the pod
Jan 30 18:07:17.841: INFO: Controller my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3: Got expected result from replica 1 [my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3-hkd89]: "my-hostname-basic-75d490dc-5076-4751-9759-9b35909151d3-hkd89", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:07:17.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-790" for this suite.
Jan 30 18:07:23.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:07:24.029: INFO: namespace replicaset-790 deletion completed in 6.180495179s

• [SLOW TEST:16.289 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:07:24.029: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-488c4ffd-b431-44ae-869a-0e39c9d20ba3 in namespace container-probe-6050
Jan 30 18:07:28.107: INFO: Started pod busybox-488c4ffd-b431-44ae-869a-0e39c9d20ba3 in namespace container-probe-6050
STEP: checking the pod's current state and verifying that restartCount is present
Jan 30 18:07:28.112: INFO: Initial restart count of pod busybox-488c4ffd-b431-44ae-869a-0e39c9d20ba3 is 0
Jan 30 18:08:22.297: INFO: Restart count of pod container-probe-6050/busybox-488c4ffd-b431-44ae-869a-0e39c9d20ba3 is now 1 (54.184886485s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:08:22.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6050" for this suite.
Jan 30 18:08:28.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:08:28.484: INFO: namespace container-probe-6050 deletion completed in 6.16133705s

• [SLOW TEST:64.455 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:08:28.485: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-9c4eb978-a9e1-45d9-805b-eb46cec07f64
STEP: Creating configMap with name cm-test-opt-upd-88f0bd79-5413-49b5-848e-f8e2998012ae
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9c4eb978-a9e1-45d9-805b-eb46cec07f64
STEP: Updating configmap cm-test-opt-upd-88f0bd79-5413-49b5-848e-f8e2998012ae
STEP: Creating configMap with name cm-test-opt-create-cba4469d-c4d7-40ed-b216-6ebe64f91827
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:08:36.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1618" for this suite.
Jan 30 18:08:58.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:08:58.917: INFO: namespace projected-1618 deletion completed in 22.172961888s

• [SLOW TEST:30.432 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:08:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:08:58.975: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:09:00.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3437" for this suite.
Jan 30 18:09:06.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:09:06.251: INFO: namespace custom-resource-definition-3437 deletion completed in 6.181049134s

• [SLOW TEST:7.333 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:09:06.251: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:09:06.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd" in namespace "projected-4593" to be "success or failure"
Jan 30 18:09:06.326: INFO: Pod "downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.807586ms
Jan 30 18:09:08.334: INFO: Pod "downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015214709s
Jan 30 18:09:10.339: INFO: Pod "downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020910765s
STEP: Saw pod success
Jan 30 18:09:10.339: INFO: Pod "downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd" satisfied condition "success or failure"
Jan 30 18:09:10.344: INFO: Trying to get logs from node aardvark pod downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd container client-container: <nil>
STEP: delete the pod
Jan 30 18:09:10.380: INFO: Waiting for pod downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd to disappear
Jan 30 18:09:10.385: INFO: Pod downwardapi-volume-225db07d-71c0-4d09-bfc6-74a8e2070bfd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:09:10.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4593" for this suite.
Jan 30 18:09:16.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:09:16.568: INFO: namespace projected-4593 deletion completed in 6.176635766s

• [SLOW TEST:10.317 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:09:16.568: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jan 30 18:09:16.633: INFO: Waiting up to 5m0s for pod "downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f" in namespace "downward-api-1354" to be "success or failure"
Jan 30 18:09:16.639: INFO: Pod "downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750348ms
Jan 30 18:09:18.646: INFO: Pod "downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013032462s
Jan 30 18:09:20.652: INFO: Pod "downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019076647s
STEP: Saw pod success
Jan 30 18:09:20.652: INFO: Pod "downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f" satisfied condition "success or failure"
Jan 30 18:09:20.657: INFO: Trying to get logs from node gazelle pod downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:09:20.691: INFO: Waiting for pod downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f to disappear
Jan 30 18:09:20.695: INFO: Pod downward-api-32eb4449-e0be-4e04-b8f2-6eb68ed4586f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:09:20.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1354" for this suite.
Jan 30 18:09:26.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:09:26.872: INFO: namespace downward-api-1354 deletion completed in 6.170628274s

• [SLOW TEST:10.304 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:09:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 18:09:26.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5279'
Jan 30 18:09:27.075: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 30 18:09:27.075: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 30 18:09:27.083: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 30 18:09:27.091: INFO: scanned /root for discovery docs: <nil>
Jan 30 18:09:27.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5279'
Jan 30 18:09:43.076: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 30 18:09:43.076: INFO: stdout: "Created e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06\nScaling up e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 30 18:09:43.076: INFO: stdout: "Created e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06\nScaling up e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 30 18:09:43.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5279'
Jan 30 18:09:43.189: INFO: stderr: ""
Jan 30 18:09:43.189: INFO: stdout: "e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06-962hb e2e-test-nginx-rc-q6lv5 "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jan 30 18:09:48.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5279'
Jan 30 18:09:48.284: INFO: stderr: ""
Jan 30 18:09:48.284: INFO: stdout: "e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06-962hb "
Jan 30 18:09:48.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06-962hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5279'
Jan 30 18:09:48.399: INFO: stderr: ""
Jan 30 18:09:48.399: INFO: stdout: "true"
Jan 30 18:09:48.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06-962hb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5279'
Jan 30 18:09:48.499: INFO: stderr: ""
Jan 30 18:09:48.499: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jan 30 18:09:48.499: INFO: e2e-test-nginx-rc-dfa99b991a0a8a1fd1151dd942159c06-962hb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Jan 30 18:09:48.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete rc e2e-test-nginx-rc --namespace=kubectl-5279'
Jan 30 18:09:48.632: INFO: stderr: ""
Jan 30 18:09:48.633: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:09:48.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5279" for this suite.
Jan 30 18:09:54.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:09:54.817: INFO: namespace kubectl-5279 deletion completed in 6.174803441s

• [SLOW TEST:27.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:09:54.817: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jan 30 18:09:54.871: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 30 18:09:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:55.205: INFO: stderr: ""
Jan 30 18:09:55.205: INFO: stdout: "service/redis-slave created\n"
Jan 30 18:09:55.205: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 30 18:09:55.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:55.461: INFO: stderr: ""
Jan 30 18:09:55.461: INFO: stdout: "service/redis-master created\n"
Jan 30 18:09:55.461: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 30 18:09:55.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:55.834: INFO: stderr: ""
Jan 30 18:09:55.834: INFO: stdout: "service/frontend created\n"
Jan 30 18:09:55.835: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 30 18:09:55.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:56.168: INFO: stderr: ""
Jan 30 18:09:56.168: INFO: stdout: "deployment.apps/frontend created\n"
Jan 30 18:09:56.168: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 30 18:09:56.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:56.488: INFO: stderr: ""
Jan 30 18:09:56.488: INFO: stdout: "deployment.apps/redis-master created\n"
Jan 30 18:09:56.488: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 30 18:09:56.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-4012'
Jan 30 18:09:56.813: INFO: stderr: ""
Jan 30 18:09:56.813: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jan 30 18:09:56.813: INFO: Waiting for all frontend pods to be Running.
Jan 30 18:10:16.864: INFO: Waiting for frontend to serve content.
Jan 30 18:10:16.892: INFO: Trying to add a new entry to the guestbook.
Jan 30 18:10:16.919: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 30 18:10:16.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.123: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.123: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 30 18:10:17.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.271: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.272: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 30 18:10:17.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.434: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.434: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 30 18:10:17.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.540: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 30 18:10:17.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.685: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.685: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 30 18:10:17.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-4012'
Jan 30 18:10:17.822: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:10:17.822: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:10:17.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4012" for this suite.
Jan 30 18:10:57.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:10:58.015: INFO: namespace kubectl-4012 deletion completed in 40.186501251s

• [SLOW TEST:63.197 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:10:58.015: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 18:10:58.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9731'
Jan 30 18:10:58.220: INFO: stderr: ""
Jan 30 18:10:58.221: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 30 18:11:03.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pod e2e-test-nginx-pod --namespace=kubectl-9731 -o json'
Jan 30 18:11:03.376: INFO: stderr: ""
Jan 30 18:11:03.376: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-01-30T18:10:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9731\",\n        \"resourceVersion\": \"1195964\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9731/pods/e2e-test-nginx-pod\",\n        \"uid\": \"ed6f6723-626a-4f24-83b6-7828cb2a6132\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lsv5n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aardvark\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lsv5n\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lsv5n\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-30T18:10:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-30T18:11:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-30T18:11:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-30T18:10:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://64b0bc6bd2efab2c622e47bae3df467f15bf1d3b41a247cc91f73f6d13ff4b1c\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-30T18:10:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"167.172.59.233\",\n        \"phase\": \"Running\",\n        \"podIP\": \"25.0.1.105\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-30T18:10:58Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 30 18:11:03.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 replace -f - --namespace=kubectl-9731'
Jan 30 18:11:03.670: INFO: stderr: ""
Jan 30 18:11:03.670: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Jan 30 18:11:03.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete pods e2e-test-nginx-pod --namespace=kubectl-9731'
Jan 30 18:11:05.463: INFO: stderr: ""
Jan 30 18:11:05.463: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:11:05.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9731" for this suite.
Jan 30 18:11:11.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:11:11.649: INFO: namespace kubectl-9731 deletion completed in 6.179886143s

• [SLOW TEST:13.634 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:11:11.650: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 18:11:11.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-3465'
Jan 30 18:11:11.820: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 30 18:11:11.820: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Jan 30 18:11:15.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3465'
Jan 30 18:11:15.971: INFO: stderr: ""
Jan 30 18:11:15.971: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:11:15.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3465" for this suite.
Jan 30 18:11:38.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:11:38.150: INFO: namespace kubectl-3465 deletion completed in 22.171527769s

• [SLOW TEST:26.499 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:11:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 30 18:11:42.761: INFO: Successfully updated pod "pod-update-6efe5321-5178-437e-a3c9-3dd0916a9ece"
STEP: verifying the updated pod is in kubernetes
Jan 30 18:11:42.771: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:11:42.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-172" for this suite.
Jan 30 18:12:04.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:12:04.957: INFO: namespace pods-172 deletion completed in 22.177161484s

• [SLOW TEST:26.806 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:12:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-978c3169-cdfa-4e4d-bd9a-28c25ad52069
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:12:05.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2259" for this suite.
Jan 30 18:12:11.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:12:11.200: INFO: namespace configmap-2259 deletion completed in 6.174571269s

• [SLOW TEST:6.243 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:12:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 30 18:12:15.298: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ace9ec5d-838f-4a7d-9064-08feb6442cdf,GenerateName:,Namespace:events-3421,SelfLink:/api/v1/namespaces/events-3421/pods/send-events-ace9ec5d-838f-4a7d-9064-08feb6442cdf,UID:8776f8db-8d62-478b-b51f-d67e9ba4911c,ResourceVersion:1196212,Generation:0,CreationTimestamp:2020-01-30 18:12:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 261632605,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zq48t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zq48t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-zq48t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035e49b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035e49d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:12:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:12:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:12:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:12:11 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.70,StartTime:2020-01-30 18:12:11 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-01-30 18:12:12 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://90727900ff4307bfbaf1baa2f5eacb0d9451a1748765968775f800244defe6cd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 30 18:12:17.306: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 30 18:12:19.314: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:12:19.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3421" for this suite.
Jan 30 18:12:59.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:12:59.519: INFO: namespace events-3421 deletion completed in 40.186652671s

• [SLOW TEST:48.318 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:12:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-18630c04-9333-46ad-ba65-678c6d8f10af
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:12:59.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4644" for this suite.
Jan 30 18:13:05.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:13:05.760: INFO: namespace secrets-4644 deletion completed in 6.167285724s

• [SLOW TEST:6.239 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:13:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jan 30 18:13:06.354: INFO: created pod pod-service-account-defaultsa
Jan 30 18:13:06.354: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 30 18:13:06.363: INFO: created pod pod-service-account-mountsa
Jan 30 18:13:06.363: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 30 18:13:06.376: INFO: created pod pod-service-account-nomountsa
Jan 30 18:13:06.376: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 30 18:13:06.388: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 30 18:13:06.388: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 30 18:13:06.397: INFO: created pod pod-service-account-mountsa-mountspec
Jan 30 18:13:06.397: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 30 18:13:06.406: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 30 18:13:06.407: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 30 18:13:06.415: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 30 18:13:06.415: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 30 18:13:06.423: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 30 18:13:06.423: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 30 18:13:06.433: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 30 18:13:06.433: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:13:06.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4881" for this suite.
Jan 30 18:13:12.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:13:12.628: INFO: namespace svcaccounts-4881 deletion completed in 6.183512715s

• [SLOW TEST:6.868 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:13:12.629: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:13:12.736: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7ad79910-63f6-4b62-b46f-4cdbbb7ab54a", Controller:(*bool)(0xc0021ba5e2), BlockOwnerDeletion:(*bool)(0xc0021ba5e3)}}
Jan 30 18:13:12.746: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"879a2572-ace9-4cc2-8947-ee800b283517", Controller:(*bool)(0xc0030a79ea), BlockOwnerDeletion:(*bool)(0xc0030a79eb)}}
Jan 30 18:13:12.753: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7971f7df-8a68-499b-b637-3449fbcba13b", Controller:(*bool)(0xc001b9620a), BlockOwnerDeletion:(*bool)(0xc001b9620b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:13:17.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4826" for this suite.
Jan 30 18:13:23.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:13:23.935: INFO: namespace gc-4826 deletion completed in 6.157966739s

• [SLOW TEST:11.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:13:23.936: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jan 30 18:13:24.000: INFO: Waiting up to 5m0s for pod "downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df" in namespace "downward-api-4524" to be "success or failure"
Jan 30 18:13:24.008: INFO: Pod "downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087523ms
Jan 30 18:13:26.014: INFO: Pod "downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014480193s
Jan 30 18:13:28.022: INFO: Pod "downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022274277s
STEP: Saw pod success
Jan 30 18:13:28.022: INFO: Pod "downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df" satisfied condition "success or failure"
Jan 30 18:13:28.027: INFO: Trying to get logs from node aardvark pod downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:13:28.061: INFO: Waiting for pod downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df to disappear
Jan 30 18:13:28.066: INFO: Pod downward-api-55d28f8f-065b-45f8-947a-cb37bb39b8df no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:13:28.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4524" for this suite.
Jan 30 18:13:34.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:13:34.254: INFO: namespace downward-api-4524 deletion completed in 6.181513659s

• [SLOW TEST:10.318 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:13:34.254: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:13:34.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1385" for this suite.
Jan 30 18:13:56.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:13:56.504: INFO: namespace pods-1385 deletion completed in 22.175124654s

• [SLOW TEST:22.250 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:13:56.504: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jan 30 18:14:01.133: INFO: Successfully updated pod "labelsupdatef29b532f-f076-4144-b016-8f6c689a9bb6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:14:03.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5603" for this suite.
Jan 30 18:14:25.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:14:25.349: INFO: namespace projected-5603 deletion completed in 22.173392609s

• [SLOW TEST:28.845 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:14:25.349: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jan 30 18:14:29.964: INFO: Successfully updated pod "labelsupdate02ccdf36-8837-4d7c-abf2-67b1e9bfd64c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:14:31.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5784" for this suite.
Jan 30 18:14:54.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:14:54.171: INFO: namespace downward-api-5784 deletion completed in 22.173706919s

• [SLOW TEST:28.822 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:14:54.175: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 30 18:14:54.273: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:54.273: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:54.273: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:54.279: INFO: Number of nodes with available pods: 0
Jan 30 18:14:54.279: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:14:55.287: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:55.287: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:55.288: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:55.292: INFO: Number of nodes with available pods: 0
Jan 30 18:14:55.292: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:14:56.287: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:56.287: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:56.287: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:56.292: INFO: Number of nodes with available pods: 0
Jan 30 18:14:56.292: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:14:57.287: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.287: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.287: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.292: INFO: Number of nodes with available pods: 2
Jan 30 18:14:57.292: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 30 18:14:57.319: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.319: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.319: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:14:57.325: INFO: Number of nodes with available pods: 2
Jan 30 18:14:57.325: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7126, will wait for the garbage collector to delete the pods
Jan 30 18:14:58.412: INFO: Deleting DaemonSet.extensions daemon-set took: 13.570611ms
Jan 30 18:14:58.712: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.367591ms
Jan 30 18:16:35.220: INFO: Number of nodes with available pods: 0
Jan 30 18:16:35.220: INFO: Number of running nodes: 0, number of available pods: 0
Jan 30 18:16:35.225: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7126/daemonsets","resourceVersion":"1197009"},"items":null}

Jan 30 18:16:35.231: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7126/pods","resourceVersion":"1197009"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:16:35.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7126" for this suite.
Jan 30 18:16:41.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:16:41.422: INFO: namespace daemonsets-7126 deletion completed in 6.168295596s

• [SLOW TEST:107.247 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:16:41.437: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:16:41.514: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74" in namespace "projected-525" to be "success or failure"
Jan 30 18:16:41.524: INFO: Pod "downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.522622ms
Jan 30 18:16:43.530: INFO: Pod "downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016114103s
Jan 30 18:16:45.538: INFO: Pod "downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023930948s
STEP: Saw pod success
Jan 30 18:16:45.538: INFO: Pod "downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74" satisfied condition "success or failure"
Jan 30 18:16:45.543: INFO: Trying to get logs from node aardvark pod downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74 container client-container: <nil>
STEP: delete the pod
Jan 30 18:16:45.576: INFO: Waiting for pod downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74 to disappear
Jan 30 18:16:45.581: INFO: Pod downwardapi-volume-ee8f5bc0-7df7-4aaf-ab62-901755162a74 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:16:45.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-525" for this suite.
Jan 30 18:16:51.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:16:51.749: INFO: namespace projected-525 deletion completed in 6.16182247s

• [SLOW TEST:10.312 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:16:51.750: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-9a5cab11-2ac0-4200-bee7-8c4dc7560939
STEP: Creating secret with name secret-projected-all-test-volume-475e871c-e3d7-492a-9049-69782a1159f1
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 30 18:16:51.833: INFO: Waiting up to 5m0s for pod "projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766" in namespace "projected-1973" to be "success or failure"
Jan 30 18:16:51.839: INFO: Pod "projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285805ms
Jan 30 18:16:53.846: INFO: Pod "projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013084729s
Jan 30 18:16:55.853: INFO: Pod "projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020409451s
STEP: Saw pod success
Jan 30 18:16:55.853: INFO: Pod "projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766" satisfied condition "success or failure"
Jan 30 18:16:55.858: INFO: Trying to get logs from node gazelle pod projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 30 18:16:55.890: INFO: Waiting for pod projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766 to disappear
Jan 30 18:16:55.895: INFO: Pod projected-volume-8ee0837b-8935-4307-88d6-ceba653f2766 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:16:55.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1973" for this suite.
Jan 30 18:17:01.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:17:02.075: INFO: namespace projected-1973 deletion completed in 6.173568524s

• [SLOW TEST:10.326 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:17:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 30 18:17:05.170: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:17:05.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7153" for this suite.
Jan 30 18:17:11.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:17:11.357: INFO: namespace container-runtime-7153 deletion completed in 6.157833348s

• [SLOW TEST:9.282 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:17:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jan 30 18:17:15.965: INFO: Successfully updated pod "annotationupdate059c90c3-56af-4ef3-9930-c4b58c03e219"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:17:17.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3362" for this suite.
Jan 30 18:17:40.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:17:40.179: INFO: namespace projected-3362 deletion completed in 22.180029929s

• [SLOW TEST:28.822 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:17:40.180: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-bd3b87fc-9f98-488f-b2bd-5ea88d4c7e5d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-bd3b87fc-9f98-488f-b2bd-5ea88d4c7e5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:19:15.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9817" for this suite.
Jan 30 18:19:37.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:19:37.216: INFO: namespace configmap-9817 deletion completed in 22.164816122s

• [SLOW TEST:117.036 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:19:37.218: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8742
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 30 18:19:37.273: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 30 18:20:03.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://25.0.1.120:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8742 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 18:20:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 18:20:03.614: INFO: Found all expected endpoints: [netserver-0]
Jan 30 18:20:03.619: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://25.0.2.81:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8742 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 18:20:03.620: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 18:20:03.830: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:20:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8742" for this suite.
Jan 30 18:20:25.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:20:26.020: INFO: namespace pod-network-test-8742 deletion completed in 22.180503579s

• [SLOW TEST:48.803 seconds]
[sig-network] Networking
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:20:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:20:30.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5983" for this suite.
Jan 30 18:21:18.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:21:18.320: INFO: namespace kubelet-test-5983 deletion completed in 48.189877773s

• [SLOW TEST:52.299 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:21:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-de0a6a65-7ded-45ff-8628-9532d60a63e1
STEP: Creating a pod to test consume secrets
Jan 30 18:21:18.393: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124" in namespace "projected-2129" to be "success or failure"
Jan 30 18:21:18.399: INFO: Pod "pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124": Phase="Pending", Reason="", readiness=false. Elapsed: 5.769075ms
Jan 30 18:21:20.404: INFO: Pod "pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011389244s
Jan 30 18:21:22.411: INFO: Pod "pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017627635s
STEP: Saw pod success
Jan 30 18:21:22.411: INFO: Pod "pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124" satisfied condition "success or failure"
Jan 30 18:21:22.415: INFO: Trying to get logs from node aardvark pod pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:21:22.453: INFO: Waiting for pod pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124 to disappear
Jan 30 18:21:22.458: INFO: Pod pod-projected-secrets-270cde81-d9b9-4d32-a102-2ce1b3d42124 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:21:22.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2129" for this suite.
Jan 30 18:21:28.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:21:28.621: INFO: namespace projected-2129 deletion completed in 6.156695428s

• [SLOW TEST:10.301 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:21:28.622: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8c5f1c9f-a4c5-430e-9c5f-cac37fc43fc9
STEP: Creating a pod to test consume configMaps
Jan 30 18:21:28.721: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493" in namespace "projected-4252" to be "success or failure"
Jan 30 18:21:28.737: INFO: Pod "pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493": Phase="Pending", Reason="", readiness=false. Elapsed: 16.296449ms
Jan 30 18:21:30.743: INFO: Pod "pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022353838s
Jan 30 18:21:32.749: INFO: Pod "pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028386089s
STEP: Saw pod success
Jan 30 18:21:32.750: INFO: Pod "pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493" satisfied condition "success or failure"
Jan 30 18:21:32.754: INFO: Trying to get logs from node gazelle pod pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:21:32.788: INFO: Waiting for pod pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493 to disappear
Jan 30 18:21:32.793: INFO: Pod pod-projected-configmaps-4647505e-a2f4-475a-8c43-c8108246e493 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:21:32.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4252" for this suite.
Jan 30 18:21:38.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:21:38.966: INFO: namespace projected-4252 deletion completed in 6.166754929s

• [SLOW TEST:10.344 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:21:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-c5ct
STEP: Creating a pod to test atomic-volume-subpath
Jan 30 18:21:39.046: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-c5ct" in namespace "subpath-2380" to be "success or failure"
Jan 30 18:21:39.058: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Pending", Reason="", readiness=false. Elapsed: 11.623787ms
Jan 30 18:21:41.064: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017227144s
Jan 30 18:21:43.070: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 4.023857136s
Jan 30 18:21:45.078: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 6.031045484s
Jan 30 18:21:47.084: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 8.037600174s
Jan 30 18:21:49.090: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 10.043742202s
Jan 30 18:21:51.150: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 12.103413157s
Jan 30 18:21:53.156: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 14.109080613s
Jan 30 18:21:55.163: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 16.116101734s
Jan 30 18:21:57.169: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 18.122386774s
Jan 30 18:21:59.175: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 20.128929614s
Jan 30 18:22:01.181: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Running", Reason="", readiness=true. Elapsed: 22.134825119s
Jan 30 18:22:03.187: INFO: Pod "pod-subpath-test-secret-c5ct": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.140113123s
STEP: Saw pod success
Jan 30 18:22:03.187: INFO: Pod "pod-subpath-test-secret-c5ct" satisfied condition "success or failure"
Jan 30 18:22:03.191: INFO: Trying to get logs from node aardvark pod pod-subpath-test-secret-c5ct container test-container-subpath-secret-c5ct: <nil>
STEP: delete the pod
Jan 30 18:22:03.224: INFO: Waiting for pod pod-subpath-test-secret-c5ct to disappear
Jan 30 18:22:03.228: INFO: Pod pod-subpath-test-secret-c5ct no longer exists
STEP: Deleting pod pod-subpath-test-secret-c5ct
Jan 30 18:22:03.228: INFO: Deleting pod "pod-subpath-test-secret-c5ct" in namespace "subpath-2380"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:22:03.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2380" for this suite.
Jan 30 18:22:09.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:22:09.418: INFO: namespace subpath-2380 deletion completed in 6.179851189s

• [SLOW TEST:30.452 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:22:09.419: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:22:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-754" for this suite.
Jan 30 18:22:51.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:22:51.693: INFO: namespace kubelet-test-754 deletion completed in 38.176649348s

• [SLOW TEST:42.274 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:22:51.694: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:22:51.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95" in namespace "downward-api-7217" to be "success or failure"
Jan 30 18:22:51.766: INFO: Pod "downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89534ms
Jan 30 18:22:53.772: INFO: Pod "downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011649369s
Jan 30 18:22:55.779: INFO: Pod "downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018133862s
STEP: Saw pod success
Jan 30 18:22:55.779: INFO: Pod "downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95" satisfied condition "success or failure"
Jan 30 18:22:55.785: INFO: Trying to get logs from node aardvark pod downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95 container client-container: <nil>
STEP: delete the pod
Jan 30 18:22:55.822: INFO: Waiting for pod downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95 to disappear
Jan 30 18:22:55.826: INFO: Pod downwardapi-volume-332df461-2cba-465c-8358-b1fc6ad87e95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:22:55.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7217" for this suite.
Jan 30 18:23:01.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:23:02.017: INFO: namespace downward-api-7217 deletion completed in 6.183844417s

• [SLOW TEST:10.323 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:23:02.018: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-147.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-147.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-147.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-147.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-147.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 241.6.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.6.241_udp@PTR;check="$$(dig +tcp +noall +answer +search 241.6.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.6.241_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-147.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-147.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-147.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-147.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-147.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-147.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-147.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 241.6.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.6.241_udp@PTR;check="$$(dig +tcp +noall +answer +search 241.6.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.6.241_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 30 18:23:06.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.162: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.169: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.222: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.228: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.234: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.240: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:06.277: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:11.286: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.293: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.306: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.353: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:11.407: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:16.286: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.293: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.306: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.348: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.359: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.365: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:16.407: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:21.285: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.292: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.304: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.351: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.365: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.371: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:21.409: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:26.286: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.292: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.305: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.345: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.356: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.362: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:26.394: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:31.285: INFO: Unable to read wheezy_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.293: INFO: Unable to read wheezy_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.306: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.356: INFO: Unable to read jessie_udp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.362: INFO: Unable to read jessie_tcp@dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.368: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local from pod dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788: the server could not find the requested resource (get pods dns-test-79062f08-d63f-47f3-8097-c15630b64788)
Jan 30 18:23:31.408: INFO: Lookups using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 failed for: [wheezy_udp@dns-test-service.dns-147.svc.cluster.local wheezy_tcp@dns-test-service.dns-147.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_udp@dns-test-service.dns-147.svc.cluster.local jessie_tcp@dns-test-service.dns-147.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-147.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-147.svc.cluster.local]

Jan 30 18:23:36.399: INFO: DNS probes using dns-147/dns-test-79062f08-d63f-47f3-8097-c15630b64788 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:23:36.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-147" for this suite.
Jan 30 18:23:42.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:23:42.650: INFO: namespace dns-147 deletion completed in 6.166295934s

• [SLOW TEST:40.632 seconds]
[sig-network] DNS
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:23:42.652: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Jan 30 18:23:42.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-3466'
Jan 30 18:23:43.249: INFO: stderr: ""
Jan 30 18:23:43.249: INFO: stdout: "pod/pause created\n"
Jan 30 18:23:43.249: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 30 18:23:43.249: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3466" to be "running and ready"
Jan 30 18:23:43.255: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.48744ms
Jan 30 18:23:45.264: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014561455s
Jan 30 18:23:47.271: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.021430303s
Jan 30 18:23:47.271: INFO: Pod "pause" satisfied condition "running and ready"
Jan 30 18:23:47.271: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 30 18:23:47.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 label pods pause testing-label=testing-label-value --namespace=kubectl-3466'
Jan 30 18:23:47.392: INFO: stderr: ""
Jan 30 18:23:47.392: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 30 18:23:47.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pod pause -L testing-label --namespace=kubectl-3466'
Jan 30 18:23:47.509: INFO: stderr: ""
Jan 30 18:23:47.509: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 30 18:23:47.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 label pods pause testing-label- --namespace=kubectl-3466'
Jan 30 18:23:47.649: INFO: stderr: ""
Jan 30 18:23:47.649: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 30 18:23:47.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pod pause -L testing-label --namespace=kubectl-3466'
Jan 30 18:23:47.776: INFO: stderr: ""
Jan 30 18:23:47.776: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Jan 30 18:23:47.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-3466'
Jan 30 18:23:47.906: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:23:47.906: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 30 18:23:47.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get rc,svc -l name=pause --no-headers --namespace=kubectl-3466'
Jan 30 18:23:48.045: INFO: stderr: "No resources found.\n"
Jan 30 18:23:48.045: INFO: stdout: ""
Jan 30 18:23:48.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -l name=pause --namespace=kubectl-3466 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 18:23:48.184: INFO: stderr: ""
Jan 30 18:23:48.184: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:23:48.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3466" for this suite.
Jan 30 18:23:54.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:23:54.355: INFO: namespace kubectl-3466 deletion completed in 6.164014727s

• [SLOW TEST:11.704 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:23:54.356: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 30 18:23:54.426: INFO: Waiting up to 5m0s for pod "pod-63bf0071-939c-4d14-81ab-f5b34460e241" in namespace "emptydir-6446" to be "success or failure"
Jan 30 18:23:54.432: INFO: Pod "pod-63bf0071-939c-4d14-81ab-f5b34460e241": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195215ms
Jan 30 18:23:56.437: INFO: Pod "pod-63bf0071-939c-4d14-81ab-f5b34460e241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011645387s
Jan 30 18:23:58.443: INFO: Pod "pod-63bf0071-939c-4d14-81ab-f5b34460e241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017748316s
STEP: Saw pod success
Jan 30 18:23:58.444: INFO: Pod "pod-63bf0071-939c-4d14-81ab-f5b34460e241" satisfied condition "success or failure"
Jan 30 18:23:58.449: INFO: Trying to get logs from node gazelle pod pod-63bf0071-939c-4d14-81ab-f5b34460e241 container test-container: <nil>
STEP: delete the pod
Jan 30 18:23:58.478: INFO: Waiting for pod pod-63bf0071-939c-4d14-81ab-f5b34460e241 to disappear
Jan 30 18:23:58.483: INFO: Pod pod-63bf0071-939c-4d14-81ab-f5b34460e241 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:23:58.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6446" for this suite.
Jan 30 18:24:04.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:24:04.663: INFO: namespace emptydir-6446 deletion completed in 6.175397351s

• [SLOW TEST:10.307 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:24:04.664: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 30 18:24:04.718: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jan 30 18:24:05.547: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 30 18:24:07.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:09.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:11.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:13.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:15.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:17.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:19.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:21.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:23.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:25.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:27.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005445, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:24:30.284: INFO: Waited 647.900373ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:24:30.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4494" for this suite.
Jan 30 18:24:36.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:24:36.983: INFO: namespace aggregator-4494 deletion completed in 6.259967223s

• [SLOW TEST:32.320 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:24:36.987: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jan 30 18:24:37.058: INFO: Waiting up to 5m0s for pod "downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34" in namespace "downward-api-6376" to be "success or failure"
Jan 30 18:24:37.064: INFO: Pod "downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782409ms
Jan 30 18:24:39.072: INFO: Pod "downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013469283s
Jan 30 18:24:41.077: INFO: Pod "downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018501572s
STEP: Saw pod success
Jan 30 18:24:41.077: INFO: Pod "downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34" satisfied condition "success or failure"
Jan 30 18:24:41.082: INFO: Trying to get logs from node aardvark pod downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34 container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:24:41.118: INFO: Waiting for pod downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34 to disappear
Jan 30 18:24:41.122: INFO: Pod downward-api-a5a3ac96-af7f-4786-8711-79804f6d4d34 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:24:41.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6376" for this suite.
Jan 30 18:24:47.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:24:47.301: INFO: namespace downward-api-6376 deletion completed in 6.170171591s

• [SLOW TEST:10.313 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:24:47.302: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:24:47.371: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2" in namespace "projected-851" to be "success or failure"
Jan 30 18:24:47.375: INFO: Pod "downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839663ms
Jan 30 18:24:49.382: INFO: Pod "downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01171508s
Jan 30 18:24:51.389: INFO: Pod "downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01832676s
STEP: Saw pod success
Jan 30 18:24:51.389: INFO: Pod "downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2" satisfied condition "success or failure"
Jan 30 18:24:51.394: INFO: Trying to get logs from node aardvark pod downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2 container client-container: <nil>
STEP: delete the pod
Jan 30 18:24:51.426: INFO: Waiting for pod downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2 to disappear
Jan 30 18:24:51.430: INFO: Pod downwardapi-volume-c4476028-29cc-43d7-9b07-163a475bbdc2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:24:51.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-851" for this suite.
Jan 30 18:24:57.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:24:57.605: INFO: namespace projected-851 deletion completed in 6.170659739s

• [SLOW TEST:10.304 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:24:57.606: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:24:57.656: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 30 18:24:57.670: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 18:25:02.677: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 30 18:25:02.677: INFO: Creating deployment "test-rolling-update-deployment"
Jan 30 18:25:02.687: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 30 18:25:02.698: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 30 18:25:04.712: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 30 18:25:04.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005502, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005502, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005502, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716005502, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 18:25:06.724: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jan 30 18:25:06.741: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-3602,SelfLink:/apis/apps/v1/namespaces/deployment-3602/deployments/test-rolling-update-deployment,UID:58cdb94f-8af7-4f42-91ef-12ff505e432a,ResourceVersion:1198659,Generation:1,CreationTimestamp:2020-01-30 18:25:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-01-30 18:25:02 +0000 UTC 2020-01-30 18:25:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-01-30 18:25:04 +0000 UTC 2020-01-30 18:25:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 18:25:06.748: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-3602,SelfLink:/apis/apps/v1/namespaces/deployment-3602/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:a0247c51-322f-4379-8e02-e5c0c4ec8952,ResourceVersion:1198648,Generation:1,CreationTimestamp:2020-01-30 18:25:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 58cdb94f-8af7-4f42-91ef-12ff505e432a 0xc001da9e47 0xc001da9e48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 30 18:25:06.748: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 30 18:25:06.748: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-3602,SelfLink:/apis/apps/v1/namespaces/deployment-3602/replicasets/test-rolling-update-controller,UID:52e69fa5-ad5a-4ba6-adab-75df7a0a45d8,ResourceVersion:1198657,Generation:2,CreationTimestamp:2020-01-30 18:24:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 58cdb94f-8af7-4f42-91ef-12ff505e432a 0xc001da9d67 0xc001da9d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 18:25:06.753: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-f69dz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-f69dz,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-3602,SelfLink:/api/v1/namespaces/deployment-3602/pods/test-rolling-update-deployment-79f6b9d75c-f69dz,UID:bf224deb-54aa-41e5-af76-f934d22011f8,ResourceVersion:1198647,Generation:0,CreationTimestamp:2020-01-30 18:25:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c a0247c51-322f-4379-8e02-e5c0c4ec8952 0xc002d0e0d7 0xc002d0e0d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bxhgg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bxhgg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bxhgg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d0e150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d0e170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:25:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:25:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:25:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:25:02 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.129,StartTime:2020-01-30 18:25:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-01-30 18:25:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://17a8d40dd6e7c464cdfdf0078ae75234f1385e9479d01101f4ac74e643af636d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:25:06.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3602" for this suite.
Jan 30 18:25:12.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:25:12.929: INFO: namespace deployment-3602 deletion completed in 6.168866927s

• [SLOW TEST:15.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:25:12.929: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8755 to expose endpoints map[]
Jan 30 18:25:13.009: INFO: Get endpoints failed (4.222082ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 30 18:25:14.015: INFO: successfully validated that service multi-endpoint-test in namespace services-8755 exposes endpoints map[] (1.009897739s elapsed)
STEP: Creating pod pod1 in namespace services-8755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8755 to expose endpoints map[pod1:[100]]
Jan 30 18:25:17.074: INFO: successfully validated that service multi-endpoint-test in namespace services-8755 exposes endpoints map[pod1:[100]] (3.045337373s elapsed)
STEP: Creating pod pod2 in namespace services-8755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8755 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 30 18:25:20.145: INFO: successfully validated that service multi-endpoint-test in namespace services-8755 exposes endpoints map[pod1:[100] pod2:[101]] (3.063877078s elapsed)
STEP: Deleting pod pod1 in namespace services-8755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8755 to expose endpoints map[pod2:[101]]
Jan 30 18:25:21.183: INFO: successfully validated that service multi-endpoint-test in namespace services-8755 exposes endpoints map[pod2:[101]] (1.028650152s elapsed)
STEP: Deleting pod pod2 in namespace services-8755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8755 to expose endpoints map[]
Jan 30 18:25:22.206: INFO: successfully validated that service multi-endpoint-test in namespace services-8755 exposes endpoints map[] (1.011681687s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:25:22.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8755" for this suite.
Jan 30 18:25:28.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:25:28.411: INFO: namespace services-8755 deletion completed in 6.156524514s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:15.482 seconds]
[sig-network] Services
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:25:28.413: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 30 18:25:33.519: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:25:34.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2172" for this suite.
Jan 30 18:25:56.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:25:56.738: INFO: namespace replicaset-2172 deletion completed in 22.180496204s

• [SLOW TEST:28.325 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:25:56.739: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jan 30 18:25:56.807: INFO: Waiting up to 5m0s for pod "downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e" in namespace "downward-api-8019" to be "success or failure"
Jan 30 18:25:56.811: INFO: Pod "downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.715403ms
Jan 30 18:25:58.822: INFO: Pod "downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014948006s
Jan 30 18:26:00.828: INFO: Pod "downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020233615s
STEP: Saw pod success
Jan 30 18:26:00.828: INFO: Pod "downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e" satisfied condition "success or failure"
Jan 30 18:26:00.831: INFO: Trying to get logs from node aardvark pod downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:26:00.864: INFO: Waiting for pod downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e to disappear
Jan 30 18:26:00.868: INFO: Pod downward-api-b4717670-4562-427c-aa15-fe9a5e35b29e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:26:00.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8019" for this suite.
Jan 30 18:26:06.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:26:07.054: INFO: namespace downward-api-8019 deletion completed in 6.177619462s

• [SLOW TEST:10.315 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:26:07.054: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-8d47147e-f733-43a2-a979-1b04d9b1c979
STEP: Creating a pod to test consume configMaps
Jan 30 18:26:07.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698" in namespace "configmap-2767" to be "success or failure"
Jan 30 18:26:07.134: INFO: Pod "pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698": Phase="Pending", Reason="", readiness=false. Elapsed: 6.219567ms
Jan 30 18:26:09.141: INFO: Pod "pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012860696s
Jan 30 18:26:11.147: INFO: Pod "pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018996703s
STEP: Saw pod success
Jan 30 18:26:11.147: INFO: Pod "pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698" satisfied condition "success or failure"
Jan 30 18:26:11.152: INFO: Trying to get logs from node aardvark pod pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:26:11.185: INFO: Waiting for pod pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698 to disappear
Jan 30 18:26:11.189: INFO: Pod pod-configmaps-2663e173-85a4-4206-9b2d-dba937076698 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:26:11.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2767" for this suite.
Jan 30 18:26:17.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:26:17.374: INFO: namespace configmap-2767 deletion completed in 6.178799717s

• [SLOW TEST:10.321 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:26:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 30 18:26:17.437: INFO: Waiting up to 5m0s for pod "pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc" in namespace "emptydir-348" to be "success or failure"
Jan 30 18:26:17.444: INFO: Pod "pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794958ms
Jan 30 18:26:19.450: INFO: Pod "pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012568689s
Jan 30 18:26:21.456: INFO: Pod "pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018520836s
STEP: Saw pod success
Jan 30 18:26:21.456: INFO: Pod "pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc" satisfied condition "success or failure"
Jan 30 18:26:21.462: INFO: Trying to get logs from node gazelle pod pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc container test-container: <nil>
STEP: delete the pod
Jan 30 18:26:21.494: INFO: Waiting for pod pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc to disappear
Jan 30 18:26:21.498: INFO: Pod pod-00586f0f-1a25-454d-af70-e9ceaa78f6bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:26:21.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-348" for this suite.
Jan 30 18:26:27.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:26:27.673: INFO: namespace emptydir-348 deletion completed in 6.170132409s

• [SLOW TEST:10.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:26:27.673: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 30 18:26:27.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6715'
Jan 30 18:26:27.848: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 30 18:26:27.848: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 30 18:26:27.860: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-w7scl]
Jan 30 18:26:27.860: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-w7scl" in namespace "kubectl-6715" to be "running and ready"
Jan 30 18:26:27.865: INFO: Pod "e2e-test-nginx-rc-w7scl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.309564ms
Jan 30 18:26:29.872: INFO: Pod "e2e-test-nginx-rc-w7scl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011874435s
Jan 30 18:26:31.878: INFO: Pod "e2e-test-nginx-rc-w7scl": Phase="Running", Reason="", readiness=true. Elapsed: 4.018120296s
Jan 30 18:26:31.878: INFO: Pod "e2e-test-nginx-rc-w7scl" satisfied condition "running and ready"
Jan 30 18:26:31.878: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-w7scl]
Jan 30 18:26:31.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs rc/e2e-test-nginx-rc --namespace=kubectl-6715'
Jan 30 18:26:32.031: INFO: stderr: ""
Jan 30 18:26:32.031: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Jan 30 18:26:32.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete rc e2e-test-nginx-rc --namespace=kubectl-6715'
Jan 30 18:26:32.168: INFO: stderr: ""
Jan 30 18:26:32.168: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:26:32.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6715" for this suite.
Jan 30 18:26:54.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:26:54.343: INFO: namespace kubectl-6715 deletion completed in 22.169168199s

• [SLOW TEST:26.670 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:26:54.344: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-2206de60-3012-4714-8c51-636caf593d00
STEP: Creating a pod to test consume configMaps
Jan 30 18:26:54.421: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718" in namespace "configmap-400" to be "success or failure"
Jan 30 18:26:54.426: INFO: Pod "pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681773ms
Jan 30 18:26:56.432: INFO: Pod "pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010608402s
Jan 30 18:26:58.437: INFO: Pod "pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016283602s
STEP: Saw pod success
Jan 30 18:26:58.437: INFO: Pod "pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718" satisfied condition "success or failure"
Jan 30 18:26:58.443: INFO: Trying to get logs from node gazelle pod pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:26:58.480: INFO: Waiting for pod pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718 to disappear
Jan 30 18:26:58.484: INFO: Pod pod-configmaps-0b19e963-2c45-4c86-bdfb-80747582c718 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:26:58.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-400" for this suite.
Jan 30 18:27:04.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:27:04.661: INFO: namespace configmap-400 deletion completed in 6.170758505s

• [SLOW TEST:10.317 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:27:04.662: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jan 30 18:27:04.718: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:27:08.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4715" for this suite.
Jan 30 18:27:14.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:27:14.226: INFO: namespace init-container-4715 deletion completed in 6.165306843s

• [SLOW TEST:9.564 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:27:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 30 18:27:24.406: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:27:24.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6747" for this suite.
Jan 30 18:27:32.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:27:32.578: INFO: namespace gc-6747 deletion completed in 8.165761865s

• [SLOW TEST:18.351 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:27:32.580: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jan 30 18:27:32.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 --namespace=kubectl-2233 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 30 18:27:35.118: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 30 18:27:35.118: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:27:37.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2233" for this suite.
Jan 30 18:27:43.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:27:43.306: INFO: namespace kubectl-2233 deletion completed in 6.1703619s

• [SLOW TEST:10.726 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:27:43.306: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 30 18:27:43.374: INFO: Waiting up to 5m0s for pod "pod-6506a421-7942-4034-8477-ae80a7f2dbbe" in namespace "emptydir-6315" to be "success or failure"
Jan 30 18:27:43.380: INFO: Pod "pod-6506a421-7942-4034-8477-ae80a7f2dbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622627ms
Jan 30 18:27:45.387: INFO: Pod "pod-6506a421-7942-4034-8477-ae80a7f2dbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013327023s
Jan 30 18:27:47.394: INFO: Pod "pod-6506a421-7942-4034-8477-ae80a7f2dbbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019773207s
STEP: Saw pod success
Jan 30 18:27:47.394: INFO: Pod "pod-6506a421-7942-4034-8477-ae80a7f2dbbe" satisfied condition "success or failure"
Jan 30 18:27:47.399: INFO: Trying to get logs from node aardvark pod pod-6506a421-7942-4034-8477-ae80a7f2dbbe container test-container: <nil>
STEP: delete the pod
Jan 30 18:27:47.428: INFO: Waiting for pod pod-6506a421-7942-4034-8477-ae80a7f2dbbe to disappear
Jan 30 18:27:47.433: INFO: Pod pod-6506a421-7942-4034-8477-ae80a7f2dbbe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:27:47.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6315" for this suite.
Jan 30 18:27:53.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:27:53.605: INFO: namespace emptydir-6315 deletion completed in 6.163802513s

• [SLOW TEST:10.299 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:27:53.605: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Jan 30 18:27:53.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-9413'
Jan 30 18:27:53.983: INFO: stderr: ""
Jan 30 18:27:53.983: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jan 30 18:27:54.990: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:27:54.990: INFO: Found 0 / 1
Jan 30 18:27:55.991: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:27:55.991: INFO: Found 0 / 1
Jan 30 18:27:56.989: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:27:56.989: INFO: Found 1 / 1
Jan 30 18:27:56.989: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 18:27:56.994: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:27:56.994: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 30 18:27:56.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413'
Jan 30 18:27:57.133: INFO: stderr: ""
Jan 30 18:27:57.133: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 30 Jan 18:27:55.576 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 30 Jan 18:27:55.576 # Server started, Redis version 3.2.12\n1:M 30 Jan 18:27:55.576 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 30 Jan 18:27:55.576 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 30 18:27:57.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413 --tail=1'
Jan 30 18:27:57.250: INFO: stderr: ""
Jan 30 18:27:57.250: INFO: stdout: "1:M 30 Jan 18:27:55.576 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 30 18:27:57.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413 --limit-bytes=1'
Jan 30 18:27:57.424: INFO: stderr: ""
Jan 30 18:27:57.424: INFO: stdout: " "
STEP: exposing timestamps
Jan 30 18:27:57.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413 --tail=1 --timestamps'
Jan 30 18:27:57.570: INFO: stderr: ""
Jan 30 18:27:57.570: INFO: stdout: "2020-01-30T18:27:55.577081129Z 1:M 30 Jan 18:27:55.576 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 30 18:28:00.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413 --since=1s'
Jan 30 18:28:00.196: INFO: stderr: ""
Jan 30 18:28:00.196: INFO: stdout: ""
Jan 30 18:28:00.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 logs redis-master-lqcjk redis-master --namespace=kubectl-9413 --since=24h'
Jan 30 18:28:00.311: INFO: stderr: ""
Jan 30 18:28:00.311: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 30 Jan 18:27:55.576 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 30 Jan 18:27:55.576 # Server started, Redis version 3.2.12\n1:M 30 Jan 18:27:55.576 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 30 Jan 18:27:55.576 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Jan 30 18:28:00.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-9413'
Jan 30 18:28:00.443: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:28:00.443: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 30 18:28:00.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get rc,svc -l name=nginx --no-headers --namespace=kubectl-9413'
Jan 30 18:28:00.547: INFO: stderr: "No resources found.\n"
Jan 30 18:28:00.547: INFO: stdout: ""
Jan 30 18:28:00.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -l name=nginx --namespace=kubectl-9413 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 18:28:00.650: INFO: stderr: ""
Jan 30 18:28:00.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:28:00.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9413" for this suite.
Jan 30 18:28:06.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:28:06.849: INFO: namespace kubectl-9413 deletion completed in 6.192143988s

• [SLOW TEST:13.244 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:28:06.849: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 30 18:28:06.942: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:06.942: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:06.942: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:06.947: INFO: Number of nodes with available pods: 0
Jan 30 18:28:06.947: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:07.954: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:07.954: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:07.954: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:07.960: INFO: Number of nodes with available pods: 0
Jan 30 18:28:07.960: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:08.956: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:08.956: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:08.956: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:08.962: INFO: Number of nodes with available pods: 0
Jan 30 18:28:08.962: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:09.954: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.954: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.955: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.961: INFO: Number of nodes with available pods: 2
Jan 30 18:28:09.961: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 30 18:28:09.993: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.993: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.993: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:09.999: INFO: Number of nodes with available pods: 1
Jan 30 18:28:09.999: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:11.006: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:11.006: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:11.006: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:11.012: INFO: Number of nodes with available pods: 1
Jan 30 18:28:11.012: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:12.007: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:12.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:12.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:12.011: INFO: Number of nodes with available pods: 1
Jan 30 18:28:12.012: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:13.006: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:13.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:13.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:13.013: INFO: Number of nodes with available pods: 1
Jan 30 18:28:13.013: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:14.007: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:14.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:14.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:14.013: INFO: Number of nodes with available pods: 1
Jan 30 18:28:14.013: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:15.007: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:15.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:15.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:15.012: INFO: Number of nodes with available pods: 1
Jan 30 18:28:15.012: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:16.006: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:16.006: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:16.006: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:16.012: INFO: Number of nodes with available pods: 1
Jan 30 18:28:16.012: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:17.006: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:17.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:17.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:17.012: INFO: Number of nodes with available pods: 1
Jan 30 18:28:17.012: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:28:18.007: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:18.007: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:18.007: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:28:18.013: INFO: Number of nodes with available pods: 2
Jan 30 18:28:18.013: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2925, will wait for the garbage collector to delete the pods
Jan 30 18:28:18.090: INFO: Deleting DaemonSet.extensions daemon-set took: 15.208807ms
Jan 30 18:28:18.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.348526ms
Jan 30 18:28:21.496: INFO: Number of nodes with available pods: 0
Jan 30 18:28:21.496: INFO: Number of running nodes: 0, number of available pods: 0
Jan 30 18:28:21.501: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2925/daemonsets","resourceVersion":"1199685"},"items":null}

Jan 30 18:28:21.505: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2925/pods","resourceVersion":"1199685"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:28:21.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2925" for this suite.
Jan 30 18:28:27.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:28:27.701: INFO: namespace daemonsets-2925 deletion completed in 6.173357015s

• [SLOW TEST:20.851 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:28:27.703: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 30 18:28:27.777: INFO: Waiting up to 5m0s for pod "pod-0c34baca-f710-4fe7-8e1e-a56749613c45" in namespace "emptydir-6916" to be "success or failure"
Jan 30 18:28:27.784: INFO: Pod "pod-0c34baca-f710-4fe7-8e1e-a56749613c45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.527001ms
Jan 30 18:28:29.790: INFO: Pod "pod-0c34baca-f710-4fe7-8e1e-a56749613c45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012563091s
Jan 30 18:28:31.797: INFO: Pod "pod-0c34baca-f710-4fe7-8e1e-a56749613c45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019663992s
STEP: Saw pod success
Jan 30 18:28:31.797: INFO: Pod "pod-0c34baca-f710-4fe7-8e1e-a56749613c45" satisfied condition "success or failure"
Jan 30 18:28:31.802: INFO: Trying to get logs from node aardvark pod pod-0c34baca-f710-4fe7-8e1e-a56749613c45 container test-container: <nil>
STEP: delete the pod
Jan 30 18:28:31.837: INFO: Waiting for pod pod-0c34baca-f710-4fe7-8e1e-a56749613c45 to disappear
Jan 30 18:28:31.842: INFO: Pod pod-0c34baca-f710-4fe7-8e1e-a56749613c45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:28:31.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6916" for this suite.
Jan 30 18:28:37.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:28:38.027: INFO: namespace emptydir-6916 deletion completed in 6.17789987s

• [SLOW TEST:10.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:28:38.027: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 30 18:29:08.643: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:29:08.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-250" for this suite.
Jan 30 18:29:14.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:29:14.819: INFO: namespace gc-250 deletion completed in 6.170248457s

• [SLOW TEST:36.792 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:29:14.820: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 30 18:29:14.895: INFO: Waiting up to 5m0s for pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85" in namespace "emptydir-8536" to be "success or failure"
Jan 30 18:29:14.902: INFO: Pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035457ms
Jan 30 18:29:16.908: INFO: Pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012958057s
Jan 30 18:29:18.925: INFO: Pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85": Phase="Running", Reason="", readiness=true. Elapsed: 4.030367317s
Jan 30 18:29:20.932: INFO: Pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036871312s
STEP: Saw pod success
Jan 30 18:29:20.932: INFO: Pod "pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85" satisfied condition "success or failure"
Jan 30 18:29:20.937: INFO: Trying to get logs from node gazelle pod pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85 container test-container: <nil>
STEP: delete the pod
Jan 30 18:29:20.969: INFO: Waiting for pod pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85 to disappear
Jan 30 18:29:20.973: INFO: Pod pod-33ec7de4-5a23-4839-89a4-c9a21bc26c85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:29:20.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8536" for this suite.
Jan 30 18:29:27.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:29:27.149: INFO: namespace emptydir-8536 deletion completed in 6.168468996s

• [SLOW TEST:12.330 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:29:27.151: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 30 18:29:31.752: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bcc5a0bb-25da-4116-a901-8dff33c7f71c"
Jan 30 18:29:31.753: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bcc5a0bb-25da-4116-a901-8dff33c7f71c" in namespace "pods-6494" to be "terminated due to deadline exceeded"
Jan 30 18:29:31.758: INFO: Pod "pod-update-activedeadlineseconds-bcc5a0bb-25da-4116-a901-8dff33c7f71c": Phase="Running", Reason="", readiness=true. Elapsed: 4.965251ms
Jan 30 18:29:33.764: INFO: Pod "pod-update-activedeadlineseconds-bcc5a0bb-25da-4116-a901-8dff33c7f71c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010956378s
Jan 30 18:29:33.764: INFO: Pod "pod-update-activedeadlineseconds-bcc5a0bb-25da-4116-a901-8dff33c7f71c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:29:33.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6494" for this suite.
Jan 30 18:29:39.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:29:39.939: INFO: namespace pods-6494 deletion completed in 6.169296445s

• [SLOW TEST:12.789 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:29:39.942: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fba23287-83c9-42ac-aa80-29ca76742c40
STEP: Creating a pod to test consume secrets
Jan 30 18:29:40.014: INFO: Waiting up to 5m0s for pod "pod-secrets-3367f493-9006-4262-8e30-893626d164c2" in namespace "secrets-2904" to be "success or failure"
Jan 30 18:29:40.021: INFO: Pod "pod-secrets-3367f493-9006-4262-8e30-893626d164c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.021578ms
Jan 30 18:29:42.028: INFO: Pod "pod-secrets-3367f493-9006-4262-8e30-893626d164c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013621197s
Jan 30 18:29:44.035: INFO: Pod "pod-secrets-3367f493-9006-4262-8e30-893626d164c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020540767s
STEP: Saw pod success
Jan 30 18:29:44.035: INFO: Pod "pod-secrets-3367f493-9006-4262-8e30-893626d164c2" satisfied condition "success or failure"
Jan 30 18:29:44.039: INFO: Trying to get logs from node gazelle pod pod-secrets-3367f493-9006-4262-8e30-893626d164c2 container secret-env-test: <nil>
STEP: delete the pod
Jan 30 18:29:44.071: INFO: Waiting for pod pod-secrets-3367f493-9006-4262-8e30-893626d164c2 to disappear
Jan 30 18:29:44.075: INFO: Pod pod-secrets-3367f493-9006-4262-8e30-893626d164c2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:29:44.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2904" for this suite.
Jan 30 18:29:50.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:29:50.256: INFO: namespace secrets-2904 deletion completed in 6.173778585s

• [SLOW TEST:10.313 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:29:50.257: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:29:50.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4" in namespace "projected-7717" to be "success or failure"
Jan 30 18:29:50.327: INFO: Pod "downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17277ms
Jan 30 18:29:52.334: INFO: Pod "downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013586353s
Jan 30 18:29:54.339: INFO: Pod "downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019283179s
STEP: Saw pod success
Jan 30 18:29:54.339: INFO: Pod "downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4" satisfied condition "success or failure"
Jan 30 18:29:54.344: INFO: Trying to get logs from node aardvark pod downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4 container client-container: <nil>
STEP: delete the pod
Jan 30 18:29:54.380: INFO: Waiting for pod downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4 to disappear
Jan 30 18:29:54.385: INFO: Pod downwardapi-volume-877d6805-4189-48e0-978d-4a5749f775c4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:29:54.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7717" for this suite.
Jan 30 18:30:00.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:30:00.566: INFO: namespace projected-7717 deletion completed in 6.174277996s

• [SLOW TEST:10.309 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:30:00.567: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:30:00.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5" in namespace "downward-api-3955" to be "success or failure"
Jan 30 18:30:00.637: INFO: Pod "downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005164ms
Jan 30 18:30:02.643: INFO: Pod "downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011703209s
Jan 30 18:30:04.648: INFO: Pod "downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017105882s
STEP: Saw pod success
Jan 30 18:30:04.648: INFO: Pod "downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5" satisfied condition "success or failure"
Jan 30 18:30:04.653: INFO: Trying to get logs from node aardvark pod downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5 container client-container: <nil>
STEP: delete the pod
Jan 30 18:30:04.689: INFO: Waiting for pod downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5 to disappear
Jan 30 18:30:04.694: INFO: Pod downwardapi-volume-afe24980-030e-46ad-87bd-3c7ab72e49a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:30:04.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3955" for this suite.
Jan 30 18:30:10.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:30:10.861: INFO: namespace downward-api-3955 deletion completed in 6.161974087s

• [SLOW TEST:10.294 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:30:10.862: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7229
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7229
STEP: Creating statefulset with conflicting port in namespace statefulset-7229
STEP: Waiting until pod test-pod will start running in namespace statefulset-7229
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7229
Jan 30 18:30:14.954: INFO: Observed stateful pod in namespace: statefulset-7229, name: ss-0, uid: 4e5d40be-ccf6-40e8-8033-a7bd2326aed7, status phase: Pending. Waiting for statefulset controller to delete.
Jan 30 18:30:15.343: INFO: Observed stateful pod in namespace: statefulset-7229, name: ss-0, uid: 4e5d40be-ccf6-40e8-8033-a7bd2326aed7, status phase: Failed. Waiting for statefulset controller to delete.
Jan 30 18:30:15.354: INFO: Observed stateful pod in namespace: statefulset-7229, name: ss-0, uid: 4e5d40be-ccf6-40e8-8033-a7bd2326aed7, status phase: Failed. Waiting for statefulset controller to delete.
Jan 30 18:30:15.359: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7229
STEP: Removing pod with conflicting port in namespace statefulset-7229
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7229 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jan 30 18:30:19.398: INFO: Deleting all statefulset in ns statefulset-7229
Jan 30 18:30:19.404: INFO: Scaling statefulset ss to 0
Jan 30 18:30:29.431: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:30:29.436: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:30:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7229" for this suite.
Jan 30 18:30:35.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:30:35.652: INFO: namespace statefulset-7229 deletion completed in 6.179611809s

• [SLOW TEST:24.791 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:30:35.653: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-bd3ac043-046c-4421-9594-c3ea567058f3
STEP: Creating secret with name s-test-opt-upd-983e9cb2-230c-4d4d-9ba0-7994ccb5809f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bd3ac043-046c-4421-9594-c3ea567058f3
STEP: Updating secret s-test-opt-upd-983e9cb2-230c-4d4d-9ba0-7994ccb5809f
STEP: Creating secret with name s-test-opt-create-52543aef-35e5-4769-b8c7-2eeaa79a0104
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:30:43.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7155" for this suite.
Jan 30 18:31:05.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:31:06.070: INFO: namespace secrets-7155 deletion completed in 22.167739254s

• [SLOW TEST:30.418 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:31:06.070: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 30 18:31:06.135: INFO: Waiting up to 5m0s for pod "pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8" in namespace "emptydir-5696" to be "success or failure"
Jan 30 18:31:06.140: INFO: Pod "pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935593ms
Jan 30 18:31:08.147: INFO: Pod "pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011056307s
Jan 30 18:31:10.152: INFO: Pod "pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016581583s
STEP: Saw pod success
Jan 30 18:31:10.152: INFO: Pod "pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8" satisfied condition "success or failure"
Jan 30 18:31:10.157: INFO: Trying to get logs from node aardvark pod pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8 container test-container: <nil>
STEP: delete the pod
Jan 30 18:31:10.189: INFO: Waiting for pod pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8 to disappear
Jan 30 18:31:10.194: INFO: Pod pod-2733a6fb-b046-4504-a34b-ecdcca7d03a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:31:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5696" for this suite.
Jan 30 18:31:16.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:31:16.370: INFO: namespace emptydir-5696 deletion completed in 6.168978703s

• [SLOW TEST:10.299 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:31:16.372: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 30 18:31:16.441: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 30 18:31:21.448: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:31:22.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-828" for this suite.
Jan 30 18:31:28.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:31:28.651: INFO: namespace replication-controller-828 deletion completed in 6.169395223s

• [SLOW TEST:12.279 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:31:28.651: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f0667a5b-2a9f-4559-bdc1-20a448bf4954
STEP: Creating a pod to test consume secrets
Jan 30 18:31:28.723: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1" in namespace "projected-4403" to be "success or failure"
Jan 30 18:31:28.734: INFO: Pod "pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.324751ms
Jan 30 18:31:30.739: INFO: Pod "pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016234079s
Jan 30 18:31:32.747: INFO: Pod "pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024157004s
STEP: Saw pod success
Jan 30 18:31:32.747: INFO: Pod "pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1" satisfied condition "success or failure"
Jan 30 18:31:32.752: INFO: Trying to get logs from node aardvark pod pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:31:32.787: INFO: Waiting for pod pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1 to disappear
Jan 30 18:31:32.792: INFO: Pod pod-projected-secrets-4580488d-d04d-4b45-a716-735b6cc75ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:31:32.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4403" for this suite.
Jan 30 18:31:38.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:31:38.970: INFO: namespace projected-4403 deletion completed in 6.170860683s

• [SLOW TEST:10.319 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:31:38.970: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 30 18:31:39.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200565,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 30 18:31:39.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200565,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 30 18:31:49.056: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200584,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 30 18:31:49.057: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200584,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 30 18:31:59.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200603,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 30 18:31:59.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200603,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 30 18:32:09.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200624,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 30 18:32:09.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-a,UID:3d52d2b5-7816-40b5-92ef-e592eda959f5,ResourceVersion:1200624,Generation:0,CreationTimestamp:2020-01-30 18:31:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 30 18:32:19.098: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-b,UID:0f2aedaf-cad9-4746-956b-d43d9dfa40d1,ResourceVersion:1200645,Generation:0,CreationTimestamp:2020-01-30 18:32:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 30 18:32:19.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-b,UID:0f2aedaf-cad9-4746-956b-d43d9dfa40d1,ResourceVersion:1200645,Generation:0,CreationTimestamp:2020-01-30 18:32:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 30 18:32:29.110: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-b,UID:0f2aedaf-cad9-4746-956b-d43d9dfa40d1,ResourceVersion:1200666,Generation:0,CreationTimestamp:2020-01-30 18:32:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 30 18:32:29.110: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-325,SelfLink:/api/v1/namespaces/watch-325/configmaps/e2e-watch-test-configmap-b,UID:0f2aedaf-cad9-4746-956b-d43d9dfa40d1,ResourceVersion:1200666,Generation:0,CreationTimestamp:2020-01-30 18:32:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:32:39.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-325" for this suite.
Jan 30 18:32:45.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:32:45.295: INFO: namespace watch-325 deletion completed in 6.176423751s

• [SLOW TEST:66.325 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:32:45.296: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:32:50.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8149" for this suite.
Jan 30 18:32:57.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:32:57.153: INFO: namespace watch-8149 deletion completed in 6.24807129s

• [SLOW TEST:11.857 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:32:57.154: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 30 18:33:05.288: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:05.293: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:07.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:07.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:09.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:09.301: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:11.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:11.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:13.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:13.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:15.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:15.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:17.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:17.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:19.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:19.301: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:21.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:21.302: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:23.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:23.300: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 18:33:25.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 18:33:25.300: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:33:25.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1217" for this suite.
Jan 30 18:33:41.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:33:41.483: INFO: namespace container-lifecycle-hook-1217 deletion completed in 16.176196878s

• [SLOW TEST:44.329 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:33:41.484: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:33:41.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9" in namespace "downward-api-8978" to be "success or failure"
Jan 30 18:33:41.552: INFO: Pod "downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.948451ms
Jan 30 18:33:43.558: INFO: Pod "downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010388888s
Jan 30 18:33:45.565: INFO: Pod "downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017002579s
STEP: Saw pod success
Jan 30 18:33:45.565: INFO: Pod "downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9" satisfied condition "success or failure"
Jan 30 18:33:45.571: INFO: Trying to get logs from node aardvark pod downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9 container client-container: <nil>
STEP: delete the pod
Jan 30 18:33:45.607: INFO: Waiting for pod downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9 to disappear
Jan 30 18:33:45.612: INFO: Pod downwardapi-volume-68763b6f-a826-47c0-b2e9-2c4aecf582d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:33:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8978" for this suite.
Jan 30 18:33:51.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:33:51.795: INFO: namespace downward-api-8978 deletion completed in 6.177145928s

• [SLOW TEST:10.311 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:33:51.795: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jan 30 18:33:51.863: INFO: Waiting up to 5m0s for pod "downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f" in namespace "downward-api-6654" to be "success or failure"
Jan 30 18:33:51.868: INFO: Pod "downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125153ms
Jan 30 18:33:53.874: INFO: Pod "downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010787475s
Jan 30 18:33:55.879: INFO: Pod "downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015968003s
STEP: Saw pod success
Jan 30 18:33:55.879: INFO: Pod "downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f" satisfied condition "success or failure"
Jan 30 18:33:55.885: INFO: Trying to get logs from node aardvark pod downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:33:55.919: INFO: Waiting for pod downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f to disappear
Jan 30 18:33:55.922: INFO: Pod downward-api-9201b4e5-44b7-40f3-8ed6-56ce9c2cea8f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:33:55.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6654" for this suite.
Jan 30 18:34:01.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:34:02.119: INFO: namespace downward-api-6654 deletion completed in 6.189476453s

• [SLOW TEST:10.324 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:34:02.120: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 30 18:34:02.187: INFO: Waiting up to 5m0s for pod "pod-942f78ee-89b9-4fa4-abb2-8945cc635b51" in namespace "emptydir-3300" to be "success or failure"
Jan 30 18:34:02.197: INFO: Pod "pod-942f78ee-89b9-4fa4-abb2-8945cc635b51": Phase="Pending", Reason="", readiness=false. Elapsed: 9.749232ms
Jan 30 18:34:04.203: INFO: Pod "pod-942f78ee-89b9-4fa4-abb2-8945cc635b51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015737083s
Jan 30 18:34:06.209: INFO: Pod "pod-942f78ee-89b9-4fa4-abb2-8945cc635b51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02199809s
STEP: Saw pod success
Jan 30 18:34:06.209: INFO: Pod "pod-942f78ee-89b9-4fa4-abb2-8945cc635b51" satisfied condition "success or failure"
Jan 30 18:34:06.213: INFO: Trying to get logs from node gazelle pod pod-942f78ee-89b9-4fa4-abb2-8945cc635b51 container test-container: <nil>
STEP: delete the pod
Jan 30 18:34:06.247: INFO: Waiting for pod pod-942f78ee-89b9-4fa4-abb2-8945cc635b51 to disappear
Jan 30 18:34:06.250: INFO: Pod pod-942f78ee-89b9-4fa4-abb2-8945cc635b51 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:34:06.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3300" for this suite.
Jan 30 18:34:12.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:34:12.438: INFO: namespace emptydir-3300 deletion completed in 6.179646986s

• [SLOW TEST:10.318 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:34:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 in namespace container-probe-8668
Jan 30 18:34:16.517: INFO: Started pod liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 in namespace container-probe-8668
STEP: checking the pod's current state and verifying that restartCount is present
Jan 30 18:34:16.523: INFO: Initial restart count of pod liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is 0
Jan 30 18:34:32.580: INFO: Restart count of pod container-probe-8668/liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is now 1 (16.057106014s elapsed)
Jan 30 18:34:50.637: INFO: Restart count of pod container-probe-8668/liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is now 2 (34.113989809s elapsed)
Jan 30 18:35:10.710: INFO: Restart count of pod container-probe-8668/liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is now 3 (54.187896988s elapsed)
Jan 30 18:35:32.791: INFO: Restart count of pod container-probe-8668/liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is now 4 (1m16.268660245s elapsed)
Jan 30 18:36:41.008: INFO: Restart count of pod container-probe-8668/liveness-3e066dda-ec75-48cb-8bef-c463cedf6f70 is now 5 (2m24.48539913s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:36:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8668" for this suite.
Jan 30 18:36:47.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:36:47.229: INFO: namespace container-probe-8668 deletion completed in 6.195086216s

• [SLOW TEST:154.790 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:36:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jan 30 18:36:47.280: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-482490662 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:36:47.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6329" for this suite.
Jan 30 18:36:53.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:36:53.578: INFO: namespace kubectl-6329 deletion completed in 6.18589814s

• [SLOW TEST:6.348 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:36:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:36:53.635: INFO: Creating deployment "nginx-deployment"
Jan 30 18:36:53.643: INFO: Waiting for observed generation 1
Jan 30 18:36:55.656: INFO: Waiting for all required pods to come up
Jan 30 18:36:55.662: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 30 18:37:09.676: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 30 18:37:09.689: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 30 18:37:09.705: INFO: Updating deployment nginx-deployment
Jan 30 18:37:09.705: INFO: Waiting for observed generation 2
Jan 30 18:37:11.764: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 30 18:37:11.771: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 30 18:37:11.776: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 30 18:37:11.791: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 30 18:37:11.791: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 30 18:37:11.797: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 30 18:37:11.808: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 30 18:37:11.808: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 30 18:37:11.823: INFO: Updating deployment nginx-deployment
Jan 30 18:37:11.823: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 30 18:37:11.835: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 30 18:37:11.842: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jan 30 18:37:11.860: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1133,SelfLink:/apis/apps/v1/namespaces/deployment-1133/deployments/nginx-deployment,UID:89357028-9813-4c4a-90e1-99d2211fec31,ResourceVersion:1201685,Generation:3,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2020-01-30 18:37:09 +0000 UTC 2020-01-30 18:36:53 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2020-01-30 18:37:11 +0000 UTC 2020-01-30 18:37:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 30 18:37:11.885: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-1133,SelfLink:/apis/apps/v1/namespaces/deployment-1133/replicasets/nginx-deployment-55fb7cb77f,UID:971c14b8-2785-4f4a-a478-f0ef6b2cccd3,ResourceVersion:1201683,Generation:3,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 89357028-9813-4c4a-90e1-99d2211fec31 0xc0031bb957 0xc0031bb958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 30 18:37:11.885: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 30 18:37:11.885: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-1133,SelfLink:/apis/apps/v1/namespaces/deployment-1133/replicasets/nginx-deployment-7b8c6f4498,UID:7f46d30e-6ae2-498f-9e50-90cc392a394f,ResourceVersion:1201680,Generation:3,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 89357028-9813-4c4a-90e1-99d2211fec31 0xc0031bba27 0xc0031bba28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 30 18:37:11.906: INFO: Pod "nginx-deployment-55fb7cb77f-6qhz7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6qhz7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-6qhz7,UID:d32311e9-7fba-4530-a6d5-d38e4fd751c3,ResourceVersion:1201637,Generation:0,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b03b7 0xc0036b03b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:,StartTime:2020-01-30 18:37:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.906: INFO: Pod "nginx-deployment-55fb7cb77f-7xv9j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7xv9j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-7xv9j,UID:139e13a9-f56f-4e67-976b-a7eafcd0a6b9,ResourceVersion:1201712,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0527 0xc0036b0528}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b05b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.906: INFO: Pod "nginx-deployment-55fb7cb77f-8x2sx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8x2sx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-8x2sx,UID:42760102-4cf5-4996-aa85-f6e2e72a9881,ResourceVersion:1201715,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0620 0xc0036b0621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b06a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b06c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.906: INFO: Pod "nginx-deployment-55fb7cb77f-9qjzw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9qjzw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-9qjzw,UID:f18967b1-7635-4176-812e-eb39e4d2fbe1,ResourceVersion:1201707,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0747 0xc0036b0748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b07b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b07d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.906: INFO: Pod "nginx-deployment-55fb7cb77f-jkrhf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jkrhf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-jkrhf,UID:8695cac3-2ccd-4348-8653-91d9267a365e,ResourceVersion:1201710,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0840 0xc0036b0841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b08c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b08e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.907: INFO: Pod "nginx-deployment-55fb7cb77f-nkh5d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nkh5d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-nkh5d,UID:8fbcab4c-cc50-44d8-9c91-3193a5934da8,ResourceVersion:1201695,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0967 0xc0036b0968}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b09e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.907: INFO: Pod "nginx-deployment-55fb7cb77f-rpk5n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rpk5n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-rpk5n,UID:ecfde075-0105-48c8-8fb7-f10ff098f2e8,ResourceVersion:1201711,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0a97 0xc0036b0a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.907: INFO: Pod "nginx-deployment-55fb7cb77f-rrx5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rrx5p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-rrx5p,UID:260c8de5-8530-4bba-84c2-ae06d3361d69,ResourceVersion:1201665,Generation:0,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0b90 0xc0036b0b91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:,StartTime:2020-01-30 18:37:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.907: INFO: Pod "nginx-deployment-55fb7cb77f-rvq67" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rvq67,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-rvq67,UID:b667fd73-c7d8-4b36-a72a-b48177ec965f,ResourceVersion:1201717,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0d07 0xc0036b0d08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.907: INFO: Pod "nginx-deployment-55fb7cb77f-sq5r6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-sq5r6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-sq5r6,UID:d369a626-0350-4c83-86ea-10f5a73c9678,ResourceVersion:1201650,Generation:0,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0e00 0xc0036b0e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b0ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:,StartTime:2020-01-30 18:37:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-55fb7cb77f-wr9wq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wr9wq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-wr9wq,UID:0f3c5a2c-e5da-4fcc-bb48-16ae95c5cd6e,ResourceVersion:1201664,Generation:0,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b0f77 0xc0036b0f78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b0ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:,StartTime:2020-01-30 18:37:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-55fb7cb77f-znlkb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-znlkb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-55fb7cb77f-znlkb,UID:9119a1b6-8ccc-4d3f-9847-004786f4cdf4,ResourceVersion:1201651,Generation:0,CreationTimestamp:2020-01-30 18:37:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 971c14b8-2785-4f4a-a478-f0ef6b2cccd3 0xc0036b10e7 0xc0036b10e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:09 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:,StartTime:2020-01-30 18:37:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-7b8c6f4498-256fn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-256fn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-256fn,UID:a334ff81-77ab-4225-9e57-2bf5315a4990,ResourceVersion:1201577,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1257 0xc0036b1258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b12d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b12f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.160,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ad4a46f1280d68c3923e0c0b894fc2f5b050b0dee284ff52f3d97e60fb43375}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-7b8c6f4498-2j8kz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2j8kz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-2j8kz,UID:7483ee49-5478-4563-8ddb-a1a944c9057c,ResourceVersion:1201720,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b13c7 0xc0036b13c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-7b8c6f4498-49vq2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-49vq2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-49vq2,UID:8d35795c-38e4-4b03-9a9e-67c3d585fc52,ResourceVersion:1201708,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b14c0 0xc0036b14c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.908: INFO: Pod "nginx-deployment-7b8c6f4498-4bcsg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4bcsg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-4bcsg,UID:82ef239b-7255-48a6-a0f3-f3a5d6f822da,ResourceVersion:1201549,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b15d7 0xc0036b15d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.109,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c526fbd12c566dae7c55c3ae2526875925464faa3ba0bb194afd35de9303d237}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-4gh24" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4gh24,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-4gh24,UID:3db8843e-c531-4c0a-aebe-14d3fe3d8d15,ResourceVersion:1201709,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b17c7 0xc0036b17c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-67qh4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-67qh4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-67qh4,UID:80a2a316-d98b-47cb-a6e9-3e50aa320bc6,ResourceVersion:1201694,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1930 0xc0036b1931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b19e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-7sxw4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7sxw4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-7sxw4,UID:7c77ecd6-19db-4517-be7a-e26a4ad7ad3f,ResourceVersion:1201573,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1a97 0xc0036b1a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.159,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://95bb69eb39dfe9b7f97fb3403303afd267ca99bd6a3085f3cc89bdb4f1019921}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-7wpns" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7wpns,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-7wpns,UID:b4a4448d-eb82-4a7c-9c6d-5addada67d1b,ResourceVersion:1201706,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1c27 0xc0036b1c28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-cn2sb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cn2sb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-cn2sb,UID:a5e7e106-da40-4ac2-8971-8615d82a444d,ResourceVersion:1201605,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1d47 0xc0036b1d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:25.0.1.163,StartTime:2020-01-30 18:36:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:58 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://62a441d3ea09adac6e468a28933a0c6f8d7c15d240ce1991fcdcebf4a08b3e12}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.909: INFO: Pod "nginx-deployment-7b8c6f4498-fc8p6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fc8p6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-fc8p6,UID:cc578e0f-f20a-41df-b217-1238fbaa0765,ResourceVersion:1201716,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1eb7 0xc0036b1eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036b1f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036b1f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-gvlgv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gvlgv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-gvlgv,UID:3a0dbeb5-fae8-4a1f-a711-5ca85d6bdf97,ResourceVersion:1201718,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc0036b1fb0 0xc0036b1fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b94040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b94230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-lws5k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lws5k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-lws5k,UID:326b672a-4d8e-4d9e-971d-fe2529c292c3,ResourceVersion:1201555,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b94420 0xc000b94421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b944a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b944c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.108,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0f51ebc099f9d05758f0787a91b190a02037278c3b3b7548d74d4563cbc1569e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-mdb2n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mdb2n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-mdb2n,UID:bdda718a-f29e-4135-8e91-893721d9576c,ResourceVersion:1201714,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b94927 0xc000b94928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b94aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b94ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-npxmx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-npxmx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-npxmx,UID:98bbcc9c-4439-4e63-a6ed-02c76fb50968,ResourceVersion:1201705,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b94b40 0xc000b94b41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b94bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b94ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.233,PodIP:,StartTime:2020-01-30 18:37:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-qn5mm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qn5mm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-qn5mm,UID:dd86fec2-12ce-4b64-bda0-2c84810d145c,ResourceVersion:1201551,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b951b7 0xc000b951b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b95250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b952b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.111,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ede66874ae7eb64e48695ea240583fd9001c4d82483aaa67de226426ae2b2d09}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.910: INFO: Pod "nginx-deployment-7b8c6f4498-rl9h7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rl9h7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-rl9h7,UID:847773d6-9801-4f25-bc60-e50bd0fe07c0,ResourceVersion:1201560,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b95467 0xc000b95468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b955e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b95600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.110,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://334767ca33d3ae64ff457cc4f60893600e779bd51cd70c60a1dcd7f1bf0fc873}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.911: INFO: Pod "nginx-deployment-7b8c6f4498-th92c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-th92c,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-th92c,UID:2d275d67-90cc-4e41-8a0b-fb5434a1cbcd,ResourceVersion:1201691,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b95787 0xc000b95788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b958a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b95910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.911: INFO: Pod "nginx-deployment-7b8c6f4498-xbbfq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xbbfq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-xbbfq,UID:63422f1c-de1e-41b2-a5b7-e387c4a8edd8,ResourceVersion:1201704,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b959e7 0xc000b959e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aardvark,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b95b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b95b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.911: INFO: Pod "nginx-deployment-7b8c6f4498-z7lql" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z7lql,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-z7lql,UID:93b5fca6-8894-41e4-88c8-393308a7f646,ResourceVersion:1201558,Generation:0,CreationTimestamp:2020-01-30 18:36:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b95c07 0xc000b95c08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b95d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b95d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:36:53 +0000 UTC  }],Message:,Reason:,HostIP:167.172.59.201,PodIP:25.0.2.107,StartTime:2020-01-30 18:36:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-30 18:36:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://82019916a0bb2e9c75aea6afee89c50a82afeebefe10158677a548bae53c80b2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 30 18:37:11.911: INFO: Pod "nginx-deployment-7b8c6f4498-zdd8j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zdd8j,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1133,SelfLink:/api/v1/namespaces/deployment-1133/pods/nginx-deployment-7b8c6f4498-zdd8j,UID:44a247d2-9fd7-4ecd-b4e0-d64467c6d667,ResourceVersion:1201713,Generation:0,CreationTimestamp:2020-01-30 18:37:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7f46d30e-6ae2-498f-9e50-90cc392a394f 0xc000b95f27 0xc000b95f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wbs5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wbs5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wbs5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gazelle,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ba000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ba020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:37:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:37:11.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1133" for this suite.
Jan 30 18:37:19.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:37:20.108: INFO: namespace deployment-1133 deletion completed in 8.183197832s

• [SLOW TEST:26.530 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:37:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan 30 18:37:24.210: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-482490662 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 30 18:37:39.318: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:37:39.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6966" for this suite.
Jan 30 18:37:45.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:37:45.517: INFO: namespace pods-6966 deletion completed in 6.187998796s

• [SLOW TEST:25.408 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:37:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:37:45.600: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 30 18:37:45.617: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:45.617: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:45.617: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:45.621: INFO: Number of nodes with available pods: 0
Jan 30 18:37:45.622: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:37:46.629: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:46.629: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:46.629: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:46.636: INFO: Number of nodes with available pods: 0
Jan 30 18:37:46.636: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:37:47.631: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:47.631: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:47.631: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:47.636: INFO: Number of nodes with available pods: 0
Jan 30 18:37:47.636: INFO: Node aardvark is running more than one daemon pod
Jan 30 18:37:48.629: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:48.629: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:48.629: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:48.634: INFO: Number of nodes with available pods: 2
Jan 30 18:37:48.634: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 30 18:37:48.676: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:48.676: INFO: Wrong image for pod: daemon-set-vqbrf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:48.683: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:48.683: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:48.683: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:49.689: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:49.689: INFO: Wrong image for pod: daemon-set-vqbrf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:49.696: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:49.696: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:49.696: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:50.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:50.690: INFO: Wrong image for pod: daemon-set-vqbrf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:50.697: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:50.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:50.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:51.688: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:51.688: INFO: Wrong image for pod: daemon-set-vqbrf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:51.688: INFO: Pod daemon-set-vqbrf is not available
Jan 30 18:37:51.695: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:51.695: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:51.695: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:52.690: INFO: Pod daemon-set-gx8cn is not available
Jan 30 18:37:52.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:52.696: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:52.696: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:52.696: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:53.690: INFO: Pod daemon-set-gx8cn is not available
Jan 30 18:37:53.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:53.696: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:53.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:53.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:54.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:54.697: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:54.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:54.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:55.688: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:55.688: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:37:55.694: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:55.694: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:55.694: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:56.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:56.690: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:37:56.697: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:56.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:56.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:57.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:57.690: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:37:57.698: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:57.698: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:57.698: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:58.692: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:58.692: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:37:58.699: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:58.699: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:58.699: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:59.689: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:37:59.689: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:37:59.696: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:59.696: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:37:59.696: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:00.689: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:38:00.689: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:38:00.695: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:00.696: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:00.696: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:01.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:38:01.690: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:38:01.698: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:01.698: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:01.698: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:02.690: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:38:02.690: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:38:02.697: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:02.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:02.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:03.691: INFO: Wrong image for pod: daemon-set-qpt98. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 30 18:38:03.691: INFO: Pod daemon-set-qpt98 is not available
Jan 30 18:38:03.699: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:03.699: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:03.699: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.689: INFO: Pod daemon-set-9wr99 is not available
Jan 30 18:38:04.697: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.697: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.697: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 30 18:38:04.702: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.702: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.702: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:04.707: INFO: Number of nodes with available pods: 1
Jan 30 18:38:04.707: INFO: Node gazelle is running more than one daemon pod
Jan 30 18:38:05.715: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:05.716: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:05.716: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:05.721: INFO: Number of nodes with available pods: 1
Jan 30 18:38:05.721: INFO: Node gazelle is running more than one daemon pod
Jan 30 18:38:06.714: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:06.714: INFO: DaemonSet pods can't tolerate node master-panther with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:06.714: INFO: DaemonSet pods can't tolerate node master-scorpion with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 30 18:38:06.720: INFO: Number of nodes with available pods: 2
Jan 30 18:38:06.720: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8307, will wait for the garbage collector to delete the pods
Jan 30 18:38:06.813: INFO: Deleting DaemonSet.extensions daemon-set took: 12.65269ms
Jan 30 18:38:07.113: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.361061ms
Jan 30 18:38:20.120: INFO: Number of nodes with available pods: 0
Jan 30 18:38:20.120: INFO: Number of running nodes: 0, number of available pods: 0
Jan 30 18:38:20.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8307/daemonsets","resourceVersion":"1202191"},"items":null}

Jan 30 18:38:20.130: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8307/pods","resourceVersion":"1202191"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:38:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8307" for this suite.
Jan 30 18:38:26.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:38:26.334: INFO: namespace daemonsets-8307 deletion completed in 6.183461567s

• [SLOW TEST:40.817 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:38:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jan 30 18:38:26.400: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1468" to be "success or failure"
Jan 30 18:38:26.404: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.470094ms
Jan 30 18:38:28.410: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010529122s
Jan 30 18:38:30.417: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017493718s
STEP: Saw pod success
Jan 30 18:38:30.417: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 30 18:38:30.423: INFO: Trying to get logs from node gazelle pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 30 18:38:30.459: INFO: Waiting for pod pod-host-path-test to disappear
Jan 30 18:38:30.463: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:38:30.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1468" for this suite.
Jan 30 18:38:36.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:38:36.639: INFO: namespace hostpath-1468 deletion completed in 6.170209705s

• [SLOW TEST:10.304 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:38:36.640: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-6eff7486-5279-4ceb-b5e2-8a1e4f4c669e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:38:40.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7174" for this suite.
Jan 30 18:39:02.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:39:02.952: INFO: namespace configmap-7174 deletion completed in 22.18114909s

• [SLOW TEST:26.313 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:39:02.953: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jan 30 18:39:03.004: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 18:39:03.020: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 18:39:03.025: INFO: 
Logging pods the kubelet thinks is on node aardvark before test
Jan 30 18:39:03.036: INFO: sonobuoy from sonobuoy started at 2020-01-30 17:19:28 +0000 UTC (1 container statuses recorded)
Jan 30 18:39:03.036: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 18:39:03.036: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-22zg6 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:39:03.036: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 30 18:39:03.036: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 18:39:03.036: INFO: kube-proxy-rlk8c from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 18:39:03.036: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 18:39:03.036: INFO: kube-flannel-ds-amd64-mlnjj from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 18:39:03.036: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 18:39:03.036: INFO: 
Logging pods the kubelet thinks is on node gazelle before test
Jan 30 18:39:03.053: INFO: kube-flannel-ds-amd64-dbdl8 from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 18:39:03.053: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 18:39:03.053: INFO: kube-proxy-l62gs from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 18:39:03.053: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 18:39:03.053: INFO: sonobuoy-e2e-job-70068240cc4e41b9 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:39:03.053: INFO: 	Container e2e ready: true, restart count 0
Jan 30 18:39:03.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 18:39:03.053: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-5w56c from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:39:03.053: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 30 18:39:03.053: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15eebe1b2778cb42], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:39:04.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7117" for this suite.
Jan 30 18:39:10.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:39:10.280: INFO: namespace sched-pred-7117 deletion completed in 6.176505691s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.326 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:39:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7638
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jan 30 18:39:10.355: INFO: Found 0 stateful pods, waiting for 3
Jan 30 18:39:20.363: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:39:20.363: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:39:20.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:39:20.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-7638 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:39:20.948: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:39:20.948: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:39:20.948: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 30 18:39:30.996: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 30 18:39:41.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-7638 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 18:39:41.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 18:39:41.330: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 18:39:41.330: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 18:39:51.367: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:39:51.367: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:39:51.367: INFO: Waiting for Pod statefulset-7638/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:39:51.367: INFO: Waiting for Pod statefulset-7638/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:40:01.380: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:40:01.380: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:40:01.380: INFO: Waiting for Pod statefulset-7638/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:40:11.382: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:40:11.382: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jan 30 18:40:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-7638 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:40:21.678: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:40:21.678: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:40:21.678: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 18:40:31.727: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 30 18:40:41.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-7638 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 18:40:42.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 18:40:42.082: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 18:40:42.082: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 18:40:52.120: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:40:52.120: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 30 18:40:52.120: INFO: Waiting for Pod statefulset-7638/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 30 18:40:52.120: INFO: Waiting for Pod statefulset-7638/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 30 18:41:02.136: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:41:02.136: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 30 18:41:02.136: INFO: Waiting for Pod statefulset-7638/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 30 18:41:12.132: INFO: Waiting for StatefulSet statefulset-7638/ss2 to complete update
Jan 30 18:41:12.132: INFO: Waiting for Pod statefulset-7638/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jan 30 18:41:22.132: INFO: Deleting all statefulset in ns statefulset-7638
Jan 30 18:41:22.137: INFO: Scaling statefulset ss2 to 0
Jan 30 18:41:52.168: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:41:52.173: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:41:52.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7638" for this suite.
Jan 30 18:41:58.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:41:58.385: INFO: namespace statefulset-7638 deletion completed in 6.181352024s

• [SLOW TEST:168.106 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:41:58.386: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 30 18:42:12.505: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:12.510: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:14.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:14.518: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:16.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:16.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:18.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:18.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:20.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:20.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:22.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:22.518: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:24.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:24.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:26.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:26.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:28.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:28.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:30.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:30.518: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 18:42:32.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 18:42:32.516: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:42:32.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1468" for this suite.
Jan 30 18:43:12.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:43:12.728: INFO: namespace container-lifecycle-hook-1468 deletion completed in 40.184354074s

• [SLOW TEST:74.342 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:43:12.729: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6c274ac7-d79d-4db1-94a9-bb3ad26f3156
STEP: Creating a pod to test consume configMaps
Jan 30 18:43:12.797: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec" in namespace "projected-3884" to be "success or failure"
Jan 30 18:43:12.803: INFO: Pod "pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144524ms
Jan 30 18:43:14.808: INFO: Pod "pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010935307s
Jan 30 18:43:16.816: INFO: Pod "pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018168664s
STEP: Saw pod success
Jan 30 18:43:16.816: INFO: Pod "pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec" satisfied condition "success or failure"
Jan 30 18:43:16.821: INFO: Trying to get logs from node aardvark pod pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:43:16.853: INFO: Waiting for pod pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec to disappear
Jan 30 18:43:16.858: INFO: Pod pod-projected-configmaps-c33baadb-c92a-4f08-80dd-0fe8572838ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:43:16.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3884" for this suite.
Jan 30 18:43:22.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:43:23.041: INFO: namespace projected-3884 deletion completed in 6.177606526s

• [SLOW TEST:10.312 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:43:23.042: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-08f8efb2-6111-481b-b504-a843a4971deb
STEP: Creating a pod to test consume secrets
Jan 30 18:43:23.117: INFO: Waiting up to 5m0s for pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961" in namespace "secrets-1872" to be "success or failure"
Jan 30 18:43:23.125: INFO: Pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961": Phase="Pending", Reason="", readiness=false. Elapsed: 8.327495ms
Jan 30 18:43:25.131: INFO: Pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014090252s
Jan 30 18:43:27.138: INFO: Pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021110595s
Jan 30 18:43:29.154: INFO: Pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036476551s
STEP: Saw pod success
Jan 30 18:43:29.154: INFO: Pod "pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961" satisfied condition "success or failure"
Jan 30 18:43:29.160: INFO: Trying to get logs from node gazelle pod pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961 container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:43:29.192: INFO: Waiting for pod pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961 to disappear
Jan 30 18:43:29.197: INFO: Pod pod-secrets-7cdbc587-4c51-40e0-ad8c-5d9a2dc41961 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:43:29.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1872" for this suite.
Jan 30 18:43:35.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:43:35.371: INFO: namespace secrets-1872 deletion completed in 6.167952859s

• [SLOW TEST:12.329 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:43:35.372: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jan 30 18:43:35.437: INFO: Waiting up to 5m0s for pod "var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b" in namespace "var-expansion-1524" to be "success or failure"
Jan 30 18:43:35.443: INFO: Pod "var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325046ms
Jan 30 18:43:37.449: INFO: Pod "var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01228371s
Jan 30 18:43:39.456: INFO: Pod "var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01889012s
STEP: Saw pod success
Jan 30 18:43:39.456: INFO: Pod "var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b" satisfied condition "success or failure"
Jan 30 18:43:39.461: INFO: Trying to get logs from node aardvark pod var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b container dapi-container: <nil>
STEP: delete the pod
Jan 30 18:43:39.496: INFO: Waiting for pod var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b to disappear
Jan 30 18:43:39.501: INFO: Pod var-expansion-086042a5-9547-4e2c-9786-0c2c11ab347b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:43:39.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1524" for this suite.
Jan 30 18:43:45.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:43:45.681: INFO: namespace var-expansion-1524 deletion completed in 6.172934817s

• [SLOW TEST:10.310 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:43:45.682: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jan 30 18:43:45.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 api-versions'
Jan 30 18:43:45.850: INFO: stderr: ""
Jan 30 18:43:45.851: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:43:45.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1131" for this suite.
Jan 30 18:43:51.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:43:52.043: INFO: namespace kubectl-1131 deletion completed in 6.184510117s

• [SLOW TEST:6.360 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:43:52.043: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 30 18:43:59.174: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:43:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5506" for this suite.
Jan 30 18:44:05.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:44:05.395: INFO: namespace container-runtime-5506 deletion completed in 6.179709214s

• [SLOW TEST:13.352 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:44:05.395: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 30 18:44:13.545: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:13.550: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:15.550: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:15.555: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:17.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:17.557: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:19.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:19.558: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:21.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:21.557: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:23.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:23.557: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 18:44:25.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 18:44:25.557: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:44:25.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3768" for this suite.
Jan 30 18:44:47.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:44:47.755: INFO: namespace container-lifecycle-hook-3768 deletion completed in 22.175768185s

• [SLOW TEST:42.360 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:44:47.755: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jan 30 18:44:47.809: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 18:44:47.821: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 18:44:47.826: INFO: 
Logging pods the kubelet thinks is on node aardvark before test
Jan 30 18:44:47.835: INFO: kube-flannel-ds-amd64-mlnjj from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 18:44:47.835: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 18:44:47.835: INFO: kube-proxy-rlk8c from kube-system started at 2020-01-23 11:11:37 +0000 UTC (1 container statuses recorded)
Jan 30 18:44:47.835: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 18:44:47.835: INFO: sonobuoy from sonobuoy started at 2020-01-30 17:19:28 +0000 UTC (1 container statuses recorded)
Jan 30 18:44:47.835: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 18:44:47.835: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-22zg6 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:44:47.835: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 30 18:44:47.835: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 18:44:47.835: INFO: 
Logging pods the kubelet thinks is on node gazelle before test
Jan 30 18:44:47.848: INFO: kube-flannel-ds-amd64-dbdl8 from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 18:44:47.848: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 30 18:44:47.848: INFO: kube-proxy-l62gs from kube-system started at 2020-01-23 11:11:57 +0000 UTC (1 container statuses recorded)
Jan 30 18:44:47.848: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 18:44:47.848: INFO: sonobuoy-e2e-job-70068240cc4e41b9 from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:44:47.848: INFO: 	Container e2e ready: true, restart count 0
Jan 30 18:44:47.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 18:44:47.848: INFO: sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-5w56c from sonobuoy started at 2020-01-30 17:19:35 +0000 UTC (2 container statuses recorded)
Jan 30 18:44:47.848: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 30 18:44:47.848: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f3b981cc-026d-4de2-8ba3-476bccbae38a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f3b981cc-026d-4de2-8ba3-476bccbae38a off the node aardvark
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f3b981cc-026d-4de2-8ba3-476bccbae38a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:44:55.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2551" for this suite.
Jan 30 18:45:03.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:45:04.147: INFO: namespace sched-pred-2551 deletion completed in 8.183121288s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.392 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:45:04.148: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5430
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5430
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5430
Jan 30 18:45:04.220: INFO: Found 0 stateful pods, waiting for 1
Jan 30 18:45:14.227: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 30 18:45:14.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:45:14.500: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:45:14.500: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:45:14.500: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 18:45:14.506: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 18:45:24.513: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 18:45:24.513: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:45:24.537: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jan 30 18:45:24.537: INFO: ss-0  gazelle  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:45:24.537: INFO: 
Jan 30 18:45:24.537: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 30 18:45:25.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994579753s
Jan 30 18:45:26.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987097358s
Jan 30 18:45:27.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979911722s
Jan 30 18:45:28.566: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972872032s
Jan 30 18:45:29.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965529954s
Jan 30 18:45:30.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954030741s
Jan 30 18:45:31.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944269463s
Jan 30 18:45:32.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936618448s
Jan 30 18:45:33.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.92661ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5430
Jan 30 18:45:34.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 18:45:44.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 30 18:45:44.565: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 18:45:44.565: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 18:45:44.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 18:45:44.879: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 18:45:44.879: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 18:45:44.879: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 18:45:44.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 30 18:45:45.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 18:45:45.180: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 30 18:45:45.180: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 30 18:45:45.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:45:45.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:45:45.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 30 18:45:45.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:45:45.477: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:45:45.477: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:45:45.477: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 18:45:45.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:45:45.814: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:45:45.814: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:45:45.814: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 18:45:45.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 exec --namespace=statefulset-5430 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 30 18:45:46.106: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 30 18:45:46.106: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 30 18:45:46.106: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 30 18:45:46.106: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:45:46.112: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 30 18:45:56.127: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 18:45:56.127: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 18:45:56.127: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 18:45:56.149: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:45:56.149: INFO: ss-0  gazelle   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:45:56.149: INFO: ss-1  aardvark  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:56.149: INFO: ss-2  gazelle   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:56.149: INFO: 
Jan 30 18:45:56.149: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 18:45:57.156: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:45:57.156: INFO: ss-0  gazelle   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:45:57.156: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:57.156: INFO: ss-2  gazelle   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:57.156: INFO: 
Jan 30 18:45:57.156: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 18:45:58.163: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:45:58.163: INFO: ss-0  gazelle   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:45:58.163: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:58.163: INFO: ss-2  gazelle   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:58.163: INFO: 
Jan 30 18:45:58.163: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 18:45:59.170: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:45:59.170: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:45:59.170: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:45:59.170: INFO: 
Jan 30 18:45:59.170: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:00.178: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:46:00.178: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:46:00.178: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:46:00.178: INFO: 
Jan 30 18:46:00.178: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:01.185: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:46:01.185: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:46:01.186: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:46:01.186: INFO: 
Jan 30 18:46:01.186: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:02.193: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:46:02.193: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:46:02.193: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:46:02.193: INFO: 
Jan 30 18:46:02.193: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:03.200: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:46:03.200: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:46:03.200: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:46:03.200: INFO: 
Jan 30 18:46:03.200: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:04.206: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 30 18:46:04.206: INFO: ss-0  gazelle   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:04 +0000 UTC  }]
Jan 30 18:46:04.206: INFO: ss-1  aardvark  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-30 18:45:24 +0000 UTC  }]
Jan 30 18:46:04.206: INFO: 
Jan 30 18:46:04.206: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 30 18:46:05.212: INFO: Verifying statefulset ss doesn't scale past 0 for another 936.402652ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5430
Jan 30 18:46:06.220: INFO: Scaling statefulset ss to 0
Jan 30 18:46:06.236: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jan 30 18:46:06.241: INFO: Deleting all statefulset in ns statefulset-5430
Jan 30 18:46:06.246: INFO: Scaling statefulset ss to 0
Jan 30 18:46:06.263: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:46:06.267: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:46:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5430" for this suite.
Jan 30 18:46:12.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:46:12.477: INFO: namespace statefulset-5430 deletion completed in 6.176405493s

• [SLOW TEST:68.329 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:46:12.477: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jan 30 18:46:12.542: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d" in namespace "downward-api-2531" to be "success or failure"
Jan 30 18:46:12.556: INFO: Pod "downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.364656ms
Jan 30 18:46:14.563: INFO: Pod "downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021360406s
Jan 30 18:46:16.571: INFO: Pod "downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028745497s
STEP: Saw pod success
Jan 30 18:46:16.571: INFO: Pod "downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d" satisfied condition "success or failure"
Jan 30 18:46:16.576: INFO: Trying to get logs from node aardvark pod downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d container client-container: <nil>
STEP: delete the pod
Jan 30 18:46:16.614: INFO: Waiting for pod downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d to disappear
Jan 30 18:46:16.619: INFO: Pod downwardapi-volume-80d1b864-eff8-48a8-a7c8-85bb3731440d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:46:16.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2531" for this suite.
Jan 30 18:46:22.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:46:22.802: INFO: namespace downward-api-2531 deletion completed in 6.174344461s

• [SLOW TEST:10.325 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:46:22.802: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-35bbbe5e-74f6-47ac-82e0-7fb4623172b6
STEP: Creating a pod to test consume secrets
Jan 30 18:46:22.880: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152" in namespace "projected-8863" to be "success or failure"
Jan 30 18:46:22.887: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448379ms
Jan 30 18:46:24.894: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013138058s
Jan 30 18:46:26.900: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020047598s
Jan 30 18:46:28.907: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026091173s
Jan 30 18:46:30.913: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032189214s
Jan 30 18:46:32.919: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.038769933s
STEP: Saw pod success
Jan 30 18:46:32.919: INFO: Pod "pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152" satisfied condition "success or failure"
Jan 30 18:46:32.923: INFO: Trying to get logs from node gazelle pod pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:46:33.016: INFO: Waiting for pod pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152 to disappear
Jan 30 18:46:33.025: INFO: Pod pod-projected-secrets-77244969-22ab-4f4a-be21-7e0fb69f8152 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:46:33.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8863" for this suite.
Jan 30 18:46:39.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:46:39.209: INFO: namespace projected-8863 deletion completed in 6.177935488s

• [SLOW TEST:16.407 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:46:39.210: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 30 18:46:49.321: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:46:49.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4330" for this suite.
Jan 30 18:46:59.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:46:59.534: INFO: namespace gc-4330 deletion completed in 10.206207369s

• [SLOW TEST:20.324 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:46:59.535: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-bcaf05fa-be56-433f-a9eb-61d780c42af7
STEP: Creating a pod to test consume configMaps
Jan 30 18:46:59.649: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954" in namespace "projected-4101" to be "success or failure"
Jan 30 18:46:59.657: INFO: Pod "pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954": Phase="Pending", Reason="", readiness=false. Elapsed: 7.378743ms
Jan 30 18:47:01.666: INFO: Pod "pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016055101s
Jan 30 18:47:03.673: INFO: Pod "pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023565101s
STEP: Saw pod success
Jan 30 18:47:03.673: INFO: Pod "pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954" satisfied condition "success or failure"
Jan 30 18:47:03.680: INFO: Trying to get logs from node aardvark pod pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 30 18:47:03.730: INFO: Waiting for pod pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954 to disappear
Jan 30 18:47:03.735: INFO: Pod pod-projected-configmaps-b09f2148-4797-4045-a2ba-f04da0952954 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:47:03.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4101" for this suite.
Jan 30 18:47:09.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:47:09.958: INFO: namespace projected-4101 deletion completed in 6.215531276s

• [SLOW TEST:10.424 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:47:09.959: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:47:10.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-765'
Jan 30 18:47:10.335: INFO: stderr: ""
Jan 30 18:47:10.335: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 30 18:47:10.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-765'
Jan 30 18:47:10.682: INFO: stderr: ""
Jan 30 18:47:10.682: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 30 18:47:11.690: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:47:11.691: INFO: Found 0 / 1
Jan 30 18:47:12.690: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:47:12.691: INFO: Found 0 / 1
Jan 30 18:47:13.689: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:47:13.689: INFO: Found 1 / 1
Jan 30 18:47:13.689: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 18:47:13.696: INFO: Selector matched 1 pods for map[app:redis]
Jan 30 18:47:13.696: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 18:47:13.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 describe pod redis-master-r2pwr --namespace=kubectl-765'
Jan 30 18:47:13.838: INFO: stderr: ""
Jan 30 18:47:13.838: INFO: stdout: "Name:           redis-master-r2pwr\nNamespace:      kubectl-765\nPriority:       0\nNode:           gazelle/167.172.59.201\nStart Time:     Thu, 30 Jan 2020 18:47:10 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             25.0.2.139\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://35d6d0716a2b196a525af226995ad7e54603c4d061be0ce5c9fab2ff6667f61f\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 30 Jan 2020 18:47:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g9qrf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-g9qrf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-g9qrf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-765/redis-master-r2pwr to gazelle\n  Normal  Pulled     2s    kubelet, gazelle   Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, gazelle   Created container redis-master\n  Normal  Started    2s    kubelet, gazelle   Started container redis-master\n"
Jan 30 18:47:13.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 describe rc redis-master --namespace=kubectl-765'
Jan 30 18:47:13.999: INFO: stderr: ""
Jan 30 18:47:13.999: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-765\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-r2pwr\n"
Jan 30 18:47:13.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 describe service redis-master --namespace=kubectl-765'
Jan 30 18:47:14.141: INFO: stderr: ""
Jan 30 18:47:14.141: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-765\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.101.147.173\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         25.0.2.139:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 30 18:47:14.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 describe node aardvark'
Jan 30 18:47:14.309: INFO: stderr: ""
Jan 30 18:47:14.309: INFO: stdout: "Name:               aardvark\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aardvark\n                    kubernetes.io/os=linux\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"02:63:b9:29:77:7c\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 167.172.59.233\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Jan 2020 11:11:37 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 30 Jan 2020 18:46:46 +0000   Thu, 23 Jan 2020 11:11:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 30 Jan 2020 18:46:46 +0000   Thu, 23 Jan 2020 11:11:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 30 Jan 2020 18:46:46 +0000   Thu, 23 Jan 2020 11:11:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 30 Jan 2020 18:46:46 +0000   Thu, 23 Jan 2020 11:11:57 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  167.172.59.233\n  Hostname:    aardvark\nCapacity:\n cpu:                8\n ephemeral-storage:  203070420Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65969716Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  187149698763\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65867316Ki\n pods:               110\nSystem Info:\n Machine ID:                 e5ac8a932ad84831a2eaf9d454773d2b\n System UUID:                E5AC8A93-2AD8-4831-A2EA-F9D454773D2B\n Boot ID:                    75f88565-5795-493b-a8d9-a6b31655efb4\n Kernel Version:             4.15.0-74-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.15.9-beta.0\n Kube-Proxy Version:         v1.15.9-beta.0\nPodCIDR:                     25.0.1.0/24\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                kube-flannel-ds-amd64-mlnjj                                100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      7d7h\n  kube-system                kube-proxy-rlk8c                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d7h\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-9dbeb6aa67f74d03-22zg6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                100m (1%)  100m (1%)\n  memory             50Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:              <none>\n"
Jan 30 18:47:14.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 describe namespace kubectl-765'
Jan 30 18:47:14.421: INFO: stderr: ""
Jan 30 18:47:14.421: INFO: stdout: "Name:         kubectl-765\nLabels:       e2e-framework=kubectl\n              e2e-run=38c88da7-0ead-4a74-8028-9d064bc24495\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:47:14.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-765" for this suite.
Jan 30 18:47:36.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:47:36.630: INFO: namespace kubectl-765 deletion completed in 22.19919451s

• [SLOW TEST:26.670 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:47:36.630: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:47:41.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8345" for this suite.
Jan 30 18:48:21.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:48:21.962: INFO: namespace replication-controller-8345 deletion completed in 40.204927707s

• [SLOW TEST:45.332 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:48:21.962: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:48:22.077: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:48:26.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4972" for this suite.
Jan 30 18:49:22.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:49:22.491: INFO: namespace pods-4972 deletion completed in 56.213612663s

• [SLOW TEST:60.529 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:49:22.491: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4422
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jan 30 18:49:22.605: INFO: Found 0 stateful pods, waiting for 3
Jan 30 18:49:32.612: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:49:32.612: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:49:32.612: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 30 18:49:32.661: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 30 18:49:42.721: INFO: Updating stateful set ss2
Jan 30 18:49:42.731: INFO: Waiting for Pod statefulset-4422/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jan 30 18:49:52.879: INFO: Found 2 stateful pods, waiting for 3
Jan 30 18:50:02.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:50:02.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 18:50:02.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 30 18:50:02.926: INFO: Updating stateful set ss2
Jan 30 18:50:02.944: INFO: Waiting for Pod statefulset-4422/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 30 18:50:12.985: INFO: Updating stateful set ss2
Jan 30 18:50:13.004: INFO: Waiting for StatefulSet statefulset-4422/ss2 to complete update
Jan 30 18:50:13.004: INFO: Waiting for Pod statefulset-4422/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jan 30 18:50:23.020: INFO: Deleting all statefulset in ns statefulset-4422
Jan 30 18:50:23.025: INFO: Scaling statefulset ss2 to 0
Jan 30 18:50:43.052: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 18:50:43.059: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:50:43.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4422" for this suite.
Jan 30 18:50:51.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:50:51.310: INFO: namespace statefulset-4422 deletion completed in 8.21468294s

• [SLOW TEST:88.818 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:50:51.310: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:51:17.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5536" for this suite.
Jan 30 18:51:23.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:51:23.749: INFO: namespace namespaces-5536 deletion completed in 6.207015564s
STEP: Destroying namespace "nsdeletetest-3556" for this suite.
Jan 30 18:51:23.754: INFO: Namespace nsdeletetest-3556 was already deleted
STEP: Destroying namespace "nsdeletetest-5927" for this suite.
Jan 30 18:51:29.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:51:29.955: INFO: namespace nsdeletetest-5927 deletion completed in 6.200794521s

• [SLOW TEST:38.645 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:51:29.957: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-89nq
STEP: Creating a pod to test atomic-volume-subpath
Jan 30 18:51:30.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-89nq" in namespace "subpath-1303" to be "success or failure"
Jan 30 18:51:30.057: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.55446ms
Jan 30 18:51:32.065: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015910597s
Jan 30 18:51:34.073: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 4.023541027s
Jan 30 18:51:36.081: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 6.031368114s
Jan 30 18:51:38.088: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 8.039113912s
Jan 30 18:51:40.095: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 10.046075345s
Jan 30 18:51:42.103: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 12.053404963s
Jan 30 18:51:44.110: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 14.060749947s
Jan 30 18:51:46.119: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 16.070181414s
Jan 30 18:51:48.128: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 18.078637898s
Jan 30 18:51:50.135: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Running", Reason="", readiness=true. Elapsed: 20.085797031s
Jan 30 18:51:52.142: INFO: Pod "pod-subpath-test-projected-89nq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.092557103s
STEP: Saw pod success
Jan 30 18:51:52.142: INFO: Pod "pod-subpath-test-projected-89nq" satisfied condition "success or failure"
Jan 30 18:51:52.148: INFO: Trying to get logs from node gazelle pod pod-subpath-test-projected-89nq container test-container-subpath-projected-89nq: <nil>
STEP: delete the pod
Jan 30 18:51:52.186: INFO: Waiting for pod pod-subpath-test-projected-89nq to disappear
Jan 30 18:51:52.191: INFO: Pod pod-subpath-test-projected-89nq no longer exists
STEP: Deleting pod pod-subpath-test-projected-89nq
Jan 30 18:51:52.191: INFO: Deleting pod "pod-subpath-test-projected-89nq" in namespace "subpath-1303"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:51:52.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1303" for this suite.
Jan 30 18:51:58.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:51:58.419: INFO: namespace subpath-1303 deletion completed in 6.214703491s

• [SLOW TEST:28.462 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:51:58.422: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jan 30 18:51:58.490: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:52:02.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6772" for this suite.
Jan 30 18:52:24.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:52:25.039: INFO: namespace init-container-6772 deletion completed in 22.224220835s

• [SLOW TEST:26.617 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:52:25.040: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1950
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 30 18:52:25.108: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 30 18:52:51.252: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 25.0.2.146 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1950 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 18:52:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 18:52:52.463: INFO: Found all expected endpoints: [netserver-0]
Jan 30 18:52:52.469: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 25.0.1.203 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1950 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 30 18:52:52.469: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
Jan 30 18:52:53.677: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:52:53.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1950" for this suite.
Jan 30 18:53:17.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:53:17.993: INFO: namespace pod-network-test-1950 deletion completed in 24.308986568s

• [SLOW TEST:52.953 seconds]
[sig-network] Networking
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:53:17.994: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9370 to expose endpoints map[]
Jan 30 18:53:18.088: INFO: Get endpoints failed (6.543374ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 30 18:53:19.113: INFO: successfully validated that service endpoint-test2 in namespace services-9370 exposes endpoints map[] (1.031752024s elapsed)
STEP: Creating pod pod1 in namespace services-9370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9370 to expose endpoints map[pod1:[80]]
Jan 30 18:53:22.179: INFO: successfully validated that service endpoint-test2 in namespace services-9370 exposes endpoints map[pod1:[80]] (3.050732295s elapsed)
STEP: Creating pod pod2 in namespace services-9370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9370 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 30 18:53:25.267: INFO: successfully validated that service endpoint-test2 in namespace services-9370 exposes endpoints map[pod1:[80] pod2:[80]] (3.079500698s elapsed)
STEP: Deleting pod pod1 in namespace services-9370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9370 to expose endpoints map[pod2:[80]]
Jan 30 18:53:26.309: INFO: successfully validated that service endpoint-test2 in namespace services-9370 exposes endpoints map[pod2:[80]] (1.026171907s elapsed)
STEP: Deleting pod pod2 in namespace services-9370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9370 to expose endpoints map[]
Jan 30 18:53:27.336: INFO: successfully validated that service endpoint-test2 in namespace services-9370 exposes endpoints map[] (1.012695354s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:53:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9370" for this suite.
Jan 30 18:53:41.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:53:41.604: INFO: namespace services-9370 deletion completed in 14.2132927s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:23.610 seconds]
[sig-network] Services
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:53:41.604: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-75eacf0e-f26d-4174-8b78-fe4f9ff06b92
STEP: Creating a pod to test consume secrets
Jan 30 18:53:41.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded" in namespace "projected-5447" to be "success or failure"
Jan 30 18:53:41.703: INFO: Pod "pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded": Phase="Pending", Reason="", readiness=false. Elapsed: 9.19592ms
Jan 30 18:53:43.710: INFO: Pod "pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015803138s
Jan 30 18:53:45.718: INFO: Pod "pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023679171s
STEP: Saw pod success
Jan 30 18:53:45.718: INFO: Pod "pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded" satisfied condition "success or failure"
Jan 30 18:53:45.724: INFO: Trying to get logs from node gazelle pod pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:53:45.763: INFO: Waiting for pod pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded to disappear
Jan 30 18:53:45.769: INFO: Pod pod-projected-secrets-5f1b19f9-913e-4200-b22e-c27326046ded no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:53:45.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5447" for this suite.
Jan 30 18:53:51.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:53:51.974: INFO: namespace projected-5447 deletion completed in 6.197214324s

• [SLOW TEST:10.369 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:53:51.975: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bb4c0b83-c65d-477e-9735-cb83aca273fa
STEP: Creating a pod to test consume secrets
Jan 30 18:53:52.058: INFO: Waiting up to 5m0s for pod "pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a" in namespace "secrets-7407" to be "success or failure"
Jan 30 18:53:52.063: INFO: Pod "pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304039ms
Jan 30 18:53:54.070: INFO: Pod "pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012569038s
Jan 30 18:53:56.078: INFO: Pod "pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020745621s
STEP: Saw pod success
Jan 30 18:53:56.078: INFO: Pod "pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a" satisfied condition "success or failure"
Jan 30 18:53:56.085: INFO: Trying to get logs from node aardvark pod pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a container secret-volume-test: <nil>
STEP: delete the pod
Jan 30 18:53:56.129: INFO: Waiting for pod pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a to disappear
Jan 30 18:53:56.138: INFO: Pod pod-secrets-6f60795d-30af-4449-a5e3-ddb1f19fd58a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:53:56.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7407" for this suite.
Jan 30 18:54:02.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:54:02.354: INFO: namespace secrets-7407 deletion completed in 6.208415528s

• [SLOW TEST:10.379 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:54:02.354: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jan 30 18:54:02.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 create -f - --namespace=kubectl-6036'
Jan 30 18:54:02.991: INFO: stderr: ""
Jan 30 18:54:02.991: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 18:54:02.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:03.109: INFO: stderr: ""
Jan 30 18:54:03.109: INFO: stdout: "update-demo-nautilus-vq2fp update-demo-nautilus-x2qp9 "
Jan 30 18:54:03.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-vq2fp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:03.221: INFO: stderr: ""
Jan 30 18:54:03.221: INFO: stdout: ""
Jan 30 18:54:03.221: INFO: update-demo-nautilus-vq2fp is created but not running
Jan 30 18:54:08.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:08.344: INFO: stderr: ""
Jan 30 18:54:08.344: INFO: stdout: "update-demo-nautilus-vq2fp update-demo-nautilus-x2qp9 "
Jan 30 18:54:08.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-vq2fp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:08.430: INFO: stderr: ""
Jan 30 18:54:08.430: INFO: stdout: "true"
Jan 30 18:54:08.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-vq2fp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:08.529: INFO: stderr: ""
Jan 30 18:54:08.529: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:08.529: INFO: validating pod update-demo-nautilus-vq2fp
Jan 30 18:54:08.540: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:08.540: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:08.540: INFO: update-demo-nautilus-vq2fp is verified up and running
Jan 30 18:54:08.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:08.656: INFO: stderr: ""
Jan 30 18:54:08.656: INFO: stdout: "true"
Jan 30 18:54:08.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:08.761: INFO: stderr: ""
Jan 30 18:54:08.761: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:08.761: INFO: validating pod update-demo-nautilus-x2qp9
Jan 30 18:54:08.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:08.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:08.771: INFO: update-demo-nautilus-x2qp9 is verified up and running
STEP: scaling down the replication controller
Jan 30 18:54:08.775: INFO: scanned /root for discovery docs: <nil>
Jan 30 18:54:08.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6036'
Jan 30 18:54:09.912: INFO: stderr: ""
Jan 30 18:54:09.912: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 18:54:09.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:10.029: INFO: stderr: ""
Jan 30 18:54:10.029: INFO: stdout: "update-demo-nautilus-vq2fp update-demo-nautilus-x2qp9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 30 18:54:15.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:15.133: INFO: stderr: ""
Jan 30 18:54:15.133: INFO: stdout: "update-demo-nautilus-vq2fp update-demo-nautilus-x2qp9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 30 18:54:20.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:20.252: INFO: stderr: ""
Jan 30 18:54:20.252: INFO: stdout: "update-demo-nautilus-x2qp9 "
Jan 30 18:54:20.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:20.359: INFO: stderr: ""
Jan 30 18:54:20.359: INFO: stdout: "true"
Jan 30 18:54:20.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:20.472: INFO: stderr: ""
Jan 30 18:54:20.472: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:20.472: INFO: validating pod update-demo-nautilus-x2qp9
Jan 30 18:54:20.482: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:20.482: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:20.482: INFO: update-demo-nautilus-x2qp9 is verified up and running
STEP: scaling up the replication controller
Jan 30 18:54:20.483: INFO: scanned /root for discovery docs: <nil>
Jan 30 18:54:20.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6036'
Jan 30 18:54:21.621: INFO: stderr: ""
Jan 30 18:54:21.621: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 30 18:54:21.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:21.746: INFO: stderr: ""
Jan 30 18:54:21.746: INFO: stdout: "update-demo-nautilus-x2qp9 update-demo-nautilus-xv2gt "
Jan 30 18:54:21.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:21.885: INFO: stderr: ""
Jan 30 18:54:21.885: INFO: stdout: "true"
Jan 30 18:54:21.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:21.981: INFO: stderr: ""
Jan 30 18:54:21.981: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:21.981: INFO: validating pod update-demo-nautilus-x2qp9
Jan 30 18:54:21.988: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:21.988: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:21.988: INFO: update-demo-nautilus-x2qp9 is verified up and running
Jan 30 18:54:21.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-xv2gt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:22.086: INFO: stderr: ""
Jan 30 18:54:22.086: INFO: stdout: ""
Jan 30 18:54:22.086: INFO: update-demo-nautilus-xv2gt is created but not running
Jan 30 18:54:27.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6036'
Jan 30 18:54:27.197: INFO: stderr: ""
Jan 30 18:54:27.197: INFO: stdout: "update-demo-nautilus-x2qp9 update-demo-nautilus-xv2gt "
Jan 30 18:54:27.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:27.293: INFO: stderr: ""
Jan 30 18:54:27.294: INFO: stdout: "true"
Jan 30 18:54:27.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-x2qp9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:27.381: INFO: stderr: ""
Jan 30 18:54:27.381: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:27.381: INFO: validating pod update-demo-nautilus-x2qp9
Jan 30 18:54:27.389: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:27.389: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:27.389: INFO: update-demo-nautilus-x2qp9 is verified up and running
Jan 30 18:54:27.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-xv2gt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:27.510: INFO: stderr: ""
Jan 30 18:54:27.510: INFO: stdout: "true"
Jan 30 18:54:27.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods update-demo-nautilus-xv2gt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6036'
Jan 30 18:54:27.613: INFO: stderr: ""
Jan 30 18:54:27.613: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 30 18:54:27.613: INFO: validating pod update-demo-nautilus-xv2gt
Jan 30 18:54:27.624: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 18:54:27.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 18:54:27.624: INFO: update-demo-nautilus-xv2gt is verified up and running
STEP: using delete to clean up resources
Jan 30 18:54:27.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 delete --grace-period=0 --force -f - --namespace=kubectl-6036'
Jan 30 18:54:27.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 18:54:27.749: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 18:54:27.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6036'
Jan 30 18:54:27.886: INFO: stderr: "No resources found.\n"
Jan 30 18:54:27.886: INFO: stdout: ""
Jan 30 18:54:27.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-482490662 get pods -l name=update-demo --namespace=kubectl-6036 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 18:54:27.973: INFO: stderr: ""
Jan 30 18:54:27.973: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:54:27.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6036" for this suite.
Jan 30 18:54:50.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:54:50.187: INFO: namespace kubectl-6036 deletion completed in 22.207929189s

• [SLOW TEST:47.833 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:54:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:54:54.319: INFO: Waiting up to 5m0s for pod "client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820" in namespace "pods-6313" to be "success or failure"
Jan 30 18:54:54.331: INFO: Pod "client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820": Phase="Pending", Reason="", readiness=false. Elapsed: 11.507898ms
Jan 30 18:54:56.339: INFO: Pod "client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01922254s
Jan 30 18:54:58.346: INFO: Pod "client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026872781s
STEP: Saw pod success
Jan 30 18:54:58.346: INFO: Pod "client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820" satisfied condition "success or failure"
Jan 30 18:54:58.352: INFO: Trying to get logs from node gazelle pod client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820 container env3cont: <nil>
STEP: delete the pod
Jan 30 18:54:58.389: INFO: Waiting for pod client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820 to disappear
Jan 30 18:54:58.395: INFO: Pod client-envvars-b44a69c1-f69f-48ab-8429-4da452ed2820 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:54:58.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6313" for this suite.
Jan 30 18:55:40.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:55:40.602: INFO: namespace pods-6313 deletion completed in 42.19975951s

• [SLOW TEST:50.415 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:55:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 30 18:55:40.684: INFO: Waiting up to 5m0s for pod "pod-a2bc6833-3426-494d-8c40-d0f0512d79b9" in namespace "emptydir-304" to be "success or failure"
Jan 30 18:55:40.690: INFO: Pod "pod-a2bc6833-3426-494d-8c40-d0f0512d79b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.470599ms
Jan 30 18:55:42.698: INFO: Pod "pod-a2bc6833-3426-494d-8c40-d0f0512d79b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013563267s
Jan 30 18:55:44.708: INFO: Pod "pod-a2bc6833-3426-494d-8c40-d0f0512d79b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023984702s
STEP: Saw pod success
Jan 30 18:55:44.708: INFO: Pod "pod-a2bc6833-3426-494d-8c40-d0f0512d79b9" satisfied condition "success or failure"
Jan 30 18:55:44.714: INFO: Trying to get logs from node aardvark pod pod-a2bc6833-3426-494d-8c40-d0f0512d79b9 container test-container: <nil>
STEP: delete the pod
Jan 30 18:55:44.757: INFO: Waiting for pod pod-a2bc6833-3426-494d-8c40-d0f0512d79b9 to disappear
Jan 30 18:55:44.762: INFO: Pod pod-a2bc6833-3426-494d-8c40-d0f0512d79b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:55:44.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-304" for this suite.
Jan 30 18:55:50.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:55:50.980: INFO: namespace emptydir-304 deletion completed in 6.213019825s

• [SLOW TEST:10.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:55:50.980: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-lpkh
STEP: Creating a pod to test atomic-volume-subpath
Jan 30 18:55:51.072: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lpkh" in namespace "subpath-3337" to be "success or failure"
Jan 30 18:55:51.081: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.784211ms
Jan 30 18:55:53.088: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015879049s
Jan 30 18:55:55.095: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 4.023557415s
Jan 30 18:55:57.103: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 6.031049891s
Jan 30 18:55:59.110: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 8.037995755s
Jan 30 18:56:01.117: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 10.045320431s
Jan 30 18:56:03.125: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 12.052762443s
Jan 30 18:56:05.132: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 14.06011994s
Jan 30 18:56:07.139: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 16.067093868s
Jan 30 18:56:09.148: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 18.075784972s
Jan 30 18:56:11.156: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Running", Reason="", readiness=true. Elapsed: 20.083928852s
Jan 30 18:56:13.164: INFO: Pod "pod-subpath-test-configmap-lpkh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.091685763s
STEP: Saw pod success
Jan 30 18:56:13.164: INFO: Pod "pod-subpath-test-configmap-lpkh" satisfied condition "success or failure"
Jan 30 18:56:13.170: INFO: Trying to get logs from node gazelle pod pod-subpath-test-configmap-lpkh container test-container-subpath-configmap-lpkh: <nil>
STEP: delete the pod
Jan 30 18:56:13.209: INFO: Waiting for pod pod-subpath-test-configmap-lpkh to disappear
Jan 30 18:56:13.222: INFO: Pod pod-subpath-test-configmap-lpkh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lpkh
Jan 30 18:56:13.222: INFO: Deleting pod "pod-subpath-test-configmap-lpkh" in namespace "subpath-3337"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:56:13.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3337" for this suite.
Jan 30 18:56:19.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:56:19.439: INFO: namespace subpath-3337 deletion completed in 6.203468805s

• [SLOW TEST:28.459 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:56:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 30 18:56:22.552: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:56:22.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-745" for this suite.
Jan 30 18:56:28.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:56:28.789: INFO: namespace container-runtime-745 deletion completed in 6.19997056s

• [SLOW TEST:9.350 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jan 30 18:56:28.791: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jan 30 18:56:28.861: INFO: >>> kubeConfig: /tmp/kubeconfig-482490662
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jan 30 18:56:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5091" for this suite.
Jan 30 18:57:16.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 30 18:57:17.136: INFO: namespace pods-5091 deletion completed in 44.210770597s

• [SLOW TEST:48.346 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.8-beta.1.30+14ede42c4fe699/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSJan 30 18:57:17.137: INFO: Running AfterSuite actions on all nodes
Jan 30 18:57:17.137: INFO: Running AfterSuite actions on node 1
Jan 30 18:57:17.137: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5839.521 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h37m21.132875404s
Test Suite Passed
