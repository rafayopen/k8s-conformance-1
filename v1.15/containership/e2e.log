I0821 15:51:23.960266      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-483237363
I0821 15:51:23.960599      15 e2e.go:241] Starting e2e run "a8d9950c-e8eb-45cb-82e5-9d6dd7865ac5" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1566402681 - Will randomize all specs
Will run 215 of 4413 specs

Aug 21 15:51:24.274: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 15:51:24.280: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 21 15:51:24.308: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 21 15:51:24.352: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 21 15:51:24.352: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 21 15:51:24.352: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 21 15:51:24.365: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 21 15:51:24.366: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-do-node' (0 seconds elapsed)
Aug 21 15:51:24.366: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 21 15:51:24.366: INFO: e2e test version: v1.15.3
Aug 21 15:51:24.370: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:51:24.381: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
Aug 21 15:51:24.619: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9fd569d5-35ff-4ce9-a05e-1947a1af4561
STEP: Creating secret with name s-test-opt-upd-d6d19af9-f206-451d-926a-44badeeb0b57
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9fd569d5-35ff-4ce9-a05e-1947a1af4561
STEP: Updating secret s-test-opt-upd-d6d19af9-f206-451d-926a-44badeeb0b57
STEP: Creating secret with name s-test-opt-create-d4b27fc7-84aa-4611-9932-4d62a21d02a8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:51:32.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8705" for this suite.
Aug 21 15:51:52.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:51:53.184: INFO: namespace secrets-8705 deletion completed in 20.307479616s

• [SLOW TEST:28.803 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:51:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 21 15:51:53.329: INFO: Waiting up to 5m0s for pod "downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30" in namespace "downward-api-6189" to be "success or failure"
Aug 21 15:51:53.335: INFO: Pod "downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383193ms
Aug 21 15:51:55.341: INFO: Pod "downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012323775s
Aug 21 15:51:57.346: INFO: Pod "downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016757756s
STEP: Saw pod success
Aug 21 15:51:57.346: INFO: Pod "downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30" satisfied condition "success or failure"
Aug 21 15:51:57.350: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30 container dapi-container: <nil>
STEP: delete the pod
Aug 21 15:51:57.386: INFO: Waiting for pod downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30 to disappear
Aug 21 15:51:57.393: INFO: Pod downward-api-d07ce7ee-ebbc-43ea-92a6-5ef292640e30 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:51:57.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6189" for this suite.
Aug 21 15:52:05.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:52:05.719: INFO: namespace downward-api-6189 deletion completed in 8.30774926s

• [SLOW TEST:12.534 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:52:05.721: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:52:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9703" for this suite.
Aug 21 15:52:26.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:52:26.570: INFO: namespace namespaces-9703 deletion completed in 12.206704039s
STEP: Destroying namespace "nsdeletetest-8441" for this suite.
Aug 21 15:52:26.574: INFO: Namespace nsdeletetest-8441 was already deleted
STEP: Destroying namespace "nsdeletetest-171" for this suite.
Aug 21 15:52:32.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:52:32.761: INFO: namespace nsdeletetest-171 deletion completed in 6.186177039s

• [SLOW TEST:27.040 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:52:32.763: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 21 15:52:32.859: INFO: Waiting up to 5m0s for pod "downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf" in namespace "downward-api-9840" to be "success or failure"
Aug 21 15:52:32.864: INFO: Pod "downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.072087ms
Aug 21 15:52:34.870: INFO: Pod "downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010312936s
STEP: Saw pod success
Aug 21 15:52:34.870: INFO: Pod "downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf" satisfied condition "success or failure"
Aug 21 15:52:34.874: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf container dapi-container: <nil>
STEP: delete the pod
Aug 21 15:52:35.207: INFO: Waiting for pod downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf to disappear
Aug 21 15:52:35.210: INFO: Pod downward-api-a29943d6-de34-4a3e-91ef-dc4a20a49daf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:52:35.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9840" for this suite.
Aug 21 15:52:47.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:52:47.397: INFO: namespace downward-api-9840 deletion completed in 12.180670813s

• [SLOW TEST:14.634 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:52:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3539/configmap-test-8da0dbe4-bec1-49f8-9c31-762755a87c03
STEP: Creating a pod to test consume configMaps
Aug 21 15:52:47.578: INFO: Waiting up to 5m0s for pod "pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780" in namespace "configmap-3539" to be "success or failure"
Aug 21 15:52:47.582: INFO: Pod "pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780": Phase="Pending", Reason="", readiness=false. Elapsed: 3.859741ms
Aug 21 15:52:49.587: INFO: Pod "pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008880075s
Aug 21 15:52:51.591: INFO: Pod "pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013589061s
STEP: Saw pod success
Aug 21 15:52:51.592: INFO: Pod "pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780" satisfied condition "success or failure"
Aug 21 15:52:51.595: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780 container env-test: <nil>
STEP: delete the pod
Aug 21 15:52:51.623: INFO: Waiting for pod pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780 to disappear
Aug 21 15:52:51.626: INFO: Pod pod-configmaps-37444bb2-fdbe-40a7-b3eb-6079a9201780 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:52:51.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3539" for this suite.
Aug 21 15:52:59.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:52:59.813: INFO: namespace configmap-3539 deletion completed in 8.174329455s

• [SLOW TEST:12.415 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:52:59.814: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 15:52:59.979: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 21 15:52:59.993: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 21 15:53:04.998: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 15:53:07.007: INFO: Creating deployment "test-rolling-update-deployment"
Aug 21 15:53:07.015: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 21 15:53:07.022: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 21 15:53:09.036: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 21 15:53:09.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701999587, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701999587, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701999587, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701999587, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:53:11.055: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 21 15:53:11.067: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2457,SelfLink:/apis/apps/v1/namespaces/deployment-2457/deployments/test-rolling-update-deployment,UID:e2adc646-d2f9-455c-bab6-1a139185d137,ResourceVersion:4259,Generation:1,CreationTimestamp:2019-08-21 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-21 15:53:07 +0000 UTC 2019-08-21 15:53:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-21 15:53:10 +0000 UTC 2019-08-21 15:53:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 21 15:53:11.074: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-2457,SelfLink:/apis/apps/v1/namespaces/deployment-2457/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:59cec19d-3058-434a-abf4-7d6fa5aa9246,ResourceVersion:4248,Generation:1,CreationTimestamp:2019-08-21 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e2adc646-d2f9-455c-bab6-1a139185d137 0xc002c8e657 0xc002c8e658}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 21 15:53:11.074: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 21 15:53:11.074: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2457,SelfLink:/apis/apps/v1/namespaces/deployment-2457/replicasets/test-rolling-update-controller,UID:8a81930a-b470-4c5d-af1a-735e367a4dfe,ResourceVersion:4258,Generation:2,CreationTimestamp:2019-08-21 15:52:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e2adc646-d2f9-455c-bab6-1a139185d137 0xc002c8e57f 0xc002c8e590}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 15:53:11.080: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-h59zm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-h59zm,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-2457,SelfLink:/api/v1/namespaces/deployment-2457/pods/test-rolling-update-deployment-79f6b9d75c-h59zm,UID:a7257247-daac-4864-8eaf-6d84912b2c10,ResourceVersion:4247,Generation:0,CreationTimestamp:2019-08-21 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.6/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 59cec19d-3058-434a-abf4-7d6fa5aa9246 0xc002c8efd7 0xc002c8efd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-2mbsj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2mbsj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-2mbsj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c8f040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c8f060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 15:53:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 15:53:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 15:53:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.6,StartTime:2019-08-21 15:53:07 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-21 15:53:09 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f5a0fef647acdabf232264e61f17d5e7cf92a4ec8b8c82bfeca1229f3470dbb5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:53:11.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2457" for this suite.
Aug 21 15:53:21.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:53:21.317: INFO: namespace deployment-2457 deletion completed in 10.227953654s

• [SLOW TEST:21.503 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:53:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-356bc643-2fe7-4e84-9ed4-056180929de3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:53:21.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8619" for this suite.
Aug 21 15:53:27.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:53:27.605: INFO: namespace configmap-8619 deletion completed in 6.16614715s

• [SLOW TEST:6.287 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:53:27.606: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1979
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 15:53:27.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 15:53:53.794: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1979 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:53:53.794: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 15:53:55.059: INFO: Found all expected endpoints: [netserver-0]
Aug 21 15:53:55.063: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1979 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:53:55.063: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 15:53:56.361: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:53:56.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1979" for this suite.
Aug 21 15:54:20.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:54:20.560: INFO: namespace pod-network-test-1979 deletion completed in 24.1930151s

• [SLOW TEST:52.955 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:54:20.563: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:54:24.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1936" for this suite.
Aug 21 15:55:04.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:55:04.862: INFO: namespace kubelet-test-1936 deletion completed in 40.166547572s

• [SLOW TEST:44.299 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:55:04.863: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-23199e4b-489a-4bcf-b13a-81623632e391
STEP: Creating a pod to test consume configMaps
Aug 21 15:55:05.071: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda" in namespace "projected-8562" to be "success or failure"
Aug 21 15:55:05.074: INFO: Pod "pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948123ms
Aug 21 15:55:07.079: INFO: Pod "pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008112725s
Aug 21 15:55:09.084: INFO: Pod "pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013256801s
STEP: Saw pod success
Aug 21 15:55:09.084: INFO: Pod "pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda" satisfied condition "success or failure"
Aug 21 15:55:09.089: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:55:09.125: INFO: Waiting for pod pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda to disappear
Aug 21 15:55:09.130: INFO: Pod pod-projected-configmaps-1450e0d5-340f-4257-b605-7865aa933fda no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:55:09.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8562" for this suite.
Aug 21 15:55:15.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:55:15.298: INFO: namespace projected-8562 deletion completed in 6.161132862s

• [SLOW TEST:10.435 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:55:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-rjg6
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 15:55:15.385: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rjg6" in namespace "subpath-3625" to be "success or failure"
Aug 21 15:55:15.388: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621569ms
Aug 21 15:55:17.393: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00832448s
Aug 21 15:55:19.399: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 4.014307105s
Aug 21 15:55:21.403: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 6.01839732s
Aug 21 15:55:23.408: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 8.023741754s
Aug 21 15:55:25.413: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 10.028392701s
Aug 21 15:55:27.419: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 12.03447679s
Aug 21 15:55:29.424: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 14.039723309s
Aug 21 15:55:31.430: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 16.04524661s
Aug 21 15:55:33.435: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 18.050364219s
Aug 21 15:55:35.440: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 20.055338671s
Aug 21 15:55:37.446: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Running", Reason="", readiness=true. Elapsed: 22.061081925s
Aug 21 15:55:39.451: INFO: Pod "pod-subpath-test-downwardapi-rjg6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.066387206s
STEP: Saw pod success
Aug 21 15:55:39.451: INFO: Pod "pod-subpath-test-downwardapi-rjg6" satisfied condition "success or failure"
Aug 21 15:55:39.456: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-subpath-test-downwardapi-rjg6 container test-container-subpath-downwardapi-rjg6: <nil>
STEP: delete the pod
Aug 21 15:55:39.487: INFO: Waiting for pod pod-subpath-test-downwardapi-rjg6 to disappear
Aug 21 15:55:39.493: INFO: Pod pod-subpath-test-downwardapi-rjg6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rjg6
Aug 21 15:55:39.493: INFO: Deleting pod "pod-subpath-test-downwardapi-rjg6" in namespace "subpath-3625"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:55:39.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3625" for this suite.
Aug 21 15:55:45.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:55:45.657: INFO: namespace subpath-3625 deletion completed in 6.154734865s

• [SLOW TEST:30.359 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:55:45.657: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 21 15:55:45.739: INFO: Waiting up to 5m0s for pod "pod-dbd5f717-1fdf-49b4-a591-5ae24159d321" in namespace "emptydir-654" to be "success or failure"
Aug 21 15:55:45.743: INFO: Pod "pod-dbd5f717-1fdf-49b4-a591-5ae24159d321": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773941ms
Aug 21 15:55:47.748: INFO: Pod "pod-dbd5f717-1fdf-49b4-a591-5ae24159d321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008705552s
Aug 21 15:55:49.754: INFO: Pod "pod-dbd5f717-1fdf-49b4-a591-5ae24159d321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014357103s
STEP: Saw pod success
Aug 21 15:55:49.754: INFO: Pod "pod-dbd5f717-1fdf-49b4-a591-5ae24159d321" satisfied condition "success or failure"
Aug 21 15:55:49.758: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-dbd5f717-1fdf-49b4-a591-5ae24159d321 container test-container: <nil>
STEP: delete the pod
Aug 21 15:55:49.785: INFO: Waiting for pod pod-dbd5f717-1fdf-49b4-a591-5ae24159d321 to disappear
Aug 21 15:55:49.790: INFO: Pod pod-dbd5f717-1fdf-49b4-a591-5ae24159d321 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:55:49.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-654" for this suite.
Aug 21 15:55:55.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:55:55.973: INFO: namespace emptydir-654 deletion completed in 6.176866776s

• [SLOW TEST:10.317 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:55:55.978: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 15:56:00.089: INFO: Waiting up to 5m0s for pod "client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12" in namespace "pods-1023" to be "success or failure"
Aug 21 15:56:00.095: INFO: Pod "client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756253ms
Aug 21 15:56:02.099: INFO: Pod "client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01013503s
Aug 21 15:56:04.104: INFO: Pod "client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015340313s
STEP: Saw pod success
Aug 21 15:56:04.105: INFO: Pod "client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12" satisfied condition "success or failure"
Aug 21 15:56:04.110: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12 container env3cont: <nil>
STEP: delete the pod
Aug 21 15:56:04.140: INFO: Waiting for pod client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12 to disappear
Aug 21 15:56:04.144: INFO: Pod client-envvars-2eb1edd7-50d1-4161-8b06-fac2a77cae12 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:56:04.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1023" for this suite.
Aug 21 15:56:44.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:56:44.339: INFO: namespace pods-1023 deletion completed in 40.18997705s

• [SLOW TEST:48.362 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:56:44.341: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6907836a-7f00-4c89-8e34-8ee034ec834a
STEP: Creating a pod to test consume secrets
Aug 21 15:56:44.442: INFO: Waiting up to 5m0s for pod "pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe" in namespace "secrets-8745" to be "success or failure"
Aug 21 15:56:44.447: INFO: Pod "pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722672ms
Aug 21 15:56:46.452: INFO: Pod "pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010520159s
Aug 21 15:56:48.457: INFO: Pod "pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015416418s
STEP: Saw pod success
Aug 21 15:56:48.457: INFO: Pod "pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe" satisfied condition "success or failure"
Aug 21 15:56:48.461: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:56:48.528: INFO: Waiting for pod pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe to disappear
Aug 21 15:56:48.532: INFO: Pod pod-secrets-9c4b6bf8-2617-41ab-a987-d421f22928fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:56:48.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8745" for this suite.
Aug 21 15:56:56.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:56:56.692: INFO: namespace secrets-8745 deletion completed in 8.154402191s

• [SLOW TEST:12.350 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:56:56.694: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 21 15:56:56.928: INFO: Waiting up to 5m0s for pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c" in namespace "containers-654" to be "success or failure"
Aug 21 15:56:56.933: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.885754ms
Aug 21 15:56:58.939: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010878389s
Aug 21 15:57:00.945: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01616828s
Aug 21 15:57:02.949: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020989765s
Aug 21 15:57:04.954: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025472834s
STEP: Saw pod success
Aug 21 15:57:04.954: INFO: Pod "client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c" satisfied condition "success or failure"
Aug 21 15:57:04.961: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c container test-container: <nil>
STEP: delete the pod
Aug 21 15:57:05.122: INFO: Waiting for pod client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c to disappear
Aug 21 15:57:05.125: INFO: Pod client-containers-f8efdf0d-44b4-4973-adbb-f88291576e2c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:57:05.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-654" for this suite.
Aug 21 15:57:11.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:11.413: INFO: namespace containers-654 deletion completed in 6.283419383s

• [SLOW TEST:14.720 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:57:11.424: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 15:57:11.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe" in namespace "downward-api-236" to be "success or failure"
Aug 21 15:57:11.540: INFO: Pod "downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930896ms
Aug 21 15:57:13.545: INFO: Pod "downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009207833s
Aug 21 15:57:15.551: INFO: Pod "downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015283935s
STEP: Saw pod success
Aug 21 15:57:15.551: INFO: Pod "downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe" satisfied condition "success or failure"
Aug 21 15:57:15.556: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe container client-container: <nil>
STEP: delete the pod
Aug 21 15:57:15.587: INFO: Waiting for pod downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe to disappear
Aug 21 15:57:15.610: INFO: Pod downwardapi-volume-6e9c13cd-05dd-40f5-8cf7-c762a7633cfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:57:15.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-236" for this suite.
Aug 21 15:57:21.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:21.824: INFO: namespace downward-api-236 deletion completed in 6.207783398s

• [SLOW TEST:10.401 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:57:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 21 15:57:21.913: INFO: Waiting up to 5m0s for pod "pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655" in namespace "emptydir-593" to be "success or failure"
Aug 21 15:57:21.919: INFO: Pod "pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442712ms
Aug 21 15:57:23.926: INFO: Pod "pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012627825s
Aug 21 15:57:25.930: INFO: Pod "pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017092263s
STEP: Saw pod success
Aug 21 15:57:25.931: INFO: Pod "pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655" satisfied condition "success or failure"
Aug 21 15:57:25.935: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655 container test-container: <nil>
STEP: delete the pod
Aug 21 15:57:25.967: INFO: Waiting for pod pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655 to disappear
Aug 21 15:57:25.971: INFO: Pod pod-9fc9132b-3ed8-40bc-b4ec-cfe6c7f3e655 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:57:25.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-593" for this suite.
Aug 21 15:57:32.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:32.143: INFO: namespace emptydir-593 deletion completed in 6.165603945s

• [SLOW TEST:10.319 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:57:32.145: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 15:57:32.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c" in namespace "downward-api-3730" to be "success or failure"
Aug 21 15:57:32.232: INFO: Pod "downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359738ms
Aug 21 15:57:34.238: INFO: Pod "downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010037223s
Aug 21 15:57:36.243: INFO: Pod "downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014803342s
STEP: Saw pod success
Aug 21 15:57:36.243: INFO: Pod "downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c" satisfied condition "success or failure"
Aug 21 15:57:36.246: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c container client-container: <nil>
STEP: delete the pod
Aug 21 15:57:36.278: INFO: Waiting for pod downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c to disappear
Aug 21 15:57:36.283: INFO: Pod downwardapi-volume-bf01fcb6-1598-4f5c-9f79-9015ed97ef6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:57:36.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3730" for this suite.
Aug 21 15:57:42.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:42.484: INFO: namespace downward-api-3730 deletion completed in 6.190489739s

• [SLOW TEST:10.339 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:57:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-439123e7-ca83-4006-941e-2c2e4a17df8a
STEP: Creating a pod to test consume secrets
Aug 21 15:57:43.589: INFO: Waiting up to 5m0s for pod "pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0" in namespace "secrets-4509" to be "success or failure"
Aug 21 15:57:43.595: INFO: Pod "pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.235152ms
Aug 21 15:57:45.602: INFO: Pod "pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012599854s
STEP: Saw pod success
Aug 21 15:57:45.602: INFO: Pod "pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0" satisfied condition "success or failure"
Aug 21 15:57:45.606: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:57:45.638: INFO: Waiting for pod pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0 to disappear
Aug 21 15:57:45.642: INFO: Pod pod-secrets-096d246b-82a9-423d-9d99-b3b9a1f8ced0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:57:45.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4509" for this suite.
Aug 21 15:57:55.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:55.857: INFO: namespace secrets-4509 deletion completed in 10.210765001s
STEP: Destroying namespace "secret-namespace-9573" for this suite.
Aug 21 15:58:01.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:02.072: INFO: namespace secret-namespace-9573 deletion completed in 6.214912218s

• [SLOW TEST:19.586 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:58:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 21 15:58:08.220: INFO: Pod pod-hostip-a5201e0b-4d05-4559-b438-ccd77efe2c4e has hostIP: 10.132.234.100
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:58:08.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-68" for this suite.
Aug 21 15:58:32.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:32.411: INFO: namespace pods-68 deletion completed in 24.18474776s

• [SLOW TEST:30.335 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:58:32.412: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e85461c6-139f-485d-b616-66273dce0ac6
STEP: Creating a pod to test consume secrets
Aug 21 15:58:32.508: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d" in namespace "projected-9667" to be "success or failure"
Aug 21 15:58:32.511: INFO: Pod "pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339316ms
Aug 21 15:58:34.518: INFO: Pod "pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009668066s
STEP: Saw pod success
Aug 21 15:58:34.518: INFO: Pod "pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d" satisfied condition "success or failure"
Aug 21 15:58:34.524: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:58:34.552: INFO: Waiting for pod pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d to disappear
Aug 21 15:58:34.557: INFO: Pod pod-projected-secrets-cca69814-da62-4b9c-bd22-f5e4fcf1625d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:58:34.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9667" for this suite.
Aug 21 15:58:46.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:46.745: INFO: namespace projected-9667 deletion completed in 12.180418293s

• [SLOW TEST:14.334 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:58:46.747: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 21 15:58:49.017: INFO: Waiting up to 5m0s for pod "pod-50675a7e-a28b-4fb7-9a80-384fc830e023" in namespace "emptydir-7984" to be "success or failure"
Aug 21 15:58:49.021: INFO: Pod "pod-50675a7e-a28b-4fb7-9a80-384fc830e023": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55356ms
Aug 21 15:58:51.026: INFO: Pod "pod-50675a7e-a28b-4fb7-9a80-384fc830e023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009087427s
STEP: Saw pod success
Aug 21 15:58:51.026: INFO: Pod "pod-50675a7e-a28b-4fb7-9a80-384fc830e023" satisfied condition "success or failure"
Aug 21 15:58:51.037: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-50675a7e-a28b-4fb7-9a80-384fc830e023 container test-container: <nil>
STEP: delete the pod
Aug 21 15:58:51.082: INFO: Waiting for pod pod-50675a7e-a28b-4fb7-9a80-384fc830e023 to disappear
Aug 21 15:58:51.091: INFO: Pod pod-50675a7e-a28b-4fb7-9a80-384fc830e023 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:58:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7984" for this suite.
Aug 21 15:58:59.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:59.296: INFO: namespace emptydir-7984 deletion completed in 8.191302424s

• [SLOW TEST:12.549 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:58:59.302: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-64c27f00-b53d-46bb-8b37-62a1264db1b0
STEP: Creating a pod to test consume configMaps
Aug 21 15:58:59.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d" in namespace "configmap-4439" to be "success or failure"
Aug 21 15:58:59.461: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583333ms
Aug 21 15:59:01.467: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009200749s
Aug 21 15:59:03.472: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013857575s
Aug 21 15:59:05.477: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019014283s
Aug 21 15:59:07.482: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024519118s
STEP: Saw pod success
Aug 21 15:59:07.482: INFO: Pod "pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d" satisfied condition "success or failure"
Aug 21 15:59:07.486: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:59:07.512: INFO: Waiting for pod pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d to disappear
Aug 21 15:59:07.515: INFO: Pod pod-configmaps-e30ce704-169a-4cb5-974c-15572d9a612d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:59:07.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4439" for this suite.
Aug 21 15:59:13.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:13.735: INFO: namespace configmap-4439 deletion completed in 6.21507089s

• [SLOW TEST:14.434 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:59:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-336e4458-14bc-4ecc-8cfa-be2775858ada
STEP: Creating a pod to test consume configMaps
Aug 21 15:59:14.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b" in namespace "configmap-9498" to be "success or failure"
Aug 21 15:59:14.207: INFO: Pod "pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551346ms
Aug 21 15:59:16.212: INFO: Pod "pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009075445s
Aug 21 15:59:18.217: INFO: Pod "pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013347272s
STEP: Saw pod success
Aug 21 15:59:18.217: INFO: Pod "pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b" satisfied condition "success or failure"
Aug 21 15:59:18.220: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:59:18.245: INFO: Waiting for pod pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b to disappear
Aug 21 15:59:18.248: INFO: Pod pod-configmaps-331d4d49-7025-4cdf-ad41-30284a0e8e3b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:59:18.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9498" for this suite.
Aug 21 15:59:28.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:28.433: INFO: namespace configmap-9498 deletion completed in 10.177847007s

• [SLOW TEST:14.694 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:59:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e7b4d784-dbc4-4cd2-af86-526dd670898f
STEP: Creating a pod to test consume configMaps
Aug 21 15:59:28.531: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5" in namespace "projected-3402" to be "success or failure"
Aug 21 15:59:28.535: INFO: Pod "pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657827ms
Aug 21 15:59:30.540: INFO: Pod "pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008338973s
Aug 21 15:59:32.546: INFO: Pod "pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014454629s
STEP: Saw pod success
Aug 21 15:59:32.546: INFO: Pod "pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5" satisfied condition "success or failure"
Aug 21 15:59:32.553: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:59:32.583: INFO: Waiting for pod pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5 to disappear
Aug 21 15:59:32.587: INFO: Pod pod-projected-configmaps-f1be2e0d-4250-4ce6-8aed-47f17c94d7c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 15:59:32.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3402" for this suite.
Aug 21 15:59:42.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:42.782: INFO: namespace projected-3402 deletion completed in 10.189122562s

• [SLOW TEST:14.348 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 15:59:42.783: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8747
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 21 15:59:45.183: INFO: Found 0 stateful pods, waiting for 3
Aug 21 15:59:55.188: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 15:59:55.188: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 15:59:55.188: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 21 16:00:05.190: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:00:05.190: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:00:05.190: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 21 16:00:05.229: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 21 16:00:15.283: INFO: Updating stateful set ss2
Aug 21 16:00:15.291: INFO: Waiting for Pod statefulset-8747/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:00:25.301: INFO: Waiting for Pod statefulset-8747/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 21 16:00:35.388: INFO: Found 1 stateful pods, waiting for 3
Aug 21 16:00:45.394: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:00:45.394: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:00:45.394: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 21 16:00:45.425: INFO: Updating stateful set ss2
Aug 21 16:00:45.433: INFO: Waiting for Pod statefulset-8747/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:00:55.469: INFO: Updating stateful set ss2
Aug 21 16:00:55.477: INFO: Waiting for StatefulSet statefulset-8747/ss2 to complete update
Aug 21 16:00:55.477: INFO: Waiting for Pod statefulset-8747/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 21 16:01:05.486: INFO: Deleting all statefulset in ns statefulset-8747
Aug 21 16:01:05.492: INFO: Scaling statefulset ss2 to 0
Aug 21 16:01:45.518: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:01:45.523: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:01:45.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8747" for this suite.
Aug 21 16:01:55.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:01:55.756: INFO: namespace statefulset-8747 deletion completed in 10.209764804s

• [SLOW TEST:132.972 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:01:55.756: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 21 16:01:55.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-1094'
Aug 21 16:01:56.431: INFO: stderr: ""
Aug 21 16:01:56.431: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:01:56.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:01:56.590: INFO: stderr: ""
Aug 21 16:01:56.590: INFO: stdout: "update-demo-nautilus-7np4b update-demo-nautilus-pw5m7 "
Aug 21 16:01:56.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:01:56.709: INFO: stderr: ""
Aug 21 16:01:56.709: INFO: stdout: ""
Aug 21 16:01:56.709: INFO: update-demo-nautilus-7np4b is created but not running
Aug 21 16:02:01.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:02:02.484: INFO: stderr: ""
Aug 21 16:02:02.484: INFO: stdout: "update-demo-nautilus-7np4b update-demo-nautilus-pw5m7 "
Aug 21 16:02:02.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:02.612: INFO: stderr: ""
Aug 21 16:02:02.612: INFO: stdout: "true"
Aug 21 16:02:02.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:02.713: INFO: stderr: ""
Aug 21 16:02:02.713: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:02.713: INFO: validating pod update-demo-nautilus-7np4b
Aug 21 16:02:02.721: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:02.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:02.721: INFO: update-demo-nautilus-7np4b is verified up and running
Aug 21 16:02:02.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-pw5m7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:02.859: INFO: stderr: ""
Aug 21 16:02:02.859: INFO: stdout: "true"
Aug 21 16:02:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-pw5m7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:02.968: INFO: stderr: ""
Aug 21 16:02:02.968: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:02.968: INFO: validating pod update-demo-nautilus-pw5m7
Aug 21 16:02:02.988: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:02.988: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:02.988: INFO: update-demo-nautilus-pw5m7 is verified up and running
STEP: scaling down the replication controller
Aug 21 16:02:02.991: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:02:02.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1094'
Aug 21 16:02:04.136: INFO: stderr: ""
Aug 21 16:02:04.136: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:02:04.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:02:04.259: INFO: stderr: ""
Aug 21 16:02:04.259: INFO: stdout: "update-demo-nautilus-7np4b update-demo-nautilus-pw5m7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 21 16:02:09.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:02:09.410: INFO: stderr: ""
Aug 21 16:02:09.410: INFO: stdout: "update-demo-nautilus-7np4b "
Aug 21 16:02:09.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:09.515: INFO: stderr: ""
Aug 21 16:02:09.515: INFO: stdout: "true"
Aug 21 16:02:09.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:09.606: INFO: stderr: ""
Aug 21 16:02:09.606: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:09.606: INFO: validating pod update-demo-nautilus-7np4b
Aug 21 16:02:09.612: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:09.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:09.612: INFO: update-demo-nautilus-7np4b is verified up and running
STEP: scaling up the replication controller
Aug 21 16:02:09.614: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:02:09.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1094'
Aug 21 16:02:10.768: INFO: stderr: ""
Aug 21 16:02:10.769: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:02:10.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:02:10.878: INFO: stderr: ""
Aug 21 16:02:10.878: INFO: stdout: "update-demo-nautilus-7np4b update-demo-nautilus-qph76 "
Aug 21 16:02:10.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:10.965: INFO: stderr: ""
Aug 21 16:02:10.965: INFO: stdout: "true"
Aug 21 16:02:10.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:11.078: INFO: stderr: ""
Aug 21 16:02:11.078: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:11.078: INFO: validating pod update-demo-nautilus-7np4b
Aug 21 16:02:11.084: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:11.084: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:11.084: INFO: update-demo-nautilus-7np4b is verified up and running
Aug 21 16:02:11.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-qph76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:11.193: INFO: stderr: ""
Aug 21 16:02:11.193: INFO: stdout: ""
Aug 21 16:02:11.193: INFO: update-demo-nautilus-qph76 is created but not running
Aug 21 16:02:16.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1094'
Aug 21 16:02:16.303: INFO: stderr: ""
Aug 21 16:02:16.303: INFO: stdout: "update-demo-nautilus-7np4b update-demo-nautilus-qph76 "
Aug 21 16:02:16.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:16.398: INFO: stderr: ""
Aug 21 16:02:16.398: INFO: stdout: "true"
Aug 21 16:02:16.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7np4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:16.509: INFO: stderr: ""
Aug 21 16:02:16.509: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:16.509: INFO: validating pod update-demo-nautilus-7np4b
Aug 21 16:02:16.514: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:16.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:16.514: INFO: update-demo-nautilus-7np4b is verified up and running
Aug 21 16:02:16.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-qph76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:16.606: INFO: stderr: ""
Aug 21 16:02:16.606: INFO: stdout: "true"
Aug 21 16:02:16.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-qph76 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1094'
Aug 21 16:02:16.723: INFO: stderr: ""
Aug 21 16:02:16.723: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:02:16.723: INFO: validating pod update-demo-nautilus-qph76
Aug 21 16:02:16.731: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:02:16.731: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:02:16.731: INFO: update-demo-nautilus-qph76 is verified up and running
STEP: using delete to clean up resources
Aug 21 16:02:16.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-1094'
Aug 21 16:02:17.244: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:02:17.244: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 21 16:02:17.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1094'
Aug 21 16:02:17.351: INFO: stderr: "No resources found.\n"
Aug 21 16:02:17.351: INFO: stdout: ""
Aug 21 16:02:17.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=update-demo --namespace=kubectl-1094 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:02:17.449: INFO: stderr: ""
Aug 21 16:02:17.449: INFO: stdout: "update-demo-nautilus-7np4b\nupdate-demo-nautilus-qph76\n"
Aug 21 16:02:17.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1094'
Aug 21 16:02:18.108: INFO: stderr: "No resources found.\n"
Aug 21 16:02:18.108: INFO: stdout: ""
Aug 21 16:02:18.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=update-demo --namespace=kubectl-1094 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:02:18.255: INFO: stderr: ""
Aug 21 16:02:18.255: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:02:18.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1094" for this suite.
Aug 21 16:02:26.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:02:26.449: INFO: namespace kubectl-1094 deletion completed in 8.187532301s

• [SLOW TEST:30.693 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:02:26.449: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 21 16:02:34.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:34.629: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:36.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:36.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:38.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:38.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:40.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:40.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:42.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:42.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:44.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:44.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:46.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:46.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:48.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:48.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:50.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:50.636: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:52.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:52.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:54.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:54.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:56.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:56.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:02:58.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:02:58.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:03:00.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:03:00.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:03:02.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:03:02.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:03:04.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:03:04.635: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:03:06.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:03:06.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 16:03:08.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 16:03:08.634: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:03:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-847" for this suite.
Aug 21 16:03:32.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:03:32.841: INFO: namespace container-lifecycle-hook-847 deletion completed in 24.187463116s

• [SLOW TEST:66.393 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:03:32.843: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 21 16:03:41.472: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1968 pod-service-account-c1f908f5-9a95-44c8-813b-46d2320a97f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 21 16:03:41.884: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1968 pod-service-account-c1f908f5-9a95-44c8-813b-46d2320a97f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 21 16:03:42.370: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1968 pod-service-account-c1f908f5-9a95-44c8-813b-46d2320a97f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:03:42.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1968" for this suite.
Aug 21 16:03:48.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:03:49.024: INFO: namespace svcaccounts-1968 deletion completed in 6.254937348s

• [SLOW TEST:16.181 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:03:49.024: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:03:49.206: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"eddeea57-2696-48d0-a1b4-31b07864d892", Controller:(*bool)(0xc0034aafd6), BlockOwnerDeletion:(*bool)(0xc0034aafd7)}}
Aug 21 16:03:49.229: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"05cab568-9d89-4748-9749-a6f3ec3d2ef3", Controller:(*bool)(0xc00299def6), BlockOwnerDeletion:(*bool)(0xc00299def7)}}
Aug 21 16:03:49.254: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a31fc6a9-dc54-4878-a8d3-ec237955b499", Controller:(*bool)(0xc0034ab19e), BlockOwnerDeletion:(*bool)(0xc0034ab19f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:03:54.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7979" for this suite.
Aug 21 16:04:00.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:00.717: INFO: namespace gc-7979 deletion completed in 6.443764101s

• [SLOW TEST:11.693 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:04:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:04:00.870: INFO: Creating ReplicaSet my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5
Aug 21 16:04:00.927: INFO: Pod name my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5: Found 0 pods out of 1
Aug 21 16:04:05.932: INFO: Pod name my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5: Found 1 pods out of 1
Aug 21 16:04:05.932: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5" is running
Aug 21 16:04:05.936: INFO: Pod "my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5-5fvm7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:04:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:04:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:04:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:04:00 +0000 UTC Reason: Message:}])
Aug 21 16:04:05.936: INFO: Trying to dial the pod
Aug 21 16:04:10.953: INFO: Controller my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5: Got expected result from replica 1 [my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5-5fvm7]: "my-hostname-basic-1d7d3931-f572-4519-a136-3df2382fa2c5-5fvm7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:04:10.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4186" for this suite.
Aug 21 16:04:16.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:17.182: INFO: namespace replicaset-4186 deletion completed in 6.222992201s

• [SLOW TEST:16.464 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:04:17.184: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 21 16:04:17.309: INFO: Waiting up to 5m0s for pod "var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc" in namespace "var-expansion-768" to be "success or failure"
Aug 21 16:04:17.313: INFO: Pod "var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130746ms
Aug 21 16:04:19.318: INFO: Pod "var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008670519s
Aug 21 16:04:21.322: INFO: Pod "var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013123031s
STEP: Saw pod success
Aug 21 16:04:21.322: INFO: Pod "var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc" satisfied condition "success or failure"
Aug 21 16:04:21.325: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc container dapi-container: <nil>
STEP: delete the pod
Aug 21 16:04:21.391: INFO: Waiting for pod var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc to disappear
Aug 21 16:04:21.403: INFO: Pod var-expansion-c086a5ad-3036-498c-9e37-ea41f313cdfc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:04:21.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-768" for this suite.
Aug 21 16:04:27.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:27.596: INFO: namespace var-expansion-768 deletion completed in 6.182396142s

• [SLOW TEST:10.413 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:04:27.597: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:04:28.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a" in namespace "downward-api-1324" to be "success or failure"
Aug 21 16:04:28.255: INFO: Pod "downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169904ms
Aug 21 16:04:30.260: INFO: Pod "downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893886s
Aug 21 16:04:32.265: INFO: Pod "downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014014834s
STEP: Saw pod success
Aug 21 16:04:32.265: INFO: Pod "downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a" satisfied condition "success or failure"
Aug 21 16:04:32.270: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a container client-container: <nil>
STEP: delete the pod
Aug 21 16:04:32.300: INFO: Waiting for pod downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a to disappear
Aug 21 16:04:32.304: INFO: Pod downwardapi-volume-1d05076b-ded7-43fd-b46c-ccd4d999102a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:04:32.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1324" for this suite.
Aug 21 16:04:38.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:38.510: INFO: namespace downward-api-1324 deletion completed in 6.19821905s

• [SLOW TEST:10.914 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:04:38.513: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-70625e4a-66d4-4886-bbac-47393af20b5b
STEP: Creating a pod to test consume configMaps
Aug 21 16:04:38.630: INFO: Waiting up to 5m0s for pod "pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a" in namespace "configmap-7039" to be "success or failure"
Aug 21 16:04:38.649: INFO: Pod "pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.050086ms
Aug 21 16:04:40.655: INFO: Pod "pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02473364s
STEP: Saw pod success
Aug 21 16:04:40.655: INFO: Pod "pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a" satisfied condition "success or failure"
Aug 21 16:04:40.660: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:04:40.717: INFO: Waiting for pod pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a to disappear
Aug 21 16:04:40.726: INFO: Pod pod-configmaps-09035448-1fe0-47a0-9f29-4676da34149a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:04:40.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7039" for this suite.
Aug 21 16:04:46.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:46.945: INFO: namespace configmap-7039 deletion completed in 6.199012766s

• [SLOW TEST:8.431 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:04:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 21 16:04:55.321: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:04:55.329: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:04:57.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:04:57.335: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:04:59.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:04:59.334: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:05:01.331: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:05:01.336: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:05:03.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:05:03.335: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:05:05.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:05:05.334: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:05:07.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:05:07.334: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 16:05:09.329: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 16:05:09.337: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:05:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9632" for this suite.
Aug 21 16:05:33.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:05:33.589: INFO: namespace container-lifecycle-hook-9632 deletion completed in 24.230743291s

• [SLOW TEST:46.644 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:05:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:05:57.735: INFO: Container started at 2019-08-21 16:05:38 +0000 UTC, pod became ready at 2019-08-21 16:05:56 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:05:57.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5603" for this suite.
Aug 21 16:06:21.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:06:21.948: INFO: namespace container-probe-5603 deletion completed in 24.204864527s

• [SLOW TEST:48.357 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:06:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 21 16:06:23.225: INFO: created pod pod-service-account-defaultsa
Aug 21 16:06:23.226: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 21 16:06:23.237: INFO: created pod pod-service-account-mountsa
Aug 21 16:06:23.237: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 21 16:06:23.250: INFO: created pod pod-service-account-nomountsa
Aug 21 16:06:23.250: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 21 16:06:23.259: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 21 16:06:23.260: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 21 16:06:23.266: INFO: created pod pod-service-account-mountsa-mountspec
Aug 21 16:06:23.266: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 21 16:06:23.273: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 21 16:06:23.273: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 21 16:06:23.281: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 21 16:06:23.281: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 21 16:06:23.286: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 21 16:06:23.287: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 21 16:06:23.292: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 21 16:06:23.292: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:06:23.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9214" for this suite.
Aug 21 16:06:49.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:06:49.482: INFO: namespace svcaccounts-9214 deletion completed in 26.184200931s

• [SLOW TEST:27.533 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:06:49.482: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 21 16:06:49.591: INFO: Waiting up to 5m0s for pod "pod-473438a4-31be-4474-91c8-43dcaaa219c7" in namespace "emptydir-992" to be "success or failure"
Aug 21 16:06:49.601: INFO: Pod "pod-473438a4-31be-4474-91c8-43dcaaa219c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.220655ms
Aug 21 16:06:51.648: INFO: Pod "pod-473438a4-31be-4474-91c8-43dcaaa219c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056996927s
STEP: Saw pod success
Aug 21 16:06:51.648: INFO: Pod "pod-473438a4-31be-4474-91c8-43dcaaa219c7" satisfied condition "success or failure"
Aug 21 16:06:51.664: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-473438a4-31be-4474-91c8-43dcaaa219c7 container test-container: <nil>
STEP: delete the pod
Aug 21 16:06:51.729: INFO: Waiting for pod pod-473438a4-31be-4474-91c8-43dcaaa219c7 to disappear
Aug 21 16:06:51.734: INFO: Pod pod-473438a4-31be-4474-91c8-43dcaaa219c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:06:51.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-992" for this suite.
Aug 21 16:06:57.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:06:57.956: INFO: namespace emptydir-992 deletion completed in 6.215283097s

• [SLOW TEST:8.474 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:06:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8726
I0821 16:07:02.340181      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8726, replica count: 1
I0821 16:07:03.390798      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:07:04.391103      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:07:05.391317      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 16:07:05.507: INFO: Created: latency-svc-fsfz6
Aug 21 16:07:05.518: INFO: Got endpoints: latency-svc-fsfz6 [26.315987ms]
Aug 21 16:07:05.554: INFO: Created: latency-svc-zmf97
Aug 21 16:07:05.557: INFO: Got endpoints: latency-svc-zmf97 [39.268352ms]
Aug 21 16:07:05.580: INFO: Created: latency-svc-fjntm
Aug 21 16:07:05.589: INFO: Got endpoints: latency-svc-fjntm [70.827078ms]
Aug 21 16:07:05.602: INFO: Created: latency-svc-vtcxr
Aug 21 16:07:05.613: INFO: Got endpoints: latency-svc-vtcxr [94.284467ms]
Aug 21 16:07:05.618: INFO: Created: latency-svc-c5jjf
Aug 21 16:07:05.636: INFO: Got endpoints: latency-svc-c5jjf [116.542585ms]
Aug 21 16:07:05.642: INFO: Created: latency-svc-hdr9f
Aug 21 16:07:05.656: INFO: Got endpoints: latency-svc-hdr9f [136.608673ms]
Aug 21 16:07:05.665: INFO: Created: latency-svc-gfpbd
Aug 21 16:07:05.683: INFO: Got endpoints: latency-svc-gfpbd [163.963278ms]
Aug 21 16:07:05.689: INFO: Created: latency-svc-2s9k6
Aug 21 16:07:05.697: INFO: Got endpoints: latency-svc-2s9k6 [177.603732ms]
Aug 21 16:07:05.711: INFO: Created: latency-svc-qjj6d
Aug 21 16:07:05.725: INFO: Got endpoints: latency-svc-qjj6d [205.557705ms]
Aug 21 16:07:05.738: INFO: Created: latency-svc-f7rgb
Aug 21 16:07:05.766: INFO: Created: latency-svc-pjfxf
Aug 21 16:07:05.766: INFO: Got endpoints: latency-svc-f7rgb [246.816944ms]
Aug 21 16:07:05.807: INFO: Created: latency-svc-kzhx5
Aug 21 16:07:05.841: INFO: Created: latency-svc-lxcr5
Aug 21 16:07:05.861: INFO: Got endpoints: latency-svc-pjfxf [341.525295ms]
Aug 21 16:07:05.862: INFO: Got endpoints: latency-svc-kzhx5 [342.085576ms]
Aug 21 16:07:05.870: INFO: Got endpoints: latency-svc-lxcr5 [350.324724ms]
Aug 21 16:07:05.874: INFO: Created: latency-svc-d2q28
Aug 21 16:07:05.891: INFO: Got endpoints: latency-svc-d2q28 [371.375783ms]
Aug 21 16:07:05.901: INFO: Created: latency-svc-8dkrr
Aug 21 16:07:05.916: INFO: Got endpoints: latency-svc-8dkrr [396.029433ms]
Aug 21 16:07:05.936: INFO: Created: latency-svc-sz79v
Aug 21 16:07:05.955: INFO: Created: latency-svc-g6dgj
Aug 21 16:07:05.961: INFO: Got endpoints: latency-svc-sz79v [441.33322ms]
Aug 21 16:07:06.004: INFO: Created: latency-svc-7ghft
Aug 21 16:07:06.019: INFO: Got endpoints: latency-svc-g6dgj [461.78686ms]
Aug 21 16:07:06.024: INFO: Got endpoints: latency-svc-7ghft [434.679138ms]
Aug 21 16:07:06.037: INFO: Created: latency-svc-ddh89
Aug 21 16:07:06.049: INFO: Got endpoints: latency-svc-ddh89 [436.203757ms]
Aug 21 16:07:06.059: INFO: Created: latency-svc-pvj5f
Aug 21 16:07:06.071: INFO: Got endpoints: latency-svc-pvj5f [434.88059ms]
Aug 21 16:07:06.082: INFO: Created: latency-svc-2mbd7
Aug 21 16:07:06.102: INFO: Got endpoints: latency-svc-2mbd7 [446.350574ms]
Aug 21 16:07:06.118: INFO: Created: latency-svc-p6n4p
Aug 21 16:07:06.129: INFO: Got endpoints: latency-svc-p6n4p [445.442637ms]
Aug 21 16:07:06.131: INFO: Created: latency-svc-k5xbr
Aug 21 16:07:06.154: INFO: Created: latency-svc-hhr4d
Aug 21 16:07:06.167: INFO: Got endpoints: latency-svc-k5xbr [469.629918ms]
Aug 21 16:07:06.175: INFO: Got endpoints: latency-svc-hhr4d [449.780628ms]
Aug 21 16:07:06.197: INFO: Created: latency-svc-d5b2c
Aug 21 16:07:06.213: INFO: Got endpoints: latency-svc-d5b2c [446.967338ms]
Aug 21 16:07:06.224: INFO: Created: latency-svc-qpv46
Aug 21 16:07:06.235: INFO: Got endpoints: latency-svc-qpv46 [373.807223ms]
Aug 21 16:07:06.254: INFO: Created: latency-svc-rswj6
Aug 21 16:07:06.277: INFO: Got endpoints: latency-svc-rswj6 [415.596542ms]
Aug 21 16:07:06.291: INFO: Created: latency-svc-8rpft
Aug 21 16:07:06.305: INFO: Got endpoints: latency-svc-8rpft [435.389434ms]
Aug 21 16:07:06.317: INFO: Created: latency-svc-9lz8v
Aug 21 16:07:06.323: INFO: Got endpoints: latency-svc-9lz8v [432.113915ms]
Aug 21 16:07:06.337: INFO: Created: latency-svc-2mv7p
Aug 21 16:07:06.347: INFO: Got endpoints: latency-svc-2mv7p [431.673188ms]
Aug 21 16:07:06.357: INFO: Created: latency-svc-pnhxn
Aug 21 16:07:06.374: INFO: Got endpoints: latency-svc-pnhxn [412.526391ms]
Aug 21 16:07:06.381: INFO: Created: latency-svc-znhfd
Aug 21 16:07:06.415: INFO: Created: latency-svc-ddkmd
Aug 21 16:07:06.444: INFO: Got endpoints: latency-svc-znhfd [424.520148ms]
Aug 21 16:07:06.451: INFO: Created: latency-svc-dk56s
Aug 21 16:07:06.474: INFO: Got endpoints: latency-svc-ddkmd [449.792311ms]
Aug 21 16:07:06.482: INFO: Created: latency-svc-6q9h4
Aug 21 16:07:06.503: INFO: Created: latency-svc-pd9rh
Aug 21 16:07:06.505: INFO: Got endpoints: latency-svc-dk56s [455.664545ms]
Aug 21 16:07:06.517: INFO: Created: latency-svc-khhl4
Aug 21 16:07:06.543: INFO: Created: latency-svc-4cdgv
Aug 21 16:07:06.558: INFO: Got endpoints: latency-svc-pd9rh [455.173214ms]
Aug 21 16:07:06.558: INFO: Got endpoints: latency-svc-6q9h4 [487.362979ms]
Aug 21 16:07:06.559: INFO: Got endpoints: latency-svc-khhl4 [429.614101ms]
Aug 21 16:07:06.559: INFO: Got endpoints: latency-svc-4cdgv [391.759931ms]
Aug 21 16:07:06.564: INFO: Created: latency-svc-8wtjc
Aug 21 16:07:06.580: INFO: Got endpoints: latency-svc-8wtjc [404.647113ms]
Aug 21 16:07:06.590: INFO: Created: latency-svc-nkkzd
Aug 21 16:07:06.614: INFO: Got endpoints: latency-svc-nkkzd [399.500409ms]
Aug 21 16:07:06.615: INFO: Created: latency-svc-7zkdt
Aug 21 16:07:06.642: INFO: Got endpoints: latency-svc-7zkdt [407.103418ms]
Aug 21 16:07:06.655: INFO: Created: latency-svc-455zd
Aug 21 16:07:06.675: INFO: Got endpoints: latency-svc-455zd [397.095374ms]
Aug 21 16:07:06.678: INFO: Created: latency-svc-5f6vr
Aug 21 16:07:06.693: INFO: Got endpoints: latency-svc-5f6vr [388.044988ms]
Aug 21 16:07:06.698: INFO: Created: latency-svc-9tpmh
Aug 21 16:07:06.714: INFO: Got endpoints: latency-svc-9tpmh [390.925238ms]
Aug 21 16:07:06.716: INFO: Created: latency-svc-8sxr5
Aug 21 16:07:06.724: INFO: Got endpoints: latency-svc-8sxr5 [376.485897ms]
Aug 21 16:07:06.746: INFO: Created: latency-svc-2pxhr
Aug 21 16:07:06.845: INFO: Got endpoints: latency-svc-2pxhr [471.097892ms]
Aug 21 16:07:06.861: INFO: Created: latency-svc-6t85m
Aug 21 16:07:06.872: INFO: Got endpoints: latency-svc-6t85m [428.398706ms]
Aug 21 16:07:06.959: INFO: Created: latency-svc-v8k82
Aug 21 16:07:06.971: INFO: Got endpoints: latency-svc-v8k82 [496.999935ms]
Aug 21 16:07:06.987: INFO: Created: latency-svc-z2nn7
Aug 21 16:07:07.005: INFO: Created: latency-svc-bxlbj
Aug 21 16:07:07.007: INFO: Got endpoints: latency-svc-z2nn7 [502.056447ms]
Aug 21 16:07:07.022: INFO: Created: latency-svc-tpv8z
Aug 21 16:07:07.044: INFO: Created: latency-svc-h9wps
Aug 21 16:07:07.055: INFO: Got endpoints: latency-svc-bxlbj [496.718885ms]
Aug 21 16:07:07.060: INFO: Got endpoints: latency-svc-tpv8z [501.035087ms]
Aug 21 16:07:07.068: INFO: Got endpoints: latency-svc-h9wps [509.407192ms]
Aug 21 16:07:07.080: INFO: Created: latency-svc-xggpq
Aug 21 16:07:07.101: INFO: Got endpoints: latency-svc-xggpq [541.613609ms]
Aug 21 16:07:07.106: INFO: Created: latency-svc-dw9f9
Aug 21 16:07:07.132: INFO: Got endpoints: latency-svc-dw9f9 [552.627206ms]
Aug 21 16:07:07.143: INFO: Created: latency-svc-7hkl8
Aug 21 16:07:07.154: INFO: Got endpoints: latency-svc-7hkl8 [540.266832ms]
Aug 21 16:07:07.180: INFO: Created: latency-svc-mz2zp
Aug 21 16:07:07.205: INFO: Created: latency-svc-zf6hz
Aug 21 16:07:07.228: INFO: Created: latency-svc-78bpd
Aug 21 16:07:07.242: INFO: Got endpoints: latency-svc-mz2zp [600.111488ms]
Aug 21 16:07:07.243: INFO: Created: latency-svc-m8mmw
Aug 21 16:07:07.261: INFO: Got endpoints: latency-svc-zf6hz [586.755696ms]
Aug 21 16:07:07.268: INFO: Created: latency-svc-mmtk5
Aug 21 16:07:07.275: INFO: Got endpoints: latency-svc-78bpd [580.774507ms]
Aug 21 16:07:07.290: INFO: Got endpoints: latency-svc-m8mmw [575.803525ms]
Aug 21 16:07:07.303: INFO: Created: latency-svc-8w7wc
Aug 21 16:07:07.306: INFO: Got endpoints: latency-svc-mmtk5 [582.036054ms]
Aug 21 16:07:07.323: INFO: Created: latency-svc-pbn8b
Aug 21 16:07:07.339: INFO: Got endpoints: latency-svc-pbn8b [466.244602ms]
Aug 21 16:07:07.339: INFO: Got endpoints: latency-svc-8w7wc [494.099505ms]
Aug 21 16:07:07.340: INFO: Created: latency-svc-kc9s4
Aug 21 16:07:07.360: INFO: Got endpoints: latency-svc-kc9s4 [388.648533ms]
Aug 21 16:07:07.365: INFO: Created: latency-svc-9kn9k
Aug 21 16:07:07.388: INFO: Got endpoints: latency-svc-9kn9k [380.828902ms]
Aug 21 16:07:07.407: INFO: Created: latency-svc-n54pb
Aug 21 16:07:07.417: INFO: Got endpoints: latency-svc-n54pb [361.835568ms]
Aug 21 16:07:07.425: INFO: Created: latency-svc-mfzh5
Aug 21 16:07:07.444: INFO: Got endpoints: latency-svc-mfzh5 [384.301619ms]
Aug 21 16:07:07.449: INFO: Created: latency-svc-hf82h
Aug 21 16:07:07.474: INFO: Created: latency-svc-m75h4
Aug 21 16:07:07.501: INFO: Created: latency-svc-xq8dh
Aug 21 16:07:07.519: INFO: Created: latency-svc-qcx6s
Aug 21 16:07:07.524: INFO: Got endpoints: latency-svc-hf82h [455.578608ms]
Aug 21 16:07:07.564: INFO: Created: latency-svc-k57rx
Aug 21 16:07:07.597: INFO: Created: latency-svc-n6m8b
Aug 21 16:07:07.636: INFO: Created: latency-svc-xlrj5
Aug 21 16:07:07.642: INFO: Got endpoints: latency-svc-xq8dh [509.919929ms]
Aug 21 16:07:07.643: INFO: Got endpoints: latency-svc-m75h4 [542.230195ms]
Aug 21 16:07:07.644: INFO: Got endpoints: latency-svc-qcx6s [489.063798ms]
Aug 21 16:07:07.644: INFO: Got endpoints: latency-svc-k57rx [400.951184ms]
Aug 21 16:07:07.650: INFO: Got endpoints: latency-svc-n6m8b [388.353466ms]
Aug 21 16:07:07.658: INFO: Created: latency-svc-ft9nr
Aug 21 16:07:07.684: INFO: Got endpoints: latency-svc-xlrj5 [409.018556ms]
Aug 21 16:07:07.690: INFO: Created: latency-svc-w8h4j
Aug 21 16:07:07.729: INFO: Got endpoints: latency-svc-ft9nr [438.033311ms]
Aug 21 16:07:07.743: INFO: Created: latency-svc-bm4fr
Aug 21 16:07:07.763: INFO: Created: latency-svc-pdzvn
Aug 21 16:07:07.765: INFO: Got endpoints: latency-svc-w8h4j [458.020548ms]
Aug 21 16:07:07.789: INFO: Created: latency-svc-8ltxq
Aug 21 16:07:07.817: INFO: Got endpoints: latency-svc-bm4fr [478.800684ms]
Aug 21 16:07:07.830: INFO: Created: latency-svc-cdjsx
Aug 21 16:07:07.854: INFO: Created: latency-svc-f7kbq
Aug 21 16:07:07.864: INFO: Got endpoints: latency-svc-pdzvn [525.254198ms]
Aug 21 16:07:07.886: INFO: Created: latency-svc-w5sz6
Aug 21 16:07:07.886: INFO: Created: latency-svc-vfvvj
Aug 21 16:07:07.908: INFO: Created: latency-svc-6htl6
Aug 21 16:07:07.924: INFO: Created: latency-svc-rgxpx
Aug 21 16:07:07.934: INFO: Got endpoints: latency-svc-8ltxq [573.769821ms]
Aug 21 16:07:07.944: INFO: Created: latency-svc-28qpp
Aug 21 16:07:07.967: INFO: Got endpoints: latency-svc-cdjsx [578.207549ms]
Aug 21 16:07:07.977: INFO: Created: latency-svc-59mfn
Aug 21 16:07:07.999: INFO: Created: latency-svc-zgkpg
Aug 21 16:07:08.018: INFO: Got endpoints: latency-svc-f7kbq [601.304597ms]
Aug 21 16:07:08.024: INFO: Created: latency-svc-c2hgl
Aug 21 16:07:08.049: INFO: Created: latency-svc-rb22c
Aug 21 16:07:08.064: INFO: Created: latency-svc-77s97
Aug 21 16:07:08.068: INFO: Got endpoints: latency-svc-vfvvj [623.87333ms]
Aug 21 16:07:08.086: INFO: Created: latency-svc-4sp6p
Aug 21 16:07:08.103: INFO: Created: latency-svc-t52s6
Aug 21 16:07:08.126: INFO: Got endpoints: latency-svc-w5sz6 [602.083954ms]
Aug 21 16:07:08.138: INFO: Created: latency-svc-kjzmm
Aug 21 16:07:08.167: INFO: Got endpoints: latency-svc-6htl6 [524.49185ms]
Aug 21 16:07:08.178: INFO: Created: latency-svc-sfbdh
Aug 21 16:07:08.199: INFO: Created: latency-svc-srlqd
Aug 21 16:07:08.217: INFO: Created: latency-svc-rz4vn
Aug 21 16:07:08.225: INFO: Got endpoints: latency-svc-rgxpx [580.870401ms]
Aug 21 16:07:08.236: INFO: Created: latency-svc-npc8s
Aug 21 16:07:08.264: INFO: Created: latency-svc-ppm27
Aug 21 16:07:08.288: INFO: Got endpoints: latency-svc-28qpp [643.392246ms]
Aug 21 16:07:08.296: INFO: Created: latency-svc-jnz2d
Aug 21 16:07:08.335: INFO: Created: latency-svc-jc6ln
Aug 21 16:07:08.418: INFO: Got endpoints: latency-svc-59mfn [773.968113ms]
Aug 21 16:07:08.418: INFO: Got endpoints: latency-svc-zgkpg [768.016208ms]
Aug 21 16:07:08.419: INFO: Got endpoints: latency-svc-c2hgl [734.77423ms]
Aug 21 16:07:08.451: INFO: Created: latency-svc-dhfrf
Aug 21 16:07:08.470: INFO: Got endpoints: latency-svc-rb22c [741.708776ms]
Aug 21 16:07:08.475: INFO: Created: latency-svc-48w2m
Aug 21 16:07:08.509: INFO: Created: latency-svc-tk8dr
Aug 21 16:07:08.530: INFO: Got endpoints: latency-svc-77s97 [765.096243ms]
Aug 21 16:07:08.539: INFO: Created: latency-svc-j6rjj
Aug 21 16:07:08.558: INFO: Created: latency-svc-p9hwm
Aug 21 16:07:08.568: INFO: Got endpoints: latency-svc-4sp6p [750.425586ms]
Aug 21 16:07:08.594: INFO: Created: latency-svc-5zxww
Aug 21 16:07:08.616: INFO: Got endpoints: latency-svc-t52s6 [751.308481ms]
Aug 21 16:07:08.643: INFO: Created: latency-svc-d4gfk
Aug 21 16:07:08.680: INFO: Got endpoints: latency-svc-kjzmm [746.013393ms]
Aug 21 16:07:08.789: INFO: Got endpoints: latency-svc-sfbdh [821.925065ms]
Aug 21 16:07:08.791: INFO: Got endpoints: latency-svc-srlqd [772.997287ms]
Aug 21 16:07:08.795: INFO: Created: latency-svc-5jjp2
Aug 21 16:07:08.820: INFO: Created: latency-svc-h6q48
Aug 21 16:07:08.821: INFO: Got endpoints: latency-svc-rz4vn [752.346669ms]
Aug 21 16:07:08.841: INFO: Created: latency-svc-hb6pw
Aug 21 16:07:08.862: INFO: Created: latency-svc-l7j9w
Aug 21 16:07:08.871: INFO: Got endpoints: latency-svc-npc8s [743.873441ms]
Aug 21 16:07:08.911: INFO: Created: latency-svc-jfspf
Aug 21 16:07:08.917: INFO: Got endpoints: latency-svc-ppm27 [749.646441ms]
Aug 21 16:07:08.948: INFO: Created: latency-svc-jtphh
Aug 21 16:07:08.964: INFO: Got endpoints: latency-svc-jnz2d [738.665697ms]
Aug 21 16:07:08.992: INFO: Created: latency-svc-wdq96
Aug 21 16:07:09.017: INFO: Got endpoints: latency-svc-jc6ln [729.387965ms]
Aug 21 16:07:09.061: INFO: Created: latency-svc-77xcz
Aug 21 16:07:09.103: INFO: Got endpoints: latency-svc-dhfrf [685.144699ms]
Aug 21 16:07:09.128: INFO: Got endpoints: latency-svc-48w2m [709.816488ms]
Aug 21 16:07:09.135: INFO: Created: latency-svc-gq4xt
Aug 21 16:07:09.161: INFO: Created: latency-svc-t9pk4
Aug 21 16:07:09.167: INFO: Got endpoints: latency-svc-tk8dr [747.889827ms]
Aug 21 16:07:09.191: INFO: Created: latency-svc-wgzws
Aug 21 16:07:09.217: INFO: Got endpoints: latency-svc-j6rjj [746.365706ms]
Aug 21 16:07:09.243: INFO: Created: latency-svc-6m8n8
Aug 21 16:07:09.266: INFO: Got endpoints: latency-svc-p9hwm [736.010263ms]
Aug 21 16:07:09.370: INFO: Created: latency-svc-kn94l
Aug 21 16:07:09.390: INFO: Got endpoints: latency-svc-5zxww [821.956799ms]
Aug 21 16:07:09.393: INFO: Got endpoints: latency-svc-d4gfk [777.285069ms]
Aug 21 16:07:09.420: INFO: Created: latency-svc-gn9pm
Aug 21 16:07:09.422: INFO: Got endpoints: latency-svc-5jjp2 [741.177973ms]
Aug 21 16:07:09.447: INFO: Created: latency-svc-hmtkx
Aug 21 16:07:09.469: INFO: Got endpoints: latency-svc-h6q48 [679.747304ms]
Aug 21 16:07:09.476: INFO: Created: latency-svc-dv7xc
Aug 21 16:07:09.493: INFO: Created: latency-svc-qp5tb
Aug 21 16:07:09.523: INFO: Got endpoints: latency-svc-hb6pw [730.9564ms]
Aug 21 16:07:09.551: INFO: Created: latency-svc-ztv4g
Aug 21 16:07:09.567: INFO: Got endpoints: latency-svc-l7j9w [745.372703ms]
Aug 21 16:07:09.590: INFO: Created: latency-svc-z5zr7
Aug 21 16:07:09.621: INFO: Got endpoints: latency-svc-jfspf [749.695731ms]
Aug 21 16:07:09.643: INFO: Created: latency-svc-kv8r4
Aug 21 16:07:09.665: INFO: Got endpoints: latency-svc-jtphh [747.924452ms]
Aug 21 16:07:09.690: INFO: Created: latency-svc-wvgdc
Aug 21 16:07:09.715: INFO: Got endpoints: latency-svc-wdq96 [751.392192ms]
Aug 21 16:07:09.738: INFO: Created: latency-svc-dt574
Aug 21 16:07:09.765: INFO: Got endpoints: latency-svc-77xcz [748.373172ms]
Aug 21 16:07:09.797: INFO: Created: latency-svc-97qd2
Aug 21 16:07:09.820: INFO: Got endpoints: latency-svc-gq4xt [717.182219ms]
Aug 21 16:07:09.847: INFO: Created: latency-svc-gg7pg
Aug 21 16:07:09.868: INFO: Got endpoints: latency-svc-t9pk4 [740.238351ms]
Aug 21 16:07:09.904: INFO: Created: latency-svc-v24db
Aug 21 16:07:09.915: INFO: Got endpoints: latency-svc-wgzws [748.230795ms]
Aug 21 16:07:09.938: INFO: Created: latency-svc-hw65l
Aug 21 16:07:09.978: INFO: Got endpoints: latency-svc-6m8n8 [761.068712ms]
Aug 21 16:07:10.003: INFO: Created: latency-svc-dmclh
Aug 21 16:07:10.027: INFO: Got endpoints: latency-svc-kn94l [760.197737ms]
Aug 21 16:07:10.060: INFO: Created: latency-svc-xcfdv
Aug 21 16:07:10.067: INFO: Got endpoints: latency-svc-gn9pm [676.185082ms]
Aug 21 16:07:10.101: INFO: Created: latency-svc-jvz85
Aug 21 16:07:10.174: INFO: Got endpoints: latency-svc-hmtkx [780.802071ms]
Aug 21 16:07:10.176: INFO: Got endpoints: latency-svc-dv7xc [754.287721ms]
Aug 21 16:07:10.209: INFO: Created: latency-svc-jlfq7
Aug 21 16:07:10.222: INFO: Got endpoints: latency-svc-qp5tb [752.645137ms]
Aug 21 16:07:10.229: INFO: Created: latency-svc-mz7ns
Aug 21 16:07:10.505: INFO: Got endpoints: latency-svc-ztv4g [982.047541ms]
Aug 21 16:07:10.506: INFO: Got endpoints: latency-svc-kv8r4 [885.042612ms]
Aug 21 16:07:10.506: INFO: Got endpoints: latency-svc-z5zr7 [939.154101ms]
Aug 21 16:07:10.507: INFO: Got endpoints: latency-svc-dt574 [791.249843ms]
Aug 21 16:07:10.507: INFO: Got endpoints: latency-svc-wvgdc [841.58982ms]
Aug 21 16:07:10.512: INFO: Created: latency-svc-k9lkz
Aug 21 16:07:10.528: INFO: Got endpoints: latency-svc-97qd2 [762.599942ms]
Aug 21 16:07:10.537: INFO: Created: latency-svc-zvttd
Aug 21 16:07:10.570: INFO: Created: latency-svc-f5hg8
Aug 21 16:07:10.570: INFO: Got endpoints: latency-svc-gg7pg [749.95703ms]
Aug 21 16:07:10.585: INFO: Created: latency-svc-mv9m8
Aug 21 16:07:10.607: INFO: Created: latency-svc-gz4sp
Aug 21 16:07:10.626: INFO: Got endpoints: latency-svc-v24db [757.991101ms]
Aug 21 16:07:10.631: INFO: Created: latency-svc-9hpvq
Aug 21 16:07:10.649: INFO: Created: latency-svc-kfnk2
Aug 21 16:07:10.672: INFO: Got endpoints: latency-svc-hw65l [756.854372ms]
Aug 21 16:07:10.675: INFO: Created: latency-svc-spsk6
Aug 21 16:07:10.701: INFO: Created: latency-svc-vmgl2
Aug 21 16:07:10.720: INFO: Created: latency-svc-btr88
Aug 21 16:07:10.720: INFO: Got endpoints: latency-svc-dmclh [742.301219ms]
Aug 21 16:07:10.751: INFO: Created: latency-svc-cfg6s
Aug 21 16:07:10.769: INFO: Got endpoints: latency-svc-xcfdv [741.934203ms]
Aug 21 16:07:10.814: INFO: Created: latency-svc-rj8gx
Aug 21 16:07:10.862: INFO: Got endpoints: latency-svc-jvz85 [795.895ms]
Aug 21 16:07:10.888: INFO: Got endpoints: latency-svc-jlfq7 [713.461819ms]
Aug 21 16:07:10.894: INFO: Created: latency-svc-k5flm
Aug 21 16:07:10.921: INFO: Created: latency-svc-fxzd8
Aug 21 16:07:10.931: INFO: Got endpoints: latency-svc-mz7ns [754.558305ms]
Aug 21 16:07:10.964: INFO: Created: latency-svc-vg8kw
Aug 21 16:07:10.965: INFO: Got endpoints: latency-svc-k9lkz [742.961251ms]
Aug 21 16:07:11.004: INFO: Created: latency-svc-4ddj9
Aug 21 16:07:11.017: INFO: Got endpoints: latency-svc-zvttd [510.421757ms]
Aug 21 16:07:11.071: INFO: Got endpoints: latency-svc-f5hg8 [564.775524ms]
Aug 21 16:07:11.071: INFO: Created: latency-svc-cz7js
Aug 21 16:07:11.094: INFO: Created: latency-svc-99lvf
Aug 21 16:07:11.114: INFO: Got endpoints: latency-svc-mv9m8 [609.165192ms]
Aug 21 16:07:11.179: INFO: Got endpoints: latency-svc-gz4sp [672.285301ms]
Aug 21 16:07:11.185: INFO: Created: latency-svc-bnxbh
Aug 21 16:07:11.231: INFO: Got endpoints: latency-svc-9hpvq [723.909783ms]
Aug 21 16:07:11.240: INFO: Created: latency-svc-6xwq7
Aug 21 16:07:11.304: INFO: Got endpoints: latency-svc-kfnk2 [775.51239ms]
Aug 21 16:07:11.322: INFO: Created: latency-svc-sqrcz
Aug 21 16:07:11.326: INFO: Got endpoints: latency-svc-spsk6 [755.146512ms]
Aug 21 16:07:11.348: INFO: Created: latency-svc-fdrqr
Aug 21 16:07:11.381: INFO: Got endpoints: latency-svc-vmgl2 [754.189528ms]
Aug 21 16:07:11.398: INFO: Created: latency-svc-rp8l9
Aug 21 16:07:11.430: INFO: Got endpoints: latency-svc-btr88 [757.428065ms]
Aug 21 16:07:11.446: INFO: Created: latency-svc-fk77h
Aug 21 16:07:11.479: INFO: Got endpoints: latency-svc-cfg6s [758.101315ms]
Aug 21 16:07:11.480: INFO: Created: latency-svc-dt66l
Aug 21 16:07:11.521: INFO: Created: latency-svc-t48nk
Aug 21 16:07:11.578: INFO: Got endpoints: latency-svc-k5flm [714.641406ms]
Aug 21 16:07:11.578: INFO: Got endpoints: latency-svc-rj8gx [809.110379ms]
Aug 21 16:07:11.608: INFO: Created: latency-svc-zn925
Aug 21 16:07:11.623: INFO: Got endpoints: latency-svc-fxzd8 [735.286691ms]
Aug 21 16:07:11.624: INFO: Created: latency-svc-mdg9v
Aug 21 16:07:11.743: INFO: Got endpoints: latency-svc-vg8kw [812.434783ms]
Aug 21 16:07:11.747: INFO: Got endpoints: latency-svc-4ddj9 [782.195082ms]
Aug 21 16:07:11.753: INFO: Created: latency-svc-ww72v
Aug 21 16:07:11.776: INFO: Got endpoints: latency-svc-cz7js [759.096219ms]
Aug 21 16:07:11.777: INFO: Created: latency-svc-nj7lp
Aug 21 16:07:11.807: INFO: Created: latency-svc-427n5
Aug 21 16:07:11.820: INFO: Got endpoints: latency-svc-99lvf [748.993412ms]
Aug 21 16:07:11.827: INFO: Created: latency-svc-fgrtp
Aug 21 16:07:11.861: INFO: Created: latency-svc-tnlqp
Aug 21 16:07:11.866: INFO: Got endpoints: latency-svc-bnxbh [751.822113ms]
Aug 21 16:07:11.902: INFO: Created: latency-svc-z95gp
Aug 21 16:07:11.915: INFO: Got endpoints: latency-svc-6xwq7 [736.133532ms]
Aug 21 16:07:11.955: INFO: Created: latency-svc-pmzvz
Aug 21 16:07:11.965: INFO: Got endpoints: latency-svc-sqrcz [733.876297ms]
Aug 21 16:07:11.986: INFO: Created: latency-svc-h6kbq
Aug 21 16:07:12.017: INFO: Got endpoints: latency-svc-fdrqr [712.774825ms]
Aug 21 16:07:12.054: INFO: Created: latency-svc-bnmvp
Aug 21 16:07:12.065: INFO: Got endpoints: latency-svc-rp8l9 [739.615677ms]
Aug 21 16:07:12.101: INFO: Created: latency-svc-f2n5r
Aug 21 16:07:12.119: INFO: Got endpoints: latency-svc-fk77h [738.234068ms]
Aug 21 16:07:12.159: INFO: Created: latency-svc-lkdzt
Aug 21 16:07:12.317: INFO: Got endpoints: latency-svc-dt66l [886.604989ms]
Aug 21 16:07:12.320: INFO: Got endpoints: latency-svc-t48nk [840.400892ms]
Aug 21 16:07:12.331: INFO: Got endpoints: latency-svc-zn925 [752.031529ms]
Aug 21 16:07:12.332: INFO: Got endpoints: latency-svc-mdg9v [753.973332ms]
Aug 21 16:07:12.360: INFO: Created: latency-svc-6vb9j
Aug 21 16:07:12.379: INFO: Got endpoints: latency-svc-ww72v [755.361393ms]
Aug 21 16:07:12.379: INFO: Created: latency-svc-n52ww
Aug 21 16:07:12.410: INFO: Created: latency-svc-hjj2n
Aug 21 16:07:12.428: INFO: Created: latency-svc-pwjcj
Aug 21 16:07:12.447: INFO: Got endpoints: latency-svc-nj7lp [703.512964ms]
Aug 21 16:07:12.448: INFO: Created: latency-svc-qrh4p
Aug 21 16:07:12.483: INFO: Got endpoints: latency-svc-427n5 [735.491893ms]
Aug 21 16:07:12.485: INFO: Created: latency-svc-88hg7
Aug 21 16:07:12.511: INFO: Created: latency-svc-sjgpq
Aug 21 16:07:12.525: INFO: Got endpoints: latency-svc-fgrtp [749.155826ms]
Aug 21 16:07:12.546: INFO: Created: latency-svc-xcf9s
Aug 21 16:07:12.572: INFO: Got endpoints: latency-svc-tnlqp [751.842529ms]
Aug 21 16:07:12.614: INFO: Created: latency-svc-6msx5
Aug 21 16:07:12.622: INFO: Got endpoints: latency-svc-z95gp [755.59504ms]
Aug 21 16:07:12.657: INFO: Created: latency-svc-9l49f
Aug 21 16:07:12.679: INFO: Got endpoints: latency-svc-pmzvz [763.40772ms]
Aug 21 16:07:12.724: INFO: Created: latency-svc-6ljlf
Aug 21 16:07:12.739: INFO: Got endpoints: latency-svc-h6kbq [774.137639ms]
Aug 21 16:07:12.768: INFO: Created: latency-svc-r5fnm
Aug 21 16:07:12.778: INFO: Got endpoints: latency-svc-bnmvp [761.200934ms]
Aug 21 16:07:12.803: INFO: Created: latency-svc-wjdjz
Aug 21 16:07:12.821: INFO: Got endpoints: latency-svc-f2n5r [756.113826ms]
Aug 21 16:07:12.870: INFO: Created: latency-svc-jw78g
Aug 21 16:07:12.877: INFO: Got endpoints: latency-svc-lkdzt [757.994842ms]
Aug 21 16:07:12.909: INFO: Created: latency-svc-f6xmd
Aug 21 16:07:13.055: INFO: Got endpoints: latency-svc-6vb9j [738.680804ms]
Aug 21 16:07:13.059: INFO: Got endpoints: latency-svc-n52ww [739.30837ms]
Aug 21 16:07:13.062: INFO: Got endpoints: latency-svc-hjj2n [731.133131ms]
Aug 21 16:07:13.074: INFO: Got endpoints: latency-svc-pwjcj [741.968769ms]
Aug 21 16:07:13.092: INFO: Created: latency-svc-88gmb
Aug 21 16:07:13.115: INFO: Created: latency-svc-5gtcb
Aug 21 16:07:13.129: INFO: Created: latency-svc-68xlg
Aug 21 16:07:13.148: INFO: Created: latency-svc-zzhdd
Aug 21 16:07:13.818: INFO: Got endpoints: latency-svc-88hg7 [1.370531539s]
Aug 21 16:07:13.818: INFO: Got endpoints: latency-svc-qrh4p [1.439131394s]
Aug 21 16:07:13.821: INFO: Got endpoints: latency-svc-6msx5 [1.249144983s]
Aug 21 16:07:13.822: INFO: Got endpoints: latency-svc-sjgpq [1.338575891s]
Aug 21 16:07:13.822: INFO: Got endpoints: latency-svc-xcf9s [1.296907041s]
Aug 21 16:07:13.862: INFO: Created: latency-svc-k6vjj
Aug 21 16:07:13.865: INFO: Got endpoints: latency-svc-6ljlf [1.185475556s]
Aug 21 16:07:13.865: INFO: Got endpoints: latency-svc-wjdjz [1.086103246s]
Aug 21 16:07:13.877: INFO: Got endpoints: latency-svc-9l49f [1.254475021s]
Aug 21 16:07:13.877: INFO: Got endpoints: latency-svc-r5fnm [1.137158001s]
Aug 21 16:07:13.877: INFO: Got endpoints: latency-svc-jw78g [1.055126016s]
Aug 21 16:07:13.892: INFO: Got endpoints: latency-svc-f6xmd [1.014266204s]
Aug 21 16:07:13.896: INFO: Got endpoints: latency-svc-88gmb [839.978664ms]
Aug 21 16:07:13.897: INFO: Created: latency-svc-dwz8f
Aug 21 16:07:13.898: INFO: Got endpoints: latency-svc-5gtcb [837.520294ms]
Aug 21 16:07:13.899: INFO: Got endpoints: latency-svc-68xlg [836.439969ms]
Aug 21 16:07:13.915: INFO: Got endpoints: latency-svc-zzhdd [841.14246ms]
Aug 21 16:07:13.919: INFO: Created: latency-svc-kksnf
Aug 21 16:07:13.932: INFO: Got endpoints: latency-svc-k6vjj [114.227398ms]
Aug 21 16:07:13.943: INFO: Got endpoints: latency-svc-dwz8f [125.017987ms]
Aug 21 16:07:13.950: INFO: Created: latency-svc-8vbvq
Aug 21 16:07:13.959: INFO: Created: latency-svc-lsqh4
Aug 21 16:07:13.971: INFO: Got endpoints: latency-svc-kksnf [149.59346ms]
Aug 21 16:07:14.022: INFO: Got endpoints: latency-svc-8vbvq [200.453817ms]
Aug 21 16:07:14.069: INFO: Got endpoints: latency-svc-lsqh4 [246.838689ms]
Aug 21 16:07:14.069: INFO: Latencies: [39.268352ms 70.827078ms 94.284467ms 114.227398ms 116.542585ms 125.017987ms 136.608673ms 149.59346ms 163.963278ms 177.603732ms 200.453817ms 205.557705ms 246.816944ms 246.838689ms 341.525295ms 342.085576ms 350.324724ms 361.835568ms 371.375783ms 373.807223ms 376.485897ms 380.828902ms 384.301619ms 388.044988ms 388.353466ms 388.648533ms 390.925238ms 391.759931ms 396.029433ms 397.095374ms 399.500409ms 400.951184ms 404.647113ms 407.103418ms 409.018556ms 412.526391ms 415.596542ms 424.520148ms 428.398706ms 429.614101ms 431.673188ms 432.113915ms 434.679138ms 434.88059ms 435.389434ms 436.203757ms 438.033311ms 441.33322ms 445.442637ms 446.350574ms 446.967338ms 449.780628ms 449.792311ms 455.173214ms 455.578608ms 455.664545ms 458.020548ms 461.78686ms 466.244602ms 469.629918ms 471.097892ms 478.800684ms 487.362979ms 489.063798ms 494.099505ms 496.718885ms 496.999935ms 501.035087ms 502.056447ms 509.407192ms 509.919929ms 510.421757ms 524.49185ms 525.254198ms 540.266832ms 541.613609ms 542.230195ms 552.627206ms 564.775524ms 573.769821ms 575.803525ms 578.207549ms 580.774507ms 580.870401ms 582.036054ms 586.755696ms 600.111488ms 601.304597ms 602.083954ms 609.165192ms 623.87333ms 643.392246ms 672.285301ms 676.185082ms 679.747304ms 685.144699ms 703.512964ms 709.816488ms 712.774825ms 713.461819ms 714.641406ms 717.182219ms 723.909783ms 729.387965ms 730.9564ms 731.133131ms 733.876297ms 734.77423ms 735.286691ms 735.491893ms 736.010263ms 736.133532ms 738.234068ms 738.665697ms 738.680804ms 739.30837ms 739.615677ms 740.238351ms 741.177973ms 741.708776ms 741.934203ms 741.968769ms 742.301219ms 742.961251ms 743.873441ms 745.372703ms 746.013393ms 746.365706ms 747.889827ms 747.924452ms 748.230795ms 748.373172ms 748.993412ms 749.155826ms 749.646441ms 749.695731ms 749.95703ms 750.425586ms 751.308481ms 751.392192ms 751.822113ms 751.842529ms 752.031529ms 752.346669ms 752.645137ms 753.973332ms 754.189528ms 754.287721ms 754.558305ms 755.146512ms 755.361393ms 755.59504ms 756.113826ms 756.854372ms 757.428065ms 757.991101ms 757.994842ms 758.101315ms 759.096219ms 760.197737ms 761.068712ms 761.200934ms 762.599942ms 763.40772ms 765.096243ms 768.016208ms 772.997287ms 773.968113ms 774.137639ms 775.51239ms 777.285069ms 780.802071ms 782.195082ms 791.249843ms 795.895ms 809.110379ms 812.434783ms 821.925065ms 821.956799ms 836.439969ms 837.520294ms 839.978664ms 840.400892ms 841.14246ms 841.58982ms 885.042612ms 886.604989ms 939.154101ms 982.047541ms 1.014266204s 1.055126016s 1.086103246s 1.137158001s 1.185475556s 1.249144983s 1.254475021s 1.296907041s 1.338575891s 1.370531539s 1.439131394s]
Aug 21 16:07:14.069: INFO: 50 %ile: 714.641406ms
Aug 21 16:07:14.069: INFO: 90 %ile: 837.520294ms
Aug 21 16:07:14.069: INFO: 99 %ile: 1.370531539s
Aug 21 16:07:14.069: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:07:14.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8726" for this suite.
Aug 21 16:07:46.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:07:46.256: INFO: namespace svc-latency-8726 deletion completed in 32.180828498s

• [SLOW TEST:48.300 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:07:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:07:46.336: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:07:47.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8986" for this suite.
Aug 21 16:07:53.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:07:53.927: INFO: namespace custom-resource-definition-8986 deletion completed in 6.194899202s

• [SLOW TEST:7.671 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:07:53.928: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 21 16:07:54.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9846'
Aug 21 16:07:54.392: INFO: stderr: ""
Aug 21 16:07:54.392: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:07:54.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9846'
Aug 21 16:07:54.481: INFO: stderr: ""
Aug 21 16:07:54.481: INFO: stdout: "update-demo-nautilus-7j76j update-demo-nautilus-r8dmw "
Aug 21 16:07:54.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7j76j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:07:54.594: INFO: stderr: ""
Aug 21 16:07:54.594: INFO: stdout: ""
Aug 21 16:07:54.594: INFO: update-demo-nautilus-7j76j is created but not running
Aug 21 16:07:59.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9846'
Aug 21 16:07:59.692: INFO: stderr: ""
Aug 21 16:07:59.692: INFO: stdout: "update-demo-nautilus-7j76j update-demo-nautilus-r8dmw "
Aug 21 16:07:59.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7j76j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:07:59.811: INFO: stderr: ""
Aug 21 16:07:59.811: INFO: stdout: "true"
Aug 21 16:07:59.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7j76j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:07:59.952: INFO: stderr: ""
Aug 21 16:07:59.952: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:07:59.952: INFO: validating pod update-demo-nautilus-7j76j
Aug 21 16:07:59.960: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:07:59.960: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:07:59.960: INFO: update-demo-nautilus-7j76j is verified up and running
Aug 21 16:07:59.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-r8dmw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:00.093: INFO: stderr: ""
Aug 21 16:08:00.093: INFO: stdout: "true"
Aug 21 16:08:00.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-r8dmw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:00.198: INFO: stderr: ""
Aug 21 16:08:00.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:08:00.198: INFO: validating pod update-demo-nautilus-r8dmw
Aug 21 16:08:00.205: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:08:00.206: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:08:00.206: INFO: update-demo-nautilus-r8dmw is verified up and running
STEP: rolling-update to new replication controller
Aug 21 16:08:00.208: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:08:00.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9846'
Aug 21 16:08:29.627: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 21 16:08:29.628: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:08:29.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9846'
Aug 21 16:08:29.737: INFO: stderr: ""
Aug 21 16:08:29.737: INFO: stdout: "update-demo-kitten-mgvbb update-demo-kitten-vw4bv "
Aug 21 16:08:29.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-kitten-mgvbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:29.832: INFO: stderr: ""
Aug 21 16:08:29.832: INFO: stdout: "true"
Aug 21 16:08:29.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-kitten-mgvbb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:29.946: INFO: stderr: ""
Aug 21 16:08:29.946: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 21 16:08:29.946: INFO: validating pod update-demo-kitten-mgvbb
Aug 21 16:08:29.953: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 21 16:08:29.953: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 21 16:08:29.953: INFO: update-demo-kitten-mgvbb is verified up and running
Aug 21 16:08:29.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-kitten-vw4bv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:30.052: INFO: stderr: ""
Aug 21 16:08:30.052: INFO: stdout: "true"
Aug 21 16:08:30.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-kitten-vw4bv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9846'
Aug 21 16:08:30.146: INFO: stderr: ""
Aug 21 16:08:30.146: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 21 16:08:30.146: INFO: validating pod update-demo-kitten-vw4bv
Aug 21 16:08:30.154: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 21 16:08:30.154: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 21 16:08:30.154: INFO: update-demo-kitten-vw4bv is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:08:30.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9846" for this suite.
Aug 21 16:08:56.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:08:56.323: INFO: namespace kubectl-9846 deletion completed in 26.155677255s

• [SLOW TEST:62.395 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:08:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8057
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8057 to expose endpoints map[]
Aug 21 16:08:56.459: INFO: Get endpoints failed (4.575756ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 21 16:08:57.463: INFO: successfully validated that service endpoint-test2 in namespace services-8057 exposes endpoints map[] (1.008944242s elapsed)
STEP: Creating pod pod1 in namespace services-8057
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8057 to expose endpoints map[pod1:[80]]
Aug 21 16:09:01.516: INFO: successfully validated that service endpoint-test2 in namespace services-8057 exposes endpoints map[pod1:[80]] (4.042431155s elapsed)
STEP: Creating pod pod2 in namespace services-8057
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8057 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 21 16:09:04.605: INFO: successfully validated that service endpoint-test2 in namespace services-8057 exposes endpoints map[pod1:[80] pod2:[80]] (3.082644639s elapsed)
STEP: Deleting pod pod1 in namespace services-8057
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8057 to expose endpoints map[pod2:[80]]
Aug 21 16:09:05.664: INFO: successfully validated that service endpoint-test2 in namespace services-8057 exposes endpoints map[pod2:[80]] (1.023478167s elapsed)
STEP: Deleting pod pod2 in namespace services-8057
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8057 to expose endpoints map[]
Aug 21 16:09:06.687: INFO: successfully validated that service endpoint-test2 in namespace services-8057 exposes endpoints map[] (1.013925167s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:09:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8057" for this suite.
Aug 21 16:09:34.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:09:34.964: INFO: namespace services-8057 deletion completed in 28.185754148s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:38.638 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:09:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d88b92a3-6eb1-4a2a-a157-c03b28e92e32 in namespace container-probe-9013
Aug 21 16:09:39.060: INFO: Started pod busybox-d88b92a3-6eb1-4a2a-a157-c03b28e92e32 in namespace container-probe-9013
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 16:09:39.064: INFO: Initial restart count of pod busybox-d88b92a3-6eb1-4a2a-a157-c03b28e92e32 is 0
Aug 21 16:10:27.192: INFO: Restart count of pod container-probe-9013/busybox-d88b92a3-6eb1-4a2a-a157-c03b28e92e32 is now 1 (48.127607616s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:10:27.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9013" for this suite.
Aug 21 16:10:33.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:10:33.393: INFO: namespace container-probe-9013 deletion completed in 6.180944878s

• [SLOW TEST:58.426 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:10:33.406: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 21 16:10:33.501: INFO: Waiting up to 5m0s for pod "client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6" in namespace "containers-1574" to be "success or failure"
Aug 21 16:10:33.505: INFO: Pod "client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482114ms
Aug 21 16:10:35.517: INFO: Pod "client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015655008s
Aug 21 16:10:37.523: INFO: Pod "client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021295645s
STEP: Saw pod success
Aug 21 16:10:37.523: INFO: Pod "client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6" satisfied condition "success or failure"
Aug 21 16:10:37.527: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6 container test-container: <nil>
STEP: delete the pod
Aug 21 16:10:37.559: INFO: Waiting for pod client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6 to disappear
Aug 21 16:10:37.561: INFO: Pod client-containers-eceb4efa-bfc6-45a2-b4b4-dad8f36657e6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:10:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1574" for this suite.
Aug 21 16:10:45.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:10:45.747: INFO: namespace containers-1574 deletion completed in 8.171713505s

• [SLOW TEST:12.341 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:10:45.747: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:10:46.262: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 21 16:10:51.276: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 16:10:51.276: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 21 16:10:53.281: INFO: Creating deployment "test-rollover-deployment"
Aug 21 16:10:53.542: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 21 16:10:55.551: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 21 16:10:55.560: INFO: Ensure that both replica sets have 1 created replica
Aug 21 16:10:55.570: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 21 16:10:55.580: INFO: Updating deployment test-rollover-deployment
Aug 21 16:10:55.580: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 21 16:10:57.591: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 21 16:10:57.600: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 21 16:10:57.609: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:10:57.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000656, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:10:59.619: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:10:59.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000656, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:01.621: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:01.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:03.622: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:03.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:05.619: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:05.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:07.619: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:07.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:09.619: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:09.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:11.619: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 16:11:11.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000660, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702000654, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:11:13.621: INFO: 
Aug 21 16:11:13.621: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 21 16:11:13.635: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7299,SelfLink:/apis/apps/v1/namespaces/deployment-7299/deployments/test-rollover-deployment,UID:a8d4dc13-fce0-4594-a9a8-e47ab90bc998,ResourceVersion:9707,Generation:2,CreationTimestamp:2019-08-21 16:10:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-21 16:10:54 +0000 UTC 2019-08-21 16:10:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-21 16:11:12 +0000 UTC 2019-08-21 16:10:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 21 16:11:13.640: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7299,SelfLink:/apis/apps/v1/namespaces/deployment-7299/replicasets/test-rollover-deployment-854595fc44,UID:2d0deb75-da64-42da-8a10-848ac6a7d60a,ResourceVersion:9694,Generation:2,CreationTimestamp:2019-08-21 16:10:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a8d4dc13-fce0-4594-a9a8-e47ab90bc998 0xc002a66197 0xc002a66198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 21 16:11:13.640: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 21 16:11:13.640: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7299,SelfLink:/apis/apps/v1/namespaces/deployment-7299/replicasets/test-rollover-controller,UID:9e495397-62fd-4f35-8e7d-a9c8547693d2,ResourceVersion:9706,Generation:2,CreationTimestamp:2019-08-21 16:10:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a8d4dc13-fce0-4594-a9a8-e47ab90bc998 0xc002a660bf 0xc002a660d0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 16:11:13.640: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7299,SelfLink:/apis/apps/v1/namespaces/deployment-7299/replicasets/test-rollover-deployment-9b8b997cf,UID:80820b5f-07d6-4cdd-8975-82d9dba15f8d,ResourceVersion:9656,Generation:2,CreationTimestamp:2019-08-21 16:10:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a8d4dc13-fce0-4594-a9a8-e47ab90bc998 0xc002a66250 0xc002a66251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 16:11:13.645: INFO: Pod "test-rollover-deployment-854595fc44-jjltx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-jjltx,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7299,SelfLink:/api/v1/namespaces/deployment-7299/pods/test-rollover-deployment-854595fc44-jjltx,UID:fbd43c5f-5950-47f9-a91d-98c7475396ae,ResourceVersion:9673,Generation:0,CreationTimestamp:2019-08-21 16:10:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.40/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 2d0deb75-da64-42da-8a10-848ac6a7d60a 0xc002a66e37 0xc002a66e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wdmsf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdmsf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wdmsf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a66ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a66ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:10:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:11:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:11:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:10:56 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.40,StartTime:2019-08-21 16:10:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-21 16:10:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b9f81f6866564f07b8e0730b51617ae4d8359971e4af1baf83b4ab5efa576335}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:11:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7299" for this suite.
Aug 21 16:11:21.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:11:21.829: INFO: namespace deployment-7299 deletion completed in 8.177795607s

• [SLOW TEST:36.082 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:11:21.829: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:11:21.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d" in namespace "projected-8449" to be "success or failure"
Aug 21 16:11:21.944: INFO: Pod "downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.883526ms
Aug 21 16:11:23.949: INFO: Pod "downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008347857s
Aug 21 16:11:25.955: INFO: Pod "downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014470222s
STEP: Saw pod success
Aug 21 16:11:25.956: INFO: Pod "downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d" satisfied condition "success or failure"
Aug 21 16:11:25.959: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d container client-container: <nil>
STEP: delete the pod
Aug 21 16:11:25.997: INFO: Waiting for pod downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d to disappear
Aug 21 16:11:26.000: INFO: Pod downwardapi-volume-3652183a-a533-483d-a5b5-f726cedc0c5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:11:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8449" for this suite.
Aug 21 16:11:34.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:11:34.187: INFO: namespace projected-8449 deletion completed in 8.181447512s

• [SLOW TEST:12.358 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:11:34.193: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:11:34.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2533'
Aug 21 16:11:34.409: INFO: stderr: ""
Aug 21 16:11:34.409: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 21 16:11:39.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pod e2e-test-nginx-pod --namespace=kubectl-2533 -o json'
Aug 21 16:11:39.574: INFO: stderr: ""
Aug 21 16:11:39.574: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.1.37/32\"\n        },\n        \"creationTimestamp\": \"2019-08-21T16:11:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2533\",\n        \"resourceVersion\": \"9844\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2533/pods/e2e-test-nginx-pod\",\n        \"uid\": \"596aeb06-8ad6-413b-a6ad-bf1d18de8343\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-96tmr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"61823cca-1200-4e72-835e-06a7dddcdaa7\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-96tmr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-96tmr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-21T16:11:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-21T16:11:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-21T16:11:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-21T16:11:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://89206791ea3d3936ab1291d82aa8a87caf6e983bc315071c0d7d5b8ea1aa87d6\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-21T16:11:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.132.234.132\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.1.37\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-21T16:11:34Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 21 16:11:39.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 replace -f - --namespace=kubectl-2533'
Aug 21 16:11:39.962: INFO: stderr: ""
Aug 21 16:11:39.962: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 21 16:11:39.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete pods e2e-test-nginx-pod --namespace=kubectl-2533'
Aug 21 16:11:42.055: INFO: stderr: ""
Aug 21 16:11:42.055: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:11:42.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2533" for this suite.
Aug 21 16:11:48.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:11:48.240: INFO: namespace kubectl-2533 deletion completed in 6.178665399s

• [SLOW TEST:14.047 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:11:48.240: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 21 16:11:48.323: INFO: namespace kubectl-1243
Aug 21 16:11:48.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-1243'
Aug 21 16:11:48.750: INFO: stderr: ""
Aug 21 16:11:48.750: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 16:11:49.755: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:49.755: INFO: Found 0 / 1
Aug 21 16:11:50.755: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:50.756: INFO: Found 0 / 1
Aug 21 16:11:51.755: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:51.755: INFO: Found 0 / 1
Aug 21 16:11:52.755: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:52.755: INFO: Found 0 / 1
Aug 21 16:11:53.756: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:53.756: INFO: Found 1 / 1
Aug 21 16:11:53.756: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 21 16:11:53.761: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:11:53.761: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 16:11:53.761: INFO: wait on redis-master startup in kubectl-1243 
Aug 21 16:11:53.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 logs redis-master-gffp8 redis-master --namespace=kubectl-1243'
Aug 21 16:11:53.897: INFO: stderr: ""
Aug 21 16:11:53.897: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Aug 16:11:52.304 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Aug 16:11:52.304 # Server started, Redis version 3.2.12\n1:M 21 Aug 16:11:52.304 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Aug 16:11:52.304 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 21 16:11:53.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1243'
Aug 21 16:11:54.074: INFO: stderr: ""
Aug 21 16:11:54.074: INFO: stdout: "service/rm2 exposed\n"
Aug 21 16:11:54.078: INFO: Service rm2 in namespace kubectl-1243 found.
STEP: exposing service
Aug 21 16:11:56.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1243'
Aug 21 16:11:56.294: INFO: stderr: ""
Aug 21 16:11:56.294: INFO: stdout: "service/rm3 exposed\n"
Aug 21 16:11:56.303: INFO: Service rm3 in namespace kubectl-1243 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:11:58.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1243" for this suite.
Aug 21 16:12:22.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:12:22.491: INFO: namespace kubectl-1243 deletion completed in 24.166242777s

• [SLOW TEST:34.251 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:12:22.493: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 21 16:12:22.623: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10018,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:12:22.623: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10018,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 21 16:12:32.633: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10040,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 21 16:12:32.634: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10040,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 21 16:12:42.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10059,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:12:42.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10059,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 21 16:12:52.654: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10079,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:12:52.654: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-a,UID:96b7efad-b083-4cf3-ab61-018d1afc6af0,ResourceVersion:10079,Generation:0,CreationTimestamp:2019-08-21 16:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 21 16:13:02.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-b,UID:3305f454-5c30-421b-849a-437125bf31e1,ResourceVersion:10099,Generation:0,CreationTimestamp:2019-08-21 16:13:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:13:02.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-b,UID:3305f454-5c30-421b-849a-437125bf31e1,ResourceVersion:10099,Generation:0,CreationTimestamp:2019-08-21 16:13:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 21 16:13:12.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-b,UID:3305f454-5c30-421b-849a-437125bf31e1,ResourceVersion:10118,Generation:0,CreationTimestamp:2019-08-21 16:13:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:13:12.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2742,SelfLink:/api/v1/namespaces/watch-2742/configmaps/e2e-watch-test-configmap-b,UID:3305f454-5c30-421b-849a-437125bf31e1,ResourceVersion:10118,Generation:0,CreationTimestamp:2019-08-21 16:13:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:13:22.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2742" for this suite.
Aug 21 16:13:28.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:13:28.868: INFO: namespace watch-2742 deletion completed in 6.183885753s

• [SLOW TEST:66.376 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:13:28.872: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-8835235b-6ca8-47b2-9309-d4e9684d2d36 in namespace container-probe-1417
Aug 21 16:13:31.074: INFO: Started pod test-webserver-8835235b-6ca8-47b2-9309-d4e9684d2d36 in namespace container-probe-1417
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 16:13:31.078: INFO: Initial restart count of pod test-webserver-8835235b-6ca8-47b2-9309-d4e9684d2d36 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:17:31.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1417" for this suite.
Aug 21 16:17:37.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:17:37.975: INFO: namespace container-probe-1417 deletion completed in 6.181220521s

• [SLOW TEST:249.103 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:17:37.975: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:17:38.079: INFO: (0) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 20.167653ms)
Aug 21 16:17:38.084: INFO: (1) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.985248ms)
Aug 21 16:17:38.091: INFO: (2) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.143017ms)
Aug 21 16:17:38.096: INFO: (3) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.256489ms)
Aug 21 16:17:38.104: INFO: (4) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.051537ms)
Aug 21 16:17:38.110: INFO: (5) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.781143ms)
Aug 21 16:17:38.124: INFO: (6) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.418527ms)
Aug 21 16:17:38.130: INFO: (7) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.175478ms)
Aug 21 16:17:38.136: INFO: (8) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.700052ms)
Aug 21 16:17:38.142: INFO: (9) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.842332ms)
Aug 21 16:17:38.152: INFO: (10) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.938644ms)
Aug 21 16:17:38.158: INFO: (11) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.647124ms)
Aug 21 16:17:38.163: INFO: (12) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.926806ms)
Aug 21 16:17:38.168: INFO: (13) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.946506ms)
Aug 21 16:17:38.174: INFO: (14) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.119008ms)
Aug 21 16:17:38.181: INFO: (15) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.942665ms)
Aug 21 16:17:38.189: INFO: (16) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.083139ms)
Aug 21 16:17:38.197: INFO: (17) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.063522ms)
Aug 21 16:17:38.203: INFO: (18) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.244171ms)
Aug 21 16:17:38.209: INFO: (19) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.998086ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:17:38.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3142" for this suite.
Aug 21 16:17:44.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:17:44.386: INFO: namespace proxy-3142 deletion completed in 6.169701586s

• [SLOW TEST:6.411 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:17:44.387: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4852
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 16:17:44.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 16:18:20.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.44:8080/dial?request=hostName&protocol=http&host=192.168.1.39&port=8080&tries=1'] Namespace:pod-network-test-4852 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:18:20.695: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:18:20.998: INFO: Waiting for endpoints: map[]
Aug 21 16:18:21.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.44:8080/dial?request=hostName&protocol=http&host=192.168.2.43&port=8080&tries=1'] Namespace:pod-network-test-4852 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:18:21.002: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:18:21.346: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:18:21.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4852" for this suite.
Aug 21 16:18:45.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:18:45.540: INFO: namespace pod-network-test-4852 deletion completed in 24.188526734s

• [SLOW TEST:61.154 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:18:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 21 16:18:45.763: INFO: Waiting up to 5m0s for pod "pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62" in namespace "emptydir-8931" to be "success or failure"
Aug 21 16:18:45.767: INFO: Pod "pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789226ms
Aug 21 16:18:47.772: INFO: Pod "pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263315s
Aug 21 16:18:49.777: INFO: Pod "pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01422748s
STEP: Saw pod success
Aug 21 16:18:49.777: INFO: Pod "pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62" satisfied condition "success or failure"
Aug 21 16:18:49.781: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62 container test-container: <nil>
STEP: delete the pod
Aug 21 16:18:49.823: INFO: Waiting for pod pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62 to disappear
Aug 21 16:18:49.826: INFO: Pod pod-9f1fdb1c-d1e2-4ae6-9647-aa9c15724f62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:18:49.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8931" for this suite.
Aug 21 16:18:55.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:18:55.998: INFO: namespace emptydir-8931 deletion completed in 6.166443873s

• [SLOW TEST:10.458 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:18:55.999: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 21 16:18:56.096: INFO: Waiting up to 5m0s for pod "pod-46cb209e-b469-4cd9-a234-ff52fc6a7085" in namespace "emptydir-5878" to be "success or failure"
Aug 21 16:18:56.100: INFO: Pod "pod-46cb209e-b469-4cd9-a234-ff52fc6a7085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.232803ms
Aug 21 16:18:58.107: INFO: Pod "pod-46cb209e-b469-4cd9-a234-ff52fc6a7085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010347684s
Aug 21 16:19:00.112: INFO: Pod "pod-46cb209e-b469-4cd9-a234-ff52fc6a7085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016017943s
STEP: Saw pod success
Aug 21 16:19:00.112: INFO: Pod "pod-46cb209e-b469-4cd9-a234-ff52fc6a7085" satisfied condition "success or failure"
Aug 21 16:19:00.116: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-46cb209e-b469-4cd9-a234-ff52fc6a7085 container test-container: <nil>
STEP: delete the pod
Aug 21 16:19:00.140: INFO: Waiting for pod pod-46cb209e-b469-4cd9-a234-ff52fc6a7085 to disappear
Aug 21 16:19:00.144: INFO: Pod pod-46cb209e-b469-4cd9-a234-ff52fc6a7085 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:19:00.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5878" for this suite.
Aug 21 16:19:06.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:06.313: INFO: namespace emptydir-5878 deletion completed in 6.164513367s

• [SLOW TEST:10.313 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:19:06.315: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cbce681d-dfec-4c6d-ac82-9fcefff8a5a0
STEP: Creating a pod to test consume configMaps
Aug 21 16:19:06.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08" in namespace "configmap-5363" to be "success or failure"
Aug 21 16:19:06.424: INFO: Pod "pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.584259ms
Aug 21 16:19:08.429: INFO: Pod "pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008765686s
Aug 21 16:19:10.435: INFO: Pod "pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01475106s
STEP: Saw pod success
Aug 21 16:19:10.435: INFO: Pod "pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08" satisfied condition "success or failure"
Aug 21 16:19:10.440: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:19:10.469: INFO: Waiting for pod pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08 to disappear
Aug 21 16:19:10.472: INFO: Pod pod-configmaps-f1022987-11d9-46d6-b2bc-444071336a08 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:19:10.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5363" for this suite.
Aug 21 16:19:16.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:16.642: INFO: namespace configmap-5363 deletion completed in 6.15930572s

• [SLOW TEST:10.326 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:19:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 21 16:19:16.731: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 16:19:16.741: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 16:19:16.744: INFO: 
Logging pods the kubelet thinks is on node 3c511303-d822-436a-a9c7-85bfe5dec0f1 before test
Aug 21 16:19:16.753: INFO: canal-4wqcb from kube-system started at 2019-08-21 15:32:50 +0000 UTC (3 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 16:19:16.753: INFO: kube-proxy-b5jkw from kube-system started at 2019-08-21 15:32:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:19:16.753: INFO: cloud-coordinator-767969ffd8-jsl2t from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container cloud-coordinator ready: true, restart count 0
Aug 21 16:19:16.753: INFO: csi-do-node-kznxp from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 16:19:16.753: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-21 15:51:17 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 16:19:16.753: INFO: sonobuoy-e2e-job-abdb7b72ca0642e2 from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container e2e ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:19:16.753: INFO: cloud-agent-6rhbp from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container cloud-agent ready: true, restart count 0
Aug 21 16:19:16.753: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-vx26t from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:19:16.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:19:16.753: INFO: 
Logging pods the kubelet thinks is on node 61823cca-1200-4e72-835e-06a7dddcdaa7 before test
Aug 21 16:19:16.765: INFO: kube-proxy-4mvfm from kube-system started at 2019-08-21 15:32:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:19:16.765: INFO: canal-q8xpb from kube-system started at 2019-08-21 15:32:33 +0000 UTC (3 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 16:19:16.765: INFO: csi-do-node-wpjp8 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 16:19:16.765: INFO: cloud-agent-lwq2n from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container cloud-agent ready: true, restart count 0
Aug 21 16:19:16.765: INFO: eventrouter-b478d5654-h8dbn from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container eventrouter ready: true, restart count 0
Aug 21 16:19:16.765: INFO: csi-do-controller-0 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (4 container statuses recorded)
Aug 21 16:19:16.765: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 21 16:19:16.765: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 21 16:19:16.765: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-p9g9l from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:19:16.766: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:19:16.766: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-28c302ae-9e84-4281-9cc1-fa7ebee7d99f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-28c302ae-9e84-4281-9cc1-fa7ebee7d99f off the node 61823cca-1200-4e72-835e-06a7dddcdaa7
STEP: verifying the node doesn't have the label kubernetes.io/e2e-28c302ae-9e84-4281-9cc1-fa7ebee7d99f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:19:24.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6626" for this suite.
Aug 21 16:19:38.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:39.064: INFO: namespace sched-pred-6626 deletion completed in 14.180745744s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.422 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:19:39.064: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 21 16:19:43.337: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-483237363 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 21 16:19:48.481: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:19:48.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4544" for this suite.
Aug 21 16:19:56.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:56.674: INFO: namespace pods-4544 deletion completed in 8.179379821s

• [SLOW TEST:17.610 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:19:56.675: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 21 16:19:57.982: INFO: Waiting up to 5m0s for pod "pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d" in namespace "emptydir-329" to be "success or failure"
Aug 21 16:19:57.986: INFO: Pod "pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.915109ms
Aug 21 16:19:59.992: INFO: Pod "pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009370917s
Aug 21 16:20:01.997: INFO: Pod "pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015073665s
STEP: Saw pod success
Aug 21 16:20:01.998: INFO: Pod "pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d" satisfied condition "success or failure"
Aug 21 16:20:02.003: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d container test-container: <nil>
STEP: delete the pod
Aug 21 16:20:02.036: INFO: Waiting for pod pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d to disappear
Aug 21 16:20:02.040: INFO: Pod pod-05e75b7c-430f-44c5-a121-4f9f6e56b05d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:20:02.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-329" for this suite.
Aug 21 16:20:10.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:10.214: INFO: namespace emptydir-329 deletion completed in 8.165987432s

• [SLOW TEST:13.539 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:20:10.217: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:20:10.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 version'
Aug 21 16:20:10.512: INFO: stderr: ""
Aug 21 16:20:10.512: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:20:10.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5187" for this suite.
Aug 21 16:20:18.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:18.729: INFO: namespace kubectl-5187 deletion completed in 8.211412598s

• [SLOW TEST:8.514 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:20:18.733: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 21 16:20:18.833: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 21 16:20:19.431: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 21 16:20:21.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:20:23.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:20:25.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:20:27.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:20:29.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702001220, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:20:34.561: INFO: Waited 3.03848031s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:20:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5195" for this suite.
Aug 21 16:20:43.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:43.596: INFO: namespace aggregator-5195 deletion completed in 8.252382552s

• [SLOW TEST:24.863 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:20:43.599: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 16:20:46.806: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:20:46.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9030" for this suite.
Aug 21 16:20:52.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:53.012: INFO: namespace container-runtime-9030 deletion completed in 6.180266383s

• [SLOW TEST:9.414 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:20:53.012: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:20:53.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9" in namespace "downward-api-7090" to be "success or failure"
Aug 21 16:20:53.163: INFO: Pod "downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.663374ms
Aug 21 16:20:55.175: INFO: Pod "downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023568374s
STEP: Saw pod success
Aug 21 16:20:55.175: INFO: Pod "downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9" satisfied condition "success or failure"
Aug 21 16:20:55.179: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9 container client-container: <nil>
STEP: delete the pod
Aug 21 16:20:55.218: INFO: Waiting for pod downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9 to disappear
Aug 21 16:20:55.221: INFO: Pod downwardapi-volume-57d8393c-d5f2-4f86-a56e-89bc58a769a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:20:55.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7090" for this suite.
Aug 21 16:21:05.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:05.401: INFO: namespace downward-api-7090 deletion completed in 10.164438991s

• [SLOW TEST:12.389 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:21:05.402: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 16:21:05.534: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:05.534: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:05.535: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:05.544: INFO: Number of nodes with available pods: 0
Aug 21 16:21:05.544: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:21:06.558: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:06.558: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:06.558: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:06.563: INFO: Number of nodes with available pods: 0
Aug 21 16:21:06.563: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:21:07.554: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:07.555: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:07.555: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:07.560: INFO: Number of nodes with available pods: 1
Aug 21 16:21:07.560: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:08.553: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.553: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.553: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.557: INFO: Number of nodes with available pods: 2
Aug 21 16:21:08.558: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 21 16:21:08.584: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.584: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.584: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:08.588: INFO: Number of nodes with available pods: 1
Aug 21 16:21:08.589: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:09.595: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:09.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:09.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:09.600: INFO: Number of nodes with available pods: 1
Aug 21 16:21:09.600: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:10.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:10.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:10.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:10.670: INFO: Number of nodes with available pods: 1
Aug 21 16:21:10.670: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:11.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:11.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:11.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:11.601: INFO: Number of nodes with available pods: 1
Aug 21 16:21:11.601: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:12.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:12.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:12.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:12.601: INFO: Number of nodes with available pods: 1
Aug 21 16:21:12.601: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:13.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:13.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:13.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:13.601: INFO: Number of nodes with available pods: 1
Aug 21 16:21:13.601: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:14.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:14.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:14.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:14.601: INFO: Number of nodes with available pods: 1
Aug 21 16:21:14.601: INFO: Node 61823cca-1200-4e72-835e-06a7dddcdaa7 is running more than one daemon pod
Aug 21 16:21:15.596: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:15.596: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:15.596: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:21:15.601: INFO: Number of nodes with available pods: 2
Aug 21 16:21:15.601: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8002, will wait for the garbage collector to delete the pods
Aug 21 16:21:15.670: INFO: Deleting DaemonSet.extensions daemon-set took: 11.736729ms
Aug 21 16:21:16.270: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.319365ms
Aug 21 16:21:27.777: INFO: Number of nodes with available pods: 0
Aug 21 16:21:27.777: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 16:21:27.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8002/daemonsets","resourceVersion":"11664"},"items":null}

Aug 21 16:21:27.787: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8002/pods","resourceVersion":"11664"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:21:27.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8002" for this suite.
Aug 21 16:21:33.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:33.992: INFO: namespace daemonsets-8002 deletion completed in 6.186836841s

• [SLOW TEST:28.591 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:21:33.999: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:21:34.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4855'
Aug 21 16:21:34.761: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:21:34.761: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 21 16:21:34.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete jobs e2e-test-nginx-job --namespace=kubectl-4855'
Aug 21 16:21:34.919: INFO: stderr: ""
Aug 21 16:21:34.919: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:21:34.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4855" for this suite.
Aug 21 16:21:40.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:41.106: INFO: namespace kubectl-4855 deletion completed in 6.178576931s

• [SLOW TEST:7.107 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:21:41.113: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-f08f8f64-bc90-4e90-8872-3e15cd2c30ff
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:21:41.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-429" for this suite.
Aug 21 16:21:49.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:49.450: INFO: namespace secrets-429 deletion completed in 8.168038715s

• [SLOW TEST:8.337 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:21:49.450: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-6e526b1c-2d01-4a5a-906c-6560a471bc2a
STEP: Creating a pod to test consume configMaps
Aug 21 16:21:49.539: INFO: Waiting up to 5m0s for pod "pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8" in namespace "configmap-871" to be "success or failure"
Aug 21 16:21:49.542: INFO: Pod "pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372997ms
Aug 21 16:21:51.547: INFO: Pod "pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008524644s
STEP: Saw pod success
Aug 21 16:21:51.547: INFO: Pod "pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8" satisfied condition "success or failure"
Aug 21 16:21:51.551: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:21:51.580: INFO: Waiting for pod pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8 to disappear
Aug 21 16:21:51.583: INFO: Pod pod-configmaps-b71e20e2-ffc4-4c2b-8bcf-5ecf9c9a9ed8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:21:51.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-871" for this suite.
Aug 21 16:21:57.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:57.766: INFO: namespace configmap-871 deletion completed in 6.176340136s

• [SLOW TEST:8.315 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:21:57.766: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-w57mv in namespace proxy-4173
I0821 16:21:57.909211      15 runners.go:180] Created replication controller with name: proxy-service-w57mv, namespace: proxy-4173, replica count: 1
I0821 16:21:58.959794      15 runners.go:180] proxy-service-w57mv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:21:59.960028      15 runners.go:180] proxy-service-w57mv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:22:00.960297      15 runners.go:180] proxy-service-w57mv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:22:01.960502      15 runners.go:180] proxy-service-w57mv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0821 16:22:02.960789      15 runners.go:180] proxy-service-w57mv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 16:22:02.967: INFO: setup took 5.092831363s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 21 16:22:02.994: INFO: (0) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 27.216754ms)
Aug 21 16:22:03.002: INFO: (0) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 34.699953ms)
Aug 21 16:22:03.002: INFO: (0) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 34.847155ms)
Aug 21 16:22:03.002: INFO: (0) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 34.467218ms)
Aug 21 16:22:03.006: INFO: (0) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 38.677227ms)
Aug 21 16:22:03.006: INFO: (0) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 39.140818ms)
Aug 21 16:22:03.006: INFO: (0) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 38.62889ms)
Aug 21 16:22:03.006: INFO: (0) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 39.114762ms)
Aug 21 16:22:03.027: INFO: (0) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 60.539275ms)
Aug 21 16:22:03.028: INFO: (0) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 59.823186ms)
Aug 21 16:22:03.028: INFO: (0) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 59.789979ms)
Aug 21 16:22:03.028: INFO: (0) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 60.343264ms)
Aug 21 16:22:03.028: INFO: (0) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 60.216649ms)
Aug 21 16:22:03.028: INFO: (0) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 60.33229ms)
Aug 21 16:22:03.027: INFO: (0) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 59.545447ms)
Aug 21 16:22:03.046: INFO: (0) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 78.433781ms)
Aug 21 16:22:03.053: INFO: (1) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 7.531131ms)
Aug 21 16:22:03.062: INFO: (1) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 14.700647ms)
Aug 21 16:22:03.062: INFO: (1) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.146009ms)
Aug 21 16:22:03.062: INFO: (1) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 16.077971ms)
Aug 21 16:22:03.062: INFO: (1) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 15.693201ms)
Aug 21 16:22:03.062: INFO: (1) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.71225ms)
Aug 21 16:22:03.063: INFO: (1) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 16.51104ms)
Aug 21 16:22:03.063: INFO: (1) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 16.46202ms)
Aug 21 16:22:03.063: INFO: (1) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 16.640352ms)
Aug 21 16:22:03.063: INFO: (1) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 17.53523ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 17.353937ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 16.871549ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 17.547835ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 17.200165ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 17.772443ms)
Aug 21 16:22:03.064: INFO: (1) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 18.213821ms)
Aug 21 16:22:03.076: INFO: (2) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.458316ms)
Aug 21 16:22:03.076: INFO: (2) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.264462ms)
Aug 21 16:22:03.076: INFO: (2) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 12.109153ms)
Aug 21 16:22:03.077: INFO: (2) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 12.339505ms)
Aug 21 16:22:03.077: INFO: (2) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 12.711098ms)
Aug 21 16:22:03.077: INFO: (2) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 12.849567ms)
Aug 21 16:22:03.078: INFO: (2) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 13.090803ms)
Aug 21 16:22:03.078: INFO: (2) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 13.468274ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 14.531936ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 14.320655ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 14.606141ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 14.970339ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 15.092552ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 15.174422ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 15.10981ms)
Aug 21 16:22:03.079: INFO: (2) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 15.397561ms)
Aug 21 16:22:03.088: INFO: (3) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 8.334091ms)
Aug 21 16:22:03.095: INFO: (3) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 15.074507ms)
Aug 21 16:22:03.096: INFO: (3) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 15.810153ms)
Aug 21 16:22:03.096: INFO: (3) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.959884ms)
Aug 21 16:22:03.096: INFO: (3) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 15.75189ms)
Aug 21 16:22:03.097: INFO: (3) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 16.890209ms)
Aug 21 16:22:03.098: INFO: (3) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 17.21807ms)
Aug 21 16:22:03.098: INFO: (3) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 17.469305ms)
Aug 21 16:22:03.099: INFO: (3) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 18.33846ms)
Aug 21 16:22:03.099: INFO: (3) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 18.892583ms)
Aug 21 16:22:03.102: INFO: (3) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 21.750644ms)
Aug 21 16:22:03.104: INFO: (3) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 23.407117ms)
Aug 21 16:22:03.104: INFO: (3) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 23.073508ms)
Aug 21 16:22:03.104: INFO: (3) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 23.800853ms)
Aug 21 16:22:03.104: INFO: (3) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 24.479162ms)
Aug 21 16:22:03.105: INFO: (3) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 24.905572ms)
Aug 21 16:22:03.118: INFO: (4) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.105342ms)
Aug 21 16:22:03.120: INFO: (4) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 13.996565ms)
Aug 21 16:22:03.120: INFO: (4) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 13.926484ms)
Aug 21 16:22:03.121: INFO: (4) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 15.448349ms)
Aug 21 16:22:03.124: INFO: (4) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 17.387295ms)
Aug 21 16:22:03.127: INFO: (4) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 21.495933ms)
Aug 21 16:22:03.127: INFO: (4) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 21.269769ms)
Aug 21 16:22:03.127: INFO: (4) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 21.501393ms)
Aug 21 16:22:03.127: INFO: (4) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 21.561791ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 21.646101ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 21.780756ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 22.075994ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 22.380918ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 22.692355ms)
Aug 21 16:22:03.128: INFO: (4) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 22.595681ms)
Aug 21 16:22:03.129: INFO: (4) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 22.894926ms)
Aug 21 16:22:03.139: INFO: (5) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 9.909718ms)
Aug 21 16:22:03.139: INFO: (5) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 9.546261ms)
Aug 21 16:22:03.139: INFO: (5) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 9.765167ms)
Aug 21 16:22:03.140: INFO: (5) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 10.837224ms)
Aug 21 16:22:03.141: INFO: (5) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.026119ms)
Aug 21 16:22:03.141: INFO: (5) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 11.081669ms)
Aug 21 16:22:03.141: INFO: (5) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 12.309747ms)
Aug 21 16:22:03.143: INFO: (5) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 13.615362ms)
Aug 21 16:22:03.143: INFO: (5) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 13.741001ms)
Aug 21 16:22:03.143: INFO: (5) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 13.43836ms)
Aug 21 16:22:03.143: INFO: (5) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 13.601959ms)
Aug 21 16:22:03.145: INFO: (5) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 15.362953ms)
Aug 21 16:22:03.148: INFO: (5) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 18.015281ms)
Aug 21 16:22:03.148: INFO: (5) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 18.498719ms)
Aug 21 16:22:03.148: INFO: (5) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 18.324856ms)
Aug 21 16:22:03.148: INFO: (5) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 18.592862ms)
Aug 21 16:22:03.156: INFO: (6) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 7.006751ms)
Aug 21 16:22:03.156: INFO: (6) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 7.165777ms)
Aug 21 16:22:03.166: INFO: (6) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 17.719629ms)
Aug 21 16:22:03.166: INFO: (6) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 17.66122ms)
Aug 21 16:22:03.167: INFO: (6) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 18.317655ms)
Aug 21 16:22:03.167: INFO: (6) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 18.217705ms)
Aug 21 16:22:03.167: INFO: (6) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 18.396713ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 19.207465ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 18.980832ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 18.968254ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 19.77403ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 19.391838ms)
Aug 21 16:22:03.168: INFO: (6) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 20.174451ms)
Aug 21 16:22:03.169: INFO: (6) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 20.036846ms)
Aug 21 16:22:03.169: INFO: (6) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 20.46038ms)
Aug 21 16:22:03.169: INFO: (6) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 20.383268ms)
Aug 21 16:22:03.174: INFO: (7) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 5.114761ms)
Aug 21 16:22:03.176: INFO: (7) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 7.082262ms)
Aug 21 16:22:03.178: INFO: (7) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 8.76379ms)
Aug 21 16:22:03.178: INFO: (7) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 8.803957ms)
Aug 21 16:22:03.179: INFO: (7) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 9.149084ms)
Aug 21 16:22:03.179: INFO: (7) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 9.458573ms)
Aug 21 16:22:03.180: INFO: (7) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 10.393591ms)
Aug 21 16:22:03.180: INFO: (7) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 10.753442ms)
Aug 21 16:22:03.180: INFO: (7) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 10.783311ms)
Aug 21 16:22:03.180: INFO: (7) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 11.12766ms)
Aug 21 16:22:03.180: INFO: (7) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 11.120397ms)
Aug 21 16:22:03.182: INFO: (7) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 12.800161ms)
Aug 21 16:22:03.182: INFO: (7) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 12.749387ms)
Aug 21 16:22:03.183: INFO: (7) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 13.400508ms)
Aug 21 16:22:03.183: INFO: (7) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 13.121033ms)
Aug 21 16:22:03.183: INFO: (7) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 13.484066ms)
Aug 21 16:22:03.195: INFO: (8) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 11.438318ms)
Aug 21 16:22:03.195: INFO: (8) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 11.76055ms)
Aug 21 16:22:03.196: INFO: (8) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.530665ms)
Aug 21 16:22:03.196: INFO: (8) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 12.269066ms)
Aug 21 16:22:03.196: INFO: (8) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 12.711546ms)
Aug 21 16:22:03.196: INFO: (8) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.848612ms)
Aug 21 16:22:03.197: INFO: (8) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 13.27837ms)
Aug 21 16:22:03.198: INFO: (8) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 14.547573ms)
Aug 21 16:22:03.198: INFO: (8) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 14.432565ms)
Aug 21 16:22:03.198: INFO: (8) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 14.372843ms)
Aug 21 16:22:03.198: INFO: (8) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 14.657862ms)
Aug 21 16:22:03.199: INFO: (8) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 15.083849ms)
Aug 21 16:22:03.199: INFO: (8) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 15.5966ms)
Aug 21 16:22:03.199: INFO: (8) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.619343ms)
Aug 21 16:22:03.199: INFO: (8) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.550285ms)
Aug 21 16:22:03.199: INFO: (8) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 15.593516ms)
Aug 21 16:22:03.215: INFO: (9) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 14.938491ms)
Aug 21 16:22:03.215: INFO: (9) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 14.236135ms)
Aug 21 16:22:03.216: INFO: (9) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 15.268011ms)
Aug 21 16:22:03.216: INFO: (9) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 15.960657ms)
Aug 21 16:22:03.216: INFO: (9) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 16.146311ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 16.830359ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 16.665813ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 16.31594ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 15.982449ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 16.430782ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 16.141707ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 16.344849ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 16.917918ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 16.668473ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 16.132768ms)
Aug 21 16:22:03.217: INFO: (9) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 16.469373ms)
Aug 21 16:22:03.230: INFO: (10) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 10.593324ms)
Aug 21 16:22:03.230: INFO: (10) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 10.320297ms)
Aug 21 16:22:03.231: INFO: (10) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 11.837039ms)
Aug 21 16:22:03.231: INFO: (10) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 11.750895ms)
Aug 21 16:22:03.231: INFO: (10) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 12.408169ms)
Aug 21 16:22:03.231: INFO: (10) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 11.781171ms)
Aug 21 16:22:03.231: INFO: (10) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.270847ms)
Aug 21 16:22:03.232: INFO: (10) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 12.20159ms)
Aug 21 16:22:03.232: INFO: (10) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.930871ms)
Aug 21 16:22:03.232: INFO: (10) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 11.667058ms)
Aug 21 16:22:03.235: INFO: (10) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 15.586093ms)
Aug 21 16:22:03.236: INFO: (10) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 17.122082ms)
Aug 21 16:22:03.236: INFO: (10) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 16.836264ms)
Aug 21 16:22:03.236: INFO: (10) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 16.588663ms)
Aug 21 16:22:03.237: INFO: (10) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 17.8555ms)
Aug 21 16:22:03.237: INFO: (10) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 17.194179ms)
Aug 21 16:22:03.248: INFO: (11) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 9.886103ms)
Aug 21 16:22:03.248: INFO: (11) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 10.464655ms)
Aug 21 16:22:03.250: INFO: (11) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 11.323171ms)
Aug 21 16:22:03.250: INFO: (11) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.153155ms)
Aug 21 16:22:03.250: INFO: (11) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.678842ms)
Aug 21 16:22:03.251: INFO: (11) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 13.289471ms)
Aug 21 16:22:03.251: INFO: (11) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 12.33333ms)
Aug 21 16:22:03.251: INFO: (11) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 12.547803ms)
Aug 21 16:22:03.251: INFO: (11) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 12.934045ms)
Aug 21 16:22:03.252: INFO: (11) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 13.744374ms)
Aug 21 16:22:03.252: INFO: (11) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 13.215221ms)
Aug 21 16:22:03.252: INFO: (11) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 13.500287ms)
Aug 21 16:22:03.252: INFO: (11) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 14.070156ms)
Aug 21 16:22:03.253: INFO: (11) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 14.762141ms)
Aug 21 16:22:03.253: INFO: (11) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 13.951182ms)
Aug 21 16:22:03.253: INFO: (11) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 14.014803ms)
Aug 21 16:22:03.264: INFO: (12) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 11.254219ms)
Aug 21 16:22:03.264: INFO: (12) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 10.794568ms)
Aug 21 16:22:03.264: INFO: (12) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 10.81275ms)
Aug 21 16:22:03.264: INFO: (12) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 11.249522ms)
Aug 21 16:22:03.265: INFO: (12) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.602772ms)
Aug 21 16:22:03.265: INFO: (12) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 11.243213ms)
Aug 21 16:22:03.265: INFO: (12) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.729232ms)
Aug 21 16:22:03.266: INFO: (12) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 12.001084ms)
Aug 21 16:22:03.266: INFO: (12) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.511248ms)
Aug 21 16:22:03.266: INFO: (12) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 11.866851ms)
Aug 21 16:22:03.266: INFO: (12) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 12.220114ms)
Aug 21 16:22:03.269: INFO: (12) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 16.242834ms)
Aug 21 16:22:03.271: INFO: (12) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 16.904695ms)
Aug 21 16:22:03.271: INFO: (12) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 17.983339ms)
Aug 21 16:22:03.271: INFO: (12) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 17.589484ms)
Aug 21 16:22:03.272: INFO: (12) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 17.676121ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.82668ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 12.145626ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 11.705532ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 11.91883ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 12.083187ms)
Aug 21 16:22:03.284: INFO: (13) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.24528ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 12.559478ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 12.748585ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 12.908631ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 12.78459ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 12.852442ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 12.861645ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 12.440122ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 13.317578ms)
Aug 21 16:22:03.285: INFO: (13) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 13.356067ms)
Aug 21 16:22:03.286: INFO: (13) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 14.146773ms)
Aug 21 16:22:03.296: INFO: (14) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 9.550223ms)
Aug 21 16:22:03.297: INFO: (14) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 10.239883ms)
Aug 21 16:22:03.297: INFO: (14) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 10.852882ms)
Aug 21 16:22:03.297: INFO: (14) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 10.494674ms)
Aug 21 16:22:03.298: INFO: (14) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.085354ms)
Aug 21 16:22:03.298: INFO: (14) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 11.341706ms)
Aug 21 16:22:03.298: INFO: (14) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.855469ms)
Aug 21 16:22:03.298: INFO: (14) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.66703ms)
Aug 21 16:22:03.299: INFO: (14) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 12.325842ms)
Aug 21 16:22:03.300: INFO: (14) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 13.844337ms)
Aug 21 16:22:03.301: INFO: (14) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 15.568394ms)
Aug 21 16:22:03.302: INFO: (14) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 15.64146ms)
Aug 21 16:22:03.304: INFO: (14) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 17.119519ms)
Aug 21 16:22:03.304: INFO: (14) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 17.434713ms)
Aug 21 16:22:03.304: INFO: (14) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 17.642739ms)
Aug 21 16:22:03.304: INFO: (14) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 17.887371ms)
Aug 21 16:22:03.315: INFO: (15) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 10.330935ms)
Aug 21 16:22:03.315: INFO: (15) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 10.118574ms)
Aug 21 16:22:03.315: INFO: (15) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 10.338384ms)
Aug 21 16:22:03.315: INFO: (15) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 10.612106ms)
Aug 21 16:22:03.315: INFO: (15) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 10.898223ms)
Aug 21 16:22:03.317: INFO: (15) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.98505ms)
Aug 21 16:22:03.317: INFO: (15) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 12.241156ms)
Aug 21 16:22:03.317: INFO: (15) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 12.324557ms)
Aug 21 16:22:03.319: INFO: (15) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 14.53904ms)
Aug 21 16:22:03.319: INFO: (15) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 14.558855ms)
Aug 21 16:22:03.320: INFO: (15) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 14.930522ms)
Aug 21 16:22:03.320: INFO: (15) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 14.82303ms)
Aug 21 16:22:03.320: INFO: (15) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 15.173243ms)
Aug 21 16:22:03.320: INFO: (15) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 15.197693ms)
Aug 21 16:22:03.320: INFO: (15) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 15.285094ms)
Aug 21 16:22:03.321: INFO: (15) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 16.536476ms)
Aug 21 16:22:03.330: INFO: (16) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 8.640907ms)
Aug 21 16:22:03.334: INFO: (16) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 11.291009ms)
Aug 21 16:22:03.334: INFO: (16) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 11.326048ms)
Aug 21 16:22:03.334: INFO: (16) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 11.268816ms)
Aug 21 16:22:03.334: INFO: (16) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 11.649011ms)
Aug 21 16:22:03.335: INFO: (16) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 13.038063ms)
Aug 21 16:22:03.335: INFO: (16) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 13.340323ms)
Aug 21 16:22:03.336: INFO: (16) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 13.965343ms)
Aug 21 16:22:03.337: INFO: (16) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 14.281577ms)
Aug 21 16:22:03.340: INFO: (16) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 18.299687ms)
Aug 21 16:22:03.344: INFO: (16) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 22.426877ms)
Aug 21 16:22:03.345: INFO: (16) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 23.363376ms)
Aug 21 16:22:03.345: INFO: (16) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 23.160889ms)
Aug 21 16:22:03.345: INFO: (16) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 23.162072ms)
Aug 21 16:22:03.345: INFO: (16) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 22.330055ms)
Aug 21 16:22:03.345: INFO: (16) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 22.90455ms)
Aug 21 16:22:03.357: INFO: (17) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 10.941401ms)
Aug 21 16:22:03.358: INFO: (17) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 11.696794ms)
Aug 21 16:22:03.358: INFO: (17) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 11.671167ms)
Aug 21 16:22:03.358: INFO: (17) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 12.254254ms)
Aug 21 16:22:03.360: INFO: (17) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 13.778604ms)
Aug 21 16:22:03.360: INFO: (17) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 13.755795ms)
Aug 21 16:22:03.361: INFO: (17) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 14.676647ms)
Aug 21 16:22:03.362: INFO: (17) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 15.847897ms)
Aug 21 16:22:03.362: INFO: (17) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 17.207718ms)
Aug 21 16:22:03.362: INFO: (17) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 17.020468ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 21.251371ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 21.572362ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 21.677308ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 20.705784ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 21.365316ms)
Aug 21 16:22:03.367: INFO: (17) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 20.890926ms)
Aug 21 16:22:03.375: INFO: (18) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 7.491846ms)
Aug 21 16:22:03.376: INFO: (18) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 8.837624ms)
Aug 21 16:22:03.377: INFO: (18) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 8.854433ms)
Aug 21 16:22:03.383: INFO: (18) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 14.966721ms)
Aug 21 16:22:03.383: INFO: (18) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 15.186151ms)
Aug 21 16:22:03.383: INFO: (18) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 14.988333ms)
Aug 21 16:22:03.383: INFO: (18) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 14.914911ms)
Aug 21 16:22:03.383: INFO: (18) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 15.065327ms)
Aug 21 16:22:03.384: INFO: (18) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 15.480566ms)
Aug 21 16:22:03.385: INFO: (18) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 16.7751ms)
Aug 21 16:22:03.386: INFO: (18) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 18.250957ms)
Aug 21 16:22:03.386: INFO: (18) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 18.284084ms)
Aug 21 16:22:03.387: INFO: (18) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 18.633631ms)
Aug 21 16:22:03.387: INFO: (18) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 19.249319ms)
Aug 21 16:22:03.388: INFO: (18) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 20.201409ms)
Aug 21 16:22:03.388: INFO: (18) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 20.661002ms)
Aug 21 16:22:03.395: INFO: (19) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7/proxy/rewriteme">test</a> (200; 6.111244ms)
Aug 21 16:22:03.395: INFO: (19) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 6.372665ms)
Aug 21 16:22:03.397: INFO: (19) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:460/proxy/: tls baz (200; 8.42966ms)
Aug 21 16:22:03.398: INFO: (19) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">... (200; 8.69693ms)
Aug 21 16:22:03.399: INFO: (19) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 9.77401ms)
Aug 21 16:22:03.399: INFO: (19) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:162/proxy/: bar (200; 10.065566ms)
Aug 21 16:22:03.400: INFO: (19) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:462/proxy/: tls qux (200; 11.33608ms)
Aug 21 16:22:03.400: INFO: (19) /api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/proxy-service-w57mv-gl7h7:1080/proxy/rewriteme">test<... (200; 11.178678ms)
Aug 21 16:22:03.400: INFO: (19) /api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4173/pods/https:proxy-service-w57mv-gl7h7:443/proxy/tlsrewritem... (200; 11.326966ms)
Aug 21 16:22:03.401: INFO: (19) /api/v1/namespaces/proxy-4173/pods/http:proxy-service-w57mv-gl7h7:160/proxy/: foo (200; 12.227438ms)
Aug 21 16:22:03.401: INFO: (19) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname1/proxy/: foo (200; 12.173723ms)
Aug 21 16:22:03.402: INFO: (19) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname1/proxy/: foo (200; 12.744242ms)
Aug 21 16:22:03.402: INFO: (19) /api/v1/namespaces/proxy-4173/services/proxy-service-w57mv:portname2/proxy/: bar (200; 13.241248ms)
Aug 21 16:22:03.403: INFO: (19) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname2/proxy/: tls qux (200; 13.298213ms)
Aug 21 16:22:03.403: INFO: (19) /api/v1/namespaces/proxy-4173/services/https:proxy-service-w57mv:tlsportname1/proxy/: tls baz (200; 13.741428ms)
Aug 21 16:22:03.403: INFO: (19) /api/v1/namespaces/proxy-4173/services/http:proxy-service-w57mv:portname2/proxy/: bar (200; 14.239709ms)
STEP: deleting ReplicationController proxy-service-w57mv in namespace proxy-4173, will wait for the garbage collector to delete the pods
Aug 21 16:22:03.468: INFO: Deleting ReplicationController proxy-service-w57mv took: 11.076079ms
Aug 21 16:22:04.069: INFO: Terminating ReplicationController proxy-service-w57mv pods took: 601.418773ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:22:06.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4173" for this suite.
Aug 21 16:22:14.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:14.441: INFO: namespace proxy-4173 deletion completed in 8.164386316s

• [SLOW TEST:16.675 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:22:14.442: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 21 16:22:14.543: INFO: Waiting up to 5m0s for pod "var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105" in namespace "var-expansion-4253" to be "success or failure"
Aug 21 16:22:14.548: INFO: Pod "var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926638ms
Aug 21 16:22:16.553: INFO: Pod "var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009661082s
Aug 21 16:22:18.560: INFO: Pod "var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016411248s
STEP: Saw pod success
Aug 21 16:22:18.560: INFO: Pod "var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105" satisfied condition "success or failure"
Aug 21 16:22:18.564: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105 container dapi-container: <nil>
STEP: delete the pod
Aug 21 16:22:18.600: INFO: Waiting for pod var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105 to disappear
Aug 21 16:22:18.603: INFO: Pod var-expansion-da27efa4-8f4b-4ddd-aa78-4d6a6f9b1105 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:22:18.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4253" for this suite.
Aug 21 16:22:24.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:24.805: INFO: namespace var-expansion-4253 deletion completed in 6.192554058s

• [SLOW TEST:10.364 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:22:24.816: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9f51f529-28c5-4e5c-bfc1-bd0eeb5e4c7a
STEP: Creating a pod to test consume configMaps
Aug 21 16:22:24.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-9adb1706-993b-4356-839a-59a89473d597" in namespace "configmap-9676" to be "success or failure"
Aug 21 16:22:24.921: INFO: Pod "pod-configmaps-9adb1706-993b-4356-839a-59a89473d597": Phase="Pending", Reason="", readiness=false. Elapsed: 3.888445ms
Aug 21 16:22:26.928: INFO: Pod "pod-configmaps-9adb1706-993b-4356-839a-59a89473d597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010781855s
Aug 21 16:22:28.933: INFO: Pod "pod-configmaps-9adb1706-993b-4356-839a-59a89473d597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01588599s
STEP: Saw pod success
Aug 21 16:22:28.933: INFO: Pod "pod-configmaps-9adb1706-993b-4356-839a-59a89473d597" satisfied condition "success or failure"
Aug 21 16:22:28.937: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-configmaps-9adb1706-993b-4356-839a-59a89473d597 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:22:28.968: INFO: Waiting for pod pod-configmaps-9adb1706-993b-4356-839a-59a89473d597 to disappear
Aug 21 16:22:28.972: INFO: Pod pod-configmaps-9adb1706-993b-4356-839a-59a89473d597 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:22:28.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9676" for this suite.
Aug 21 16:22:34.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:35.149: INFO: namespace configmap-9676 deletion completed in 6.17099465s

• [SLOW TEST:10.333 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:22:35.149: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:22:35.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-865" for this suite.
Aug 21 16:22:57.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:57.435: INFO: namespace pods-865 deletion completed in 22.181491771s

• [SLOW TEST:22.286 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:22:57.437: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 21 16:22:57.519: INFO: Waiting up to 5m0s for pod "pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56" in namespace "emptydir-4748" to be "success or failure"
Aug 21 16:22:57.524: INFO: Pod "pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809991ms
Aug 21 16:22:59.527: INFO: Pod "pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00841073s
Aug 21 16:23:01.534: INFO: Pod "pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015502218s
STEP: Saw pod success
Aug 21 16:23:01.534: INFO: Pod "pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56" satisfied condition "success or failure"
Aug 21 16:23:01.539: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56 container test-container: <nil>
STEP: delete the pod
Aug 21 16:23:01.564: INFO: Waiting for pod pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56 to disappear
Aug 21 16:23:01.568: INFO: Pod pod-477bd3f7-6f80-4347-a0da-d1c6ba128a56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:23:01.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4748" for this suite.
Aug 21 16:23:07.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:07.767: INFO: namespace emptydir-4748 deletion completed in 6.194146353s

• [SLOW TEST:10.330 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:23:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-a57ce1dd-0ca0-4c7b-826e-a72316ea1c2e
STEP: Creating secret with name secret-projected-all-test-volume-d0219a3a-cfd2-49a2-8fe3-818158b40a93
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 21 16:23:07.963: INFO: Waiting up to 5m0s for pod "projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107" in namespace "projected-4275" to be "success or failure"
Aug 21 16:23:07.967: INFO: Pod "projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032078ms
Aug 21 16:23:09.975: INFO: Pod "projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011832469s
STEP: Saw pod success
Aug 21 16:23:09.975: INFO: Pod "projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107" satisfied condition "success or failure"
Aug 21 16:23:09.980: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 21 16:23:10.016: INFO: Waiting for pod projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107 to disappear
Aug 21 16:23:10.027: INFO: Pod projected-volume-ab5ed3d1-fcaa-4b45-91e7-d1a63f84f107 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:23:10.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4275" for this suite.
Aug 21 16:23:18.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:18.205: INFO: namespace projected-4275 deletion completed in 8.171923963s

• [SLOW TEST:10.438 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:23:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-3aaae4a7-d645-47c9-bdb5-737f804492d8
STEP: Creating a pod to test consume configMaps
Aug 21 16:23:18.321: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246" in namespace "projected-9295" to be "success or failure"
Aug 21 16:23:18.325: INFO: Pod "pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785987ms
Aug 21 16:23:20.329: INFO: Pod "pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008371237s
Aug 21 16:23:22.334: INFO: Pod "pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013621159s
STEP: Saw pod success
Aug 21 16:23:22.335: INFO: Pod "pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246" satisfied condition "success or failure"
Aug 21 16:23:22.339: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:23:22.365: INFO: Waiting for pod pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246 to disappear
Aug 21 16:23:22.369: INFO: Pod pod-projected-configmaps-43163217-0521-4b29-939a-9692478d4246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:23:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9295" for this suite.
Aug 21 16:23:28.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:28.554: INFO: namespace projected-9295 deletion completed in 6.178743463s

• [SLOW TEST:10.344 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:23:28.556: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 21 16:23:28.656: INFO: Waiting up to 5m0s for pod "pod-ebe66caa-94b4-4651-a143-892f67d72ffb" in namespace "emptydir-4102" to be "success or failure"
Aug 21 16:23:28.662: INFO: Pod "pod-ebe66caa-94b4-4651-a143-892f67d72ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.904175ms
Aug 21 16:23:30.669: INFO: Pod "pod-ebe66caa-94b4-4651-a143-892f67d72ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012627769s
Aug 21 16:23:32.675: INFO: Pod "pod-ebe66caa-94b4-4651-a143-892f67d72ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018326343s
STEP: Saw pod success
Aug 21 16:23:32.675: INFO: Pod "pod-ebe66caa-94b4-4651-a143-892f67d72ffb" satisfied condition "success or failure"
Aug 21 16:23:32.679: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-ebe66caa-94b4-4651-a143-892f67d72ffb container test-container: <nil>
STEP: delete the pod
Aug 21 16:23:32.712: INFO: Waiting for pod pod-ebe66caa-94b4-4651-a143-892f67d72ffb to disappear
Aug 21 16:23:32.722: INFO: Pod pod-ebe66caa-94b4-4651-a143-892f67d72ffb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:23:32.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4102" for this suite.
Aug 21 16:23:38.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:38.896: INFO: namespace emptydir-4102 deletion completed in 6.167476922s

• [SLOW TEST:10.340 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:23:38.896: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-4f0a14a2-4380-4596-b45b-d7ba964ec7b5
STEP: Creating a pod to test consume secrets
Aug 21 16:23:39.096: INFO: Waiting up to 5m0s for pod "pod-secrets-becead2c-548f-41ce-badb-e813abb90a93" in namespace "secrets-9590" to be "success or failure"
Aug 21 16:23:39.101: INFO: Pod "pod-secrets-becead2c-548f-41ce-badb-e813abb90a93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505246ms
Aug 21 16:23:41.108: INFO: Pod "pod-secrets-becead2c-548f-41ce-badb-e813abb90a93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011684899s
STEP: Saw pod success
Aug 21 16:23:41.108: INFO: Pod "pod-secrets-becead2c-548f-41ce-badb-e813abb90a93" satisfied condition "success or failure"
Aug 21 16:23:41.115: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-secrets-becead2c-548f-41ce-badb-e813abb90a93 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:23:41.168: INFO: Waiting for pod pod-secrets-becead2c-548f-41ce-badb-e813abb90a93 to disappear
Aug 21 16:23:41.196: INFO: Pod pod-secrets-becead2c-548f-41ce-badb-e813abb90a93 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:23:41.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9590" for this suite.
Aug 21 16:23:47.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:47.366: INFO: namespace secrets-9590 deletion completed in 6.162405773s

• [SLOW TEST:8.470 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:23:47.367: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 21 16:24:17.990: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:24:17.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0821 16:24:17.990064      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3459" for this suite.
Aug 21 16:24:24.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:24.172: INFO: namespace gc-3459 deletion completed in 6.176524273s

• [SLOW TEST:36.805 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:24:24.172: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 21 16:24:24.289: INFO: Waiting up to 5m0s for pod "pod-7425232e-6a07-4a8e-8dad-6a47a85b9109" in namespace "emptydir-3201" to be "success or failure"
Aug 21 16:24:24.293: INFO: Pod "pod-7425232e-6a07-4a8e-8dad-6a47a85b9109": Phase="Pending", Reason="", readiness=false. Elapsed: 3.353676ms
Aug 21 16:24:26.301: INFO: Pod "pod-7425232e-6a07-4a8e-8dad-6a47a85b9109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011268645s
STEP: Saw pod success
Aug 21 16:24:26.301: INFO: Pod "pod-7425232e-6a07-4a8e-8dad-6a47a85b9109" satisfied condition "success or failure"
Aug 21 16:24:26.304: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-7425232e-6a07-4a8e-8dad-6a47a85b9109 container test-container: <nil>
STEP: delete the pod
Aug 21 16:24:26.344: INFO: Waiting for pod pod-7425232e-6a07-4a8e-8dad-6a47a85b9109 to disappear
Aug 21 16:24:26.349: INFO: Pod pod-7425232e-6a07-4a8e-8dad-6a47a85b9109 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:24:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3201" for this suite.
Aug 21 16:24:32.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:32.537: INFO: namespace emptydir-3201 deletion completed in 6.177052888s

• [SLOW TEST:8.365 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:24:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:24:32.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932" in namespace "downward-api-636" to be "success or failure"
Aug 21 16:24:32.668: INFO: Pod "downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643961ms
Aug 21 16:24:34.674: INFO: Pod "downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00928663s
STEP: Saw pod success
Aug 21 16:24:34.674: INFO: Pod "downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932" satisfied condition "success or failure"
Aug 21 16:24:34.678: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932 container client-container: <nil>
STEP: delete the pod
Aug 21 16:24:34.703: INFO: Waiting for pod downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932 to disappear
Aug 21 16:24:34.706: INFO: Pod downwardapi-volume-f1ea3a36-0c49-4f08-b74b-93aba30c0932 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:24:34.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-636" for this suite.
Aug 21 16:24:40.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:40.887: INFO: namespace downward-api-636 deletion completed in 6.176254782s

• [SLOW TEST:8.347 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:24:40.889: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 21 16:24:40.980: INFO: Waiting up to 5m0s for pod "pod-f8cc006d-264a-49b5-bffe-78a6068be49f" in namespace "emptydir-549" to be "success or failure"
Aug 21 16:24:40.984: INFO: Pod "pod-f8cc006d-264a-49b5-bffe-78a6068be49f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051067ms
Aug 21 16:24:42.989: INFO: Pod "pod-f8cc006d-264a-49b5-bffe-78a6068be49f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009064064s
STEP: Saw pod success
Aug 21 16:24:42.989: INFO: Pod "pod-f8cc006d-264a-49b5-bffe-78a6068be49f" satisfied condition "success or failure"
Aug 21 16:24:42.994: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-f8cc006d-264a-49b5-bffe-78a6068be49f container test-container: <nil>
STEP: delete the pod
Aug 21 16:24:43.035: INFO: Waiting for pod pod-f8cc006d-264a-49b5-bffe-78a6068be49f to disappear
Aug 21 16:24:43.044: INFO: Pod pod-f8cc006d-264a-49b5-bffe-78a6068be49f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:24:43.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-549" for this suite.
Aug 21 16:24:49.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:49.264: INFO: namespace emptydir-549 deletion completed in 6.212854646s

• [SLOW TEST:8.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:24:49.266: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 21 16:24:49.354: INFO: Waiting up to 5m0s for pod "var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197" in namespace "var-expansion-4062" to be "success or failure"
Aug 21 16:24:49.362: INFO: Pod "var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197": Phase="Pending", Reason="", readiness=false. Elapsed: 7.3767ms
Aug 21 16:24:51.367: INFO: Pod "var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012743759s
Aug 21 16:24:53.372: INFO: Pod "var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017348629s
STEP: Saw pod success
Aug 21 16:24:53.372: INFO: Pod "var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197" satisfied condition "success or failure"
Aug 21 16:24:53.375: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197 container dapi-container: <nil>
STEP: delete the pod
Aug 21 16:24:53.399: INFO: Waiting for pod var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197 to disappear
Aug 21 16:24:53.402: INFO: Pod var-expansion-1161f1ad-d40d-4d9e-b8eb-233170420197 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:24:53.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4062" for this suite.
Aug 21 16:24:59.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:59.582: INFO: namespace var-expansion-4062 deletion completed in 6.173453786s

• [SLOW TEST:10.316 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:24:59.585: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 21 16:25:04.238: INFO: Successfully updated pod "labelsupdatea9f344ad-47bc-4894-aaac-63193f4cf0f6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:25:06.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7299" for this suite.
Aug 21 16:25:30.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:25:30.449: INFO: namespace downward-api-7299 deletion completed in 24.179792024s

• [SLOW TEST:30.864 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:25:30.450: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-5e2f39c5-0be3-4471-803a-a9ac27c3ca65 in namespace container-probe-4267
Aug 21 16:25:34.552: INFO: Started pod busybox-5e2f39c5-0be3-4471-803a-a9ac27c3ca65 in namespace container-probe-4267
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 16:25:34.556: INFO: Initial restart count of pod busybox-5e2f39c5-0be3-4471-803a-a9ac27c3ca65 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:29:35.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4267" for this suite.
Aug 21 16:29:43.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:29:43.800: INFO: namespace container-probe-4267 deletion completed in 8.189007342s

• [SLOW TEST:253.350 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:29:43.805: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8274
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8274
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8274
Aug 21 16:29:43.893: INFO: Found 0 stateful pods, waiting for 1
Aug 21 16:29:53.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 21 16:30:03.899: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 21 16:30:13.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 21 16:30:13.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:30:14.549: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:30:14.549: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:30:14.549: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:30:14.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 21 16:30:24.560: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:30:24.560: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:30:24.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999489s
Aug 21 16:30:25.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996053801s
Aug 21 16:30:26.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.940441691s
Aug 21 16:30:27.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.934934176s
Aug 21 16:30:28.656: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.923709018s
Aug 21 16:30:29.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.917264488s
Aug 21 16:30:30.666: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.912045842s
Aug 21 16:30:31.671: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.907194301s
Aug 21 16:30:32.676: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.902383706s
Aug 21 16:30:33.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 897.030159ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8274
Aug 21 16:30:34.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:30:38.829: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:30:38.829: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:30:38.829: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:30:38.835: INFO: Found 1 stateful pods, waiting for 3
Aug 21 16:30:48.841: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:30:48.841: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:30:48.841: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 21 16:30:58.841: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:30:58.841: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:30:58.841: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 21 16:30:58.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:30:59.241: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:30:59.241: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:30:59.241: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:30:59.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:30:59.688: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:30:59.688: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:30:59.688: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:30:59.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:31:00.104: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:31:00.104: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:31:00.104: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:31:00.104: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:31:00.108: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 21 16:31:10.119: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:31:10.119: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:31:10.119: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:31:10.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999986267s
Aug 21 16:31:11.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994184789s
Aug 21 16:31:12.155: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988022632s
Aug 21 16:31:13.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982617298s
Aug 21 16:31:14.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975628054s
Aug 21 16:31:15.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969888948s
Aug 21 16:31:16.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962795624s
Aug 21 16:31:17.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957374997s
Aug 21 16:31:18.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952008154s
Aug 21 16:31:19.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.225681ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8274
Aug 21 16:31:20.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:20.768: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:31:20.768: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:31:20.768: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:31:20.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:21.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:31:21.184: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:31:21.184: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:31:21.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:21.730: INFO: rc: 126
Aug 21 16:31:21.730: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "read init-p: connection reset by peer": unknown
 command terminated with exit code 126
 [] <nil> 0xc001d573e0 exit status 126 <nil> <nil> true [0xc001e6a4e0 0xc001e6a4f8 0xc001e6a510] [0xc001e6a4e0 0xc001e6a4f8 0xc001e6a510] [0xc001e6a4f0 0xc001e6a508] [0x9d21f0 0x9d21f0] 0xc001ec1ec0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "read init-p: connection reset by peer": unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Aug 21 16:31:31.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:31.822: INFO: rc: 1
Aug 21 16:31:31.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025d01e0 exit status 1 <nil> <nil> true [0xc0021764c8 0xc0021764e0 0xc0021764f8] [0xc0021764c8 0xc0021764e0 0xc0021764f8] [0xc0021764d8 0xc0021764f0] [0x9d21f0 0x9d21f0] 0xc002d230e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:31:41.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:42.541: INFO: rc: 1
Aug 21 16:31:42.541: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025d05a0 exit status 1 <nil> <nil> true [0xc002176500 0xc002176518 0xc002176530] [0xc002176500 0xc002176518 0xc002176530] [0xc002176510 0xc002176528] [0x9d21f0 0x9d21f0] 0xc002d23800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:31:52.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:31:52.675: INFO: rc: 1
Aug 21 16:31:52.675: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001d57950 exit status 1 <nil> <nil> true [0xc001e6a518 0xc001e6a530 0xc001e6a548] [0xc001e6a518 0xc001e6a530 0xc001e6a548] [0xc001e6a528 0xc001e6a540] [0x9d21f0 0x9d21f0] 0xc002bfc360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:02.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:02.781: INFO: rc: 1
Aug 21 16:32:02.781: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025d0a50 exit status 1 <nil> <nil> true [0xc002176538 0xc002176550 0xc002176568] [0xc002176538 0xc002176550 0xc002176568] [0xc002176548 0xc002176560] [0x9d21f0 0x9d21f0] 0xc0029301e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:12.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:12.899: INFO: rc: 1
Aug 21 16:32:12.899: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4510 exit status 1 <nil> <nil> true [0xc002176028 0xc002176040 0xc002176070] [0xc002176028 0xc002176040 0xc002176070] [0xc002176038 0xc002176050] [0x9d21f0 0x9d21f0] 0xc002d222a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:22.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:23.009: INFO: rc: 1
Aug 21 16:32:23.009: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223c360 exit status 1 <nil> <nil> true [0xc001e6a010 0xc001e6a040 0xc001e6a080] [0xc001e6a010 0xc001e6a040 0xc001e6a080] [0xc001e6a038 0xc001e6a068] [0x9d21f0 0x9d21f0] 0xc001ec03c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:33.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:33.118: INFO: rc: 1
Aug 21 16:32:33.118: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a48a0 exit status 1 <nil> <nil> true [0xc002176088 0xc0021760a0 0xc0021760c0] [0xc002176088 0xc0021760a0 0xc0021760c0] [0xc002176098 0xc0021760b0] [0x9d21f0 0x9d21f0] 0xc002d22960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:43.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:43.217: INFO: rc: 1
Aug 21 16:32:43.218: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223c750 exit status 1 <nil> <nil> true [0xc001e6a090 0xc001e6a0d0 0xc001e6a0e8] [0xc001e6a090 0xc001e6a0d0 0xc001e6a0e8] [0xc001e6a0c8 0xc001e6a0e0] [0x9d21f0 0x9d21f0] 0xc001ec0720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:32:53.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:32:53.350: INFO: rc: 1
Aug 21 16:32:53.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4c60 exit status 1 <nil> <nil> true [0xc0021760d8 0xc002176128 0xc002176150] [0xc0021760d8 0xc002176128 0xc002176150] [0xc002176108 0xc002176148] [0x9d21f0 0x9d21f0] 0xc002d22f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:03.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:03.470: INFO: rc: 1
Aug 21 16:33:03.470: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223cb40 exit status 1 <nil> <nil> true [0xc001e6a0f0 0xc001e6a108 0xc001e6a150] [0xc001e6a0f0 0xc001e6a108 0xc001e6a150] [0xc001e6a100 0xc001e6a138] [0x9d21f0 0x9d21f0] 0xc001ec0a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:13.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:13.603: INFO: rc: 1
Aug 21 16:33:13.603: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223cf00 exit status 1 <nil> <nil> true [0xc001e6a160 0xc001e6a188 0xc001e6a1a0] [0xc001e6a160 0xc001e6a188 0xc001e6a1a0] [0xc001e6a180 0xc001e6a198] [0x9d21f0 0x9d21f0] 0xc001ec0ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:23.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:23.725: INFO: rc: 1
Aug 21 16:33:23.725: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223d2f0 exit status 1 <nil> <nil> true [0xc001e6a1b0 0xc001e6a1c8 0xc001e6a1e0] [0xc001e6a1b0 0xc001e6a1c8 0xc001e6a1e0] [0xc001e6a1c0 0xc001e6a1d8] [0x9d21f0 0x9d21f0] 0xc001ec1200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:33.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:33.825: INFO: rc: 1
Aug 21 16:33:33.825: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4ff0 exit status 1 <nil> <nil> true [0xc002176158 0xc002176170 0xc002176190] [0xc002176158 0xc002176170 0xc002176190] [0xc002176168 0xc002176180] [0x9d21f0 0x9d21f0] 0xc002d23440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:43.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:43.957: INFO: rc: 1
Aug 21 16:33:43.957: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a5350 exit status 1 <nil> <nil> true [0xc002176198 0xc0021761b0 0xc0021761e0] [0xc002176198 0xc0021761b0 0xc0021761e0] [0xc0021761a8 0xc0021761c0] [0x9d21f0 0x9d21f0] 0xc002d23bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:33:53.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:33:54.067: INFO: rc: 1
Aug 21 16:33:54.067: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223d860 exit status 1 <nil> <nil> true [0xc001e6a1e8 0xc001e6a200 0xc001e6a218] [0xc001e6a1e8 0xc001e6a200 0xc001e6a218] [0xc001e6a1f8 0xc001e6a210] [0x9d21f0 0x9d21f0] 0xc001ec1560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:04.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:04.185: INFO: rc: 1
Aug 21 16:34:04.185: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223dbf0 exit status 1 <nil> <nil> true [0xc001e6a220 0xc001e6a238 0xc001e6a250] [0xc001e6a220 0xc001e6a238 0xc001e6a250] [0xc001e6a230 0xc001e6a248] [0x9d21f0 0x9d21f0] 0xc001ec19e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:14.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:14.295: INFO: rc: 1
Aug 21 16:34:14.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4540 exit status 1 <nil> <nil> true [0xc002176028 0xc002176040 0xc002176070] [0xc002176028 0xc002176040 0xc002176070] [0xc002176038 0xc002176050] [0x9d21f0 0x9d21f0] 0xc002d222a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:24.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:24.395: INFO: rc: 1
Aug 21 16:34:24.395: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4900 exit status 1 <nil> <nil> true [0xc002176088 0xc0021760a0 0xc0021760c0] [0xc002176088 0xc0021760a0 0xc0021760c0] [0xc002176098 0xc0021760b0] [0x9d21f0 0x9d21f0] 0xc002d22960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:34.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:34.505: INFO: rc: 1
Aug 21 16:34:34.505: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4cf0 exit status 1 <nil> <nil> true [0xc0021760d8 0xc002176128 0xc002176150] [0xc0021760d8 0xc002176128 0xc002176150] [0xc002176108 0xc002176148] [0x9d21f0 0x9d21f0] 0xc002d22f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:44.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:44.639: INFO: rc: 1
Aug 21 16:34:44.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a50b0 exit status 1 <nil> <nil> true [0xc002176158 0xc002176170 0xc002176190] [0xc002176158 0xc002176170 0xc002176190] [0xc002176168 0xc002176180] [0x9d21f0 0x9d21f0] 0xc002d23440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:34:54.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:34:54.776: INFO: rc: 1
Aug 21 16:34:54.777: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223c390 exit status 1 <nil> <nil> true [0xc001e6a010 0xc001e6a040 0xc001e6a080] [0xc001e6a010 0xc001e6a040 0xc001e6a080] [0xc001e6a038 0xc001e6a068] [0x9d21f0 0x9d21f0] 0xc001ec03c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:04.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:04.876: INFO: rc: 1
Aug 21 16:35:04.876: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223c7e0 exit status 1 <nil> <nil> true [0xc001e6a090 0xc001e6a0d0 0xc001e6a0e8] [0xc001e6a090 0xc001e6a0d0 0xc001e6a0e8] [0xc001e6a0c8 0xc001e6a0e0] [0x9d21f0 0x9d21f0] 0xc001ec0720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:14.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:14.987: INFO: rc: 1
Aug 21 16:35:14.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a54a0 exit status 1 <nil> <nil> true [0xc002176198 0xc0021761b0 0xc0021761e0] [0xc002176198 0xc0021761b0 0xc0021761e0] [0xc0021761a8 0xc0021761c0] [0x9d21f0 0x9d21f0] 0xc002d23bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:24.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:25.075: INFO: rc: 1
Aug 21 16:35:25.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223cc00 exit status 1 <nil> <nil> true [0xc001e6a0f0 0xc001e6a108 0xc001e6a150] [0xc001e6a0f0 0xc001e6a108 0xc001e6a150] [0xc001e6a100 0xc001e6a138] [0x9d21f0 0x9d21f0] 0xc001ec0a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:35.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:35.179: INFO: rc: 1
Aug 21 16:35:35.180: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a5d40 exit status 1 <nil> <nil> true [0xc0021761e8 0xc002176200 0xc002176228] [0xc0021761e8 0xc002176200 0xc002176228] [0xc0021761f8 0xc002176210] [0x9d21f0 0x9d21f0] 0xc0033a02a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:45.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:45.304: INFO: rc: 1
Aug 21 16:35:45.304: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022461b0 exit status 1 <nil> <nil> true [0xc002176248 0xc002176270 0xc002176288] [0xc002176248 0xc002176270 0xc002176288] [0xc002176268 0xc002176280] [0x9d21f0 0x9d21f0] 0xc0033a0600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:35:55.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:35:55.406: INFO: rc: 1
Aug 21 16:35:55.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00223cfc0 exit status 1 <nil> <nil> true [0xc001e6a160 0xc001e6a188 0xc001e6a1a0] [0xc001e6a160 0xc001e6a188 0xc001e6a1a0] [0xc001e6a180 0xc001e6a198] [0x9d21f0 0x9d21f0] 0xc001ec0ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:36:05.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:36:05.496: INFO: rc: 1
Aug 21 16:36:05.497: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002246600 exit status 1 <nil> <nil> true [0xc002176290 0xc0021762a8 0xc0021762c0] [0xc002176290 0xc0021762a8 0xc0021762c0] [0xc0021762a0 0xc0021762b8] [0x9d21f0 0x9d21f0] 0xc0033a09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:36:15.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:36:15.616: INFO: rc: 1
Aug 21 16:36:15.617: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033a4510 exit status 1 <nil> <nil> true [0xc002176028 0xc002176040 0xc002176070] [0xc002176028 0xc002176040 0xc002176070] [0xc002176038 0xc002176050] [0x9d21f0 0x9d21f0] 0xc002d222a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 16:36:25.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8274 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:36:25.730: INFO: rc: 1
Aug 21 16:36:25.730: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Aug 21 16:36:25.730: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 21 16:36:25.748: INFO: Deleting all statefulset in ns statefulset-8274
Aug 21 16:36:25.753: INFO: Scaling statefulset ss to 0
Aug 21 16:36:25.766: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:36:25.771: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:36:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8274" for this suite.
Aug 21 16:36:36.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:36:36.713: INFO: namespace statefulset-8274 deletion completed in 10.833402266s

• [SLOW TEST:412.908 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:36:36.714: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 16:36:36.903: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:36.903: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:36.903: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:36.913: INFO: Number of nodes with available pods: 0
Aug 21 16:36:36.913: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:36:37.942: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:37.942: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:37.942: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:37.952: INFO: Number of nodes with available pods: 0
Aug 21 16:36:37.952: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:36:38.923: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:38.923: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:38.923: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:38.928: INFO: Number of nodes with available pods: 1
Aug 21 16:36:38.928: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:36:39.926: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.926: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.926: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.931: INFO: Number of nodes with available pods: 2
Aug 21 16:36:39.931: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 21 16:36:39.959: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.959: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.960: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 16:36:39.964: INFO: Number of nodes with available pods: 2
Aug 21 16:36:39.964: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3083, will wait for the garbage collector to delete the pods
Aug 21 16:36:41.063: INFO: Deleting DaemonSet.extensions daemon-set took: 18.918943ms
Aug 21 16:36:41.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.277565ms
Aug 21 16:36:47.869: INFO: Number of nodes with available pods: 0
Aug 21 16:36:47.869: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 16:36:47.872: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3083/daemonsets","resourceVersion":"14363"},"items":null}

Aug 21 16:36:47.875: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3083/pods","resourceVersion":"14363"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:36:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3083" for this suite.
Aug 21 16:36:55.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:36:55.364: INFO: namespace daemonsets-3083 deletion completed in 7.469189901s

• [SLOW TEST:18.650 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:36:55.367: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6051
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6051
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6051
Aug 21 16:36:55.479: INFO: Found 0 stateful pods, waiting for 1
Aug 21 16:37:05.487: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 21 16:37:05.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:37:05.896: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:37:05.896: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:37:05.896: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:37:05.901: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 21 16:37:15.907: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:37:15.907: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:37:15.923: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:15.923: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:15.923: INFO: 
Aug 21 16:37:15.923: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 21 16:37:16.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996425944s
Aug 21 16:37:17.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990281705s
Aug 21 16:37:18.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984067692s
Aug 21 16:37:19.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977869047s
Aug 21 16:37:20.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970203192s
Aug 21 16:37:21.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964714501s
Aug 21 16:37:22.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95793901s
Aug 21 16:37:23.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951226824s
Aug 21 16:37:24.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.047618ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6051
Aug 21 16:37:25.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:37:26.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:37:26.392: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:37:26.392: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:37:26.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:37:26.796: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 21 16:37:26.796: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:37:26.796: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:37:26.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:37:27.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 21 16:37:27.302: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:37:27.302: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:37:27.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:37:27.310: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:37:27.310: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 21 16:37:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:37:27.738: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:37:27.738: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:37:27.738: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:37:27.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:37:28.149: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:37:28.149: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:37:28.149: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:37:28.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-6051 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:37:28.928: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:37:28.928: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:37:28.928: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:37:28.928: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:37:28.935: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 21 16:37:38.946: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:37:38.946: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:37:38.946: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 16:37:38.962: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:38.962: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:38.962: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:38.962: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:38.963: INFO: 
Aug 21 16:37:38.963: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:39.970: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:39.970: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:39.970: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:39.970: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:39.970: INFO: 
Aug 21 16:37:39.970: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:40.979: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:40.979: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:40.979: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:40.979: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:40.979: INFO: 
Aug 21 16:37:40.979: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:41.985: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:41.985: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:41.985: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:41.985: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:41.985: INFO: 
Aug 21 16:37:41.985: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:42.992: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:42.992: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:42.992: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:42.992: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:42.992: INFO: 
Aug 21 16:37:42.992: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:43.999: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:43.999: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:43.999: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:43.999: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:43.999: INFO: 
Aug 21 16:37:43.999: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:45.067: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:45.067: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:45.067: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:45.067: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:45.067: INFO: 
Aug 21 16:37:45.067: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:46.075: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:46.075: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:46.075: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:46.075: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:46.075: INFO: 
Aug 21 16:37:46.075: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:47.084: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Aug 21 16:37:47.084: INFO: ss-0  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:36:55 +0000 UTC  }]
Aug 21 16:37:47.084: INFO: ss-1  3c511303-d822-436a-a9c7-85bfe5dec0f1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:47.084: INFO: ss-2  61823cca-1200-4e72-835e-06a7dddcdaa7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:37:15 +0000 UTC  }]
Aug 21 16:37:47.084: INFO: 
Aug 21 16:37:47.084: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 16:37:48.090: INFO: Verifying statefulset ss doesn't scale past 0 for another 872.616481ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6051
Aug 21 16:37:49.098: INFO: Scaling statefulset ss to 0
Aug 21 16:37:49.110: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 21 16:37:49.115: INFO: Deleting all statefulset in ns statefulset-6051
Aug 21 16:37:49.120: INFO: Scaling statefulset ss to 0
Aug 21 16:37:49.136: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:37:49.140: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:37:49.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6051" for this suite.
Aug 21 16:37:55.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:37:55.391: INFO: namespace statefulset-6051 deletion completed in 6.222312699s

• [SLOW TEST:60.024 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:37:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8d0ac79d-81bc-4b63-b88b-f1b8e9893cef
STEP: Creating a pod to test consume secrets
Aug 21 16:37:55.491: INFO: Waiting up to 5m0s for pod "pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1" in namespace "secrets-6019" to be "success or failure"
Aug 21 16:37:55.495: INFO: Pod "pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574305ms
Aug 21 16:37:57.589: INFO: Pod "pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.098196167s
STEP: Saw pod success
Aug 21 16:37:57.589: INFO: Pod "pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1" satisfied condition "success or failure"
Aug 21 16:37:57.598: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:37:57.714: INFO: Waiting for pod pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1 to disappear
Aug 21 16:37:57.720: INFO: Pod pod-secrets-03bf793e-c39f-48a5-87e9-6a243e61bfc1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:37:57.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6019" for this suite.
Aug 21 16:38:03.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:03.961: INFO: namespace secrets-6019 deletion completed in 6.232562576s

• [SLOW TEST:8.568 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:38:03.963: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0821 16:38:13.794614      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 16:38:13.794: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:38:13.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8472" for this suite.
Aug 21 16:38:21.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:22.028: INFO: namespace gc-8472 deletion completed in 8.228325947s

• [SLOW TEST:18.065 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:38:22.028: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9b6c7bfd-d83c-4ca4-b336-5367e4294fd6
STEP: Creating a pod to test consume configMaps
Aug 21 16:38:22.215: INFO: Waiting up to 5m0s for pod "pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6" in namespace "configmap-2226" to be "success or failure"
Aug 21 16:38:22.221: INFO: Pod "pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764984ms
Aug 21 16:38:24.227: INFO: Pod "pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011914691s
Aug 21 16:38:26.232: INFO: Pod "pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016651648s
STEP: Saw pod success
Aug 21 16:38:26.232: INFO: Pod "pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6" satisfied condition "success or failure"
Aug 21 16:38:26.236: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:38:26.284: INFO: Waiting for pod pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6 to disappear
Aug 21 16:38:26.290: INFO: Pod pod-configmaps-809bce22-2b3a-4722-b10a-3ac0ec5869b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:38:26.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2226" for this suite.
Aug 21 16:38:32.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:32.513: INFO: namespace configmap-2226 deletion completed in 6.216957955s

• [SLOW TEST:10.485 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:38:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:38:32.603: INFO: Creating deployment "test-recreate-deployment"
Aug 21 16:38:32.614: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 21 16:38:32.625: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Aug 21 16:38:34.636: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 21 16:38:34.640: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 21 16:38:34.653: INFO: Updating deployment test-recreate-deployment
Aug 21 16:38:34.653: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 21 16:38:34.988: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4295,SelfLink:/apis/apps/v1/namespaces/deployment-4295/deployments/test-recreate-deployment,UID:24636886-c8f9-4be9-94ff-428dccc0d679,ResourceVersion:15132,Generation:2,CreationTimestamp:2019-08-21 16:38:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-21 16:38:34 +0000 UTC 2019-08-21 16:38:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-21 16:38:34 +0000 UTC 2019-08-21 16:38:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 21 16:38:34.994: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-4295,SelfLink:/apis/apps/v1/namespaces/deployment-4295/replicasets/test-recreate-deployment-5c8c9cc69d,UID:2836380f-b3db-46b0-aa3d-f2aa407cde3a,ResourceVersion:15131,Generation:1,CreationTimestamp:2019-08-21 16:38:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 24636886-c8f9-4be9-94ff-428dccc0d679 0xc001ac0c67 0xc001ac0c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 16:38:34.994: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 21 16:38:34.994: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-4295,SelfLink:/apis/apps/v1/namespaces/deployment-4295/replicasets/test-recreate-deployment-6df85df6b9,UID:d2f20800-61e9-4605-ad5a-58f7c559b6f7,ResourceVersion:15121,Generation:2,CreationTimestamp:2019-08-21 16:38:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 24636886-c8f9-4be9-94ff-428dccc0d679 0xc001ac0d37 0xc001ac0d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 16:38:34.999: INFO: Pod "test-recreate-deployment-5c8c9cc69d-n87xd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-n87xd,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-4295,SelfLink:/api/v1/namespaces/deployment-4295/pods/test-recreate-deployment-5c8c9cc69d-n87xd,UID:ae7b71d0-e2e9-46c1-b450-cb64ba834c67,ResourceVersion:15127,Generation:0,CreationTimestamp:2019-08-21 16:38:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 2836380f-b3db-46b0-aa3d-f2aa407cde3a 0xc00035da77 0xc00035da78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p7tfs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p7tfs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p7tfs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00035dbb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00035dbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 16:38:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:38:34.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4295" for this suite.
Aug 21 16:38:43.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:43.206: INFO: namespace deployment-4295 deletion completed in 8.20185605s

• [SLOW TEST:10.693 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:38:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 21 16:38:44.937: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0821 16:38:44.937804      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:38:44.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8134" for this suite.
Aug 21 16:38:50.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:51.271: INFO: namespace gc-8134 deletion completed in 6.327834179s

• [SLOW TEST:8.064 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:38:51.271: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:38:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4011" for this suite.
Aug 21 16:39:01.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:39:01.775: INFO: namespace emptydir-wrapper-4011 deletion completed in 6.305145981s

• [SLOW TEST:10.505 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:39:01.777: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 21 16:39:09.184: INFO: Successfully updated pod "labelsupdate93ed6a6a-816e-4014-a013-b9c7e1052bd1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:39:11.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3008" for this suite.
Aug 21 16:39:43.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:39:43.477: INFO: namespace projected-3008 deletion completed in 32.195176993s

• [SLOW TEST:41.701 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:39:43.480: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 21 16:39:43.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-1354'
Aug 21 16:39:43.906: INFO: stderr: ""
Aug 21 16:39:43.906: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:39:43.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1354'
Aug 21 16:39:44.024: INFO: stderr: ""
Aug 21 16:39:44.024: INFO: stdout: "update-demo-nautilus-7q2r9 update-demo-nautilus-vh4dz "
Aug 21 16:39:44.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7q2r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1354'
Aug 21 16:39:44.159: INFO: stderr: ""
Aug 21 16:39:44.159: INFO: stdout: ""
Aug 21 16:39:44.159: INFO: update-demo-nautilus-7q2r9 is created but not running
Aug 21 16:39:49.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1354'
Aug 21 16:39:49.290: INFO: stderr: ""
Aug 21 16:39:49.290: INFO: stdout: "update-demo-nautilus-7q2r9 update-demo-nautilus-vh4dz "
Aug 21 16:39:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7q2r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1354'
Aug 21 16:39:49.420: INFO: stderr: ""
Aug 21 16:39:49.420: INFO: stdout: "true"
Aug 21 16:39:49.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-7q2r9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1354'
Aug 21 16:39:49.544: INFO: stderr: ""
Aug 21 16:39:49.544: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:39:49.544: INFO: validating pod update-demo-nautilus-7q2r9
Aug 21 16:39:49.552: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:39:49.552: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:39:49.552: INFO: update-demo-nautilus-7q2r9 is verified up and running
Aug 21 16:39:49.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-vh4dz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1354'
Aug 21 16:39:49.661: INFO: stderr: ""
Aug 21 16:39:49.661: INFO: stdout: "true"
Aug 21 16:39:49.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods update-demo-nautilus-vh4dz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1354'
Aug 21 16:39:49.788: INFO: stderr: ""
Aug 21 16:39:49.788: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:39:49.788: INFO: validating pod update-demo-nautilus-vh4dz
Aug 21 16:39:49.796: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:39:49.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:39:49.796: INFO: update-demo-nautilus-vh4dz is verified up and running
STEP: using delete to clean up resources
Aug 21 16:39:49.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-1354'
Aug 21 16:39:49.919: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:39:49.919: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 21 16:39:49.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1354'
Aug 21 16:39:50.052: INFO: stderr: "No resources found.\n"
Aug 21 16:39:50.052: INFO: stdout: ""
Aug 21 16:39:50.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=update-demo --namespace=kubectl-1354 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:39:50.170: INFO: stderr: ""
Aug 21 16:39:50.170: INFO: stdout: "update-demo-nautilus-7q2r9\nupdate-demo-nautilus-vh4dz\n"
Aug 21 16:39:50.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1354'
Aug 21 16:39:50.859: INFO: stderr: "No resources found.\n"
Aug 21 16:39:50.859: INFO: stdout: ""
Aug 21 16:39:50.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=update-demo --namespace=kubectl-1354 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:39:51.031: INFO: stderr: ""
Aug 21 16:39:51.031: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:39:51.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1354" for this suite.
Aug 21 16:40:15.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:40:15.218: INFO: namespace kubectl-1354 deletion completed in 24.165511532s

• [SLOW TEST:31.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:40:15.220: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-1176cace-6ae2-418f-a336-724e9537fbcd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:40:19.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7899" for this suite.
Aug 21 16:40:43.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:40:43.723: INFO: namespace configmap-7899 deletion completed in 24.243681951s

• [SLOW TEST:28.503 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:40:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-dc69627a-0042-499a-b558-07eeea303e13
STEP: Creating a pod to test consume secrets
Aug 21 16:40:45.065: INFO: Waiting up to 5m0s for pod "pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b" in namespace "secrets-3566" to be "success or failure"
Aug 21 16:40:45.072: INFO: Pod "pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.672051ms
Aug 21 16:40:47.077: INFO: Pod "pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01128479s
Aug 21 16:40:49.278: INFO: Pod "pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.212292639s
STEP: Saw pod success
Aug 21 16:40:49.278: INFO: Pod "pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b" satisfied condition "success or failure"
Aug 21 16:40:49.284: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:40:49.364: INFO: Waiting for pod pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b to disappear
Aug 21 16:40:49.369: INFO: Pod pod-secrets-628f38d4-eddf-4c4d-8880-5b935418190b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:40:49.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3566" for this suite.
Aug 21 16:40:57.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:40:57.599: INFO: namespace secrets-3566 deletion completed in 8.223083003s

• [SLOW TEST:13.874 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:40:57.601: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c
Aug 21 16:40:59.703: INFO: Pod name my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c: Found 0 pods out of 1
Aug 21 16:41:04.711: INFO: Pod name my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c: Found 1 pods out of 1
Aug 21 16:41:04.711: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c" are running
Aug 21 16:41:04.716: INFO: Pod "my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c-n5xtt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:40:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:41:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:41:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-21 16:40:59 +0000 UTC Reason: Message:}])
Aug 21 16:41:04.716: INFO: Trying to dial the pod
Aug 21 16:41:09.734: INFO: Controller my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c: Got expected result from replica 1 [my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c-n5xtt]: "my-hostname-basic-1f3483d6-e73b-458a-91e7-647b340c7a8c-n5xtt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:41:09.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9430" for this suite.
Aug 21 16:41:15.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:41:15.969: INFO: namespace replication-controller-9430 deletion completed in 6.22796587s

• [SLOW TEST:18.369 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:41:15.972: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 21 16:41:18.686: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a"
Aug 21 16:41:18.687: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a" in namespace "pods-604" to be "terminated due to deadline exceeded"
Aug 21 16:41:18.691: INFO: Pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a": Phase="Running", Reason="", readiness=true. Elapsed: 4.00702ms
Aug 21 16:41:20.696: INFO: Pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009627372s
Aug 21 16:41:22.701: INFO: Pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014803621s
Aug 21 16:41:22.702: INFO: Pod "pod-update-activedeadlineseconds-c7162d56-63b4-4f5e-8e08-51d78e4d253a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:41:22.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-604" for this suite.
Aug 21 16:41:28.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:41:28.931: INFO: namespace pods-604 deletion completed in 6.223732503s

• [SLOW TEST:12.959 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:41:28.931: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6053, will wait for the garbage collector to delete the pods
Aug 21 16:41:37.471: INFO: Deleting Job.batch foo took: 19.007297ms
Aug 21 16:41:38.071: INFO: Terminating Job.batch foo pods took: 600.230724ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:42:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6053" for this suite.
Aug 21 16:42:26.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:42:26.630: INFO: namespace job-6053 deletion completed in 8.547688823s

• [SLOW TEST:57.699 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:42:26.631: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6ts4
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 16:42:26.895: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6ts4" in namespace "subpath-8826" to be "success or failure"
Aug 21 16:42:26.989: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Pending", Reason="", readiness=false. Elapsed: 93.845336ms
Aug 21 16:42:28.995: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099915957s
Aug 21 16:42:31.005: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 4.109570317s
Aug 21 16:42:33.012: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 6.117010315s
Aug 21 16:42:35.017: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 8.122348668s
Aug 21 16:42:37.023: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 10.127629511s
Aug 21 16:42:39.029: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 12.133784083s
Aug 21 16:42:43.134: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 16.238573199s
Aug 21 16:42:45.141: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 18.245752246s
Aug 21 16:42:47.146: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 20.250856458s
Aug 21 16:42:49.154: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Running", Reason="", readiness=true. Elapsed: 22.258632899s
Aug 21 16:42:51.160: INFO: Pod "pod-subpath-test-configmap-6ts4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.265019035s
STEP: Saw pod success
Aug 21 16:42:51.160: INFO: Pod "pod-subpath-test-configmap-6ts4" satisfied condition "success or failure"
Aug 21 16:42:51.167: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-subpath-test-configmap-6ts4 container test-container-subpath-configmap-6ts4: <nil>
STEP: delete the pod
Aug 21 16:42:51.226: INFO: Waiting for pod pod-subpath-test-configmap-6ts4 to disappear
Aug 21 16:42:51.232: INFO: Pod pod-subpath-test-configmap-6ts4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6ts4
Aug 21 16:42:51.232: INFO: Deleting pod "pod-subpath-test-configmap-6ts4" in namespace "subpath-8826"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:42:51.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8826" for this suite.
Aug 21 16:42:57.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:42:57.573: INFO: namespace subpath-8826 deletion completed in 6.324880473s

• [SLOW TEST:30.941 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:42:57.573: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 21 16:42:57.764: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 16:42:57.780: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 16:42:57.784: INFO: 
Logging pods the kubelet thinks is on node 3c511303-d822-436a-a9c7-85bfe5dec0f1 before test
Aug 21 16:42:57.795: INFO: canal-4wqcb from kube-system started at 2019-08-21 15:32:50 +0000 UTC (3 container statuses recorded)
Aug 21 16:42:57.796: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:42:57.796: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 16:42:57.796: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 16:42:57.796: INFO: kube-proxy-b5jkw from kube-system started at 2019-08-21 15:32:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.796: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:42:57.796: INFO: cloud-coordinator-767969ffd8-jsl2t from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.796: INFO: 	Container cloud-coordinator ready: true, restart count 0
Aug 21 16:42:57.796: INFO: csi-do-node-kznxp from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 16:42:57.796: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:42:57.797: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 16:42:57.797: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-21 15:51:17 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.797: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 16:42:57.797: INFO: sonobuoy-e2e-job-abdb7b72ca0642e2 from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:42:57.797: INFO: 	Container e2e ready: true, restart count 0
Aug 21 16:42:57.797: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:42:57.797: INFO: cloud-agent-6rhbp from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.797: INFO: 	Container cloud-agent ready: true, restart count 0
Aug 21 16:42:57.797: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-vx26t from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:42:57.797: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:42:57.797: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:42:57.798: INFO: 
Logging pods the kubelet thinks is on node 61823cca-1200-4e72-835e-06a7dddcdaa7 before test
Aug 21 16:42:57.807: INFO: csi-do-controller-0 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (4 container statuses recorded)
Aug 21 16:42:57.807: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 21 16:42:57.807: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:42:57.807: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 21 16:42:57.808: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 21 16:42:57.808: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-p9g9l from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 16:42:57.808: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:42:57.808: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:42:57.808: INFO: eventrouter-b478d5654-h8dbn from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.808: INFO: 	Container eventrouter ready: true, restart count 0
Aug 21 16:42:57.808: INFO: canal-q8xpb from kube-system started at 2019-08-21 15:32:33 +0000 UTC (3 container statuses recorded)
Aug 21 16:42:57.808: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:42:57.808: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 16:42:57.808: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 16:42:57.809: INFO: csi-do-node-wpjp8 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 16:42:57.809: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 16:42:57.809: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 16:42:57.809: INFO: cloud-agent-lwq2n from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.809: INFO: 	Container cloud-agent ready: true, restart count 0
Aug 21 16:42:57.809: INFO: kube-proxy-4mvfm from kube-system started at 2019-08-21 15:32:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:42:57.809: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [containership.15bcfdc16cc9e44a], Reason = [UpdateServiceAccountSecrets], Message = [Detected out-of-date image pull secrets]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bcfdc17baf124d], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:43:01.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4970" for this suite.
Aug 21 16:43:07.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:43:07.933: INFO: namespace sched-pred-4970 deletion completed in 6.233426399s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:10.360 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:43:07.933: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 21 16:43:08.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 --namespace=kubectl-938 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 21 16:43:11.442: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 21 16:43:11.442: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:43:13.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-938" for this suite.
Aug 21 16:43:20.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:43:20.264: INFO: namespace kubectl-938 deletion completed in 6.804459695s

• [SLOW TEST:12.331 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:43:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-25291f0d-a5bd-4258-ab98-a990e785958f
STEP: Creating a pod to test consume configMaps
Aug 21 16:43:20.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841" in namespace "projected-7753" to be "success or failure"
Aug 21 16:43:20.378: INFO: Pod "pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841": Phase="Pending", Reason="", readiness=false. Elapsed: 14.918252ms
Aug 21 16:43:22.385: INFO: Pod "pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021114279s
STEP: Saw pod success
Aug 21 16:43:22.385: INFO: Pod "pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841" satisfied condition "success or failure"
Aug 21 16:43:22.392: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:43:22.435: INFO: Waiting for pod pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841 to disappear
Aug 21 16:43:22.443: INFO: Pod pod-projected-configmaps-c3cb89a9-52e2-4d67-a632-41d30fa94841 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:43:22.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7753" for this suite.
Aug 21 16:43:28.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:43:28.701: INFO: namespace projected-7753 deletion completed in 6.251876081s

• [SLOW TEST:8.437 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:43:28.706: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:43:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6669'
Aug 21 16:43:28.966: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:43:28.966: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 21 16:43:30.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6669'
Aug 21 16:43:31.089: INFO: stderr: ""
Aug 21 16:43:31.089: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:43:31.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6669" for this suite.
Aug 21 16:43:37.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:43:37.324: INFO: namespace kubectl-6669 deletion completed in 6.227780798s

• [SLOW TEST:8.619 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:43:37.325: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 21 16:43:37.426: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:43:41.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9446" for this suite.
Aug 21 16:44:05.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:44:05.770: INFO: namespace init-container-9446 deletion completed in 24.190093908s

• [SLOW TEST:28.445 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:44:05.771: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:44:05.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6322'
Aug 21 16:44:06.013: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:44:06.013: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 21 16:44:06.023: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-7c7jz]
Aug 21 16:44:06.023: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-7c7jz" in namespace "kubectl-6322" to be "running and ready"
Aug 21 16:44:06.030: INFO: Pod "e2e-test-nginx-rc-7c7jz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.361543ms
Aug 21 16:44:08.037: INFO: Pod "e2e-test-nginx-rc-7c7jz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013847449s
Aug 21 16:44:08.037: INFO: Pod "e2e-test-nginx-rc-7c7jz" satisfied condition "running and ready"
Aug 21 16:44:08.037: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-7c7jz]
Aug 21 16:44:08.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 logs rc/e2e-test-nginx-rc --namespace=kubectl-6322'
Aug 21 16:44:08.178: INFO: stderr: ""
Aug 21 16:44:08.178: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 21 16:44:08.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete rc e2e-test-nginx-rc --namespace=kubectl-6322'
Aug 21 16:44:08.288: INFO: stderr: ""
Aug 21 16:44:08.288: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:44:08.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6322" for this suite.
Aug 21 16:44:14.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:44:14.510: INFO: namespace kubectl-6322 deletion completed in 6.215358308s

• [SLOW TEST:8.739 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:44:14.513: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:44:14.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37" in namespace "projected-1972" to be "success or failure"
Aug 21 16:44:14.616: INFO: Pod "downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.974755ms
Aug 21 16:44:16.622: INFO: Pod "downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017048962s
Aug 21 16:44:18.628: INFO: Pod "downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023031491s
STEP: Saw pod success
Aug 21 16:44:18.628: INFO: Pod "downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37" satisfied condition "success or failure"
Aug 21 16:44:18.632: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37 container client-container: <nil>
STEP: delete the pod
Aug 21 16:44:18.679: INFO: Waiting for pod downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37 to disappear
Aug 21 16:44:18.685: INFO: Pod downwardapi-volume-e24c4f80-208e-465a-99e9-6c64e2215f37 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:44:18.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1972" for this suite.
Aug 21 16:44:24.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:44:24.925: INFO: namespace projected-1972 deletion completed in 6.23109988s

• [SLOW TEST:10.412 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:44:24.928: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-294bb020-6a9b-4bd9-9014-874fa18649ba
STEP: Creating configMap with name cm-test-opt-upd-be231a77-1d5d-43f8-8970-5b0f5b274c07
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-294bb020-6a9b-4bd9-9014-874fa18649ba
STEP: Updating configmap cm-test-opt-upd-be231a77-1d5d-43f8-8970-5b0f5b274c07
STEP: Creating configMap with name cm-test-opt-create-402ef0fc-4445-426d-8c3c-5b6bcaacce46
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:44:33.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4256" for this suite.
Aug 21 16:44:57.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:44:57.423: INFO: namespace projected-4256 deletion completed in 24.224628644s

• [SLOW TEST:32.495 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:44:57.423: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:44:57.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9880'
Aug 21 16:44:57.638: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:44:57.638: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Aug 21 16:44:57.646: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 21 16:44:57.691: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 21 16:44:57.725: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:44:57.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9880'
Aug 21 16:45:13.749: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 21 16:45:13.749: INFO: stdout: "Created e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942\nScaling up e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 21 16:45:13.749: INFO: stdout: "Created e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942\nScaling up e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 21 16:45:13.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:13.879: INFO: stderr: ""
Aug 21 16:45:13.879: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:18.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:19.015: INFO: stderr: ""
Aug 21 16:45:19.015: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:24.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:24.151: INFO: stderr: ""
Aug 21 16:45:24.151: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:29.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:29.253: INFO: stderr: ""
Aug 21 16:45:29.253: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:34.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:34.368: INFO: stderr: ""
Aug 21 16:45:34.368: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:39.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:39.482: INFO: stderr: ""
Aug 21 16:45:39.482: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:44.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:44.580: INFO: stderr: ""
Aug 21 16:45:44.580: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:49.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:49.709: INFO: stderr: ""
Aug 21 16:45:49.709: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:54.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:54.809: INFO: stderr: ""
Aug 21 16:45:54.809: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:45:59.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:45:59.944: INFO: stderr: ""
Aug 21 16:45:59.945: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:46:04.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:46:05.065: INFO: stderr: ""
Aug 21 16:46:05.066: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:46:10.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:46:10.166: INFO: stderr: ""
Aug 21 16:46:10.166: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:46:15.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:46:15.298: INFO: stderr: ""
Aug 21 16:46:15.298: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk e2e-test-nginx-rc-scttk "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug 21 16:46:20.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:46:20.409: INFO: stderr: ""
Aug 21 16:46:20.409: INFO: stdout: "e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk "
Aug 21 16:46:20.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9880'
Aug 21 16:46:20.526: INFO: stderr: ""
Aug 21 16:46:20.526: INFO: stdout: "true"
Aug 21 16:46:20.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9880'
Aug 21 16:46:20.613: INFO: stderr: ""
Aug 21 16:46:20.613: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 21 16:46:20.613: INFO: e2e-test-nginx-rc-c0a8635eb77ecb2896c8f9597dff1942-2fbmk is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 21 16:46:20.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete rc e2e-test-nginx-rc --namespace=kubectl-9880'
Aug 21 16:46:20.737: INFO: stderr: ""
Aug 21 16:46:20.737: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:46:20.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9880" for this suite.
Aug 21 16:46:28.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:46:28.955: INFO: namespace kubectl-9880 deletion completed in 8.211038458s

• [SLOW TEST:91.532 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:46:28.955: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:46:33.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6370" for this suite.
Aug 21 16:47:13.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:47:13.353: INFO: namespace kubelet-test-6370 deletion completed in 40.254084148s

• [SLOW TEST:44.398 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:47:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:47:42.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5344" for this suite.
Aug 21 16:47:48.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:47:49.138: INFO: namespace namespaces-5344 deletion completed in 6.272943825s
STEP: Destroying namespace "nsdeletetest-1800" for this suite.
Aug 21 16:47:49.143: INFO: Namespace nsdeletetest-1800 was already deleted
STEP: Destroying namespace "nsdeletetest-2894" for this suite.
Aug 21 16:47:57.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:47:57.360: INFO: namespace nsdeletetest-2894 deletion completed in 8.217035627s

• [SLOW TEST:44.003 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:47:57.360: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-d4168da9-ad42-49a6-8fe1-d703e0ca508e
STEP: Creating a pod to test consume secrets
Aug 21 16:47:57.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff" in namespace "projected-2069" to be "success or failure"
Aug 21 16:47:57.506: INFO: Pod "pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.777643ms
Aug 21 16:47:59.789: INFO: Pod "pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287899811s
Aug 21 16:48:01.795: INFO: Pod "pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.294313241s
STEP: Saw pod success
Aug 21 16:48:01.795: INFO: Pod "pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff" satisfied condition "success or failure"
Aug 21 16:48:01.802: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:48:01.852: INFO: Waiting for pod pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff to disappear
Aug 21 16:48:01.864: INFO: Pod pod-projected-secrets-fb7f48dd-e7d5-4cd3-b654-d123d3acbfff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:48:01.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2069" for this suite.
Aug 21 16:48:09.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:48:10.099: INFO: namespace projected-2069 deletion completed in 8.224475181s

• [SLOW TEST:12.738 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:48:10.100: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-kjsg
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 16:48:14.246: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kjsg" in namespace "subpath-4766" to be "success or failure"
Aug 21 16:48:14.251: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.979336ms
Aug 21 16:48:16.257: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01072968s
Aug 21 16:48:18.274: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 4.027959419s
Aug 21 16:48:20.283: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 6.036363313s
Aug 21 16:48:22.294: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 8.047503279s
Aug 21 16:48:24.299: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 10.052736604s
Aug 21 16:48:26.305: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 12.058217537s
Aug 21 16:48:28.310: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 14.063741848s
Aug 21 16:48:30.316: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 16.069173452s
Aug 21 16:48:32.321: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 18.074281166s
Aug 21 16:48:34.326: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Running", Reason="", readiness=true. Elapsed: 20.079575003s
Aug 21 16:48:36.332: INFO: Pod "pod-subpath-test-configmap-kjsg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.085811139s
STEP: Saw pod success
Aug 21 16:48:36.332: INFO: Pod "pod-subpath-test-configmap-kjsg" satisfied condition "success or failure"
Aug 21 16:48:36.337: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-subpath-test-configmap-kjsg container test-container-subpath-configmap-kjsg: <nil>
STEP: delete the pod
Aug 21 16:48:36.371: INFO: Waiting for pod pod-subpath-test-configmap-kjsg to disappear
Aug 21 16:48:36.375: INFO: Pod pod-subpath-test-configmap-kjsg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kjsg
Aug 21 16:48:36.375: INFO: Deleting pod "pod-subpath-test-configmap-kjsg" in namespace "subpath-4766"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:48:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4766" for this suite.
Aug 21 16:48:42.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:48:42.607: INFO: namespace subpath-4766 deletion completed in 6.22179615s

• [SLOW TEST:32.507 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:48:42.617: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4755.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4755.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 16:48:54.875: INFO: DNS probes using dns-4755/dns-test-d802c3d5-4a1f-4968-968b-b10cf0eb0b3c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:48:54.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4755" for this suite.
Aug 21 16:49:02.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:49:03.150: INFO: namespace dns-4755 deletion completed in 8.22979711s

• [SLOW TEST:20.533 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:49:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 21 16:49:06.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-6283'
Aug 21 16:49:06.918: INFO: stderr: ""
Aug 21 16:49:06.918: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 21 16:49:07.951: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:49:07.951: INFO: Found 0 / 1
Aug 21 16:49:08.924: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:49:08.924: INFO: Found 1 / 1
Aug 21 16:49:08.924: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 21 16:49:08.928: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:49:08.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 21 16:49:08.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 logs redis-master-k8vvm redis-master --namespace=kubectl-6283'
Aug 21 16:49:09.095: INFO: stderr: ""
Aug 21 16:49:09.095: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Aug 16:49:08.251 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Aug 16:49:08.251 # Server started, Redis version 3.2.12\n1:M 21 Aug 16:49:08.251 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Aug 16:49:08.251 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 21 16:49:09.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 log redis-master-k8vvm redis-master --namespace=kubectl-6283 --tail=1'
Aug 21 16:49:09.276: INFO: stderr: ""
Aug 21 16:49:09.276: INFO: stdout: "1:M 21 Aug 16:49:08.251 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 21 16:49:09.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 log redis-master-k8vvm redis-master --namespace=kubectl-6283 --limit-bytes=1'
Aug 21 16:49:09.417: INFO: stderr: ""
Aug 21 16:49:09.417: INFO: stdout: " "
STEP: exposing timestamps
Aug 21 16:49:09.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 log redis-master-k8vvm redis-master --namespace=kubectl-6283 --tail=1 --timestamps'
Aug 21 16:49:09.557: INFO: stderr: ""
Aug 21 16:49:09.557: INFO: stdout: "2019-08-21T16:49:08.252383811Z 1:M 21 Aug 16:49:08.251 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 21 16:49:12.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 log redis-master-k8vvm redis-master --namespace=kubectl-6283 --since=1s'
Aug 21 16:49:12.215: INFO: stderr: ""
Aug 21 16:49:12.215: INFO: stdout: ""
Aug 21 16:49:12.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 log redis-master-k8vvm redis-master --namespace=kubectl-6283 --since=24h'
Aug 21 16:49:12.370: INFO: stderr: ""
Aug 21 16:49:12.370: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Aug 16:49:08.251 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Aug 16:49:08.251 # Server started, Redis version 3.2.12\n1:M 21 Aug 16:49:08.251 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Aug 16:49:08.251 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 21 16:49:12.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-6283'
Aug 21 16:49:12.489: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:49:12.489: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 21 16:49:12.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6283'
Aug 21 16:49:12.625: INFO: stderr: "No resources found.\n"
Aug 21 16:49:12.625: INFO: stdout: ""
Aug 21 16:49:12.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=nginx --namespace=kubectl-6283 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:49:12.729: INFO: stderr: ""
Aug 21 16:49:12.729: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:49:12.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6283" for this suite.
Aug 21 16:49:20.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:49:20.938: INFO: namespace kubectl-6283 deletion completed in 8.201960878s

• [SLOW TEST:17.787 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:49:20.939: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:49:21.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-3823'
Aug 21 16:49:21.242: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:49:21.242: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 21 16:49:23.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3823'
Aug 21 16:49:23.437: INFO: stderr: ""
Aug 21 16:49:23.437: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:49:23.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3823" for this suite.
Aug 21 16:49:47.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:49:47.738: INFO: namespace kubectl-3823 deletion completed in 24.295441607s

• [SLOW TEST:26.800 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:49:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:49:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6986" for this suite.
Aug 21 16:49:59.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:00.099: INFO: namespace kubelet-test-6986 deletion completed in 8.220659862s

• [SLOW TEST:12.360 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:50:00.099: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:50:00.190: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12" in namespace "projected-8157" to be "success or failure"
Aug 21 16:50:00.195: INFO: Pod "downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898552ms
Aug 21 16:50:02.204: INFO: Pod "downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01313466s
Aug 21 16:50:04.209: INFO: Pod "downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018270822s
STEP: Saw pod success
Aug 21 16:50:04.209: INFO: Pod "downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12" satisfied condition "success or failure"
Aug 21 16:50:04.213: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12 container client-container: <nil>
STEP: delete the pod
Aug 21 16:50:04.249: INFO: Waiting for pod downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12 to disappear
Aug 21 16:50:04.253: INFO: Pod downwardapi-volume-e59cf045-47b2-4746-a883-f8b5f3d1ac12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:50:04.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8157" for this suite.
Aug 21 16:50:12.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:12.568: INFO: namespace projected-8157 deletion completed in 8.299660535s

• [SLOW TEST:12.469 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:50:12.568: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 21 16:50:15.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec pod-sharedvolume-be5ae13f-0c82-44a3-a1cb-3e28126299d5 -c busybox-main-container --namespace=emptydir-7687 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 21 16:50:15.984: INFO: stderr: ""
Aug 21 16:50:15.984: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:50:15.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7687" for this suite.
Aug 21 16:50:24.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:24.204: INFO: namespace emptydir-7687 deletion completed in 8.213289154s

• [SLOW TEST:11.636 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:50:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8870
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 21 16:50:25.283: INFO: Found 0 stateful pods, waiting for 3
Aug 21 16:50:35.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:50:35.290: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:50:35.290: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 21 16:50:45.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:50:45.290: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:50:45.290: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:50:45.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8870 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:50:45.687: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:50:45.687: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:50:45.687: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 21 16:50:55.733: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 21 16:51:05.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8870 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:51:06.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:51:06.173: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:51:06.173: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:51:16.204: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:51:16.204: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:51:16.204: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:51:16.204: INFO: Waiting for Pod statefulset-8870/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:51:26.216: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:51:26.216: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:51:26.217: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 21 16:51:36.215: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:51:36.215: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Aug 21 16:51:46.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8870 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 21 16:51:49.411: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 21 16:51:49.411: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 21 16:51:49.411: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 21 16:51:49.519: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 21 16:51:59.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 exec --namespace=statefulset-8870 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 21 16:51:59.949: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 21 16:51:59.949: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 21 16:51:59.950: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 21 16:52:09.986: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:52:09.986: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:09.986: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:09.986: INFO: Waiting for Pod statefulset-8870/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:20.003: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:52:20.003: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:20.003: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:20.003: INFO: Waiting for Pod statefulset-8870/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:30.000: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:52:30.000: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:30.000: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:30.000: INFO: Waiting for Pod statefulset-8870/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:39.997: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:52:39.997: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:39.997: INFO: Waiting for Pod statefulset-8870/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:39.997: INFO: Waiting for Pod statefulset-8870/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:49.997: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
Aug 21 16:52:49.998: INFO: Waiting for Pod statefulset-8870/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 21 16:52:59.997: INFO: Waiting for StatefulSet statefulset-8870/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 21 16:53:09.996: INFO: Deleting all statefulset in ns statefulset-8870
Aug 21 16:53:10.000: INFO: Scaling statefulset ss2 to 0
Aug 21 16:53:30.021: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:53:30.025: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:53:30.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8870" for this suite.
Aug 21 16:53:40.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:40.237: INFO: namespace statefulset-8870 deletion completed in 10.1828983s

• [SLOW TEST:196.032 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:53:40.243: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 21 16:53:40.360: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18585,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:53:40.360: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18586,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 21 16:53:40.361: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18587,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 21 16:53:50.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18607,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:53:50.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18608,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 21 16:53:50.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-label-changed,UID:0f452882-4890-4de3-a133-a4c7c295de48,ResourceVersion:18609,Generation:0,CreationTimestamp:2019-08-21 16:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:53:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4451" for this suite.
Aug 21 16:53:56.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:56.635: INFO: namespace watch-4451 deletion completed in 6.219483871s

• [SLOW TEST:16.393 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:53:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:53:56.734: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 21 16:53:56.749: INFO: Number of nodes with available pods: 0
Aug 21 16:53:56.749: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 21 16:53:56.771: INFO: Number of nodes with available pods: 0
Aug 21 16:53:56.771: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:53:57.777: INFO: Number of nodes with available pods: 0
Aug 21 16:53:57.777: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:53:58.777: INFO: Number of nodes with available pods: 1
Aug 21 16:53:58.777: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 21 16:53:58.800: INFO: Number of nodes with available pods: 1
Aug 21 16:53:58.800: INFO: Number of running nodes: 0, number of available pods: 1
Aug 21 16:53:59.805: INFO: Number of nodes with available pods: 0
Aug 21 16:53:59.805: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 21 16:53:59.830: INFO: Number of nodes with available pods: 0
Aug 21 16:53:59.830: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:54:00.837: INFO: Number of nodes with available pods: 0
Aug 21 16:54:00.837: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:54:01.837: INFO: Number of nodes with available pods: 0
Aug 21 16:54:01.837: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:54:02.837: INFO: Number of nodes with available pods: 0
Aug 21 16:54:02.837: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:54:03.836: INFO: Number of nodes with available pods: 0
Aug 21 16:54:03.836: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 16:54:04.838: INFO: Number of nodes with available pods: 1
Aug 21 16:54:04.838: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3624, will wait for the garbage collector to delete the pods
Aug 21 16:54:04.913: INFO: Deleting DaemonSet.extensions daemon-set took: 11.164553ms
Aug 21 16:54:05.614: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.278884ms
Aug 21 16:54:08.720: INFO: Number of nodes with available pods: 0
Aug 21 16:54:08.720: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 16:54:08.725: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3624/daemonsets","resourceVersion":"18716"},"items":null}

Aug 21 16:54:08.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3624/pods","resourceVersion":"18716"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:54:08.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3624" for this suite.
Aug 21 16:54:14.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:54:14.990: INFO: namespace daemonsets-3624 deletion completed in 6.223260938s

• [SLOW TEST:18.350 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:54:14.995: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:54:15.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e" in namespace "downward-api-9235" to be "success or failure"
Aug 21 16:54:15.079: INFO: Pod "downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511036ms
Aug 21 16:54:17.085: INFO: Pod "downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011464996s
STEP: Saw pod success
Aug 21 16:54:17.085: INFO: Pod "downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e" satisfied condition "success or failure"
Aug 21 16:54:17.090: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e container client-container: <nil>
STEP: delete the pod
Aug 21 16:54:17.125: INFO: Waiting for pod downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e to disappear
Aug 21 16:54:17.130: INFO: Pod downwardapi-volume-625a51bd-d91f-4c59-a402-b40a98420a9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:54:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9235" for this suite.
Aug 21 16:54:23.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:54:23.332: INFO: namespace downward-api-9235 deletion completed in 6.196116556s

• [SLOW TEST:8.338 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:54:23.335: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-a4ea627d-1a6f-4988-9041-abcdf0bf63f0
STEP: Creating a pod to test consume secrets
Aug 21 16:54:23.429: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a" in namespace "projected-122" to be "success or failure"
Aug 21 16:54:23.435: INFO: Pod "pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063379ms
Aug 21 16:54:25.440: INFO: Pod "pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011259529s
Aug 21 16:54:27.445: INFO: Pod "pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015743497s
STEP: Saw pod success
Aug 21 16:54:27.445: INFO: Pod "pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a" satisfied condition "success or failure"
Aug 21 16:54:27.449: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:54:27.495: INFO: Waiting for pod pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a to disappear
Aug 21 16:54:27.505: INFO: Pod pod-projected-secrets-6d239d97-2bdd-40bb-8e7a-22c1b0c2573a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:54:27.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-122" for this suite.
Aug 21 16:54:33.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:54:33.767: INFO: namespace projected-122 deletion completed in 6.242902939s

• [SLOW TEST:10.432 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:54:33.769: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 21 16:54:41.984: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:41.984: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:42.241: INFO: Exec stderr: ""
Aug 21 16:54:42.241: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:42.553: INFO: Exec stderr: ""
Aug 21 16:54:42.553: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:42.553: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:42.869: INFO: Exec stderr: ""
Aug 21 16:54:42.869: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:42.869: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:43.202: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 21 16:54:43.202: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:43.202: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:43.503: INFO: Exec stderr: ""
Aug 21 16:54:43.503: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:43.854: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 21 16:54:43.854: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:43.854: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:44.116: INFO: Exec stderr: ""
Aug 21 16:54:44.116: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:44.116: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:45.615: INFO: Exec stderr: ""
Aug 21 16:54:45.615: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:45.919: INFO: Exec stderr: ""
Aug 21 16:54:45.919: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:54:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:54:46.251: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:54:46.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4078" for this suite.
Aug 21 16:55:30.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:55:30.457: INFO: namespace e2e-kubelet-etc-hosts-4078 deletion completed in 44.198987867s

• [SLOW TEST:56.689 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:55:30.457: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:55:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1934" for this suite.
Aug 21 16:56:14.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:14.841: INFO: namespace kubelet-test-1934 deletion completed in 42.217033975s

• [SLOW TEST:44.383 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:56:14.841: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 21 16:56:14.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-190'
Aug 21 16:56:16.393: INFO: stderr: ""
Aug 21 16:56:16.393: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 16:56:17.402: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:56:17.402: INFO: Found 0 / 1
Aug 21 16:56:18.399: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:56:18.399: INFO: Found 0 / 1
Aug 21 16:56:19.400: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:56:19.400: INFO: Found 1 / 1
Aug 21 16:56:19.400: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 21 16:56:19.404: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:56:19.404: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 16:56:19.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 patch pod redis-master-mxjcm --namespace=kubectl-190 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 21 16:56:19.512: INFO: stderr: ""
Aug 21 16:56:19.512: INFO: stdout: "pod/redis-master-mxjcm patched\n"
STEP: checking annotations
Aug 21 16:56:19.517: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:56:19.517: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:56:19.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-190" for this suite.
Aug 21 16:56:41.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:41.777: INFO: namespace kubectl-190 deletion completed in 22.238013435s

• [SLOW TEST:26.936 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:56:41.779: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:56:42.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61" in namespace "projected-3766" to be "success or failure"
Aug 21 16:56:42.726: INFO: Pod "downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751814ms
Aug 21 16:56:44.906: INFO: Pod "downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186246589s
Aug 21 16:56:46.913: INFO: Pod "downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193391284s
STEP: Saw pod success
Aug 21 16:56:46.913: INFO: Pod "downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61" satisfied condition "success or failure"
Aug 21 16:56:46.918: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61 container client-container: <nil>
STEP: delete the pod
Aug 21 16:56:46.952: INFO: Waiting for pod downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61 to disappear
Aug 21 16:56:46.956: INFO: Pod downwardapi-volume-b7e11f5d-08be-4cfa-9eda-abaf41d41f61 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:56:46.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3766" for this suite.
Aug 21 16:56:52.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:53.178: INFO: namespace projected-3766 deletion completed in 6.216530909s

• [SLOW TEST:11.399 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:56:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1128
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1128
STEP: Deleting pre-stop pod
Aug 21 16:57:06.370: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:57:06.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1128" for this suite.
Aug 21 16:57:44.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:57:44.577: INFO: namespace prestop-1128 deletion completed in 38.184168251s

• [SLOW TEST:51.397 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:57:44.580: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 16:57:44.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-8706'
Aug 21 16:57:46.047: INFO: stderr: ""
Aug 21 16:57:46.047: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 21 16:57:46.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-8706'
Aug 21 16:57:46.328: INFO: stderr: ""
Aug 21 16:57:46.329: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 16:57:47.334: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:57:47.334: INFO: Found 0 / 1
Aug 21 16:57:48.334: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:57:48.334: INFO: Found 0 / 1
Aug 21 16:57:49.335: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:57:49.335: INFO: Found 1 / 1
Aug 21 16:57:49.335: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 21 16:57:49.340: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:57:49.340: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 16:57:49.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 describe pod redis-master-8f5wp --namespace=kubectl-8706'
Aug 21 16:57:49.451: INFO: stderr: ""
Aug 21 16:57:49.451: INFO: stdout: "Name:           redis-master-8f5wp\nNamespace:      kubectl-8706\nPriority:       0\nNode:           3c511303-d822-436a-a9c7-85bfe5dec0f1/10.132.234.100\nStart Time:     Wed, 21 Aug 2019 16:57:46 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 192.168.2.97/32\nStatus:         Running\nIP:             192.168.2.97\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9d3830e6b1f6c98084488532841b6dc45cb890a74a8a8ffdddd41b9faca25fd2\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 21 Aug 2019 16:57:47 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-72skb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-72skb:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-72skb\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                           Message\n  ----    ------     ----  ----                                           -------\n  Normal  Scheduled  3s    default-scheduler                              Successfully assigned kubectl-8706/redis-master-8f5wp to 3c511303-d822-436a-a9c7-85bfe5dec0f1\n  Normal  Pulled     2s    kubelet, 3c511303-d822-436a-a9c7-85bfe5dec0f1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, 3c511303-d822-436a-a9c7-85bfe5dec0f1  Created container redis-master\n  Normal  Started    2s    kubelet, 3c511303-d822-436a-a9c7-85bfe5dec0f1  Started container redis-master\n"
Aug 21 16:57:49.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 describe rc redis-master --namespace=kubectl-8706'
Aug 21 16:57:49.597: INFO: stderr: ""
Aug 21 16:57:49.597: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8706\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-8f5wp\n"
Aug 21 16:57:49.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 describe service redis-master --namespace=kubectl-8706'
Aug 21 16:57:49.706: INFO: stderr: ""
Aug 21 16:57:49.706: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8706\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.105.133.159\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.2.97:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 21 16:57:49.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 describe node 3c511303-d822-436a-a9c7-85bfe5dec0f1'
Aug 21 16:57:49.897: INFO: stderr: ""
Aug 21 16:57:49.897: INFO: stdout: "Name:               3c511303-d822-436a-a9c7-85bfe5dec0f1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s-2vcpu-4gb\n                    beta.kubernetes.io/os=linux\n                    cluster.containership.io/environment=sonobuoy\n                    cluster.containership.io/name=Conformance\n                    containership.io/cluster-id=bbc5ccc9-f393-4339-bfba-3b0e0942d3ad\n                    containership.io/gpu-enabled=false\n                    containership.io/managed=true\n                    containership.io/node-id=3c511303-d822-436a-a9c7-85bfe5dec0f1\n                    containership.io/node-pool-id=785333d7-ba97-4662-ac38-e7cb74be4f5c\n                    containership.io/organization-id=7bc31d09-f215-4654-b300-7bf6e28a075e\n                    failure-domain.beta.kubernetes.io/region=nyc3\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=3c511303-d822-436a-a9c7-85bfe5dec0f1\n                    kubernetes.io/os=linux\n                    region=nyc3\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"155769841\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"86:05:4d:45:c3:a2\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 167.71.241.98\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 21 Aug 2019 15:32:50 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 21 Aug 2019 16:57:15 +0000   Wed, 21 Aug 2019 15:32:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 21 Aug 2019 16:57:15 +0000   Wed, 21 Aug 2019 15:32:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 21 Aug 2019 16:57:15 +0000   Wed, 21 Aug 2019 15:32:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 21 Aug 2019 16:57:15 +0000   Wed, 21 Aug 2019 15:33:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    3c511303-d822-436a-a9c7-85bfe5dec0f1\n  InternalIP:  10.132.234.100\n  ExternalIP:  167.71.241.98\nCapacity:\n attachable-volumes-csi-dobs.csi.digitalocean.com:  7\n cpu:                                               2\n ephemeral-storage:                                 81120924Ki\n hugepages-1Gi:                                     0\n hugepages-2Mi:                                     0\n memory:                                            4046320Ki\n pods:                                              110\nAllocatable:\n attachable-volumes-csi-dobs.csi.digitalocean.com:  7\n cpu:                                               2\n ephemeral-storage:                                 74761043435\n hugepages-1Gi:                                     0\n hugepages-2Mi:                                     0\n memory:                                            3943920Ki\n pods:                                              110\nSystem Info:\n Machine ID:                 e3d734e411c3497fb8a85f8d3b6fa367\n System UUID:                E3D734E4-11C3-497F-B8A8-5F8D3B6FA367\n Boot ID:                    35cd7dc1-23da-47d9-b1b0-dd965577e045\n Kernel Version:             4.4.0-157-generic\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.7\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     192.168.2.0/24\nProviderID:                  digitalocean://155769841\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  containership-core         cloud-agent-6rhbp                                          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         82m\n  containership-core         cloud-coordinator-767969ffd8-jsl2t                         50m (2%)      0 (0%)      50Mi (1%)        0 (0%)         82m\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         66m\n  heptio-sonobuoy            sonobuoy-e2e-job-abdb7b72ca0642e2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         66m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-vx26t    0 (0%)        0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                canal-4wqcb                                                250m (12%)    0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                csi-do-node-kznxp                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\n  kube-system                kube-proxy-b5jkw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kubectl-8706               redis-master-8f5wp                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                                          Requests    Limits\n  --------                                          --------    ------\n  cpu                                               310m (15%)  0 (0%)\n  memory                                            70Mi (1%)   0 (0%)\n  ephemeral-storage                                 0 (0%)      0 (0%)\n  attachable-volumes-csi-dobs.csi.digitalocean.com  0           0\nEvents:\n  Type    Reason                   Age                From                                              Message\n  ----    ------                   ----               ----                                              -------\n  Normal  NodeHasSufficientMemory  84m (x8 over 85m)  kubelet, 3c511303-d822-436a-a9c7-85bfe5dec0f1     Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    84m (x8 over 85m)  kubelet, 3c511303-d822-436a-a9c7-85bfe5dec0f1     Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 84m                kube-proxy, 3c511303-d822-436a-a9c7-85bfe5dec0f1  Starting kube-proxy.\n"
Aug 21 16:57:49.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 describe namespace kubectl-8706'
Aug 21 16:57:50.005: INFO: stderr: ""
Aug 21 16:57:50.005: INFO: stdout: "Name:         kubectl-8706\nLabels:       e2e-framework=kubectl\n              e2e-run=a8d9950c-e8eb-45cb-82e5-9d6dd7865ac5\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:57:50.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8706" for this suite.
Aug 21 16:58:12.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:58:12.259: INFO: namespace kubectl-8706 deletion completed in 22.244690779s

• [SLOW TEST:27.680 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:58:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 21 16:58:12.313: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 21 16:58:12.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:12.909: INFO: stderr: ""
Aug 21 16:58:12.909: INFO: stdout: "service/redis-slave created\n"
Aug 21 16:58:12.909: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 21 16:58:12.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:13.254: INFO: stderr: ""
Aug 21 16:58:13.254: INFO: stdout: "service/redis-master created\n"
Aug 21 16:58:13.254: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 21 16:58:13.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:13.636: INFO: stderr: ""
Aug 21 16:58:13.636: INFO: stdout: "service/frontend created\n"
Aug 21 16:58:13.636: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 21 16:58:13.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:14.027: INFO: stderr: ""
Aug 21 16:58:14.027: INFO: stdout: "deployment.apps/frontend created\n"
Aug 21 16:58:14.027: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 21 16:58:14.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:14.286: INFO: stderr: ""
Aug 21 16:58:14.286: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 21 16:58:14.287: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 21 16:58:14.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-9668'
Aug 21 16:58:14.738: INFO: stderr: ""
Aug 21 16:58:14.738: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 21 16:58:14.738: INFO: Waiting for all frontend pods to be Running.
Aug 21 16:58:34.791: INFO: Waiting for frontend to serve content.
Aug 21 16:58:34.815: INFO: Trying to add a new entry to the guestbook.
Aug 21 16:58:34.835: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 21 16:58:34.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:34.992: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:34.992: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:58:34.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:35.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:35.170: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:58:35.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:35.369: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:35.369: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:58:35.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:35.516: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:35.516: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:58:35.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:35.649: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:35.649: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:58:35.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-9668'
Aug 21 16:58:35.814: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:58:35.814: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:58:35.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9668" for this suite.
Aug 21 16:59:19.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:20.028: INFO: namespace kubectl-9668 deletion completed in 44.20501647s

• [SLOW TEST:67.769 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:59:20.031: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 21 16:59:20.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9938'
Aug 21 16:59:20.209: INFO: stderr: ""
Aug 21 16:59:20.209: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 21 16:59:20.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete pods e2e-test-nginx-pod --namespace=kubectl-9938'
Aug 21 16:59:27.812: INFO: stderr: ""
Aug 21 16:59:27.812: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:59:27.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9938" for this suite.
Aug 21 16:59:33.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:34.018: INFO: namespace kubectl-9938 deletion completed in 6.199537509s

• [SLOW TEST:13.987 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 16:59:34.018: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-704
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 16:59:34.084: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 16:59:54.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.95:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-704 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:59:54.196: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:59:54.411: INFO: Found all expected endpoints: [netserver-0]
Aug 21 16:59:54.418: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.101:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-704 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:59:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 16:59:54.714: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 16:59:54.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-704" for this suite.
Aug 21 17:00:16.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:16.937: INFO: namespace pod-network-test-704 deletion completed in 22.213413339s

• [SLOW TEST:42.919 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:00:16.944: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0821 17:00:27.188418      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:00:27.188: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:00:27.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1288" for this suite.
Aug 21 17:00:33.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:33.431: INFO: namespace gc-1288 deletion completed in 6.237595061s

• [SLOW TEST:16.488 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:00:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 21 17:00:33.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 create -f - --namespace=kubectl-1710'
Aug 21 17:00:34.075: INFO: stderr: ""
Aug 21 17:00:34.075: INFO: stdout: "pod/pause created\n"
Aug 21 17:00:34.075: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 21 17:00:34.075: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1710" to be "running and ready"
Aug 21 17:00:34.082: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554527ms
Aug 21 17:00:36.087: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012632889s
Aug 21 17:00:38.094: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.019689812s
Aug 21 17:00:38.095: INFO: Pod "pause" satisfied condition "running and ready"
Aug 21 17:00:38.095: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 21 17:00:38.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 label pods pause testing-label=testing-label-value --namespace=kubectl-1710'
Aug 21 17:00:38.241: INFO: stderr: ""
Aug 21 17:00:38.241: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 21 17:00:38.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pod pause -L testing-label --namespace=kubectl-1710'
Aug 21 17:00:38.360: INFO: stderr: ""
Aug 21 17:00:38.360: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 21 17:00:38.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 label pods pause testing-label- --namespace=kubectl-1710'
Aug 21 17:00:38.586: INFO: stderr: ""
Aug 21 17:00:38.586: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 21 17:00:38.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pod pause -L testing-label --namespace=kubectl-1710'
Aug 21 17:00:38.727: INFO: stderr: ""
Aug 21 17:00:38.728: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 21 17:00:38.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 delete --grace-period=0 --force -f - --namespace=kubectl-1710'
Aug 21 17:00:39.014: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 17:00:39.014: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 21 17:00:39.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get rc,svc -l name=pause --no-headers --namespace=kubectl-1710'
Aug 21 17:00:39.208: INFO: stderr: "No resources found.\n"
Aug 21 17:00:39.208: INFO: stdout: ""
Aug 21 17:00:39.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 get pods -l name=pause --namespace=kubectl-1710 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 17:00:39.406: INFO: stderr: ""
Aug 21 17:00:39.406: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:00:39.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1710" for this suite.
Aug 21 17:00:45.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:45.618: INFO: namespace kubectl-1710 deletion completed in 6.199317189s

• [SLOW TEST:12.187 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:00:45.620: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:00:45.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd" in namespace "downward-api-482" to be "success or failure"
Aug 21 17:00:45.724: INFO: Pod "downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206203ms
Aug 21 17:00:47.733: INFO: Pod "downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013279611s
STEP: Saw pod success
Aug 21 17:00:47.733: INFO: Pod "downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd" satisfied condition "success or failure"
Aug 21 17:00:47.739: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd container client-container: <nil>
STEP: delete the pod
Aug 21 17:00:47.782: INFO: Waiting for pod downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd to disappear
Aug 21 17:00:47.792: INFO: Pod downwardapi-volume-61ed5f6e-8c22-4ae2-b22c-495be0e668dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:00:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-482" for this suite.
Aug 21 17:00:53.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:53.988: INFO: namespace downward-api-482 deletion completed in 6.189653898s

• [SLOW TEST:8.369 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:00:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1954643a-9943-4f18-97ea-0cf73aced63e
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-1954643a-9943-4f18-97ea-0cf73aced63e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:00:58.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8575" for this suite.
Aug 21 17:01:20.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:01:20.407: INFO: namespace projected-8575 deletion completed in 22.191189833s

• [SLOW TEST:26.416 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:01:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 21 17:01:23.538: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:01:23.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-269" for this suite.
Aug 21 17:01:45.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:01:46.069: INFO: namespace replicaset-269 deletion completed in 22.408502091s

• [SLOW TEST:25.662 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:01:46.070: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:01:46.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7592" for this suite.
Aug 21 17:01:52.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:01:52.417: INFO: namespace kubelet-test-7592 deletion completed in 6.207944815s

• [SLOW TEST:6.348 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:01:52.428: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 21 17:02:00.608: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:00.613: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:02.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:02.618: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:04.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:04.618: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:06.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:06.619: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:08.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:08.621: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:10.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:10.619: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:02:12.613: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:02:12.619: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:02:12.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1693" for this suite.
Aug 21 17:02:34.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:02:34.828: INFO: namespace container-lifecycle-hook-1693 deletion completed in 22.201573524s

• [SLOW TEST:42.399 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:02:34.836: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:03:06.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3955" for this suite.
Aug 21 17:03:12.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:12.670: INFO: namespace container-runtime-3955 deletion completed in 6.230872656s

• [SLOW TEST:37.834 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:03:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1697/configmap-test-db1ed7c0-78cb-47c0-99c4-e2f2f0490fee
STEP: Creating a pod to test consume configMaps
Aug 21 17:03:12.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281" in namespace "configmap-1697" to be "success or failure"
Aug 21 17:03:12.797: INFO: Pod "pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281": Phase="Pending", Reason="", readiness=false. Elapsed: 14.47798ms
Aug 21 17:03:14.803: INFO: Pod "pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02028978s
Aug 21 17:03:16.809: INFO: Pod "pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026046017s
STEP: Saw pod success
Aug 21 17:03:16.809: INFO: Pod "pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281" satisfied condition "success or failure"
Aug 21 17:03:16.813: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281 container env-test: <nil>
STEP: delete the pod
Aug 21 17:03:16.841: INFO: Waiting for pod pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281 to disappear
Aug 21 17:03:16.844: INFO: Pod pod-configmaps-bbfb2827-10ca-495a-9420-9a2f2945c281 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:03:16.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1697" for this suite.
Aug 21 17:03:24.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:25.088: INFO: namespace configmap-1697 deletion completed in 8.236491384s

• [SLOW TEST:12.414 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:03:25.093: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:03:25.216: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 21 17:03:30.223: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 17:03:30.223: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 21 17:03:30.249: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2692,SelfLink:/apis/apps/v1/namespaces/deployment-2692/deployments/test-cleanup-deployment,UID:357674aa-ba6b-4435-8b45-f4c0aec6bc37,ResourceVersion:21084,Generation:1,CreationTimestamp:2019-08-21 17:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Aug 21 17:03:30.261: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-2692,SelfLink:/apis/apps/v1/namespaces/deployment-2692/replicasets/test-cleanup-deployment-55bbcbc84c,UID:95bd0257-71a5-4cbb-826c-462eceac7215,ResourceVersion:21086,Generation:1,CreationTimestamp:2019-08-21 17:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 357674aa-ba6b-4435-8b45-f4c0aec6bc37 0xc002a67507 0xc002a67508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 17:03:30.261: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 21 17:03:30.262: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-2692,SelfLink:/apis/apps/v1/namespaces/deployment-2692/replicasets/test-cleanup-controller,UID:949c7c7a-ab05-4374-877f-5f0f4e1eaec5,ResourceVersion:21085,Generation:1,CreationTimestamp:2019-08-21 17:03:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 357674aa-ba6b-4435-8b45-f4c0aec6bc37 0xc002a67437 0xc002a67438}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 21 17:03:30.295: INFO: Pod "test-cleanup-controller-km47n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-km47n,GenerateName:test-cleanup-controller-,Namespace:deployment-2692,SelfLink:/api/v1/namespaces/deployment-2692/pods/test-cleanup-controller-km47n,UID:4dbec8ae-d5a5-4d63-a873-1a5bb566c836,ResourceVersion:21079,Generation:0,CreationTimestamp:2019-08-21 17:03:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.106/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 949c7c7a-ab05-4374-877f-5f0f4e1eaec5 0xc002a67f87 0xc002a67f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gptwb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gptwb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gptwb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a3a040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a3a060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:03:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:03:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:03:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:03:25 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.106,StartTime:2019-08-21 17:03:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:03:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://38e0c22d465053411d1da9df6b61192284d079505b0383a9c3ad4d91520a9378}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:03:30.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2692" for this suite.
Aug 21 17:03:36.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:36.614: INFO: namespace deployment-2692 deletion completed in 6.297359607s

• [SLOW TEST:11.522 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:03:36.615: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:04:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6185" for this suite.
Aug 21 17:04:58.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:04:58.945: INFO: namespace container-probe-6185 deletion completed in 22.211770094s

• [SLOW TEST:82.330 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:04:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:05:02.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4076" for this suite.
Aug 21 17:05:26.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:05:26.364: INFO: namespace replication-controller-4076 deletion completed in 24.22015019s

• [SLOW TEST:27.418 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:05:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 21 17:05:30.519: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:30.525: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:32.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:32.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:34.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:34.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:36.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:36.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:38.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:38.532: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:40.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:40.532: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:42.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:42.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:44.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:44.532: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:46.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:46.530: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:48.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:48.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:50.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:50.532: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:52.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:52.531: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:54.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:54.532: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:56.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:56.533: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 17:05:58.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 17:05:58.533: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:05:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8643" for this suite.
Aug 21 17:06:20.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:06:20.785: INFO: namespace container-lifecycle-hook-8643 deletion completed in 22.244179997s

• [SLOW TEST:54.421 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:06:20.785: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2524e703-3208-416f-a2ac-8780f67dc985
STEP: Creating a pod to test consume secrets
Aug 21 17:06:20.876: INFO: Waiting up to 5m0s for pod "pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46" in namespace "secrets-8797" to be "success or failure"
Aug 21 17:06:20.882: INFO: Pod "pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46": Phase="Pending", Reason="", readiness=false. Elapsed: 6.117318ms
Aug 21 17:06:22.891: INFO: Pod "pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015149919s
Aug 21 17:06:24.898: INFO: Pod "pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021707573s
STEP: Saw pod success
Aug 21 17:06:24.898: INFO: Pod "pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46" satisfied condition "success or failure"
Aug 21 17:06:24.903: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46 container secret-env-test: <nil>
STEP: delete the pod
Aug 21 17:06:24.933: INFO: Waiting for pod pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46 to disappear
Aug 21 17:06:24.939: INFO: Pod pod-secrets-f5f47245-e66c-44ea-a848-159e3361ad46 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:06:24.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8797" for this suite.
Aug 21 17:06:30.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:06:31.219: INFO: namespace secrets-8797 deletion completed in 6.273211406s

• [SLOW TEST:10.434 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:06:31.223: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0821 17:06:41.372645      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:06:41.372: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:06:41.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7295" for this suite.
Aug 21 17:06:47.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:06:47.631: INFO: namespace gc-7295 deletion completed in 6.248155823s

• [SLOW TEST:16.409 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:06:47.633: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:06:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:06:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-825" for this suite.
Aug 21 17:07:44.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:07:44.281: INFO: namespace pods-825 deletion completed in 54.193650084s

• [SLOW TEST:56.648 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:07:44.282: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-9xzs
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 17:07:44.898: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9xzs" in namespace "subpath-2695" to be "success or failure"
Aug 21 17:07:44.903: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.846213ms
Aug 21 17:07:46.909: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010739873s
Aug 21 17:07:48.915: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.017707385s
Aug 21 17:07:50.921: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.023598288s
Aug 21 17:07:52.927: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.029026691s
Aug 21 17:07:54.933: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.035549019s
Aug 21 17:07:56.939: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.041159369s
Aug 21 17:07:58.945: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.04680912s
Aug 21 17:08:00.951: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.053507542s
Aug 21 17:08:02.957: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.059547044s
Aug 21 17:08:04.967: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.068977177s
Aug 21 17:08:06.973: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Running", Reason="", readiness=true. Elapsed: 22.075116901s
Aug 21 17:08:08.979: INFO: Pod "pod-subpath-test-projected-9xzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.081487719s
STEP: Saw pod success
Aug 21 17:08:08.979: INFO: Pod "pod-subpath-test-projected-9xzs" satisfied condition "success or failure"
Aug 21 17:08:08.984: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-subpath-test-projected-9xzs container test-container-subpath-projected-9xzs: <nil>
STEP: delete the pod
Aug 21 17:08:09.012: INFO: Waiting for pod pod-subpath-test-projected-9xzs to disappear
Aug 21 17:08:09.015: INFO: Pod pod-subpath-test-projected-9xzs no longer exists
STEP: Deleting pod pod-subpath-test-projected-9xzs
Aug 21 17:08:09.015: INFO: Deleting pod "pod-subpath-test-projected-9xzs" in namespace "subpath-2695"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:08:09.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2695" for this suite.
Aug 21 17:08:15.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:08:15.218: INFO: namespace subpath-2695 deletion completed in 6.184231513s

• [SLOW TEST:30.937 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:08:15.219: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 21 17:08:15.378: INFO: Waiting up to 5m0s for pod "downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40" in namespace "downward-api-7034" to be "success or failure"
Aug 21 17:08:15.383: INFO: Pod "downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207911ms
Aug 21 17:08:17.394: INFO: Pod "downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015628288s
Aug 21 17:08:19.399: INFO: Pod "downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021170547s
STEP: Saw pod success
Aug 21 17:08:19.399: INFO: Pod "downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40" satisfied condition "success or failure"
Aug 21 17:08:19.405: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40 container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:08:19.439: INFO: Waiting for pod downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40 to disappear
Aug 21 17:08:19.456: INFO: Pod downward-api-d6011242-ebe9-4f96-8b7d-1a0f453ebb40 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:08:19.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7034" for this suite.
Aug 21 17:08:25.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:08:25.705: INFO: namespace downward-api-7034 deletion completed in 6.237156558s

• [SLOW TEST:10.487 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:08:25.706: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 17:08:27.819: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:08:27.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9882" for this suite.
Aug 21 17:08:33.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:08:34.058: INFO: namespace container-runtime-9882 deletion completed in 6.190505648s

• [SLOW TEST:8.353 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:08:34.059: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 21 17:08:34.758: INFO: Pod name wrapped-volume-race-dcac8565-208b-4c63-b15d-e82704fa7da5: Found 0 pods out of 5
Aug 21 17:08:39.767: INFO: Pod name wrapped-volume-race-dcac8565-208b-4c63-b15d-e82704fa7da5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-dcac8565-208b-4c63-b15d-e82704fa7da5 in namespace emptydir-wrapper-6808, will wait for the garbage collector to delete the pods
Aug 21 17:08:51.887: INFO: Deleting ReplicationController wrapped-volume-race-dcac8565-208b-4c63-b15d-e82704fa7da5 took: 24.151366ms
Aug 21 17:08:52.488: INFO: Terminating ReplicationController wrapped-volume-race-dcac8565-208b-4c63-b15d-e82704fa7da5 pods took: 600.857877ms
STEP: Creating RC which spawns configmap-volume pods
Aug 21 17:09:38.133: INFO: Pod name wrapped-volume-race-f2d38730-b7be-4247-8671-6bcdadf3e68f: Found 0 pods out of 5
Aug 21 17:09:43.146: INFO: Pod name wrapped-volume-race-f2d38730-b7be-4247-8671-6bcdadf3e68f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f2d38730-b7be-4247-8671-6bcdadf3e68f in namespace emptydir-wrapper-6808, will wait for the garbage collector to delete the pods
Aug 21 17:09:55.253: INFO: Deleting ReplicationController wrapped-volume-race-f2d38730-b7be-4247-8671-6bcdadf3e68f took: 19.863345ms
Aug 21 17:09:55.956: INFO: Terminating ReplicationController wrapped-volume-race-f2d38730-b7be-4247-8671-6bcdadf3e68f pods took: 702.866399ms
STEP: Creating RC which spawns configmap-volume pods
Aug 21 17:10:38.997: INFO: Pod name wrapped-volume-race-e7cf6642-0ddc-4617-882e-ea80603e0b9e: Found 0 pods out of 5
Aug 21 17:10:44.006: INFO: Pod name wrapped-volume-race-e7cf6642-0ddc-4617-882e-ea80603e0b9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e7cf6642-0ddc-4617-882e-ea80603e0b9e in namespace emptydir-wrapper-6808, will wait for the garbage collector to delete the pods
Aug 21 17:10:56.112: INFO: Deleting ReplicationController wrapped-volume-race-e7cf6642-0ddc-4617-882e-ea80603e0b9e took: 17.741243ms
Aug 21 17:10:56.729: INFO: Terminating ReplicationController wrapped-volume-race-e7cf6642-0ddc-4617-882e-ea80603e0b9e pods took: 616.519474ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:11:38.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6808" for this suite.
Aug 21 17:11:46.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:11:46.991: INFO: namespace emptydir-wrapper-6808 deletion completed in 8.221344985s

• [SLOW TEST:192.933 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:11:46.994: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 21 17:11:47.083: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1116" to be "success or failure"
Aug 21 17:11:47.089: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.294111ms
Aug 21 17:11:49.095: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011880368s
Aug 21 17:11:51.102: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018758514s
STEP: Saw pod success
Aug 21 17:11:51.102: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 21 17:11:51.108: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 21 17:11:51.171: INFO: Waiting for pod pod-host-path-test to disappear
Aug 21 17:11:51.176: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:11:51.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1116" for this suite.
Aug 21 17:11:57.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:11:57.453: INFO: namespace hostpath-1116 deletion completed in 6.264129639s

• [SLOW TEST:10.459 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:11:57.453: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:11:57.675: INFO: (0) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.745218ms)
Aug 21 17:11:57.695: INFO: (1) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 20.492395ms)
Aug 21 17:11:57.702: INFO: (2) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.491064ms)
Aug 21 17:11:57.708: INFO: (3) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.292346ms)
Aug 21 17:11:57.735: INFO: (4) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.915192ms)
Aug 21 17:11:57.744: INFO: (5) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.288027ms)
Aug 21 17:11:57.750: INFO: (6) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.415073ms)
Aug 21 17:11:57.761: INFO: (7) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.009531ms)
Aug 21 17:11:57.774: INFO: (8) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.898333ms)
Aug 21 17:11:57.781: INFO: (9) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.269693ms)
Aug 21 17:11:57.787: INFO: (10) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.380712ms)
Aug 21 17:11:57.794: INFO: (11) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.628029ms)
Aug 21 17:11:57.801: INFO: (12) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.490441ms)
Aug 21 17:11:57.806: INFO: (13) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.248565ms)
Aug 21 17:11:57.812: INFO: (14) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.220612ms)
Aug 21 17:11:57.822: INFO: (15) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.072578ms)
Aug 21 17:11:57.833: INFO: (16) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.380431ms)
Aug 21 17:11:57.840: INFO: (17) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.788864ms)
Aug 21 17:11:57.847: INFO: (18) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.052753ms)
Aug 21 17:11:57.853: INFO: (19) /api/v1/nodes/3c511303-d822-436a-a9c7-85bfe5dec0f1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.249917ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:11:57.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8014" for this suite.
Aug 21 17:12:03.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:12:04.051: INFO: namespace proxy-8014 deletion completed in 6.191358357s

• [SLOW TEST:6.599 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:12:04.055: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:12:04.158: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c" in namespace "projected-3257" to be "success or failure"
Aug 21 17:12:04.170: INFO: Pod "downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.43111ms
Aug 21 17:12:06.176: INFO: Pod "downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017939402s
STEP: Saw pod success
Aug 21 17:12:06.176: INFO: Pod "downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c" satisfied condition "success or failure"
Aug 21 17:12:06.180: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c container client-container: <nil>
STEP: delete the pod
Aug 21 17:12:06.213: INFO: Waiting for pod downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c to disappear
Aug 21 17:12:06.217: INFO: Pod downwardapi-volume-b3fdfbe8-ef6e-47c0-b36f-a691361c798c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:12:06.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3257" for this suite.
Aug 21 17:12:12.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:12:12.429: INFO: namespace projected-3257 deletion completed in 6.204068294s

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:12:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 21 17:12:12.503: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 21 17:12:17.512: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:12:18.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7973" for this suite.
Aug 21 17:12:24.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:12:24.753: INFO: namespace replication-controller-7973 deletion completed in 6.204533268s

• [SLOW TEST:12.322 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:12:24.754: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 21 17:12:24.862: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:12:28.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5073" for this suite.
Aug 21 17:12:34.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:12:34.346: INFO: namespace init-container-5073 deletion completed in 6.206365809s

• [SLOW TEST:9.592 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:12:34.348: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:12:34.413: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:12:38.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7319" for this suite.
Aug 21 17:13:18.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:18.683: INFO: namespace pods-7319 deletion completed in 40.210759798s

• [SLOW TEST:44.336 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:13:18.684: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 21 17:13:18.817: INFO: Waiting up to 5m0s for pod "client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6" in namespace "containers-7335" to be "success or failure"
Aug 21 17:13:18.827: INFO: Pod "client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452571ms
Aug 21 17:13:20.841: INFO: Pod "client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024548569s
Aug 21 17:13:22.847: INFO: Pod "client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030659417s
STEP: Saw pod success
Aug 21 17:13:22.847: INFO: Pod "client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6" satisfied condition "success or failure"
Aug 21 17:13:22.854: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6 container test-container: <nil>
STEP: delete the pod
Aug 21 17:13:22.915: INFO: Waiting for pod client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6 to disappear
Aug 21 17:13:22.921: INFO: Pod client-containers-1e7fb7f6-0ca0-4d32-aa92-595a5d1403c6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:13:22.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7335" for this suite.
Aug 21 17:13:28.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:29.156: INFO: namespace containers-7335 deletion completed in 6.229506388s

• [SLOW TEST:10.472 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:13:29.158: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:13:29.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb" in namespace "downward-api-897" to be "success or failure"
Aug 21 17:13:29.260: INFO: Pod "downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.770919ms
Aug 21 17:13:31.267: INFO: Pod "downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021066011s
Aug 21 17:13:33.273: INFO: Pod "downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026703351s
STEP: Saw pod success
Aug 21 17:13:33.273: INFO: Pod "downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb" satisfied condition "success or failure"
Aug 21 17:13:33.277: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb container client-container: <nil>
STEP: delete the pod
Aug 21 17:13:33.360: INFO: Waiting for pod downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb to disappear
Aug 21 17:13:33.370: INFO: Pod downwardapi-volume-480e1082-10e7-433c-a214-b1b4cac551eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:13:33.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-897" for this suite.
Aug 21 17:13:39.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:39.567: INFO: namespace downward-api-897 deletion completed in 6.188412265s

• [SLOW TEST:10.409 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:13:39.568: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:13:39.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736" in namespace "downward-api-9463" to be "success or failure"
Aug 21 17:13:39.653: INFO: Pod "downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18618ms
Aug 21 17:13:41.658: INFO: Pod "downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009384915s
STEP: Saw pod success
Aug 21 17:13:41.658: INFO: Pod "downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736" satisfied condition "success or failure"
Aug 21 17:13:41.664: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736 container client-container: <nil>
STEP: delete the pod
Aug 21 17:13:41.707: INFO: Waiting for pod downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736 to disappear
Aug 21 17:13:41.712: INFO: Pod downwardapi-volume-2c416dff-cff8-4920-b919-e79841c22736 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:13:41.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9463" for this suite.
Aug 21 17:13:47.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:47.916: INFO: namespace downward-api-9463 deletion completed in 6.197986162s

• [SLOW TEST:8.348 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:13:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-733739cf-7c75-4b51-b116-d4f62d77be82 in namespace container-probe-776
Aug 21 17:13:52.032: INFO: Started pod liveness-733739cf-7c75-4b51-b116-d4f62d77be82 in namespace container-probe-776
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 17:13:52.037: INFO: Initial restart count of pod liveness-733739cf-7c75-4b51-b116-d4f62d77be82 is 0
Aug 21 17:14:16.115: INFO: Restart count of pod container-probe-776/liveness-733739cf-7c75-4b51-b116-d4f62d77be82 is now 1 (24.078163876s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:14:16.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-776" for this suite.
Aug 21 17:14:22.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:22.376: INFO: namespace container-probe-776 deletion completed in 6.220278864s

• [SLOW TEST:34.455 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:14:22.379: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 21 17:14:22.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 api-versions'
Aug 21 17:14:24.130: INFO: stderr: ""
Aug 21 17:14:24.130: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauth.containership.io/v3\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncerebral.containership.io/v1alpha1\ncertificates.k8s.io/v1beta1\ncontainership.io/v3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nprovision.containership.io/v3\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:14:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6253" for this suite.
Aug 21 17:14:30.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:30.342: INFO: namespace kubectl-6253 deletion completed in 6.203536127s

• [SLOW TEST:7.964 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:14:30.343: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 21 17:14:30.417: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:14:34.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9931" for this suite.
Aug 21 17:14:40.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:40.745: INFO: namespace init-container-9931 deletion completed in 6.221458162s

• [SLOW TEST:10.402 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:14:40.746: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 17:14:46.988: INFO: DNS probes using dns-2514/dns-test-25703443-2f61-4fa6-b3a9-41ab23b1bc19 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:14:47.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2514" for this suite.
Aug 21 17:14:53.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:53.266: INFO: namespace dns-2514 deletion completed in 6.215835422s

• [SLOW TEST:12.520 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:14:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 21 17:14:55.896: INFO: Successfully updated pod "pod-update-6333f732-b5b0-4fb4-84fa-3f3b888f97c9"
STEP: verifying the updated pod is in kubernetes
Aug 21 17:14:55.908: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:14:55.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6955" for this suite.
Aug 21 17:15:19.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:15:20.163: INFO: namespace pods-6955 deletion completed in 24.246878639s

• [SLOW TEST:26.895 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:15:20.164: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2119
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2119
STEP: Creating statefulset with conflicting port in namespace statefulset-2119
STEP: Waiting until pod test-pod will start running in namespace statefulset-2119
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2119
Aug 21 17:15:24.286: INFO: Observed stateful pod in namespace: statefulset-2119, name: ss-0, uid: 129d6902-3f87-4a32-8069-75ce16a3f8ab, status phase: Failed. Waiting for statefulset controller to delete.
Aug 21 17:15:24.288: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2119
STEP: Removing pod with conflicting port in namespace statefulset-2119
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2119 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 21 17:15:28.348: INFO: Deleting all statefulset in ns statefulset-2119
Aug 21 17:15:28.353: INFO: Scaling statefulset ss to 0
Aug 21 17:15:38.375: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:15:38.380: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:15:38.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2119" for this suite.
Aug 21 17:15:44.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:15:44.615: INFO: namespace statefulset-2119 deletion completed in 6.211030053s

• [SLOW TEST:24.452 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:15:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 21 17:15:47.253: INFO: Successfully updated pod "annotationupdatec7550829-dc84-4efb-b7b9-e2468b464f4d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:15:49.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5771" for this suite.
Aug 21 17:16:11.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:16:11.559: INFO: namespace downward-api-5771 deletion completed in 22.274283834s

• [SLOW TEST:26.944 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:16:11.560: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 21 17:16:51.692: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:16:51.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0821 17:16:51.692742      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2831" for this suite.
Aug 21 17:16:59.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:16:59.961: INFO: namespace gc-2831 deletion completed in 8.262557869s

• [SLOW TEST:48.401 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:16:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 21 17:17:02.698: INFO: Successfully updated pod "annotationupdate1003ebac-af94-463f-8590-bc6cac9ac5ae"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:17:04.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-569" for this suite.
Aug 21 17:17:26.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:26.996: INFO: namespace projected-569 deletion completed in 22.254271174s

• [SLOW TEST:27.034 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:17:26.996: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-73210647-dd9d-4b74-983b-2b6584aa75b9
STEP: Creating a pod to test consume secrets
Aug 21 17:17:27.160: INFO: Waiting up to 5m0s for pod "pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d" in namespace "secrets-8656" to be "success or failure"
Aug 21 17:17:27.167: INFO: Pod "pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.780629ms
Aug 21 17:17:29.172: INFO: Pod "pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011641798s
Aug 21 17:17:31.177: INFO: Pod "pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017235681s
STEP: Saw pod success
Aug 21 17:17:31.177: INFO: Pod "pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d" satisfied condition "success or failure"
Aug 21 17:17:31.184: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:17:31.248: INFO: Waiting for pod pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d to disappear
Aug 21 17:17:31.259: INFO: Pod pod-secrets-9458e503-0e84-4dce-8529-32c6323da02d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:17:31.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8656" for this suite.
Aug 21 17:17:37.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:37.581: INFO: namespace secrets-8656 deletion completed in 6.294594568s

• [SLOW TEST:10.585 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:17:37.583: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-89a3b8d2-b0c5-47e0-9209-3732bb20a1e9
STEP: Creating a pod to test consume configMaps
Aug 21 17:17:37.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade" in namespace "projected-6103" to be "success or failure"
Aug 21 17:17:37.836: INFO: Pod "pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade": Phase="Pending", Reason="", readiness=false. Elapsed: 5.9943ms
Aug 21 17:17:39.841: INFO: Pod "pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010874463s
STEP: Saw pod success
Aug 21 17:17:39.841: INFO: Pod "pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade" satisfied condition "success or failure"
Aug 21 17:17:39.847: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:17:39.889: INFO: Waiting for pod pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade to disappear
Aug 21 17:17:39.895: INFO: Pod pod-projected-configmaps-5e72fd5f-4bd9-40aa-a1e5-12f5a69d9ade no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:17:39.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6103" for this suite.
Aug 21 17:17:45.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:46.126: INFO: namespace projected-6103 deletion completed in 6.221912756s

• [SLOW TEST:8.543 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:17:46.126: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-c39834e6-8b5e-4589-83fa-bf60823e55c9
STEP: Creating a pod to test consume configMaps
Aug 21 17:17:46.275: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df" in namespace "projected-6038" to be "success or failure"
Aug 21 17:17:46.280: INFO: Pod "pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402796ms
Aug 21 17:17:48.288: INFO: Pod "pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013177175s
STEP: Saw pod success
Aug 21 17:17:48.288: INFO: Pod "pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df" satisfied condition "success or failure"
Aug 21 17:17:48.293: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:17:48.330: INFO: Waiting for pod pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df to disappear
Aug 21 17:17:48.335: INFO: Pod pod-projected-configmaps-648479ba-d7e9-4268-96c6-3ac5575c55df no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:17:48.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6038" for this suite.
Aug 21 17:17:54.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:54.555: INFO: namespace projected-6038 deletion completed in 6.20657105s

• [SLOW TEST:8.429 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:17:54.556: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 21 17:17:54.855: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4016,SelfLink:/api/v1/namespaces/watch-4016/configmaps/e2e-watch-test-watch-closed,UID:ecaf5782-3dea-4b8d-9d0e-3205867394bd,ResourceVersion:25134,Generation:0,CreationTimestamp:2019-08-21 17:17:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 17:17:54.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4016,SelfLink:/api/v1/namespaces/watch-4016/configmaps/e2e-watch-test-watch-closed,UID:ecaf5782-3dea-4b8d-9d0e-3205867394bd,ResourceVersion:25135,Generation:0,CreationTimestamp:2019-08-21 17:17:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 21 17:17:54.909: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4016,SelfLink:/api/v1/namespaces/watch-4016/configmaps/e2e-watch-test-watch-closed,UID:ecaf5782-3dea-4b8d-9d0e-3205867394bd,ResourceVersion:25136,Generation:0,CreationTimestamp:2019-08-21 17:17:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 17:17:54.910: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4016,SelfLink:/api/v1/namespaces/watch-4016/configmaps/e2e-watch-test-watch-closed,UID:ecaf5782-3dea-4b8d-9d0e-3205867394bd,ResourceVersion:25137,Generation:0,CreationTimestamp:2019-08-21 17:17:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:17:54.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4016" for this suite.
Aug 21 17:18:00.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:18:01.240: INFO: namespace watch-4016 deletion completed in 6.3237223s

• [SLOW TEST:6.684 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:18:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:18:01.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea" in namespace "projected-2017" to be "success or failure"
Aug 21 17:18:01.411: INFO: Pod "downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.331367ms
Aug 21 17:18:03.418: INFO: Pod "downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019918569s
Aug 21 17:18:05.425: INFO: Pod "downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026336695s
STEP: Saw pod success
Aug 21 17:18:05.425: INFO: Pod "downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea" satisfied condition "success or failure"
Aug 21 17:18:05.430: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea container client-container: <nil>
STEP: delete the pod
Aug 21 17:18:05.460: INFO: Waiting for pod downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea to disappear
Aug 21 17:18:05.464: INFO: Pod downwardapi-volume-1c2a2c31-5c0a-41a1-b55e-d0176062d7ea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:18:05.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2017" for this suite.
Aug 21 17:18:11.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:18:11.724: INFO: namespace projected-2017 deletion completed in 6.242889715s

• [SLOW TEST:10.482 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:18:11.728: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 21 17:18:11.786: INFO: PodSpec: initContainers in spec.initContainers
Aug 21 17:19:00.413: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed40f561-9407-4728-96a4-d407e92280a2", GenerateName:"", Namespace:"init-container-1702", SelfLink:"/api/v1/namespaces/init-container-1702/pods/pod-init-ed40f561-9407-4728-96a4-d407e92280a2", UID:"6532abc9-9728-49a9-b455-2f182490fb03", ResourceVersion:"25344", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63702004691, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"786795633"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.1.125/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rp7l4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00245a180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rp7l4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rp7l4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rp7l4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0037e22a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"61823cca-1200-4e72-835e-06a7dddcdaa7", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002534060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037e2320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037e2340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0037e2348), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0037e234c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702004691, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702004691, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702004691, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702004691, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.132.234.132", PodIP:"192.168.1.125", StartTime:(*v1.Time)(0xc003412120), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ab61c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ab6230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://27b87cb5f4f8c1e4f12575eba6ec45bb63b29ac36a6cee7f24af5364963b787f"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003412160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003412140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:19:00.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1702" for this suite.
Aug 21 17:19:22.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:19:22.679: INFO: namespace init-container-1702 deletion completed in 22.256543198s

• [SLOW TEST:70.951 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:19:22.680: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:19:22.789: INFO: Create a RollingUpdate DaemonSet
Aug 21 17:19:22.800: INFO: Check that daemon pods launch on every node of the cluster
Aug 21 17:19:22.811: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:22.811: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:22.811: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:22.816: INFO: Number of nodes with available pods: 0
Aug 21 17:19:22.816: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:19:23.825: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:23.825: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:23.825: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:23.830: INFO: Number of nodes with available pods: 0
Aug 21 17:19:23.830: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:19:24.826: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:24.826: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:24.826: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:24.830: INFO: Number of nodes with available pods: 0
Aug 21 17:19:24.830: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:19:25.824: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:25.824: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:25.824: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:25.830: INFO: Number of nodes with available pods: 2
Aug 21 17:19:25.830: INFO: Number of running nodes: 2, number of available pods: 2
Aug 21 17:19:25.830: INFO: Update the DaemonSet to trigger a rollout
Aug 21 17:19:25.845: INFO: Updating DaemonSet daemon-set
Aug 21 17:19:32.883: INFO: Roll back the DaemonSet before rollout is complete
Aug 21 17:19:32.895: INFO: Updating DaemonSet daemon-set
Aug 21 17:19:32.895: INFO: Make sure DaemonSet rollback is complete
Aug 21 17:19:32.900: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:32.900: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:32.906: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:32.906: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:32.906: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:33.914: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:33.914: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:33.921: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:33.921: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:33.921: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:34.912: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:34.912: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:34.918: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:34.919: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:34.919: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:35.913: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:35.914: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:35.920: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:35.920: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:35.920: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:36.911: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:36.911: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:36.919: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:36.919: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:36.919: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:37.913: INFO: Wrong image for pod: daemon-set-5kp5n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 21 17:19:37.913: INFO: Pod daemon-set-5kp5n is not available
Aug 21 17:19:37.923: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:37.923: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:37.923: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:38.913: INFO: Pod daemon-set-npwq4 is not available
Aug 21 17:19:38.919: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:38.919: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:19:38.919: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2880, will wait for the garbage collector to delete the pods
Aug 21 17:19:39.006: INFO: Deleting DaemonSet.extensions daemon-set took: 19.709419ms
Aug 21 17:19:39.607: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.476676ms
Aug 21 17:19:44.713: INFO: Number of nodes with available pods: 0
Aug 21 17:19:44.713: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 17:19:44.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2880/daemonsets","resourceVersion":"25530"},"items":null}

Aug 21 17:19:44.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2880/pods","resourceVersion":"25530"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:19:44.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2880" for this suite.
Aug 21 17:19:50.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:19:50.953: INFO: namespace daemonsets-2880 deletion completed in 6.207480635s

• [SLOW TEST:28.274 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:19:50.954: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5652
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 17:19:51.207: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 17:20:39.375: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.128:8080/dial?request=hostName&protocol=udp&host=192.168.2.156&port=8081&tries=1'] Namespace:pod-network-test-5652 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:20:39.375: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 17:20:39.624: INFO: Waiting for endpoints: map[]
Aug 21 17:20:39.630: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.128:8080/dial?request=hostName&protocol=udp&host=192.168.1.127&port=8081&tries=1'] Namespace:pod-network-test-5652 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:20:39.630: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
Aug 21 17:20:39.982: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:20:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5652" for this suite.
Aug 21 17:21:04.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:04.235: INFO: namespace pod-network-test-5652 deletion completed in 24.246005456s

• [SLOW TEST:73.282 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:04.237: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:21:04.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5790" for this suite.
Aug 21 17:21:10.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:10.637: INFO: namespace services-5790 deletion completed in 6.235085527s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.401 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:10.638: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 17:21:12.796: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:21:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8967" for this suite.
Aug 21 17:21:18.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:19.074: INFO: namespace container-runtime-8967 deletion completed in 6.228116859s

• [SLOW TEST:8.436 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 21 17:21:19.196: INFO: Waiting up to 5m0s for pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22" in namespace "emptydir-9236" to be "success or failure"
Aug 21 17:21:19.202: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.860886ms
Aug 21 17:21:21.207: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010937552s
Aug 21 17:21:23.212: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016312023s
Aug 21 17:21:25.218: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021563187s
Aug 21 17:21:27.224: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02805334s
Aug 21 17:21:29.231: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035340884s
Aug 21 17:21:31.237: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040955456s
Aug 21 17:21:33.242: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Pending", Reason="", readiness=false. Elapsed: 14.046249453s
Aug 21 17:21:35.248: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.052359625s
STEP: Saw pod success
Aug 21 17:21:35.248: INFO: Pod "pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22" satisfied condition "success or failure"
Aug 21 17:21:35.254: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22 container test-container: <nil>
STEP: delete the pod
Aug 21 17:21:35.297: INFO: Waiting for pod pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22 to disappear
Aug 21 17:21:35.302: INFO: Pod pod-7eaed46a-ba4a-4ec4-b83c-9341802f8e22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:21:35.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9236" for this suite.
Aug 21 17:21:41.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:41.608: INFO: namespace emptydir-9236 deletion completed in 6.295022017s

• [SLOW TEST:22.534 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:41.610: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 21 17:21:41.675: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 17:21:41.692: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 17:21:41.697: INFO: 
Logging pods the kubelet thinks is on node 3c511303-d822-436a-a9c7-85bfe5dec0f1 before test
Aug 21 17:21:41.710: INFO: cloud-agent-6rhbp from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container cloud-agent ready: true, restart count 0
Aug 21 17:21:41.710: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-vx26t from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 17:21:41.710: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 21 17:21:41.710: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-21 15:51:17 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 17:21:41.710: INFO: sonobuoy-e2e-job-abdb7b72ca0642e2 from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container e2e ready: true, restart count 0
Aug 21 17:21:41.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 17:21:41.710: INFO: canal-4wqcb from kube-system started at 2019-08-21 15:32:50 +0000 UTC (3 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 17:21:41.710: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 17:21:41.710: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 17:21:41.710: INFO: kube-proxy-b5jkw from kube-system started at 2019-08-21 15:32:50 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.710: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 17:21:41.710: INFO: cloud-coordinator-767969ffd8-jsl2t from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.711: INFO: 	Container cloud-coordinator ready: true, restart count 0
Aug 21 17:21:41.711: INFO: csi-do-node-kznxp from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 17:21:41.711: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 17:21:41.711: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 17:21:41.711: INFO: 
Logging pods the kubelet thinks is on node 61823cca-1200-4e72-835e-06a7dddcdaa7 before test
Aug 21 17:21:41.722: INFO: eventrouter-b478d5654-h8dbn from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container eventrouter ready: true, restart count 0
Aug 21 17:21:41.722: INFO: csi-do-controller-0 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (4 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 21 17:21:41.722: INFO: sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-p9g9l from heptio-sonobuoy started at 2019-08-21 15:51:19 +0000 UTC (2 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 17:21:41.722: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 21 17:21:41.722: INFO: kube-proxy-4mvfm from kube-system started at 2019-08-21 15:32:33 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 17:21:41.722: INFO: canal-q8xpb from kube-system started at 2019-08-21 15:32:33 +0000 UTC (3 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container install-cni ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 21 17:21:41.722: INFO: csi-do-node-wpjp8 from kube-system started at 2019-08-21 15:36:45 +0000 UTC (2 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container csi-do-plugin ready: true, restart count 0
Aug 21 17:21:41.722: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 21 17:21:41.722: INFO: cloud-agent-lwq2n from containership-core started at 2019-08-21 15:35:20 +0000 UTC (1 container statuses recorded)
Aug 21 17:21:41.722: INFO: 	Container cloud-agent ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node 3c511303-d822-436a-a9c7-85bfe5dec0f1
STEP: verifying the node has the label node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.796: INFO: Pod cloud-agent-6rhbp requesting resource cpu=10m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod cloud-agent-lwq2n requesting resource cpu=10m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod cloud-coordinator-767969ffd8-jsl2t requesting resource cpu=50m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod eventrouter-b478d5654-h8dbn requesting resource cpu=10m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod sonobuoy requesting resource cpu=0m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod sonobuoy-e2e-job-abdb7b72ca0642e2 requesting resource cpu=0m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-p9g9l requesting resource cpu=0m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-3ecd7e6572c44f5e-vx26t requesting resource cpu=0m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod canal-4wqcb requesting resource cpu=250m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod canal-q8xpb requesting resource cpu=250m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod csi-do-controller-0 requesting resource cpu=0m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod csi-do-node-kznxp requesting resource cpu=0m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
Aug 21 17:21:41.797: INFO: Pod csi-do-node-wpjp8 requesting resource cpu=0m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod kube-proxy-4mvfm requesting resource cpu=0m on Node 61823cca-1200-4e72-835e-06a7dddcdaa7
Aug 21 17:21:41.797: INFO: Pod kube-proxy-b5jkw requesting resource cpu=0m on Node 3c511303-d822-436a-a9c7-85bfe5dec0f1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [containership.15bcffde88825d57], Reason = [UpdateServiceAccountSecrets], Message = [Detected out-of-date image pull secrets]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5.15bcffde92a0a61c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3619/filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5 to 61823cca-1200-4e72-835e-06a7dddcdaa7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5.15bcffdee947ff5c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5.15bcffdeeca97e02], Reason = [Created], Message = [Created container filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5.15bcffdf023cd6c4], Reason = [Started], Message = [Started container filler-pod-180168d6-ede3-4ae6-9fef-f5389a59afe5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3.15bcffde9348e147], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3619/filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3 to 3c511303-d822-436a-a9c7-85bfe5dec0f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3.15bcffdeddac71ba], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3.15bcffdee3845eaa], Reason = [Created], Message = [Created container filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3.15bcffdefd14f7d6], Reason = [Started], Message = [Started container filler-pod-77993233-fa3e-465d-acc5-6d92d37ab0e3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bcffdf8347b543], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node 3c511303-d822-436a-a9c7-85bfe5dec0f1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 61823cca-1200-4e72-835e-06a7dddcdaa7
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:21:46.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3619" for this suite.
Aug 21 17:21:52.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:53.168: INFO: namespace sched-pred-3619 deletion completed in 6.238649957s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.559 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:53.169: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 21 17:21:53.352: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1301,SelfLink:/api/v1/namespaces/watch-1301/configmaps/e2e-watch-test-resource-version,UID:fe163366-a1ad-414f-82f2-1760bbbd7ece,ResourceVersion:26055,Generation:0,CreationTimestamp:2019-08-21 17:21:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 17:21:53.353: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1301,SelfLink:/api/v1/namespaces/watch-1301/configmaps/e2e-watch-test-resource-version,UID:fe163366-a1ad-414f-82f2-1760bbbd7ece,ResourceVersion:26056,Generation:0,CreationTimestamp:2019-08-21 17:21:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:21:53.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1301" for this suite.
Aug 21 17:21:59.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:59.598: INFO: namespace watch-1301 deletion completed in 6.239410189s

• [SLOW TEST:6.430 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:21:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:21:59.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd" in namespace "projected-8300" to be "success or failure"
Aug 21 17:21:59.730: INFO: Pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333186ms
Aug 21 17:22:01.746: INFO: Pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02041619s
Aug 21 17:22:03.752: INFO: Pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026277595s
Aug 21 17:22:05.758: INFO: Pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032387482s
STEP: Saw pod success
Aug 21 17:22:05.758: INFO: Pod "downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd" satisfied condition "success or failure"
Aug 21 17:22:05.763: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd container client-container: <nil>
STEP: delete the pod
Aug 21 17:22:05.796: INFO: Waiting for pod downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd to disappear
Aug 21 17:22:05.805: INFO: Pod downwardapi-volume-ae3e0064-8bc0-450e-8adf-30e7432b55cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:22:05.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8300" for this suite.
Aug 21 17:22:11.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:12.071: INFO: namespace projected-8300 deletion completed in 6.254805737s

• [SLOW TEST:12.473 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:22:12.071: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6467
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6467 to expose endpoints map[]
Aug 21 17:22:12.234: INFO: Get endpoints failed (5.254484ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 21 17:22:13.241: INFO: Get endpoints failed (1.011655804s elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 21 17:22:14.248: INFO: Get endpoints failed (2.018526675s elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 21 17:22:15.253: INFO: successfully validated that service multi-endpoint-test in namespace services-6467 exposes endpoints map[] (3.023688723s elapsed)
STEP: Creating pod pod1 in namespace services-6467
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6467 to expose endpoints map[pod1:[100]]
Aug 21 17:22:18.336: INFO: successfully validated that service multi-endpoint-test in namespace services-6467 exposes endpoints map[pod1:[100]] (3.071129533s elapsed)
STEP: Creating pod pod2 in namespace services-6467
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6467 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 21 17:22:20.417: INFO: successfully validated that service multi-endpoint-test in namespace services-6467 exposes endpoints map[pod1:[100] pod2:[101]] (2.070915847s elapsed)
STEP: Deleting pod pod1 in namespace services-6467
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6467 to expose endpoints map[pod2:[101]]
Aug 21 17:22:21.452: INFO: successfully validated that service multi-endpoint-test in namespace services-6467 exposes endpoints map[pod2:[101]] (1.026085172s elapsed)
STEP: Deleting pod pod2 in namespace services-6467
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6467 to expose endpoints map[]
Aug 21 17:22:22.474: INFO: successfully validated that service multi-endpoint-test in namespace services-6467 exposes endpoints map[] (1.009701975s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:22:22.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6467" for this suite.
Aug 21 17:22:44.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:44.766: INFO: namespace services-6467 deletion completed in 22.22727332s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:32.694 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:22:44.766: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:22:44.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286" in namespace "projected-4556" to be "success or failure"
Aug 21 17:22:44.902: INFO: Pod "downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286": Phase="Pending", Reason="", readiness=false. Elapsed: 5.871579ms
Aug 21 17:22:46.908: INFO: Pod "downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012030207s
Aug 21 17:22:48.913: INFO: Pod "downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017051258s
STEP: Saw pod success
Aug 21 17:22:48.913: INFO: Pod "downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286" satisfied condition "success or failure"
Aug 21 17:22:48.918: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286 container client-container: <nil>
STEP: delete the pod
Aug 21 17:22:48.951: INFO: Waiting for pod downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286 to disappear
Aug 21 17:22:48.955: INFO: Pod downwardapi-volume-1cf2a4ba-1012-40dc-aadb-2ae9d998e286 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:22:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4556" for this suite.
Aug 21 17:22:54.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:55.195: INFO: namespace projected-4556 deletion completed in 6.234012647s

• [SLOW TEST:10.429 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:22:55.200: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-8e170d52-2f51-413c-ac5b-8fde7104b4b2
STEP: Creating a pod to test consume secrets
Aug 21 17:22:55.303: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba" in namespace "projected-9543" to be "success or failure"
Aug 21 17:22:55.310: INFO: Pod "pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.577818ms
Aug 21 17:22:57.315: INFO: Pod "pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955872s
Aug 21 17:22:59.324: INFO: Pod "pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020465179s
STEP: Saw pod success
Aug 21 17:22:59.324: INFO: Pod "pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba" satisfied condition "success or failure"
Aug 21 17:22:59.328: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:22:59.373: INFO: Waiting for pod pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba to disappear
Aug 21 17:22:59.378: INFO: Pod pod-projected-secrets-86071291-5815-416c-a585-d7752b3cc7ba no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:22:59.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9543" for this suite.
Aug 21 17:23:05.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:23:05.601: INFO: namespace projected-9543 deletion completed in 6.215551692s

• [SLOW TEST:10.401 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:23:05.603: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3885.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3885.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3885.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3885.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 50.96.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.96.50_udp@PTR;check="$$(dig +tcp +noall +answer +search 50.96.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.96.50_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3885.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3885.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3885.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3885.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3885.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3885.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 50.96.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.96.50_udp@PTR;check="$$(dig +tcp +noall +answer +search 50.96.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.96.50_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 17:23:09.963: INFO: Unable to read wheezy_udp@dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:09.968: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:09.974: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:10.029: INFO: Unable to read jessie_udp@dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:10.043: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:10.049: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local from pod dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab: the server could not find the requested resource (get pods dns-test-066f8092-423a-4544-947c-daeee4430cab)
Aug 21 17:23:10.090: INFO: Lookups using dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab failed for: [wheezy_udp@dns-test-service.dns-3885.svc.cluster.local wheezy_tcp@dns-test-service.dns-3885.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local jessie_udp@dns-test-service.dns-3885.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3885.svc.cluster.local]

Aug 21 17:23:15.227: INFO: DNS probes using dns-3885/dns-test-066f8092-423a-4544-947c-daeee4430cab succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:23:15.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3885" for this suite.
Aug 21 17:23:21.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:23:21.682: INFO: namespace dns-3885 deletion completed in 6.211648623s

• [SLOW TEST:16.079 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:23:21.683: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 21 17:23:23.801: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-84b5cc1b-4135-4ea0-9c06-e0cc8ebb0c52,GenerateName:,Namespace:events-7873,SelfLink:/api/v1/namespaces/events-7873/pods/send-events-84b5cc1b-4135-4ea0-9c06-e0cc8ebb0c52,UID:f5c7e62c-8130-4fd9-88de-d05ffa5d5799,ResourceVersion:26496,Generation:0,CreationTimestamp:2019-08-21 17:23:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 761078763,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.133/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6l675 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6l675,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-6l675 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023f51c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023f51e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:23:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:23:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:23:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:23:21 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.133,StartTime:2019-08-21 17:23:21 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-21 17:23:23 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://7f2d185e5aa81945f7cd387f641b1b3ada6ab409c770148d1f03f658f5a938be}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 21 17:23:25.807: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 21 17:23:27.813: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:23:27.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7873" for this suite.
Aug 21 17:24:07.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:08.059: INFO: namespace events-7873 deletion completed in 40.228289591s

• [SLOW TEST:46.376 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:08.065: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-dbeffffd-9f8b-41cc-b7fb-3a57219131a7
STEP: Creating a pod to test consume secrets
Aug 21 17:24:08.215: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c" in namespace "projected-3660" to be "success or failure"
Aug 21 17:24:08.221: INFO: Pod "pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3869ms
Aug 21 17:24:10.227: INFO: Pod "pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012295043s
Aug 21 17:24:12.238: INFO: Pod "pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022782468s
STEP: Saw pod success
Aug 21 17:24:12.238: INFO: Pod "pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c" satisfied condition "success or failure"
Aug 21 17:24:12.243: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:24:12.274: INFO: Waiting for pod pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c to disappear
Aug 21 17:24:12.279: INFO: Pod pod-projected-secrets-4e7eda4d-caf7-43b2-b421-dfe7ddcb0d8c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:24:12.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3660" for this suite.
Aug 21 17:24:18.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:18.498: INFO: namespace projected-3660 deletion completed in 6.209794864s

• [SLOW TEST:10.434 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:18.500: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 21 17:24:18.985: INFO: Waiting up to 5m0s for pod "downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03" in namespace "downward-api-1416" to be "success or failure"
Aug 21 17:24:18.997: INFO: Pod "downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03": Phase="Pending", Reason="", readiness=false. Elapsed: 12.541555ms
Aug 21 17:24:21.004: INFO: Pod "downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018979122s
Aug 21 17:24:23.009: INFO: Pod "downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024298085s
STEP: Saw pod success
Aug 21 17:24:23.009: INFO: Pod "downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03" satisfied condition "success or failure"
Aug 21 17:24:23.014: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03 container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:24:23.074: INFO: Waiting for pod downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03 to disappear
Aug 21 17:24:23.079: INFO: Pod downward-api-2d88c5cd-a4c2-46c9-913d-ef69a1721d03 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:24:23.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1416" for this suite.
Aug 21 17:24:29.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:29.309: INFO: namespace downward-api-1416 deletion completed in 6.222466614s

• [SLOW TEST:10.810 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:29.312: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:24:29.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23" in namespace "projected-7206" to be "success or failure"
Aug 21 17:24:29.478: INFO: Pod "downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.48649ms
Aug 21 17:24:31.484: INFO: Pod "downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010865297s
Aug 21 17:24:33.492: INFO: Pod "downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018715795s
STEP: Saw pod success
Aug 21 17:24:33.492: INFO: Pod "downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23" satisfied condition "success or failure"
Aug 21 17:24:33.502: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23 container client-container: <nil>
STEP: delete the pod
Aug 21 17:24:33.586: INFO: Waiting for pod downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23 to disappear
Aug 21 17:24:33.600: INFO: Pod downwardapi-volume-5e48cf62-238b-4b82-a03d-2754d5bb4e23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:24:33.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7206" for this suite.
Aug 21 17:24:39.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:39.829: INFO: namespace projected-7206 deletion completed in 6.221268621s

• [SLOW TEST:10.518 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:39.832: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 21 17:24:40.034: INFO: Waiting up to 5m0s for pod "pod-a6fe00fe-3e60-457e-8d6c-898cad81d712" in namespace "emptydir-1365" to be "success or failure"
Aug 21 17:24:40.041: INFO: Pod "pod-a6fe00fe-3e60-457e-8d6c-898cad81d712": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919323ms
Aug 21 17:24:42.047: INFO: Pod "pod-a6fe00fe-3e60-457e-8d6c-898cad81d712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012477828s
STEP: Saw pod success
Aug 21 17:24:42.047: INFO: Pod "pod-a6fe00fe-3e60-457e-8d6c-898cad81d712" satisfied condition "success or failure"
Aug 21 17:24:42.051: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-a6fe00fe-3e60-457e-8d6c-898cad81d712 container test-container: <nil>
STEP: delete the pod
Aug 21 17:24:42.109: INFO: Waiting for pod pod-a6fe00fe-3e60-457e-8d6c-898cad81d712 to disappear
Aug 21 17:24:42.113: INFO: Pod pod-a6fe00fe-3e60-457e-8d6c-898cad81d712 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:24:42.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1365" for this suite.
Aug 21 17:24:48.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:48.364: INFO: namespace emptydir-1365 deletion completed in 6.221355817s

• [SLOW TEST:8.532 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:48.364: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 21 17:24:48.623: INFO: Waiting up to 5m0s for pod "pod-253331e1-40b2-4c6c-9f00-a641f9918918" in namespace "emptydir-5091" to be "success or failure"
Aug 21 17:24:48.642: INFO: Pod "pod-253331e1-40b2-4c6c-9f00-a641f9918918": Phase="Pending", Reason="", readiness=false. Elapsed: 18.684926ms
Aug 21 17:24:50.648: INFO: Pod "pod-253331e1-40b2-4c6c-9f00-a641f9918918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024590514s
Aug 21 17:24:52.654: INFO: Pod "pod-253331e1-40b2-4c6c-9f00-a641f9918918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030991627s
STEP: Saw pod success
Aug 21 17:24:52.654: INFO: Pod "pod-253331e1-40b2-4c6c-9f00-a641f9918918" satisfied condition "success or failure"
Aug 21 17:24:52.667: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-253331e1-40b2-4c6c-9f00-a641f9918918 container test-container: <nil>
STEP: delete the pod
Aug 21 17:24:52.699: INFO: Waiting for pod pod-253331e1-40b2-4c6c-9f00-a641f9918918 to disappear
Aug 21 17:24:52.715: INFO: Pod pod-253331e1-40b2-4c6c-9f00-a641f9918918 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:24:52.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5091" for this suite.
Aug 21 17:24:58.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:24:58.963: INFO: namespace emptydir-5091 deletion completed in 6.233421786s

• [SLOW TEST:10.599 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:24:58.964: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-6drw
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 17:24:59.080: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6drw" in namespace "subpath-4394" to be "success or failure"
Aug 21 17:24:59.086: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.254121ms
Aug 21 17:25:01.094: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013095735s
Aug 21 17:25:03.101: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 4.020454514s
Aug 21 17:25:05.106: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 6.025743228s
Aug 21 17:25:07.112: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 8.031397469s
Aug 21 17:25:09.118: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 10.037898949s
Aug 21 17:25:11.136: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 12.055933283s
Aug 21 17:25:13.142: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 14.061788098s
Aug 21 17:25:15.148: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 16.067916254s
Aug 21 17:25:17.154: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 18.074029129s
Aug 21 17:25:19.161: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 20.080077503s
Aug 21 17:25:21.166: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 22.085312447s
Aug 21 17:25:23.172: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Running", Reason="", readiness=true. Elapsed: 24.092023716s
Aug 21 17:25:25.178: INFO: Pod "pod-subpath-test-secret-6drw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.097349505s
STEP: Saw pod success
Aug 21 17:25:25.178: INFO: Pod "pod-subpath-test-secret-6drw" satisfied condition "success or failure"
Aug 21 17:25:25.183: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-subpath-test-secret-6drw container test-container-subpath-secret-6drw: <nil>
STEP: delete the pod
Aug 21 17:25:25.221: INFO: Waiting for pod pod-subpath-test-secret-6drw to disappear
Aug 21 17:25:25.226: INFO: Pod pod-subpath-test-secret-6drw no longer exists
STEP: Deleting pod pod-subpath-test-secret-6drw
Aug 21 17:25:25.226: INFO: Deleting pod "pod-subpath-test-secret-6drw" in namespace "subpath-4394"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:25:25.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4394" for this suite.
Aug 21 17:25:31.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:25:31.539: INFO: namespace subpath-4394 deletion completed in 6.301473626s

• [SLOW TEST:32.576 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:25:31.542: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-4fd6893b-dbc6-4807-a94d-a22b16834fdd
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4fd6893b-dbc6-4807-a94d-a22b16834fdd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:25:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8400" for this suite.
Aug 21 17:25:58.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:25:58.228: INFO: namespace configmap-8400 deletion completed in 22.218987438s

• [SLOW TEST:26.686 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:25:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d41ab511-a841-47ce-9f92-37bc89affc31
STEP: Creating a pod to test consume secrets
Aug 21 17:25:58.338: INFO: Waiting up to 5m0s for pod "pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce" in namespace "secrets-2958" to be "success or failure"
Aug 21 17:25:58.346: INFO: Pod "pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.206699ms
Aug 21 17:26:00.353: INFO: Pod "pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015496456s
Aug 21 17:26:02.358: INFO: Pod "pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020866594s
STEP: Saw pod success
Aug 21 17:26:02.359: INFO: Pod "pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce" satisfied condition "success or failure"
Aug 21 17:26:02.363: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:26:02.405: INFO: Waiting for pod pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce to disappear
Aug 21 17:26:02.410: INFO: Pod pod-secrets-46946628-7b49-42c1-9aa8-fc3eadc618ce no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:26:02.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2958" for this suite.
Aug 21 17:26:08.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:26:08.646: INFO: namespace secrets-2958 deletion completed in 6.228694641s

• [SLOW TEST:10.417 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:26:08.646: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 21 17:26:08.784: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-483237363 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:26:08.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8306" for this suite.
Aug 21 17:26:14.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:26:15.108: INFO: namespace kubectl-8306 deletion completed in 6.206644669s

• [SLOW TEST:6.462 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:26:15.109: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 21 17:26:15.364: INFO: Waiting up to 5m0s for pod "downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc" in namespace "downward-api-508" to be "success or failure"
Aug 21 17:26:15.376: INFO: Pod "downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.029284ms
Aug 21 17:26:17.389: INFO: Pod "downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024151421s
STEP: Saw pod success
Aug 21 17:26:17.389: INFO: Pod "downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc" satisfied condition "success or failure"
Aug 21 17:26:17.396: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:26:17.456: INFO: Waiting for pod downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc to disappear
Aug 21 17:26:17.467: INFO: Pod downward-api-ffe98832-4fc4-4063-98ee-9737e54079dc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:26:17.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-508" for this suite.
Aug 21 17:26:23.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:26:23.715: INFO: namespace downward-api-508 deletion completed in 6.240941266s

• [SLOW TEST:8.607 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:26:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 17:26:26.886: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:26:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-350" for this suite.
Aug 21 17:26:32.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:26:33.161: INFO: namespace container-runtime-350 deletion completed in 6.238453785s

• [SLOW TEST:9.445 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:26:33.161: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-e490e2d6-020b-4cfd-bd5e-b13415eb14f5
STEP: Creating secret with name s-test-opt-upd-4f35410b-ff01-4615-bf5b-63fb70bac392
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e490e2d6-020b-4cfd-bd5e-b13415eb14f5
STEP: Updating secret s-test-opt-upd-4f35410b-ff01-4615-bf5b-63fb70bac392
STEP: Creating secret with name s-test-opt-create-6f0aa538-c596-4ef7-b1f6-8408cb2f0e1a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:26:39.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-847" for this suite.
Aug 21 17:27:03.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:27:03.733: INFO: namespace projected-847 deletion completed in 24.226654395s

• [SLOW TEST:30.572 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:27:03.733: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 17:27:15.858: INFO: DNS probes using dns-test-9d0fc081-b602-4e2c-b2e8-d52a8a7c72ac succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 17:27:19.927: INFO: File wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:19.936: INFO: File jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:19.936: INFO: Lookups using dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 failed for: [wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local]

Aug 21 17:27:24.943: INFO: File wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:24.948: INFO: File jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:24.949: INFO: Lookups using dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 failed for: [wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local]

Aug 21 17:27:29.943: INFO: File wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:29.950: INFO: File jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:29.950: INFO: Lookups using dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 failed for: [wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local]

Aug 21 17:27:34.942: INFO: File wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:34.948: INFO: File jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local from pod  dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 17:27:34.948: INFO: Lookups using dns-3884/dns-test-db6085eb-f945-400c-b654-779711da9dd9 failed for: [wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local]

Aug 21 17:27:39.948: INFO: DNS probes using dns-test-db6085eb-f945-400c-b654-779711da9dd9 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3884.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3884.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 17:27:44.144: INFO: DNS probes using dns-test-d723403c-4ddc-4462-a9c5-9286e7e5b9df succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:27:44.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3884" for this suite.
Aug 21 17:27:52.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:27:52.423: INFO: namespace dns-3884 deletion completed in 8.237909832s

• [SLOW TEST:48.691 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:27:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:27:52.530: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 17:27:52.556: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:52.556: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:52.556: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:52.562: INFO: Number of nodes with available pods: 0
Aug 21 17:27:52.562: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:27:53.577: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:53.577: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:53.577: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:53.583: INFO: Number of nodes with available pods: 0
Aug 21 17:27:53.583: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:27:54.568: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:54.568: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:54.568: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:54.573: INFO: Number of nodes with available pods: 2
Aug 21 17:27:54.573: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 21 17:27:54.620: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:54.620: INFO: Wrong image for pod: daemon-set-hvj9q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:54.627: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:54.629: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:54.629: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:55.637: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:55.637: INFO: Wrong image for pod: daemon-set-hvj9q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:55.645: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:55.645: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:55.645: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:56.636: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:56.636: INFO: Wrong image for pod: daemon-set-hvj9q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:56.642: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:56.642: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:56.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:57.638: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:57.638: INFO: Wrong image for pod: daemon-set-hvj9q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:57.638: INFO: Pod daemon-set-hvj9q is not available
Aug 21 17:27:57.645: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:57.645: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:57.645: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:58.647: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:58.647: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:27:58.657: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:58.657: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:58.658: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:59.636: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:27:59.636: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:27:59.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:59.643: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:27:59.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:00.634: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:00.634: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:28:00.641: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:00.641: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:00.642: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:01.637: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:01.637: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:28:01.662: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:01.663: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:01.663: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:02.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:02.635: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:28:02.642: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:02.642: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:02.642: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:03.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:03.635: INFO: Pod daemon-set-x77lc is not available
Aug 21 17:28:03.641: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:03.641: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:03.641: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:04.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:04.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:04.643: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:04.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:05.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:05.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:05.643: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:05.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:06.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:06.635: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:06.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:06.644: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:06.644: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:07.642: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:07.643: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:07.651: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:07.651: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:07.651: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:08.636: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:08.636: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:08.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:08.644: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:08.644: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:09.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:09.635: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:09.641: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:09.642: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:09.642: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:10.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:10.635: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:10.644: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:10.644: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:10.644: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:11.637: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:11.637: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:11.644: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:11.644: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:11.644: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:12.634: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:12.634: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:12.641: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:12.641: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:12.641: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:13.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:13.635: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:13.642: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:13.642: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:13.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:14.635: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:14.635: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:14.642: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:14.642: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:14.642: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:15.637: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:15.637: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:15.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:15.643: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:15.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:16.636: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:16.636: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:16.644: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:16.644: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:16.644: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:17.636: INFO: Wrong image for pod: daemon-set-8vk4s. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 21 17:28:17.636: INFO: Pod daemon-set-8vk4s is not available
Aug 21 17:28:17.646: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:17.646: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:17.646: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.636: INFO: Pod daemon-set-h455v is not available
Aug 21 17:28:18.643: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.643: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.643: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 21 17:28:18.650: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.651: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.651: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:18.655: INFO: Number of nodes with available pods: 1
Aug 21 17:28:18.655: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:28:19.664: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:19.664: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:19.664: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:19.670: INFO: Number of nodes with available pods: 1
Aug 21 17:28:19.670: INFO: Node 3c511303-d822-436a-a9c7-85bfe5dec0f1 is running more than one daemon pod
Aug 21 17:28:20.666: INFO: DaemonSet pods can't tolerate node 6a587ac4-5741-4a0a-a6ef-f33e1624837d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:20.666: INFO: DaemonSet pods can't tolerate node 774a2e5a-a29b-4e15-a442-39652cf7d8c0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:20.666: INFO: DaemonSet pods can't tolerate node eb7fae3a-223c-4630-ae18-7487867ac48f with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 21 17:28:20.673: INFO: Number of nodes with available pods: 2
Aug 21 17:28:20.673: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4954, will wait for the garbage collector to delete the pods
Aug 21 17:28:20.763: INFO: Deleting DaemonSet.extensions daemon-set took: 11.821697ms
Aug 21 17:28:21.367: INFO: Terminating DaemonSet.extensions daemon-set pods took: 603.679943ms
Aug 21 17:28:27.872: INFO: Number of nodes with available pods: 0
Aug 21 17:28:27.872: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 17:28:27.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4954/daemonsets","resourceVersion":"27736"},"items":null}

Aug 21 17:28:27.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4954/pods","resourceVersion":"27736"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:28:27.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4954" for this suite.
Aug 21 17:28:33.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:28:34.134: INFO: namespace daemonsets-4954 deletion completed in 6.231478493s

• [SLOW TEST:41.706 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:28:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 21 17:28:34.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-483237363 cluster-info'
Aug 21 17:28:34.779: INFO: stderr: ""
Aug 21 17:28:34.779: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:28:34.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3465" for this suite.
Aug 21 17:28:40.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:28:41.034: INFO: namespace kubectl-3465 deletion completed in 6.247224962s

• [SLOW TEST:6.901 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:28:41.045: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-57d0042f-2595-47f7-a40a-15b9088ecbfe
STEP: Creating a pod to test consume configMaps
Aug 21 17:28:41.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396" in namespace "projected-9059" to be "success or failure"
Aug 21 17:28:41.176: INFO: Pod "pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396": Phase="Pending", Reason="", readiness=false. Elapsed: 8.472596ms
Aug 21 17:28:43.183: INFO: Pod "pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014908498s
Aug 21 17:28:45.191: INFO: Pod "pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02304596s
STEP: Saw pod success
Aug 21 17:28:45.191: INFO: Pod "pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396" satisfied condition "success or failure"
Aug 21 17:28:45.196: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:28:45.234: INFO: Waiting for pod pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396 to disappear
Aug 21 17:28:45.241: INFO: Pod pod-projected-configmaps-07988f4d-9a92-4e58-afb0-dc455cac9396 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:28:45.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9059" for this suite.
Aug 21 17:28:51.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:28:51.561: INFO: namespace projected-9059 deletion completed in 6.309512885s

• [SLOW TEST:10.517 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:28:51.563: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c in namespace container-probe-2294
Aug 21 17:28:55.669: INFO: Started pod liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c in namespace container-probe-2294
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 17:28:55.677: INFO: Initial restart count of pod liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is 0
Aug 21 17:29:05.708: INFO: Restart count of pod container-probe-2294/liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is now 1 (10.030392378s elapsed)
Aug 21 17:29:27.785: INFO: Restart count of pod container-probe-2294/liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is now 2 (32.108230079s elapsed)
Aug 21 17:29:45.843: INFO: Restart count of pod container-probe-2294/liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is now 3 (50.165788525s elapsed)
Aug 21 17:30:05.906: INFO: Restart count of pod container-probe-2294/liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is now 4 (1m10.228944011s elapsed)
Aug 21 17:31:06.204: INFO: Restart count of pod container-probe-2294/liveness-8bed7dda-f0eb-42e1-8220-37579e26e57c is now 5 (2m10.527151235s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:31:06.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2294" for this suite.
Aug 21 17:31:12.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:31:12.440: INFO: namespace container-probe-2294 deletion completed in 6.207681611s

• [SLOW TEST:140.877 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:31:12.442: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:31:17.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9851" for this suite.
Aug 21 17:31:24.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:31:24.337: INFO: namespace watch-9851 deletion completed in 6.32686008s

• [SLOW TEST:11.895 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:31:24.337: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-83b2eb06-8eab-43f8-ae12-f227dbd399d4
STEP: Creating a pod to test consume secrets
Aug 21 17:31:24.453: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e" in namespace "projected-4712" to be "success or failure"
Aug 21 17:31:24.470: INFO: Pod "pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131592ms
Aug 21 17:31:26.483: INFO: Pod "pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029954217s
STEP: Saw pod success
Aug 21 17:31:26.483: INFO: Pod "pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e" satisfied condition "success or failure"
Aug 21 17:31:26.489: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:31:26.539: INFO: Waiting for pod pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e to disappear
Aug 21 17:31:26.545: INFO: Pod pod-projected-secrets-38bd7bec-f853-4423-af0f-44fa6400cf8e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:31:26.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4712" for this suite.
Aug 21 17:31:32.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:31:32.793: INFO: namespace projected-4712 deletion completed in 6.24195714s

• [SLOW TEST:8.456 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:31:32.794: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 21 17:31:33.039: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:31:47.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7090" for this suite.
Aug 21 17:31:53.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:31:53.944: INFO: namespace pods-7090 deletion completed in 6.198304995s

• [SLOW TEST:21.151 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:31:53.945: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:31:54.072: INFO: Creating deployment "nginx-deployment"
Aug 21 17:31:54.088: INFO: Waiting for observed generation 1
Aug 21 17:31:56.100: INFO: Waiting for all required pods to come up
Aug 21 17:31:56.107: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 21 17:32:02.123: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 21 17:32:02.135: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 21 17:32:02.153: INFO: Updating deployment nginx-deployment
Aug 21 17:32:02.153: INFO: Waiting for observed generation 2
Aug 21 17:32:04.168: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 21 17:32:04.172: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 21 17:32:04.176: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 21 17:32:04.190: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 21 17:32:04.190: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 21 17:32:04.195: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 21 17:32:04.205: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 21 17:32:04.205: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 21 17:32:04.300: INFO: Updating deployment nginx-deployment
Aug 21 17:32:04.300: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 21 17:32:04.387: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 21 17:32:06.408: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 21 17:32:06.417: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3592,SelfLink:/apis/apps/v1/namespaces/deployment-3592/deployments/nginx-deployment,UID:1d318a8e-f3e7-4028-b29b-32b070acad28,ResourceVersion:28785,Generation:3,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-21 17:32:04 +0000 UTC 2019-08-21 17:32:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-21 17:32:04 +0000 UTC 2019-08-21 17:31:54 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 21 17:32:06.423: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-3592,SelfLink:/apis/apps/v1/namespaces/deployment-3592/replicasets/nginx-deployment-55fb7cb77f,UID:a7412fcc-0859-4acc-81cd-aef04f30ffb3,ResourceVersion:28782,Generation:3,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1d318a8e-f3e7-4028-b29b-32b070acad28 0xc00375f237 0xc00375f238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 21 17:32:06.423: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 21 17:32:06.423: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-3592,SelfLink:/apis/apps/v1/namespaces/deployment-3592/replicasets/nginx-deployment-7b8c6f4498,UID:6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7,ResourceVersion:28775,Generation:3,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1d318a8e-f3e7-4028-b29b-32b070acad28 0xc00375f307 0xc00375f308}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 21 17:32:06.432: INFO: Pod "nginx-deployment-55fb7cb77f-8nkz7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8nkz7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-8nkz7,UID:e413998d-0e19-403c-b047-fdfd0119da0a,ResourceVersion:28771,Generation:0,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.178/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0033c7cd7 0xc0033c7cd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033c7d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033c7d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.432: INFO: Pod "nginx-deployment-55fb7cb77f-8pf77" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8pf77,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-8pf77,UID:cbf87fe3-bd91-4a5f-a6d1-3838b5e9f107,ResourceVersion:28787,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0033c7e40 0xc0033c7e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033c7eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033c7ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.432: INFO: Pod "nginx-deployment-55fb7cb77f-b2h2f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-b2h2f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-b2h2f,UID:44262fef-1552-479a-995e-aaf8776b5a24,ResourceVersion:28812,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0033c7fa0 0xc0033c7fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.433: INFO: Pod "nginx-deployment-55fb7cb77f-gl4bk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gl4bk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-gl4bk,UID:86feaaf7-0c0e-4e0c-a74a-32e559d958f0,ResourceVersion:28765,Generation:0,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.151/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8110 0xc0035a8111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a81a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.433: INFO: Pod "nginx-deployment-55fb7cb77f-grk8q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-grk8q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-grk8q,UID:b259839c-e8f7-4b24-a677-31b8c37069b5,ResourceVersion:28830,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8270 0xc0035a8271}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a82e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.433: INFO: Pod "nginx-deployment-55fb7cb77f-j2tb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j2tb5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-j2tb5,UID:2e1dd1d3-e63f-456f-8664-414370e090ac,ResourceVersion:28799,Generation:0,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.153/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a83e0 0xc0035a83e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.434: INFO: Pod "nginx-deployment-55fb7cb77f-j5dq5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j5dq5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-j5dq5,UID:2b97b9cc-a77b-49d1-b3ba-171f39d9494f,ResourceVersion:28783,Generation:0,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.152/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8550 0xc0035a8551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a85c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a85e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.434: INFO: Pod "nginx-deployment-55fb7cb77f-k6q4m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-k6q4m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-k6q4m,UID:f1b8790f-5837-43a5-b160-fba62e0fc353,ResourceVersion:28837,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a86b0 0xc0035a86b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.434: INFO: Pod "nginx-deployment-55fb7cb77f-mj7sw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mj7sw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-mj7sw,UID:8f2fd3bc-061b-4329-8dd7-499e6046dbf8,ResourceVersion:28753,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8810 0xc0035a8811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a88a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.434: INFO: Pod "nginx-deployment-55fb7cb77f-qjl66" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qjl66,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-qjl66,UID:458b3b52-d247-47a2-a496-b6b1fe1f6dcf,ResourceVersion:28774,Generation:0,CreationTimestamp:2019-08-21 17:32:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.179/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8980 0xc0035a8981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a89f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:02 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.179,StartTime:2019-08-21 17:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.434: INFO: Pod "nginx-deployment-55fb7cb77f-srz28" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-srz28,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-srz28,UID:b02972e6-718a-4484-bd1d-37c1721812e8,ResourceVersion:28836,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8b00 0xc0035a8b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.435: INFO: Pod "nginx-deployment-55fb7cb77f-st5cj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-st5cj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-st5cj,UID:3055071d-303b-4eeb-bff3-dc52ed589bf1,ResourceVersion:28784,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8c60 0xc0035a8c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.435: INFO: Pod "nginx-deployment-55fb7cb77f-wbmsc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wbmsc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-55fb7cb77f-wbmsc,UID:ad5a3315-f931-4d45-ae9c-aca27e274a1b,ResourceVersion:28797,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a7412fcc-0859-4acc-81cd-aef04f30ffb3 0xc0035a8dc0 0xc0035a8dc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.435: INFO: Pod "nginx-deployment-7b8c6f4498-46rjt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-46rjt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-46rjt,UID:1c221141-b915-44c8-b1a8-8c2244a36f49,ResourceVersion:28633,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.149/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a8f30 0xc0035a8f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a8f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a8fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.149,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://95ca4772b3bcf82dbb9c97b852d978222ec7e1bf183852bb171ed23c14a6a4e8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.435: INFO: Pod "nginx-deployment-7b8c6f4498-4b9tq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4b9tq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-4b9tq,UID:0baa3e2b-1b98-432c-8dc4-a64e675b224e,ResourceVersion:28839,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9087 0xc0035a9088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a90f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.436: INFO: Pod "nginx-deployment-7b8c6f4498-4n4n7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4n4n7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-4n4n7,UID:c7ba5143-3774-4595-bd38-7bc10261d1a6,ResourceVersion:28621,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.146/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a91e7 0xc0035a91e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.146,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://533b9fdd1a1945decdab63862d0cde98cd940c57069c10d0d1062a62d8b15909}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.436: INFO: Pod "nginx-deployment-7b8c6f4498-75xl7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-75xl7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-75xl7,UID:72386f79-044f-4e2d-b3a8-4ad4c79c1dc9,ResourceVersion:28615,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.147/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9357 0xc0035a9358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a93c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a93e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:192.168.1.147,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://64319ba0121d569ce719ba51daca887c8ae8a77c40eed5f9d4cda72a623c7fef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.436: INFO: Pod "nginx-deployment-7b8c6f4498-8mbmg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8mbmg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-8mbmg,UID:e5b2ae64-a4f1-4bcb-94d1-8a0813ff23a2,ResourceVersion:28760,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a94b7 0xc0035a94b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.436: INFO: Pod "nginx-deployment-7b8c6f4498-bm6mt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bm6mt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-bm6mt,UID:f2836a02-ef3a-4abf-95cc-3ede08241504,ResourceVersion:28802,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a95c0 0xc0035a95c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.436: INFO: Pod "nginx-deployment-7b8c6f4498-cjjhx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cjjhx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-cjjhx,UID:0935cc26-ffe8-4bba-88c6-cdcbd5d9a4a4,ResourceVersion:28604,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.177/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9717 0xc0035a9718}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a97a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.177,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dada2eb6f3d5f43d2e2cca5c8e0411f6e63f711bc6ae771c831a9a24e10f5eb2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-dqdpp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dqdpp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-dqdpp,UID:23aa0ca2-c085-4560-8482-1063330b33d2,ResourceVersion:28599,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.176/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9887 0xc0035a9888}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a98f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.176,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2e6c8105903f69099dd991e486b025dc10b15ce7ffb0bd1d8a9ce88c1ed129c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-fdbwz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fdbwz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-fdbwz,UID:1203c9aa-584e-4f7b-baab-2dbc19c49b34,ResourceVersion:28816,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a99e7 0xc0035a99e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-fjtwb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fjtwb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-fjtwb,UID:4c5f2b42-1dfc-4638-bb9d-74cb940675a6,ResourceVersion:28763,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9b37 0xc0035a9b38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-fk772" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fk772,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-fk772,UID:ed0193ad-450e-413f-b57b-7159f7a7b940,ResourceVersion:28758,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9c40 0xc0035a9c41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-fphrv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fphrv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-fphrv,UID:70ce9625-8d05-46da-8538-6dd640a73c9b,ResourceVersion:28601,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.173/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9d97 0xc0035a9d98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.173,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bae5e65a6dc1574082b0c97b1cb9f9c9fac131e904eda52e29e5127425f760a9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.437: INFO: Pod "nginx-deployment-7b8c6f4498-g89zl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-g89zl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-g89zl,UID:420f943d-2a7c-45dd-a061-b4cc286a5efb,ResourceVersion:28826,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc0035a9ef7 0xc0035a9ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035a9f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035a9f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.438: INFO: Pod "nginx-deployment-7b8c6f4498-gghhn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gghhn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-gghhn,UID:865d1356-c631-4cd5-851f-65c63244b779,ResourceVersion:28781,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e047 0xc00353e048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.438: INFO: Pod "nginx-deployment-7b8c6f4498-kblvd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kblvd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-kblvd,UID:7bcc2bc3-3829-4889-bacc-21d5b2651bf6,ResourceVersion:28596,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.174/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e1a7 0xc00353e1a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.174,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cda4bbdafe3e5081e5ff8686b5edef03c5c41a411ed51e532f3200c704e06cd4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.438: INFO: Pod "nginx-deployment-7b8c6f4498-kv8bn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kv8bn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-kv8bn,UID:b3e9b717-6d12-4afe-bfd2-ff3ac9a71838,ResourceVersion:28764,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e307 0xc00353e308}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.438: INFO: Pod "nginx-deployment-7b8c6f4498-kvnn4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kvnn4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-kvnn4,UID:9770a9d8-d956-4bd9-b002-8bfcda0d95cf,ResourceVersion:28813,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e410 0xc00353e411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:61823cca-1200-4e72-835e-06a7dddcdaa7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.132,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.438: INFO: Pod "nginx-deployment-7b8c6f4498-qclts" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qclts,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-qclts,UID:df55bd84-b9d2-4638-8d06-957da376a516,ResourceVersion:28817,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e557 0xc00353e558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e5c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e5e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.439: INFO: Pod "nginx-deployment-7b8c6f4498-x8lkb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x8lkb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-x8lkb,UID:9a103165-6177-460d-bc8c-525f44929ee4,ResourceVersion:28779,Generation:0,CreationTimestamp:2019-08-21 17:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e6a7 0xc00353e6a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:32:04 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:,StartTime:2019-08-21 17:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 21 17:32:06.439: INFO: Pod "nginx-deployment-7b8c6f4498-xlhq5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xlhq5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3592,SelfLink:/api/v1/namespaces/deployment-3592/pods/nginx-deployment-7b8c6f4498-xlhq5,UID:6f5e2477-7497-4c7b-83ba-76de1623d30d,ResourceVersion:28593,Generation:0,CreationTimestamp:2019-08-21 17:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.175/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6d486cd1-4001-4d47-bb7b-c28b5aa4c2b7 0xc00353e807 0xc00353e808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b6s2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b6s2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b6s2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:3c511303-d822-436a-a9c7-85bfe5dec0f1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00353e870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00353e890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-21 17:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.132.234.100,PodIP:192.168.2.175,StartTime:2019-08-21 17:31:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-21 17:31:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://612584fea84af765467786eb426778326b97724c4190f02a2b60e45b9bef8147}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:32:06.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3592" for this suite.
Aug 21 17:32:16.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:32:16.743: INFO: namespace deployment-3592 deletion completed in 10.271142815s

• [SLOW TEST:22.798 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:32:16.743: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-83550518-77b1-4307-87ac-38028fc5c2d9
STEP: Creating configMap with name cm-test-opt-upd-c77a85bf-de69-40f2-b524-b7a60fe69ad5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-83550518-77b1-4307-87ac-38028fc5c2d9
STEP: Updating configmap cm-test-opt-upd-c77a85bf-de69-40f2-b524-b7a60fe69ad5
STEP: Creating configMap with name cm-test-opt-create-963e8e7c-bbf7-4fda-bc17-30bddb00b086
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:32:25.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9298" for this suite.
Aug 21 17:32:45.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:32:45.805: INFO: namespace configmap-9298 deletion completed in 20.220751686s

• [SLOW TEST:29.063 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:32:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 21 17:32:45.886: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 21 17:32:48.209: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:32:48.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5718" for this suite.
Aug 21 17:32:54.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:32:54.454: INFO: namespace replication-controller-5718 deletion completed in 6.232270767s

• [SLOW TEST:8.647 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:32:54.455: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 21 17:32:54.610: INFO: Waiting up to 5m0s for pod "client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e" in namespace "containers-3382" to be "success or failure"
Aug 21 17:32:54.615: INFO: Pod "client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.499835ms
Aug 21 17:32:56.837: INFO: Pod "client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227006524s
Aug 21 17:32:58.843: INFO: Pod "client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.232748591s
STEP: Saw pod success
Aug 21 17:32:58.843: INFO: Pod "client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e" satisfied condition "success or failure"
Aug 21 17:32:58.849: INFO: Trying to get logs from node 3c511303-d822-436a-a9c7-85bfe5dec0f1 pod client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e container test-container: <nil>
STEP: delete the pod
Aug 21 17:32:58.898: INFO: Waiting for pod client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e to disappear
Aug 21 17:32:58.902: INFO: Pod client-containers-25d9d489-704d-43e7-a3cd-6dbc156e8b8e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:32:58.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3382" for this suite.
Aug 21 17:33:04.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:05.138: INFO: namespace containers-3382 deletion completed in 6.230019138s

• [SLOW TEST:10.683 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:33:05.139: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-6599/secret-test-968e8013-aecc-4b40-98bb-f84e7802d0a5
STEP: Creating a pod to test consume secrets
Aug 21 17:33:05.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f" in namespace "secrets-6599" to be "success or failure"
Aug 21 17:33:05.251: INFO: Pod "pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.447174ms
Aug 21 17:33:07.257: INFO: Pod "pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015108445s
Aug 21 17:33:09.262: INFO: Pod "pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020437298s
STEP: Saw pod success
Aug 21 17:33:09.262: INFO: Pod "pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f" satisfied condition "success or failure"
Aug 21 17:33:09.267: INFO: Trying to get logs from node 61823cca-1200-4e72-835e-06a7dddcdaa7 pod pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f container env-test: <nil>
STEP: delete the pod
Aug 21 17:33:09.432: INFO: Waiting for pod pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f to disappear
Aug 21 17:33:09.436: INFO: Pod pod-configmaps-b2461505-9028-49c3-b1e3-32973339370f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:33:09.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6599" for this suite.
Aug 21 17:33:15.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:15.691: INFO: namespace secrets-6599 deletion completed in 6.24724901s

• [SLOW TEST:10.553 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 21 17:33:15.691: INFO: >>> kubeConfig: /tmp/kubeconfig-483237363
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 21 17:33:15.917: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-483237363 proxy --unix-socket=/tmp/kubectl-proxy-unix360759370/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 21 17:33:15.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-673" for this suite.
Aug 21 17:33:22.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:22.252: INFO: namespace kubectl-673 deletion completed in 6.251463801s

• [SLOW TEST:6.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSAug 21 17:33:22.254: INFO: Running AfterSuite actions on all nodes
Aug 21 17:33:22.254: INFO: Running AfterSuite actions on node 1
Aug 21 17:33:22.254: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6117.986 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h42m0.438812027s
Test Suite Passed
