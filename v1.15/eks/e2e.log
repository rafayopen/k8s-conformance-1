I0305 19:01:55.459292      20 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-370450052
I0305 19:01:55.459398      20 e2e.go:243] Starting e2e run "66f9321d-0b96-4935-8701-8ec853197070" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583434913 - Will randomize all specs
Will run 215 of 4412 specs

Mar  5 19:01:55.606: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:01:55.609: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  5 19:01:55.633: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  5 19:01:55.666: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  5 19:01:55.666: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar  5 19:01:55.666: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  5 19:01:55.676: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'aws-node' (0 seconds elapsed)
Mar  5 19:01:55.676: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  5 19:01:55.676: INFO: e2e test version: v1.15.10
Mar  5 19:01:55.678: INFO: kube-apiserver version: v1.15.10-eks-bac369
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:01:55.678: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
Mar  5 19:01:55.726: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  5 19:01:55.738: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2012
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0305 19:02:26.401018      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 19:02:26.401: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:02:26.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2012" for this suite.
Mar  5 19:02:32.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:02:32.597: INFO: namespace gc-2012 deletion completed in 6.189052891s

• [SLOW TEST:36.919 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:02:32.598: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  5 19:02:32.756: INFO: Waiting up to 5m0s for pod "downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62" in namespace "downward-api-8070" to be "success or failure"
Mar  5 19:02:32.762: INFO: Pod "downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.817324ms
Mar  5 19:02:34.768: INFO: Pod "downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011377471s
Mar  5 19:02:36.773: INFO: Pod "downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016969909s
STEP: Saw pod success
Mar  5 19:02:36.773: INFO: Pod "downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62" satisfied condition "success or failure"
Mar  5 19:02:36.778: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62 container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:02:36.814: INFO: Waiting for pod downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62 to disappear
Mar  5 19:02:36.818: INFO: Pod downward-api-0486d67a-e0ea-4735-8941-cd2de1673b62 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:02:36.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8070" for this suite.
Mar  5 19:02:42.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:02:43.002: INFO: namespace downward-api-8070 deletion completed in 6.17616176s

• [SLOW TEST:10.404 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:02:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  5 19:02:43.163: INFO: Waiting up to 5m0s for pod "downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b" in namespace "downward-api-3380" to be "success or failure"
Mar  5 19:02:43.169: INFO: Pod "downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.215755ms
Mar  5 19:02:45.175: INFO: Pod "downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011790161s
Mar  5 19:02:47.180: INFO: Pod "downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017344458s
STEP: Saw pod success
Mar  5 19:02:47.180: INFO: Pod "downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b" satisfied condition "success or failure"
Mar  5 19:02:47.185: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:02:47.223: INFO: Waiting for pod downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b to disappear
Mar  5 19:02:47.228: INFO: Pod downward-api-9c4e9d7b-d6b9-41f1-9bd0-39c8f1d7dc1b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:02:47.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3380" for this suite.
Mar  5 19:02:53.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:02:53.414: INFO: namespace downward-api-3380 deletion completed in 6.17698223s

• [SLOW TEST:10.412 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:02:53.415: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6337
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6337
STEP: Creating statefulset with conflicting port in namespace statefulset-6337
STEP: Waiting until pod test-pod will start running in namespace statefulset-6337
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6337
Mar  5 19:02:57.609: INFO: Observed stateful pod in namespace: statefulset-6337, name: ss-0, uid: 351d7aa1-b943-4606-b96d-f8095d2fb3d4, status phase: Failed. Waiting for statefulset controller to delete.
Mar  5 19:02:57.615: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6337
STEP: Removing pod with conflicting port in namespace statefulset-6337
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6337 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  5 19:03:01.646: INFO: Deleting all statefulset in ns statefulset-6337
Mar  5 19:03:01.651: INFO: Scaling statefulset ss to 0
Mar  5 19:03:11.673: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:03:11.678: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:03:11.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6337" for this suite.
Mar  5 19:03:17.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:03:17.883: INFO: namespace statefulset-6337 deletion completed in 6.177667971s

• [SLOW TEST:24.469 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:03:17.884: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  5 19:03:18.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-2519'
Mar  5 19:03:18.335: INFO: stderr: ""
Mar  5 19:03:18.336: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:03:18.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:18.417: INFO: stderr: ""
Mar  5 19:03:18.417: INFO: stdout: "update-demo-nautilus-9nf7z update-demo-nautilus-dr6zc "
Mar  5 19:03:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-9nf7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:18.492: INFO: stderr: ""
Mar  5 19:03:18.492: INFO: stdout: ""
Mar  5 19:03:18.492: INFO: update-demo-nautilus-9nf7z is created but not running
Mar  5 19:03:23.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:23.573: INFO: stderr: ""
Mar  5 19:03:23.573: INFO: stdout: "update-demo-nautilus-9nf7z update-demo-nautilus-dr6zc "
Mar  5 19:03:23.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-9nf7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:23.663: INFO: stderr: ""
Mar  5 19:03:23.663: INFO: stdout: "true"
Mar  5 19:03:23.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-9nf7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:23.737: INFO: stderr: ""
Mar  5 19:03:23.737: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:03:23.737: INFO: validating pod update-demo-nautilus-9nf7z
Mar  5 19:03:23.745: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:03:23.745: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:03:23.745: INFO: update-demo-nautilus-9nf7z is verified up and running
Mar  5 19:03:23.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:23.819: INFO: stderr: ""
Mar  5 19:03:23.819: INFO: stdout: "true"
Mar  5 19:03:23.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:23.896: INFO: stderr: ""
Mar  5 19:03:23.896: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:03:23.896: INFO: validating pod update-demo-nautilus-dr6zc
Mar  5 19:03:23.903: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:03:23.903: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:03:23.903: INFO: update-demo-nautilus-dr6zc is verified up and running
STEP: scaling down the replication controller
Mar  5 19:03:23.904: INFO: scanned /root for discovery docs: <nil>
Mar  5 19:03:23.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2519'
Mar  5 19:03:25.018: INFO: stderr: ""
Mar  5 19:03:25.018: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:03:25.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:25.103: INFO: stderr: ""
Mar  5 19:03:25.103: INFO: stdout: "update-demo-nautilus-9nf7z update-demo-nautilus-dr6zc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  5 19:03:30.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:30.180: INFO: stderr: ""
Mar  5 19:03:30.180: INFO: stdout: "update-demo-nautilus-dr6zc "
Mar  5 19:03:30.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:30.258: INFO: stderr: ""
Mar  5 19:03:30.258: INFO: stdout: "true"
Mar  5 19:03:30.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:30.333: INFO: stderr: ""
Mar  5 19:03:30.333: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:03:30.333: INFO: validating pod update-demo-nautilus-dr6zc
Mar  5 19:03:30.339: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:03:30.339: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:03:30.339: INFO: update-demo-nautilus-dr6zc is verified up and running
STEP: scaling up the replication controller
Mar  5 19:03:30.340: INFO: scanned /root for discovery docs: <nil>
Mar  5 19:03:30.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2519'
Mar  5 19:03:31.450: INFO: stderr: ""
Mar  5 19:03:31.450: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:03:31.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:31.530: INFO: stderr: ""
Mar  5 19:03:31.530: INFO: stdout: "update-demo-nautilus-2s92l update-demo-nautilus-dr6zc "
Mar  5 19:03:31.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-2s92l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:31.613: INFO: stderr: ""
Mar  5 19:03:31.613: INFO: stdout: ""
Mar  5 19:03:31.613: INFO: update-demo-nautilus-2s92l is created but not running
Mar  5 19:03:36.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2519'
Mar  5 19:03:36.692: INFO: stderr: ""
Mar  5 19:03:36.693: INFO: stdout: "update-demo-nautilus-2s92l update-demo-nautilus-dr6zc "
Mar  5 19:03:36.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-2s92l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:36.772: INFO: stderr: ""
Mar  5 19:03:36.772: INFO: stdout: "true"
Mar  5 19:03:36.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-2s92l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:36.847: INFO: stderr: ""
Mar  5 19:03:36.847: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:03:36.847: INFO: validating pod update-demo-nautilus-2s92l
Mar  5 19:03:36.853: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:03:36.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:03:36.854: INFO: update-demo-nautilus-2s92l is verified up and running
Mar  5 19:03:36.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:36.930: INFO: stderr: ""
Mar  5 19:03:36.930: INFO: stdout: "true"
Mar  5 19:03:36.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-dr6zc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2519'
Mar  5 19:03:37.004: INFO: stderr: ""
Mar  5 19:03:37.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:03:37.004: INFO: validating pod update-demo-nautilus-dr6zc
Mar  5 19:03:37.010: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:03:37.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:03:37.011: INFO: update-demo-nautilus-dr6zc is verified up and running
STEP: using delete to clean up resources
Mar  5 19:03:37.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-2519'
Mar  5 19:03:37.100: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:03:37.100: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  5 19:03:37.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2519'
Mar  5 19:03:37.198: INFO: stderr: "No resources found.\n"
Mar  5 19:03:37.198: INFO: stdout: ""
Mar  5 19:03:37.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -l name=update-demo --namespace=kubectl-2519 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 19:03:37.279: INFO: stderr: ""
Mar  5 19:03:37.279: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:03:37.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2519" for this suite.
Mar  5 19:03:43.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:03:43.481: INFO: namespace kubectl-2519 deletion completed in 6.18562373s

• [SLOW TEST:25.598 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:03:43.482: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  5 19:03:43.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-1136'
Mar  5 19:03:43.815: INFO: stderr: ""
Mar  5 19:03:43.815: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  5 19:03:44.821: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:03:44.821: INFO: Found 0 / 1
Mar  5 19:03:45.820: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:03:45.821: INFO: Found 0 / 1
Mar  5 19:03:46.820: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:03:46.820: INFO: Found 1 / 1
Mar  5 19:03:46.820: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  5 19:03:46.825: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:03:46.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 19:03:46.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 patch pod redis-master-2wgk6 --namespace=kubectl-1136 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  5 19:03:46.909: INFO: stderr: ""
Mar  5 19:03:46.909: INFO: stdout: "pod/redis-master-2wgk6 patched\n"
STEP: checking annotations
Mar  5 19:03:46.913: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:03:46.914: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:03:46.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1136" for this suite.
Mar  5 19:04:08.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:04:09.108: INFO: namespace kubectl-1136 deletion completed in 22.187250896s

• [SLOW TEST:25.627 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:04:09.109: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9679c5d2-57d3-42a6-8949-464f21e22ef0
STEP: Creating a pod to test consume secrets
Mar  5 19:04:09.277: INFO: Waiting up to 5m0s for pod "pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a" in namespace "secrets-511" to be "success or failure"
Mar  5 19:04:09.281: INFO: Pod "pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.734803ms
Mar  5 19:04:11.287: INFO: Pod "pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010087705s
Mar  5 19:04:13.292: INFO: Pod "pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015571559s
STEP: Saw pod success
Mar  5 19:04:13.292: INFO: Pod "pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a" satisfied condition "success or failure"
Mar  5 19:04:13.297: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:04:13.331: INFO: Waiting for pod pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a to disappear
Mar  5 19:04:13.335: INFO: Pod pod-secrets-7589c9b8-d105-4031-8c27-0171d2eb952a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:04:13.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-511" for this suite.
Mar  5 19:04:19.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:04:19.523: INFO: namespace secrets-511 deletion completed in 6.180215942s

• [SLOW TEST:10.414 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:04:19.523: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c938a90c-25a8-48a8-94c0-04b107318618
STEP: Creating a pod to test consume secrets
Mar  5 19:04:19.691: INFO: Waiting up to 5m0s for pod "pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246" in namespace "secrets-6004" to be "success or failure"
Mar  5 19:04:19.697: INFO: Pod "pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246": Phase="Pending", Reason="", readiness=false. Elapsed: 6.260873ms
Mar  5 19:04:21.703: INFO: Pod "pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01177542s
Mar  5 19:04:23.708: INFO: Pod "pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017014803s
STEP: Saw pod success
Mar  5 19:04:23.708: INFO: Pod "pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246" satisfied condition "success or failure"
Mar  5 19:04:23.713: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246 container secret-env-test: <nil>
STEP: delete the pod
Mar  5 19:04:23.748: INFO: Waiting for pod pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246 to disappear
Mar  5 19:04:23.752: INFO: Pod pod-secrets-291ed481-6bf9-40e8-85a8-efc079eff246 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:04:23.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6004" for this suite.
Mar  5 19:04:29.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:04:29.939: INFO: namespace secrets-6004 deletion completed in 6.179349822s

• [SLOW TEST:10.416 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:04:29.940: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-976
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-976
STEP: Deleting pre-stop pod
Mar  5 19:04:43.146: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:04:43.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-976" for this suite.
Mar  5 19:05:21.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:05:21.360: INFO: namespace prestop-976 deletion completed in 38.195390066s

• [SLOW TEST:51.420 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:05:21.362: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0305 19:05:31.554118      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 19:05:31.554: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:05:31.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6360" for this suite.
Mar  5 19:05:37.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:05:37.748: INFO: namespace gc-6360 deletion completed in 6.186678118s

• [SLOW TEST:16.386 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:05:37.748: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 19:05:37.907: INFO: Waiting up to 5m0s for pod "pod-b4580480-0e87-4bca-9212-e719a5416a07" in namespace "emptydir-5369" to be "success or failure"
Mar  5 19:05:37.912: INFO: Pod "pod-b4580480-0e87-4bca-9212-e719a5416a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967494ms
Mar  5 19:05:39.918: INFO: Pod "pod-b4580480-0e87-4bca-9212-e719a5416a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010527507s
STEP: Saw pod success
Mar  5 19:05:39.918: INFO: Pod "pod-b4580480-0e87-4bca-9212-e719a5416a07" satisfied condition "success or failure"
Mar  5 19:05:39.922: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-b4580480-0e87-4bca-9212-e719a5416a07 container test-container: <nil>
STEP: delete the pod
Mar  5 19:05:39.952: INFO: Waiting for pod pod-b4580480-0e87-4bca-9212-e719a5416a07 to disappear
Mar  5 19:05:39.957: INFO: Pod pod-b4580480-0e87-4bca-9212-e719a5416a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:05:39.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5369" for this suite.
Mar  5 19:05:45.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:05:46.153: INFO: namespace emptydir-5369 deletion completed in 6.189555906s

• [SLOW TEST:8.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:05:46.154: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  5 19:05:56.366: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:05:56.371: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:05:58.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:05:58.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:00.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:00.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:02.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:02.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:04.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:04.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:06.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:06.376: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:08.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:08.376: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:10.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:10.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:12.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:12.376: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:14.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:14.376: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:16.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:16.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:18.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:18.377: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 19:06:20.371: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 19:06:20.377: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:06:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1646" for this suite.
Mar  5 19:06:42.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:06:42.584: INFO: namespace container-lifecycle-hook-1646 deletion completed in 22.187147857s

• [SLOW TEST:56.430 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:06:42.584: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:06:46.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6936" for this suite.
Mar  5 19:06:52.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:06:52.944: INFO: namespace kubelet-test-6936 deletion completed in 6.182249711s

• [SLOW TEST:10.360 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:06:52.944: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-23e155b3-7506-40a0-81bf-c4b5a85a80bb
STEP: Creating a pod to test consume secrets
Mar  5 19:06:53.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3" in namespace "projected-8483" to be "success or failure"
Mar  5 19:06:53.116: INFO: Pod "pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.946991ms
Mar  5 19:06:55.121: INFO: Pod "pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011389048s
Mar  5 19:06:57.127: INFO: Pod "pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01703226s
STEP: Saw pod success
Mar  5 19:06:57.127: INFO: Pod "pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3" satisfied condition "success or failure"
Mar  5 19:06:57.132: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:06:57.161: INFO: Waiting for pod pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3 to disappear
Mar  5 19:06:57.166: INFO: Pod pod-projected-secrets-e393e16a-b926-46a5-8adf-fec15da055d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:06:57.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8483" for this suite.
Mar  5 19:07:03.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:07:03.366: INFO: namespace projected-8483 deletion completed in 6.192189897s

• [SLOW TEST:10.421 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:07:03.366: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-236
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:07:03.516: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:07:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-236" for this suite.
Mar  5 19:07:10.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:07:10.834: INFO: namespace custom-resource-definition-236 deletion completed in 6.194909971s

• [SLOW TEST:7.468 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:07:10.834: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  5 19:07:15.543: INFO: Successfully updated pod "annotationupdate84ed2aff-92eb-4e9e-9ccb-f8bb8b23660a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:07:19.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5339" for this suite.
Mar  5 19:07:41.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:07:41.778: INFO: namespace downward-api-5339 deletion completed in 22.186165104s

• [SLOW TEST:30.944 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:07:41.779: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  5 19:07:41.932: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:07:44.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8652" for this suite.
Mar  5 19:07:50.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:07:51.097: INFO: namespace init-container-8652 deletion completed in 6.1809562s

• [SLOW TEST:9.319 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:07:51.098: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:07:51.257: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b" in namespace "downward-api-3990" to be "success or failure"
Mar  5 19:07:51.261: INFO: Pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69205ms
Mar  5 19:07:53.267: INFO: Pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010112349s
Mar  5 19:07:55.272: INFO: Pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b": Phase="Running", Reason="", readiness=true. Elapsed: 4.015589258s
Mar  5 19:07:57.278: INFO: Pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020921057s
STEP: Saw pod success
Mar  5 19:07:57.278: INFO: Pod "downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b" satisfied condition "success or failure"
Mar  5 19:07:57.282: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b container client-container: <nil>
STEP: delete the pod
Mar  5 19:07:57.312: INFO: Waiting for pod downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b to disappear
Mar  5 19:07:57.316: INFO: Pod downwardapi-volume-7e03d400-ee9c-4511-b85a-aa0aae2af04b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:07:57.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3990" for this suite.
Mar  5 19:08:03.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:03.508: INFO: namespace downward-api-3990 deletion completed in 6.18467982s

• [SLOW TEST:12.410 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:03.509: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Mar  5 19:08:03.670: INFO: Waiting up to 5m0s for pod "client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b" in namespace "containers-9816" to be "success or failure"
Mar  5 19:08:03.675: INFO: Pod "client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.861926ms
Mar  5 19:08:05.680: INFO: Pod "client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010449079s
Mar  5 19:08:07.686: INFO: Pod "client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015812154s
STEP: Saw pod success
Mar  5 19:08:07.686: INFO: Pod "client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b" satisfied condition "success or failure"
Mar  5 19:08:07.690: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b container test-container: <nil>
STEP: delete the pod
Mar  5 19:08:07.720: INFO: Waiting for pod client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b to disappear
Mar  5 19:08:07.725: INFO: Pod client-containers-ae2ea1e4-a2ab-425b-82f2-522408062e0b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:08:07.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9816" for this suite.
Mar  5 19:08:13.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:13.917: INFO: namespace containers-9816 deletion completed in 6.18432289s

• [SLOW TEST:10.408 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:13.918: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-27ec827f-d98d-49c3-92d2-044831f515e4
STEP: Creating a pod to test consume secrets
Mar  5 19:08:14.086: INFO: Waiting up to 5m0s for pod "pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d" in namespace "secrets-9119" to be "success or failure"
Mar  5 19:08:14.091: INFO: Pod "pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77936ms
Mar  5 19:08:16.097: INFO: Pod "pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010829767s
Mar  5 19:08:18.102: INFO: Pod "pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016429911s
STEP: Saw pod success
Mar  5 19:08:18.102: INFO: Pod "pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d" satisfied condition "success or failure"
Mar  5 19:08:18.107: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:08:18.137: INFO: Waiting for pod pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d to disappear
Mar  5 19:08:18.142: INFO: Pod pod-secrets-84d00a47-6242-4e4a-9134-243cba6a571d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:08:18.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9119" for this suite.
Mar  5 19:08:24.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:24.331: INFO: namespace secrets-9119 deletion completed in 6.181558423s

• [SLOW TEST:10.413 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-a150ae04-7e97-4582-bb8c-e658e0f721f3
STEP: Creating a pod to test consume configMaps
Mar  5 19:08:24.497: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d" in namespace "projected-8924" to be "success or failure"
Mar  5 19:08:24.502: INFO: Pod "pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807394ms
Mar  5 19:08:26.507: INFO: Pod "pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010338093s
STEP: Saw pod success
Mar  5 19:08:26.507: INFO: Pod "pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d" satisfied condition "success or failure"
Mar  5 19:08:26.512: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:08:26.540: INFO: Waiting for pod pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d to disappear
Mar  5 19:08:26.544: INFO: Pod pod-projected-configmaps-a5a4371d-2936-4a1f-9496-38abb0cf324d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:08:26.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8924" for this suite.
Mar  5 19:08:32.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:32.732: INFO: namespace projected-8924 deletion completed in 6.179858445s

• [SLOW TEST:8.401 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:32.732: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Mar  5 19:08:32.891: INFO: Waiting up to 5m0s for pod "client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae" in namespace "containers-1226" to be "success or failure"
Mar  5 19:08:32.896: INFO: Pod "client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.706429ms
Mar  5 19:08:34.902: INFO: Pod "client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010211652s
Mar  5 19:08:36.907: INFO: Pod "client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015552554s
STEP: Saw pod success
Mar  5 19:08:36.907: INFO: Pod "client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae" satisfied condition "success or failure"
Mar  5 19:08:36.913: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae container test-container: <nil>
STEP: delete the pod
Mar  5 19:08:36.940: INFO: Waiting for pod client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae to disappear
Mar  5 19:08:36.945: INFO: Pod client-containers-d8a17a1c-0228-410f-96fa-978b3cb5f7ae no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:08:36.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1226" for this suite.
Mar  5 19:08:42.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:43.134: INFO: namespace containers-1226 deletion completed in 6.181566201s

• [SLOW TEST:10.402 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:43.134: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 19:08:43.294: INFO: Waiting up to 5m0s for pod "pod-bd6c1aec-5169-46d2-952f-b64bf36451de" in namespace "emptydir-9609" to be "success or failure"
Mar  5 19:08:43.299: INFO: Pod "pod-bd6c1aec-5169-46d2-952f-b64bf36451de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787695ms
Mar  5 19:08:45.305: INFO: Pod "pod-bd6c1aec-5169-46d2-952f-b64bf36451de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010104453s
STEP: Saw pod success
Mar  5 19:08:45.305: INFO: Pod "pod-bd6c1aec-5169-46d2-952f-b64bf36451de" satisfied condition "success or failure"
Mar  5 19:08:45.309: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-bd6c1aec-5169-46d2-952f-b64bf36451de container test-container: <nil>
STEP: delete the pod
Mar  5 19:08:45.338: INFO: Waiting for pod pod-bd6c1aec-5169-46d2-952f-b64bf36451de to disappear
Mar  5 19:08:45.343: INFO: Pod pod-bd6c1aec-5169-46d2-952f-b64bf36451de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:08:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9609" for this suite.
Mar  5 19:08:51.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:08:51.531: INFO: namespace emptydir-9609 deletion completed in 6.180929571s

• [SLOW TEST:8.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:08:51.531: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6871
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8986
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:09:18.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5910" for this suite.
Mar  5 19:09:24.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:09:24.212: INFO: namespace namespaces-5910 deletion completed in 6.198214616s
STEP: Destroying namespace "nsdeletetest-6871" for this suite.
Mar  5 19:09:24.217: INFO: Namespace nsdeletetest-6871 was already deleted
STEP: Destroying namespace "nsdeletetest-8986" for this suite.
Mar  5 19:09:30.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:09:30.399: INFO: namespace nsdeletetest-8986 deletion completed in 6.181675497s

• [SLOW TEST:38.867 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:09:30.400: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-hgz8
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 19:09:30.573: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hgz8" in namespace "subpath-6446" to be "success or failure"
Mar  5 19:09:30.578: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690491ms
Mar  5 19:09:32.583: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010303087s
Mar  5 19:09:34.589: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 4.015977016s
Mar  5 19:09:36.594: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 6.021339104s
Mar  5 19:09:38.600: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 8.026967769s
Mar  5 19:09:40.606: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 10.032631283s
Mar  5 19:09:42.611: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 12.03824045s
Mar  5 19:09:44.617: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 14.043930173s
Mar  5 19:09:46.623: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 16.049698143s
Mar  5 19:09:48.628: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 18.055345194s
Mar  5 19:09:50.634: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Running", Reason="", readiness=true. Elapsed: 20.061082868s
Mar  5 19:09:52.640: INFO: Pod "pod-subpath-test-projected-hgz8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.066559114s
STEP: Saw pod success
Mar  5 19:09:52.640: INFO: Pod "pod-subpath-test-projected-hgz8" satisfied condition "success or failure"
Mar  5 19:09:52.644: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-subpath-test-projected-hgz8 container test-container-subpath-projected-hgz8: <nil>
STEP: delete the pod
Mar  5 19:09:52.674: INFO: Waiting for pod pod-subpath-test-projected-hgz8 to disappear
Mar  5 19:09:52.679: INFO: Pod pod-subpath-test-projected-hgz8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hgz8
Mar  5 19:09:52.679: INFO: Deleting pod "pod-subpath-test-projected-hgz8" in namespace "subpath-6446"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:09:52.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6446" for this suite.
Mar  5 19:09:58.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:09:58.873: INFO: namespace subpath-6446 deletion completed in 6.182030755s

• [SLOW TEST:28.473 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:09:58.875: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  5 19:10:03.566: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7e44a3ba-32a9-4094-97fe-0e3d61906df5"
Mar  5 19:10:03.566: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7e44a3ba-32a9-4094-97fe-0e3d61906df5" in namespace "pods-519" to be "terminated due to deadline exceeded"
Mar  5 19:10:03.571: INFO: Pod "pod-update-activedeadlineseconds-7e44a3ba-32a9-4094-97fe-0e3d61906df5": Phase="Running", Reason="", readiness=true. Elapsed: 4.783409ms
Mar  5 19:10:05.576: INFO: Pod "pod-update-activedeadlineseconds-7e44a3ba-32a9-4094-97fe-0e3d61906df5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01033174s
Mar  5 19:10:05.576: INFO: Pod "pod-update-activedeadlineseconds-7e44a3ba-32a9-4094-97fe-0e3d61906df5" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:10:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-519" for this suite.
Mar  5 19:10:11.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:10:11.779: INFO: namespace pods-519 deletion completed in 6.194964206s

• [SLOW TEST:12.904 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:10:11.779: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Mar  5 19:10:11.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-7905'
Mar  5 19:10:12.149: INFO: stderr: ""
Mar  5 19:10:12.149: INFO: stdout: "pod/pause created\n"
Mar  5 19:10:12.149: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  5 19:10:12.149: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7905" to be "running and ready"
Mar  5 19:10:12.154: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699134ms
Mar  5 19:10:14.159: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010112307s
Mar  5 19:10:14.160: INFO: Pod "pause" satisfied condition "running and ready"
Mar  5 19:10:14.160: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  5 19:10:14.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 label pods pause testing-label=testing-label-value --namespace=kubectl-7905'
Mar  5 19:10:14.246: INFO: stderr: ""
Mar  5 19:10:14.246: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  5 19:10:14.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pod pause -L testing-label --namespace=kubectl-7905'
Mar  5 19:10:14.323: INFO: stderr: ""
Mar  5 19:10:14.323: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  5 19:10:14.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 label pods pause testing-label- --namespace=kubectl-7905'
Mar  5 19:10:14.403: INFO: stderr: ""
Mar  5 19:10:14.403: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  5 19:10:14.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pod pause -L testing-label --namespace=kubectl-7905'
Mar  5 19:10:14.477: INFO: stderr: ""
Mar  5 19:10:14.477: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Mar  5 19:10:14.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-7905'
Mar  5 19:10:14.567: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:10:14.567: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  5 19:10:14.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get rc,svc -l name=pause --no-headers --namespace=kubectl-7905'
Mar  5 19:10:14.650: INFO: stderr: "No resources found.\n"
Mar  5 19:10:14.650: INFO: stdout: ""
Mar  5 19:10:14.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -l name=pause --namespace=kubectl-7905 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 19:10:14.728: INFO: stderr: ""
Mar  5 19:10:14.728: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:10:14.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7905" for this suite.
Mar  5 19:10:20.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:10:20.917: INFO: namespace kubectl-7905 deletion completed in 6.181730443s

• [SLOW TEST:9.139 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:10:20.918: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9d8aa259-508d-403c-89bd-ad9acdd7de35
STEP: Creating a pod to test consume configMaps
Mar  5 19:10:21.082: INFO: Waiting up to 5m0s for pod "pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7" in namespace "configmap-1125" to be "success or failure"
Mar  5 19:10:21.087: INFO: Pod "pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63342ms
Mar  5 19:10:23.092: INFO: Pod "pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009963279s
Mar  5 19:10:25.098: INFO: Pod "pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015569174s
STEP: Saw pod success
Mar  5 19:10:25.098: INFO: Pod "pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7" satisfied condition "success or failure"
Mar  5 19:10:25.103: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:10:25.132: INFO: Waiting for pod pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7 to disappear
Mar  5 19:10:25.137: INFO: Pod pod-configmaps-34a42b45-8d9c-4742-96b1-52c899e187b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:10:25.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1125" for this suite.
Mar  5 19:10:31.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:10:31.327: INFO: namespace configmap-1125 deletion completed in 6.182297713s

• [SLOW TEST:10.409 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:10:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9712.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9712.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 100.56.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.56.100_udp@PTR;check="$$(dig +tcp +noall +answer +search 100.56.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.56.100_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9712.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9712.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9712.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 100.56.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.56.100_udp@PTR;check="$$(dig +tcp +noall +answer +search 100.56.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.56.100_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 19:10:45.533: INFO: Unable to read wheezy_udp@dns-test-service.dns-9712.svc.cluster.local from pod dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd: the server could not find the requested resource (get pods dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd)
Mar  5 19:10:45.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9712.svc.cluster.local from pod dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd: the server could not find the requested resource (get pods dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd)
Mar  5 19:10:45.583: INFO: Unable to read jessie_udp@dns-test-service.dns-9712.svc.cluster.local from pod dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd: the server could not find the requested resource (get pods dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd)
Mar  5 19:10:45.588: INFO: Unable to read jessie_tcp@dns-test-service.dns-9712.svc.cluster.local from pod dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd: the server could not find the requested resource (get pods dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd)
Mar  5 19:10:45.593: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local from pod dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd: the server could not find the requested resource (get pods dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd)
Mar  5 19:10:45.628: INFO: Lookups using dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd failed for: [wheezy_udp@dns-test-service.dns-9712.svc.cluster.local wheezy_tcp@dns-test-service.dns-9712.svc.cluster.local jessie_udp@dns-test-service.dns-9712.svc.cluster.local jessie_tcp@dns-test-service.dns-9712.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9712.svc.cluster.local]

Mar  5 19:10:50.729: INFO: DNS probes using dns-9712/dns-test-59b2602c-ca1f-46d3-bb60-faa1dd9ae9cd succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:10:50.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9712" for this suite.
Mar  5 19:10:56.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:10:56.995: INFO: namespace dns-9712 deletion completed in 6.186334473s

• [SLOW TEST:25.668 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:10:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7895
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  5 19:10:57.164: INFO: Found 0 stateful pods, waiting for 3
Mar  5 19:11:07.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:11:07.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:11:07.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Mar  5 19:11:17.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:11:17.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:11:17.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:11:17.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-7895 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:11:17.398: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:11:17.398: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:11:17.398: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  5 19:11:27.440: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  5 19:11:37.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-7895 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:11:37.666: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 19:11:37.666: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 19:11:37.666: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 19:11:47.696: INFO: Waiting for StatefulSet statefulset-7895/ss2 to complete update
Mar  5 19:11:47.696: INFO: Waiting for Pod statefulset-7895/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar  5 19:11:47.696: INFO: Waiting for Pod statefulset-7895/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar  5 19:11:57.707: INFO: Waiting for StatefulSet statefulset-7895/ss2 to complete update
Mar  5 19:11:57.707: INFO: Waiting for Pod statefulset-7895/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Mar  5 19:12:07.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-7895 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:12:07.921: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:12:07.921: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:12:07.921: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 19:12:17.963: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  5 19:12:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-7895 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:12:28.193: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 19:12:28.193: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 19:12:28.193: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 19:12:38.222: INFO: Waiting for StatefulSet statefulset-7895/ss2 to complete update
Mar  5 19:12:38.223: INFO: Waiting for Pod statefulset-7895/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Mar  5 19:12:38.223: INFO: Waiting for Pod statefulset-7895/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Mar  5 19:12:48.233: INFO: Waiting for StatefulSet statefulset-7895/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  5 19:12:58.234: INFO: Deleting all statefulset in ns statefulset-7895
Mar  5 19:12:58.238: INFO: Scaling statefulset ss2 to 0
Mar  5 19:13:18.259: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:13:18.264: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:13:18.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7895" for this suite.
Mar  5 19:13:26.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:13:26.485: INFO: namespace statefulset-7895 deletion completed in 8.180974661s

• [SLOW TEST:149.489 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:13:26.486: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Mar  5 19:13:26.645: INFO: Waiting up to 5m0s for pod "client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576" in namespace "containers-4104" to be "success or failure"
Mar  5 19:13:26.650: INFO: Pod "client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576": Phase="Pending", Reason="", readiness=false. Elapsed: 4.829214ms
Mar  5 19:13:28.655: INFO: Pod "client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010402953s
STEP: Saw pod success
Mar  5 19:13:28.655: INFO: Pod "client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576" satisfied condition "success or failure"
Mar  5 19:13:28.660: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576 container test-container: <nil>
STEP: delete the pod
Mar  5 19:13:28.691: INFO: Waiting for pod client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576 to disappear
Mar  5 19:13:28.696: INFO: Pod client-containers-26e58b0a-530f-49bb-8f91-a95b556f3576 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:13:28.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4104" for this suite.
Mar  5 19:13:34.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:13:34.877: INFO: namespace containers-4104 deletion completed in 6.17392386s

• [SLOW TEST:8.391 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:13:34.877: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:13:35.033: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  5 19:13:40.039: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 19:13:40.039: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  5 19:13:42.045: INFO: Creating deployment "test-rollover-deployment"
Mar  5 19:13:42.056: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  5 19:13:44.067: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  5 19:13:44.077: INFO: Ensure that both replica sets have 1 created replica
Mar  5 19:13:44.087: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  5 19:13:44.098: INFO: Updating deployment test-rollover-deployment
Mar  5 19:13:44.098: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  5 19:13:46.108: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  5 19:13:46.120: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  5 19:13:46.131: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:46.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032424, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:48.142: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:48.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032427, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:50.142: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:50.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032427, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:52.142: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:52.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032427, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:54.142: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:54.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032427, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:56.142: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 19:13:56.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032427, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719032422, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:13:58.142: INFO: 
Mar  5 19:13:58.142: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  5 19:13:58.156: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3121,SelfLink:/apis/apps/v1/namespaces/deployment-3121/deployments/test-rollover-deployment,UID:2db3fd22-326f-4195-98d7-e5d7b9dce3dd,ResourceVersion:644458,Generation:2,CreationTimestamp:2020-03-05 19:13:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-05 19:13:42 +0000 UTC 2020-03-05 19:13:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-05 19:13:57 +0000 UTC 2020-03-05 19:13:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 19:13:58.162: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-3121,SelfLink:/apis/apps/v1/namespaces/deployment-3121/replicasets/test-rollover-deployment-854595fc44,UID:3bdfdb9a-7e3b-479c-93a6-9c16da159320,ResourceVersion:644447,Generation:2,CreationTimestamp:2020-03-05 19:13:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2db3fd22-326f-4195-98d7-e5d7b9dce3dd 0xc00385e1f7 0xc00385e1f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 19:13:58.162: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  5 19:13:58.162: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3121,SelfLink:/apis/apps/v1/namespaces/deployment-3121/replicasets/test-rollover-controller,UID:3bfee5d5-9cb9-4f47-b8cc-5a106d285f2e,ResourceVersion:644456,Generation:2,CreationTimestamp:2020-03-05 19:13:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2db3fd22-326f-4195-98d7-e5d7b9dce3dd 0xc00385e127 0xc00385e128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:13:58.162: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-3121,SelfLink:/apis/apps/v1/namespaces/deployment-3121/replicasets/test-rollover-deployment-9b8b997cf,UID:88748092-58c4-4c41-85b7-1637b5592fa7,ResourceVersion:644405,Generation:2,CreationTimestamp:2020-03-05 19:13:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2db3fd22-326f-4195-98d7-e5d7b9dce3dd 0xc00385e2c0 0xc00385e2c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:13:58.167: INFO: Pod "test-rollover-deployment-854595fc44-hbhqs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-hbhqs,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-3121,SelfLink:/api/v1/namespaces/deployment-3121/pods/test-rollover-deployment-854595fc44-hbhqs,UID:ce54646c-4dbe-4179-b2b5-9475bf7b7b91,ResourceVersion:644423,Generation:0,CreationTimestamp:2020-03-05 19:13:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 3bdfdb9a-7e3b-479c-93a6-9c16da159320 0xc00385eed7 0xc00385eed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ptfmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ptfmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ptfmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00385ef40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00385ef60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:13:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:13:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:13:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:13:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:192.168.82.155,StartTime:2020-03-05 19:13:44 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-05 19:13:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6c5778c2f8ff68d6766c052e29d58dc481bd381f7b5191ddb96fb8f5069bb2a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:13:58.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3121" for this suite.
Mar  5 19:14:04.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:14:04.364: INFO: namespace deployment-3121 deletion completed in 6.18994741s

• [SLOW TEST:29.487 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:14:04.364: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-e1bfc1b3-a0fa-4ab1-9a44-c1f9f6813dd9
STEP: Creating a pod to test consume configMaps
Mar  5 19:14:04.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540" in namespace "configmap-6949" to be "success or failure"
Mar  5 19:14:04.537: INFO: Pod "pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055129ms
Mar  5 19:14:06.542: INFO: Pod "pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011145901s
STEP: Saw pod success
Mar  5 19:14:06.542: INFO: Pod "pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540" satisfied condition "success or failure"
Mar  5 19:14:06.547: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:14:06.581: INFO: Waiting for pod pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540 to disappear
Mar  5 19:14:06.586: INFO: Pod pod-configmaps-cc0fc72a-ad0e-4394-ad15-be268a555540 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:14:06.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6949" for this suite.
Mar  5 19:14:12.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:14:12.786: INFO: namespace configmap-6949 deletion completed in 6.187723435s

• [SLOW TEST:8.422 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:14:12.786: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:14:12.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3" in namespace "downward-api-4112" to be "success or failure"
Mar  5 19:14:12.947: INFO: Pod "downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137396ms
Mar  5 19:14:14.953: INFO: Pod "downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010688944s
STEP: Saw pod success
Mar  5 19:14:14.953: INFO: Pod "downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3" satisfied condition "success or failure"
Mar  5 19:14:14.958: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3 container client-container: <nil>
STEP: delete the pod
Mar  5 19:14:14.990: INFO: Waiting for pod downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3 to disappear
Mar  5 19:14:14.995: INFO: Pod downwardapi-volume-1eabb74c-b941-4b4d-ab93-b4c1352e83c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:14:14.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4112" for this suite.
Mar  5 19:14:21.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:14:21.175: INFO: namespace downward-api-4112 deletion completed in 6.173467155s

• [SLOW TEST:8.389 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:14:21.175: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9866
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 19:14:21.320: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 19:14:47.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.145.128:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:47.488: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:47.621: INFO: Found all expected endpoints: [netserver-0]
Mar  5 19:14:47.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.65.224:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:47.626: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:47.751: INFO: Found all expected endpoints: [netserver-1]
Mar  5 19:14:47.757: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.110.191:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:47.757: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:47.873: INFO: Found all expected endpoints: [netserver-2]
Mar  5 19:14:47.878: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.239.73:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:47.878: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:48.004: INFO: Found all expected endpoints: [netserver-3]
Mar  5 19:14:48.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.132.123:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:48.009: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:48.129: INFO: Found all expected endpoints: [netserver-4]
Mar  5 19:14:48.135: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.205.7:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 19:14:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 19:14:48.248: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:14:48.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9866" for this suite.
Mar  5 19:15:12.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:15:12.441: INFO: namespace pod-network-test-9866 deletion completed in 24.184750151s

• [SLOW TEST:51.265 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:15:12.443: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:15:12.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d" in namespace "projected-1994" to be "success or failure"
Mar  5 19:15:12.606: INFO: Pod "downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.717927ms
Mar  5 19:15:14.611: INFO: Pod "downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010063883s
STEP: Saw pod success
Mar  5 19:15:14.611: INFO: Pod "downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d" satisfied condition "success or failure"
Mar  5 19:15:14.615: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d container client-container: <nil>
STEP: delete the pod
Mar  5 19:15:14.645: INFO: Waiting for pod downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d to disappear
Mar  5 19:15:14.649: INFO: Pod downwardapi-volume-f474e6ef-9e42-4ea5-9365-6601e4d66b4d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:15:14.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1994" for this suite.
Mar  5 19:15:20.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:15:20.835: INFO: namespace projected-1994 deletion completed in 6.17772477s

• [SLOW TEST:8.392 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:15:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-wtxm
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 19:15:21.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wtxm" in namespace "subpath-6699" to be "success or failure"
Mar  5 19:15:21.009: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78234ms
Mar  5 19:15:23.014: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 2.010097769s
Mar  5 19:15:25.019: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 4.015707215s
Mar  5 19:15:27.025: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 6.021307237s
Mar  5 19:15:29.030: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 8.026633882s
Mar  5 19:15:31.036: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 10.031764144s
Mar  5 19:15:33.041: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 12.037413427s
Mar  5 19:15:35.047: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 14.043013634s
Mar  5 19:15:37.053: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 16.04877983s
Mar  5 19:15:39.058: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 18.054646415s
Mar  5 19:15:41.065: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Running", Reason="", readiness=true. Elapsed: 20.061137058s
Mar  5 19:15:43.071: INFO: Pod "pod-subpath-test-configmap-wtxm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.067218208s
STEP: Saw pod success
Mar  5 19:15:43.071: INFO: Pod "pod-subpath-test-configmap-wtxm" satisfied condition "success or failure"
Mar  5 19:15:43.076: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-subpath-test-configmap-wtxm container test-container-subpath-configmap-wtxm: <nil>
STEP: delete the pod
Mar  5 19:15:43.107: INFO: Waiting for pod pod-subpath-test-configmap-wtxm to disappear
Mar  5 19:15:43.111: INFO: Pod pod-subpath-test-configmap-wtxm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wtxm
Mar  5 19:15:43.111: INFO: Deleting pod "pod-subpath-test-configmap-wtxm" in namespace "subpath-6699"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:15:43.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6699" for this suite.
Mar  5 19:15:49.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:15:49.299: INFO: namespace subpath-6699 deletion completed in 6.175824174s

• [SLOW TEST:28.464 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:15:49.300: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:15:49.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3" in namespace "downward-api-5797" to be "success or failure"
Mar  5 19:15:49.462: INFO: Pod "downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912446ms
Mar  5 19:15:51.468: INFO: Pod "downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010332777s
STEP: Saw pod success
Mar  5 19:15:51.468: INFO: Pod "downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3" satisfied condition "success or failure"
Mar  5 19:15:51.472: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3 container client-container: <nil>
STEP: delete the pod
Mar  5 19:15:51.500: INFO: Waiting for pod downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3 to disappear
Mar  5 19:15:51.504: INFO: Pod downwardapi-volume-52ae43b6-0c98-4002-a1ac-e7afbca748c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:15:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5797" for this suite.
Mar  5 19:15:57.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:15:57.693: INFO: namespace downward-api-5797 deletion completed in 6.179740964s

• [SLOW TEST:8.393 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:15:57.693: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:15:57.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1147" for this suite.
Mar  5 19:16:19.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:16:20.044: INFO: namespace pods-1147 deletion completed in 22.178817107s

• [SLOW TEST:22.351 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:16:20.045: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  5 19:16:20.204: INFO: Waiting up to 5m0s for pod "pod-e1b63175-f4d8-420e-9a20-440023532451" in namespace "emptydir-8518" to be "success or failure"
Mar  5 19:16:20.208: INFO: Pod "pod-e1b63175-f4d8-420e-9a20-440023532451": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524019ms
Mar  5 19:16:22.214: INFO: Pod "pod-e1b63175-f4d8-420e-9a20-440023532451": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01026551s
STEP: Saw pod success
Mar  5 19:16:22.214: INFO: Pod "pod-e1b63175-f4d8-420e-9a20-440023532451" satisfied condition "success or failure"
Mar  5 19:16:22.219: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-e1b63175-f4d8-420e-9a20-440023532451 container test-container: <nil>
STEP: delete the pod
Mar  5 19:16:22.247: INFO: Waiting for pod pod-e1b63175-f4d8-420e-9a20-440023532451 to disappear
Mar  5 19:16:22.251: INFO: Pod pod-e1b63175-f4d8-420e-9a20-440023532451 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:16:22.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8518" for this suite.
Mar  5 19:16:28.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:16:28.439: INFO: namespace emptydir-8518 deletion completed in 6.180489171s

• [SLOW TEST:8.394 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:16:28.440: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0305 19:16:38.670311      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 19:16:38.670: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:16:38.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2546" for this suite.
Mar  5 19:16:46.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:16:46.867: INFO: namespace gc-2546 deletion completed in 8.189843151s

• [SLOW TEST:18.427 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:16:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  5 19:16:51.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 19:16:51.105: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 19:16:53.105: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 19:16:53.111: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 19:16:55.105: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 19:16:55.111: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:16:55.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5002" for this suite.
Mar  5 19:17:17.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:17:17.301: INFO: namespace container-lifecycle-hook-5002 deletion completed in 22.182789618s

• [SLOW TEST:30.433 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:17:17.301: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-17ae8d4f-7b86-4853-bf85-a6af3f1df7a6
STEP: Creating a pod to test consume configMaps
Mar  5 19:17:17.467: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8" in namespace "projected-5868" to be "success or failure"
Mar  5 19:17:17.472: INFO: Pod "pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773378ms
Mar  5 19:17:19.477: INFO: Pod "pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009633993s
STEP: Saw pod success
Mar  5 19:17:19.477: INFO: Pod "pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8" satisfied condition "success or failure"
Mar  5 19:17:19.481: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:17:19.509: INFO: Waiting for pod pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8 to disappear
Mar  5 19:17:19.515: INFO: Pod pod-projected-configmaps-750ab403-00eb-4949-8b9a-af3619971fe8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:17:19.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5868" for this suite.
Mar  5 19:17:25.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:17:25.707: INFO: namespace projected-5868 deletion completed in 6.184355813s

• [SLOW TEST:8.406 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:17:25.707: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Mar  5 19:17:25.870: INFO: Waiting up to 5m0s for pod "var-expansion-f95831d6-2672-4c75-bde8-e89a70196681" in namespace "var-expansion-8069" to be "success or failure"
Mar  5 19:17:25.876: INFO: Pod "var-expansion-f95831d6-2672-4c75-bde8-e89a70196681": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88496ms
Mar  5 19:17:27.881: INFO: Pod "var-expansion-f95831d6-2672-4c75-bde8-e89a70196681": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011323654s
STEP: Saw pod success
Mar  5 19:17:27.881: INFO: Pod "var-expansion-f95831d6-2672-4c75-bde8-e89a70196681" satisfied condition "success or failure"
Mar  5 19:17:27.886: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod var-expansion-f95831d6-2672-4c75-bde8-e89a70196681 container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:17:27.916: INFO: Waiting for pod var-expansion-f95831d6-2672-4c75-bde8-e89a70196681 to disappear
Mar  5 19:17:27.921: INFO: Pod var-expansion-f95831d6-2672-4c75-bde8-e89a70196681 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:17:27.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8069" for this suite.
Mar  5 19:17:33.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:17:34.120: INFO: namespace var-expansion-8069 deletion completed in 6.189338056s

• [SLOW TEST:8.413 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:17:34.121: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:17:34.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12" in namespace "downward-api-5479" to be "success or failure"
Mar  5 19:17:34.286: INFO: Pod "downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708629ms
Mar  5 19:17:36.291: INFO: Pod "downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011258284s
STEP: Saw pod success
Mar  5 19:17:36.291: INFO: Pod "downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12" satisfied condition "success or failure"
Mar  5 19:17:36.296: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12 container client-container: <nil>
STEP: delete the pod
Mar  5 19:17:36.324: INFO: Waiting for pod downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12 to disappear
Mar  5 19:17:36.329: INFO: Pod downwardapi-volume-1c4a3281-75fd-4b72-8025-89eaec7a6c12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:17:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5479" for this suite.
Mar  5 19:17:42.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:17:42.515: INFO: namespace downward-api-5479 deletion completed in 6.179278646s

• [SLOW TEST:8.395 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:17:42.516: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Mar  5 19:17:44.696: INFO: Pod pod-hostip-3b554ebc-9924-4267-89d6-19806cf066f1 has hostIP: 192.168.155.60
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:17:44.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3543" for this suite.
Mar  5 19:18:06.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:18:06.898: INFO: namespace pods-3543 deletion completed in 22.193462362s

• [SLOW TEST:24.382 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:18:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  5 19:18:07.056: INFO: Waiting up to 5m0s for pod "downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c" in namespace "downward-api-1292" to be "success or failure"
Mar  5 19:18:07.061: INFO: Pod "downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650558ms
Mar  5 19:18:09.067: INFO: Pod "downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011205152s
STEP: Saw pod success
Mar  5 19:18:09.067: INFO: Pod "downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c" satisfied condition "success or failure"
Mar  5 19:18:09.072: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:18:09.101: INFO: Waiting for pod downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c to disappear
Mar  5 19:18:09.106: INFO: Pod downward-api-b8f9aa5f-d928-4ac1-a321-d5796fee444c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:18:09.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1292" for this suite.
Mar  5 19:18:15.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:18:15.304: INFO: namespace downward-api-1292 deletion completed in 6.190048241s

• [SLOW TEST:8.406 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:18:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:18:15.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70" in namespace "projected-665" to be "success or failure"
Mar  5 19:18:15.475: INFO: Pod "downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599409ms
Mar  5 19:18:17.480: INFO: Pod "downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010211188s
Mar  5 19:18:19.486: INFO: Pod "downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015809514s
STEP: Saw pod success
Mar  5 19:18:19.486: INFO: Pod "downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70" satisfied condition "success or failure"
Mar  5 19:18:19.491: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70 container client-container: <nil>
STEP: delete the pod
Mar  5 19:18:19.518: INFO: Waiting for pod downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70 to disappear
Mar  5 19:18:19.523: INFO: Pod downwardapi-volume-314dce30-3efd-448c-80af-cae5551f7b70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:18:19.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-665" for this suite.
Mar  5 19:18:25.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:18:25.715: INFO: namespace projected-665 deletion completed in 6.18475345s

• [SLOW TEST:10.411 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:18:25.716: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-85511c6c-1e6c-483b-8c57-6a731f15d125
STEP: Creating a pod to test consume configMaps
Mar  5 19:18:25.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d" in namespace "configmap-9647" to be "success or failure"
Mar  5 19:18:25.892: INFO: Pod "pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586365ms
Mar  5 19:18:27.898: INFO: Pod "pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010155533s
STEP: Saw pod success
Mar  5 19:18:27.898: INFO: Pod "pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d" satisfied condition "success or failure"
Mar  5 19:18:27.902: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:18:27.930: INFO: Waiting for pod pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d to disappear
Mar  5 19:18:27.936: INFO: Pod pod-configmaps-ab915d9e-2810-4d54-9220-152e4317e40d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:18:27.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9647" for this suite.
Mar  5 19:18:33.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:18:34.120: INFO: namespace configmap-9647 deletion completed in 6.1765139s

• [SLOW TEST:8.405 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:18:34.120: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e in namespace container-probe-2664
Mar  5 19:18:36.289: INFO: Started pod liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e in namespace container-probe-2664
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 19:18:36.293: INFO: Initial restart count of pod liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is 0
Mar  5 19:18:48.331: INFO: Restart count of pod container-probe-2664/liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is now 1 (12.037767972s elapsed)
Mar  5 19:19:08.387: INFO: Restart count of pod container-probe-2664/liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is now 2 (32.094071914s elapsed)
Mar  5 19:19:28.444: INFO: Restart count of pod container-probe-2664/liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is now 3 (52.150991701s elapsed)
Mar  5 19:19:48.502: INFO: Restart count of pod container-probe-2664/liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is now 4 (1m12.208821351s elapsed)
Mar  5 19:20:58.695: INFO: Restart count of pod container-probe-2664/liveness-bd40ec97-1444-4cd9-b2c6-5170f354697e is now 5 (2m22.401532049s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:20:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2664" for this suite.
Mar  5 19:21:04.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:21:04.899: INFO: namespace container-probe-2664 deletion completed in 6.18136965s

• [SLOW TEST:150.779 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:21:04.900: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-848
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-5fd6db6c-3344-4878-b33c-a22e5ece3342
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:21:09.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-848" for this suite.
Mar  5 19:21:31.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:21:31.298: INFO: namespace configmap-848 deletion completed in 22.179229433s

• [SLOW TEST:26.398 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:21:31.298: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:21:31.445: INFO: Creating deployment "nginx-deployment"
Mar  5 19:21:31.451: INFO: Waiting for observed generation 1
Mar  5 19:21:33.461: INFO: Waiting for all required pods to come up
Mar  5 19:21:33.466: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  5 19:21:35.478: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  5 19:21:35.488: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  5 19:21:35.498: INFO: Updating deployment nginx-deployment
Mar  5 19:21:35.498: INFO: Waiting for observed generation 2
Mar  5 19:21:37.508: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  5 19:21:37.513: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  5 19:21:37.517: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  5 19:21:37.532: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  5 19:21:37.532: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  5 19:21:37.537: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  5 19:21:37.546: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  5 19:21:37.546: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  5 19:21:37.556: INFO: Updating deployment nginx-deployment
Mar  5 19:21:37.556: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  5 19:21:37.568: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  5 19:21:37.578: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  5 19:21:37.595: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1070,SelfLink:/apis/apps/v1/namespaces/deployment-1070/deployments/nginx-deployment,UID:e5af02d8-c370-4f42-a08a-4b9d094b72a9,ResourceVersion:646529,Generation:3,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2020-03-05 19:21:35 +0000 UTC 2020-03-05 19:21:31 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2020-03-05 19:21:37 +0000 UTC 2020-03-05 19:21:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  5 19:21:37.603: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-1070,SelfLink:/apis/apps/v1/namespaces/deployment-1070/replicasets/nginx-deployment-55fb7cb77f,UID:8f2e340a-f4ad-4145-8746-03d3ccb0dfa7,ResourceVersion:646523,Generation:3,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment e5af02d8-c370-4f42-a08a-4b9d094b72a9 0xc00269c437 0xc00269c438}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:21:37.603: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  5 19:21:37.603: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-1070,SelfLink:/apis/apps/v1/namespaces/deployment-1070/replicasets/nginx-deployment-7b8c6f4498,UID:1b9e0e4e-b742-4aa1-aef3-11362dc33a2e,ResourceVersion:646522,Generation:3,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment e5af02d8-c370-4f42-a08a-4b9d094b72a9 0xc00269c507 0xc00269c508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  5 19:21:37.619: INFO: Pod "nginx-deployment-55fb7cb77f-7ljsf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7ljsf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-7ljsf,UID:d6a5ab2e-05a9-4187-803e-48502fe82f63,ResourceVersion:646518,Generation:0,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269ce87 0xc00269ce88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-239-209.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269cef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269cf10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.239.209,PodIP:192.168.237.10,StartTime:2020-03-05 19:21:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.619: INFO: Pod "nginx-deployment-55fb7cb77f-7lnxl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7lnxl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-7lnxl,UID:4afb69f4-3b0d-4df3-91f3-4f7c6a292276,ResourceVersion:646507,Generation:0,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d000 0xc00269d001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:192.168.73.189,StartTime:2020-03-05 19:21:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.619: INFO: Pod "nginx-deployment-55fb7cb77f-8mcls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8mcls,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-8mcls,UID:0eb44117-95ce-4dce-8154-6c0b0ac83e14,ResourceVersion:646547,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d180 0xc00269d181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.620: INFO: Pod "nginx-deployment-55fb7cb77f-b877n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-b877n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-b877n,UID:141befb1-f250-47c4-b91e-99fbef6a974e,ResourceVersion:646558,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d280 0xc00269d281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d2f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.620: INFO: Pod "nginx-deployment-55fb7cb77f-hfhck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hfhck,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-hfhck,UID:665908f0-f88e-426b-8ac1-998572654ff0,ResourceVersion:646510,Generation:0,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d390 0xc00269d391}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-112-247.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.112.247,PodIP:192.168.118.207,StartTime:2020-03-05 19:21:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.620: INFO: Pod "nginx-deployment-55fb7cb77f-hpsxt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hpsxt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-hpsxt,UID:1c4c0062-334b-4a4e-b3fb-bb4cf38044cb,ResourceVersion:646516,Generation:0,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d510 0xc00269d511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-155-60.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.155.60,PodIP:,StartTime:2020-03-05 19:21:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.620: INFO: Pod "nginx-deployment-55fb7cb77f-kqhnr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kqhnr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-kqhnr,UID:51a1f31a-bbef-41c3-9346-c54e8ed38476,ResourceVersion:646549,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d690 0xc00269d691}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.621: INFO: Pod "nginx-deployment-55fb7cb77f-nrbjn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nrbjn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-nrbjn,UID:90146a8b-57e4-4881-a6d7-597ac47c4325,ResourceVersion:646540,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d790 0xc00269d791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.621: INFO: Pod "nginx-deployment-55fb7cb77f-nswl8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nswl8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-nswl8,UID:80c814d7-ec14-42a8-80eb-3078e2f1d793,ResourceVersion:646560,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269d8a0 0xc00269d8a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269d910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269d930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.148.164,PodIP:,StartTime:2020-03-05 19:21:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.621: INFO: Pod "nginx-deployment-55fb7cb77f-pc8c4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pc8c4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-pc8c4,UID:a2424bec-834b-479e-9487-8ed6c1ddb5a3,ResourceVersion:646543,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269da00 0xc00269da01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-239-209.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269da70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269da90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.621: INFO: Pod "nginx-deployment-55fb7cb77f-pg2q5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pg2q5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-pg2q5,UID:bfae7224-1873-4bfc-921b-b5cce7fc7471,ResourceVersion:646546,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269db10 0xc00269db11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269db80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269dba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.621: INFO: Pod "nginx-deployment-55fb7cb77f-t84h6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t84h6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-55fb7cb77f-t84h6,UID:6645754a-b2f1-484a-9761-7731acb529da,ResourceVersion:646504,Generation:0,CreationTimestamp:2020-03-05 19:21:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8f2e340a-f4ad-4145-8746-03d3ccb0dfa7 0xc00269dc10 0xc00269dc11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269dc80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269dca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.148.164,PodIP:192.168.155.82,StartTime:2020-03-05 19:21:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.622: INFO: Pod "nginx-deployment-7b8c6f4498-4d82g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4d82g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-4d82g,UID:21885f26-f24f-4100-9de8-61b9e8bf2047,ResourceVersion:646554,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc00269dd90 0xc00269dd91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269ddf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269de10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.622: INFO: Pod "nginx-deployment-7b8c6f4498-866nv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-866nv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-866nv,UID:1976000d-add6-4303-a1ca-2bb6840d5192,ResourceVersion:646561,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc00269de90 0xc00269de91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269def0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00269df10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.622: INFO: Pod "nginx-deployment-7b8c6f4498-8fsfj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8fsfj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-8fsfj,UID:0609108a-14aa-46bf-8d2a-55068d3363c7,ResourceVersion:646418,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc00269df80 0xc00269df81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00269dfe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:192.168.82.155,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://81747f0f2147a36872b9fac7cbc6d1d4b202aea61a86876931c9a4834dcf6c61}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.622: INFO: Pod "nginx-deployment-7b8c6f4498-dnh66" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dnh66,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-dnh66,UID:c0a3cd21-2290-412f-bf8a-fcf94be591d7,ResourceVersion:646553,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc0028620d7 0xc0028620d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-155-60.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.623: INFO: Pod "nginx-deployment-7b8c6f4498-fzs29" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fzs29,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-fzs29,UID:083808bb-87de-4c03-8f10-faa77edd225e,ResourceVersion:646429,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc0028621e0 0xc0028621e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.218.169,PodIP:192.168.227.54,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f34c69c35f607914dd0d901c215e75e9e35d6f4919c7e6cccc39effb50d9683b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.623: INFO: Pod "nginx-deployment-7b8c6f4498-hz5gn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hz5gn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-hz5gn,UID:1cc0fa4a-b710-415c-b91c-5bafadca03ce,ResourceVersion:646551,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862337 0xc002862338}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-239-209.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028623a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028623c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.623: INFO: Pod "nginx-deployment-7b8c6f4498-mv4f4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mv4f4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-mv4f4,UID:0d18276f-9dc8-483c-bdf0-2621694fdf57,ResourceVersion:646422,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862440 0xc002862441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028624a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028624c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:192.168.80.53,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b2f7a49acb7acc5eb49bebc9c007bb3b58fa720e023fb9405e481390624ee973}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.623: INFO: Pod "nginx-deployment-7b8c6f4498-nfjkl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nfjkl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-nfjkl,UID:48e6eef0-376f-43a6-aa1d-62fcd9b85992,ResourceVersion:646404,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862597 0xc002862598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-155-60.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.155.60,PodIP:192.168.153.245,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7c5a231373f227654336c55376832b99c1e3f4af25b10f146c9818328987ddbd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.624: INFO: Pod "nginx-deployment-7b8c6f4498-nv4fb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nv4fb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-nv4fb,UID:cac66b49-e862-4200-94b7-dd5eec94a816,ResourceVersion:646557,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc0028626f7 0xc0028626f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-112-247.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.624: INFO: Pod "nginx-deployment-7b8c6f4498-sgb4c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sgb4c,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-sgb4c,UID:90bdc6e8-a42d-47e5-8743-9da772bde90f,ResourceVersion:646555,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862800 0xc002862801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-239-209.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.239.209,PodIP:,StartTime:2020-03-05 19:21:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.624: INFO: Pod "nginx-deployment-7b8c6f4498-tmcbv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tmcbv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-tmcbv,UID:58848ffe-c118-4abc-bdd2-137b0e7056ee,ResourceVersion:646410,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862947 0xc002862948}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028629b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028629d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.148.164,PodIP:192.168.181.88,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ac509ea01f99b3f44e3ef98fa9d3c6439a9bfbaaf7cbc8a268429a5f21849a7b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.624: INFO: Pod "nginx-deployment-7b8c6f4498-tmr28" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tmr28,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-tmr28,UID:6136700e-683f-4c4b-8627-f89fbb574af5,ResourceVersion:646559,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862ab7 0xc002862ab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-155-60.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.155.60,PodIP:,StartTime:2020-03-05 19:21:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.625: INFO: Pod "nginx-deployment-7b8c6f4498-w7dbv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w7dbv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-w7dbv,UID:81a907f7-af6a-4e98-b14a-ba525b479423,ResourceVersion:646415,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862c07 0xc002862c08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-112-247.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.112.247,PodIP:192.168.115.120,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cbafac93c99869ba995b1df4a21807f8577a25576b499d0d1a3519c4fbaf2204}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.625: INFO: Pod "nginx-deployment-7b8c6f4498-wxvlg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wxvlg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-wxvlg,UID:3513e425-c5a3-4353-a8b1-62ce6d8a5d2e,ResourceVersion:646426,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862d67 0xc002862d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.218.169,PodIP:192.168.204.112,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:33 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f728f963eac96113353cc2cc36674e30a9a22650d4098fb21b6b525ec422283e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.625: INFO: Pod "nginx-deployment-7b8c6f4498-xr57p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xr57p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-xr57p,UID:93a3c9b1-fbd2-4f3c-9e9b-c3ada93fc1f9,ResourceVersion:646541,Generation:0,CreationTimestamp:2020-03-05 19:21:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862ec7 0xc002862ec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-112-247.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002862f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002862f50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:37 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  5 19:21:37.625: INFO: Pod "nginx-deployment-7b8c6f4498-zk6jr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zk6jr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1070,SelfLink:/api/v1/namespaces/deployment-1070/pods/nginx-deployment-7b8c6f4498-zk6jr,UID:2fca18bb-9b50-4175-915c-c254767a3fb9,ResourceVersion:646413,Generation:0,CreationTimestamp:2020-03-05 19:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1b9e0e4e-b742-4aa1-aef3-11362dc33a2e 0xc002862fd0 0xc002862fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ngh5k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ngh5k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ngh5k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002863030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002863050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.148.164,PodIP:192.168.145.22,StartTime:2020-03-05 19:21:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-05 19:21:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://57193a5d89ab043f7b7dd1bc312e37ab702438da8db93d0354882a41c017ded8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:21:37.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1070" for this suite.
Mar  5 19:21:45.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:21:45.830: INFO: namespace deployment-1070 deletion completed in 8.192571004s

• [SLOW TEST:14.532 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:21:45.830: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:22:12.004: INFO: Container started at 2020-03-05 19:21:48 +0000 UTC, pod became ready at 2020-03-05 19:22:10 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:22:12.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6368" for this suite.
Mar  5 19:22:34.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:22:34.200: INFO: namespace container-probe-6368 deletion completed in 22.187825557s

• [SLOW TEST:48.370 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:22:34.200: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:22:34.347: INFO: Creating ReplicaSet my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b
Mar  5 19:22:34.359: INFO: Pod name my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b: Found 0 pods out of 1
Mar  5 19:22:39.364: INFO: Pod name my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b: Found 1 pods out of 1
Mar  5 19:22:39.364: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b" is running
Mar  5 19:22:39.369: INFO: Pod "my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b-2k4fm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 19:22:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 19:22:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 19:22:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 19:22:34 +0000 UTC Reason: Message:}])
Mar  5 19:22:39.369: INFO: Trying to dial the pod
Mar  5 19:22:44.386: INFO: Controller my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b: Got expected result from replica 1 [my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b-2k4fm]: "my-hostname-basic-6909f328-4569-4153-bc3c-19c42fd9eb9b-2k4fm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:22:44.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8313" for this suite.
Mar  5 19:22:50.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:22:50.578: INFO: namespace replicaset-8313 deletion completed in 6.184722934s

• [SLOW TEST:16.378 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:22:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9090
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-9090
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9090
Mar  5 19:22:50.745: INFO: Found 0 stateful pods, waiting for 1
Mar  5 19:23:00.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  5 19:23:00.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:23:01.082: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:23:01.082: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:23:01.082: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 19:23:01.087: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  5 19:23:11.093: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 19:23:11.093: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:23:11.114: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar  5 19:23:11.114: INFO: ss-0  ip-192-168-106-110.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  }]
Mar  5 19:23:11.115: INFO: 
Mar  5 19:23:11.115: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  5 19:23:12.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994567669s
Mar  5 19:23:13.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988003641s
Mar  5 19:23:14.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982209105s
Mar  5 19:23:15.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976554897s
Mar  5 19:23:16.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970658501s
Mar  5 19:23:17.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964925135s
Mar  5 19:23:18.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957462252s
Mar  5 19:23:19.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9514751s
Mar  5 19:23:20.169: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.780618ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9090
Mar  5 19:23:21.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:23:21.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 19:23:21.386: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 19:23:21.386: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 19:23:21.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:23:21.588: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  5 19:23:21.588: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 19:23:21.588: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 19:23:21.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:23:21.822: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  5 19:23:21.823: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 19:23:21.823: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 19:23:21.829: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:23:21.829: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:23:21.829: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  5 19:23:21.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:23:22.045: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:23:22.045: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:23:22.045: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 19:23:22.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:23:22.248: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:23:22.248: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:23:22.248: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 19:23:22.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 19:23:22.458: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 19:23:22.458: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 19:23:22.458: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 19:23:22.458: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:23:22.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  5 19:23:32.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 19:23:32.475: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 19:23:32.475: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 19:23:32.491: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar  5 19:23:32.491: INFO: ss-0  ip-192-168-106-110.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  }]
Mar  5 19:23:32.491: INFO: ss-1  ip-192-168-155-60.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:32.492: INFO: ss-2  ip-192-168-239-209.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:32.492: INFO: 
Mar  5 19:23:32.492: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 19:23:33.497: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar  5 19:23:33.497: INFO: ss-0  ip-192-168-106-110.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  }]
Mar  5 19:23:33.497: INFO: ss-1  ip-192-168-155-60.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:33.497: INFO: ss-2  ip-192-168-239-209.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:33.497: INFO: 
Mar  5 19:23:33.497: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 19:23:34.503: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar  5 19:23:34.503: INFO: ss-0  ip-192-168-106-110.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:22:50 +0000 UTC  }]
Mar  5 19:23:34.503: INFO: ss-1  ip-192-168-155-60.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:34.503: INFO: 
Mar  5 19:23:34.503: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  5 19:23:35.509: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:35.509: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:35.509: INFO: 
Mar  5 19:23:35.509: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:36.515: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:36.515: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:36.515: INFO: 
Mar  5 19:23:36.515: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:37.521: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:37.521: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:37.521: INFO: 
Mar  5 19:23:37.521: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:38.526: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:38.526: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:38.526: INFO: 
Mar  5 19:23:38.526: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:39.532: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:39.532: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:39.532: INFO: 
Mar  5 19:23:39.532: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:40.538: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:40.538: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:40.538: INFO: 
Mar  5 19:23:40.538: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  5 19:23:41.544: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Mar  5 19:23:41.544: INFO: ss-1  ip-192-168-155-60.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:23:11 +0000 UTC  }]
Mar  5 19:23:41.544: INFO: 
Mar  5 19:23:41.544: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9090
Mar  5 19:23:42.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:23:42.661: INFO: rc: 1
Mar  5 19:23:42.661: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000990ea0 exit status 1 <nil> <nil> true [0xc0017a0600 0xc0017a0618 0xc0017a0630] [0xc0017a0600 0xc0017a0618 0xc0017a0630] [0xc0017a0610 0xc0017a0628] [0xba70a0 0xba70a0] 0xc0027a13e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Mar  5 19:23:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:23:52.743: INFO: rc: 1
Mar  5 19:23:52.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000c2ebd0 exit status 1 <nil> <nil> true [0xc00352e398 0xc00352e3b0 0xc00352e3c8] [0xc00352e398 0xc00352e3b0 0xc00352e3c8] [0xc00352e3a8 0xc00352e3c0] [0xba70a0 0xba70a0] 0xc0026d74a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:02.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:02.822: INFO: rc: 1
Mar  5 19:24:02.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000c2ef30 exit status 1 <nil> <nil> true [0xc00352e3d0 0xc00352e3f0 0xc00352e408] [0xc00352e3d0 0xc00352e3f0 0xc00352e408] [0xc00352e3e0 0xc00352e400] [0xba70a0 0xba70a0] 0xc0026d7920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:12.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:12.900: INFO: rc: 1
Mar  5 19:24:12.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1e3f0 exit status 1 <nil> <nil> true [0xc0014b62c8 0xc0014b6448 0xc0014b6770] [0xc0014b62c8 0xc0014b6448 0xc0014b6770] [0xc0014b63f8 0xc0014b6608] [0xba70a0 0xba70a0] 0xc0028482a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:22.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:22.981: INFO: rc: 1
Mar  5 19:24:22.982: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1e7e0 exit status 1 <nil> <nil> true [0xc0014b6a30 0xc0014b6b58 0xc0014b6f38] [0xc0014b6a30 0xc0014b6b58 0xc0014b6f38] [0xc0014b6ab8 0xc0014b6e30] [0xba70a0 0xba70a0] 0xc002848600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:32.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:33.066: INFO: rc: 1
Mar  5 19:24:33.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc390 exit status 1 <nil> <nil> true [0xc0000c4320 0xc0000c5150 0xc0000c5550] [0xc0000c4320 0xc0000c5150 0xc0000c5550] [0xc0000c4948 0xc0000c53b0] [0xba70a0 0xba70a0] 0xc001e962a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:43.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:43.148: INFO: rc: 1
Mar  5 19:24:43.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc7b0 exit status 1 <nil> <nil> true [0xc0010aa098 0xc0010aa1d8 0xc0010aa420] [0xc0010aa098 0xc0010aa1d8 0xc0010aa420] [0xc0010aa168 0xc0010aa350] [0xba70a0 0xba70a0] 0xc001e96600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:24:53.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:24:53.225: INFO: rc: 1
Mar  5 19:24:53.225: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011ccb40 exit status 1 <nil> <nil> true [0xc0010aa4b8 0xc0010aa670 0xc0010aa810] [0xc0010aa4b8 0xc0010aa670 0xc0010aa810] [0xc0010aa600 0xc0010aa7e0] [0xba70a0 0xba70a0] 0xc001e96960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:03.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:03.306: INFO: rc: 1
Mar  5 19:25:03.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011ccf00 exit status 1 <nil> <nil> true [0xc0010aa820 0xc0010aa940 0xc0010aa9b0] [0xc0010aa820 0xc0010aa940 0xc0010aa9b0] [0xc0010aa8f8 0xc0010aa990] [0xba70a0 0xba70a0] 0xc001e96de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:13.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:13.391: INFO: rc: 1
Mar  5 19:25:13.391: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cd260 exit status 1 <nil> <nil> true [0xc0010aa9d0 0xc0010aaa18 0xc0010aab58] [0xc0010aa9d0 0xc0010aaa18 0xc0010aab58] [0xc0010aaa08 0xc0010aab10] [0xba70a0 0xba70a0] 0xc001e978c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:23.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:23.470: INFO: rc: 1
Mar  5 19:25:23.470: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cd5f0 exit status 1 <nil> <nil> true [0xc0010aabc8 0xc0010aaca8 0xc0010aad78] [0xc0010aabc8 0xc0010aaca8 0xc0010aad78] [0xc0010aac68 0xc0010aad28] [0xba70a0 0xba70a0] 0xc001f8c480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:33.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:33.550: INFO: rc: 1
Mar  5 19:25:33.550: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1eb40 exit status 1 <nil> <nil> true [0xc0014b6fa0 0xc0014b7188 0xc0014b73e8] [0xc0014b6fa0 0xc0014b7188 0xc0014b73e8] [0xc0014b7158 0xc0014b73e0] [0xba70a0 0xba70a0] 0xc002848960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:43.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:43.644: INFO: rc: 1
Mar  5 19:25:43.644: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1ef00 exit status 1 <nil> <nil> true [0xc0014b7470 0xc0014b75e0 0xc0014b7650] [0xc0014b7470 0xc0014b75e0 0xc0014b7650] [0xc0014b7548 0xc0014b7628] [0xba70a0 0xba70a0] 0xc002848d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:25:53.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:25:53.730: INFO: rc: 1
Mar  5 19:25:53.731: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cd9b0 exit status 1 <nil> <nil> true [0xc0010aad98 0xc0010aade0 0xc0010aaf20] [0xc0010aad98 0xc0010aade0 0xc0010aaf20] [0xc0010aadd0 0xc0010aaef8] [0xba70a0 0xba70a0] 0xc001f8cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:03.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:03.813: INFO: rc: 1
Mar  5 19:26:03.813: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cdd70 exit status 1 <nil> <nil> true [0xc0010aaf48 0xc0010aafe8 0xc0010ab190] [0xc0010aaf48 0xc0010aafe8 0xc0010ab190] [0xc0010aafd8 0xc0010ab0c0] [0xba70a0 0xba70a0] 0xc001f8d320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:13.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:13.894: INFO: rc: 1
Mar  5 19:26:13.894: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc360 exit status 1 <nil> <nil> true [0xc0000c4720 0xc0000c51e8 0xc0010aa098] [0xc0000c4720 0xc0000c51e8 0xc0010aa098] [0xc0000c5150 0xc0000c5550] [0xba70a0 0xba70a0] 0xc001e962a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:23.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:23.975: INFO: rc: 1
Mar  5 19:26:23.975: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc7e0 exit status 1 <nil> <nil> true [0xc0010aa148 0xc0010aa298 0xc0010aa4b8] [0xc0010aa148 0xc0010aa298 0xc0010aa4b8] [0xc0010aa1d8 0xc0010aa420] [0xba70a0 0xba70a0] 0xc001e96600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:33.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:34.055: INFO: rc: 1
Mar  5 19:26:34.055: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011ccba0 exit status 1 <nil> <nil> true [0xc0010aa538 0xc0010aa7b8 0xc0010aa820] [0xc0010aa538 0xc0010aa7b8 0xc0010aa820] [0xc0010aa670 0xc0010aa810] [0xba70a0 0xba70a0] 0xc001e96960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:44.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:44.139: INFO: rc: 1
Mar  5 19:26:44.139: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011ccf90 exit status 1 <nil> <nil> true [0xc0010aa848 0xc0010aa970 0xc0010aa9d0] [0xc0010aa848 0xc0010aa970 0xc0010aa9d0] [0xc0010aa940 0xc0010aa9b0] [0xba70a0 0xba70a0] 0xc001e96de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:26:54.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:26:54.224: INFO: rc: 1
Mar  5 19:26:54.224: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1e420 exit status 1 <nil> <nil> true [0xc0014b60f8 0xc0014b63f8 0xc0014b6608] [0xc0014b60f8 0xc0014b63f8 0xc0014b6608] [0xc0014b6300 0xc0014b6558] [0xba70a0 0xba70a0] 0xc001f8c8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:04.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:04.301: INFO: rc: 1
Mar  5 19:27:04.302: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000f1e840 exit status 1 <nil> <nil> true [0xc0014b6770 0xc0014b6ab8 0xc0014b6e30] [0xc0014b6770 0xc0014b6ab8 0xc0014b6e30] [0xc0014b6aa8 0xc0014b6cc0] [0xba70a0 0xba70a0] 0xc001f8cfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:14.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:14.380: INFO: rc: 1
Mar  5 19:27:14.380: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cd320 exit status 1 <nil> <nil> true [0xc0010aa9f8 0xc0010aaa90 0xc0010aabc8] [0xc0010aa9f8 0xc0010aaa90 0xc0010aabc8] [0xc0010aaa18 0xc0010aab58] [0xba70a0 0xba70a0] 0xc001e978c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:24.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:24.458: INFO: rc: 1
Mar  5 19:27:24.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cd710 exit status 1 <nil> <nil> true [0xc0010aac40 0xc0010aace8 0xc0010aad98] [0xc0010aac40 0xc0010aace8 0xc0010aad98] [0xc0010aaca8 0xc0010aad78] [0xba70a0 0xba70a0] 0xc0028480c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:34.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:34.539: INFO: rc: 1
Mar  5 19:27:34.539: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cdb00 exit status 1 <nil> <nil> true [0xc0010aadb8 0xc0010aae50 0xc0010aaf48] [0xc0010aadb8 0xc0010aae50 0xc0010aaf48] [0xc0010aade0 0xc0010aaf20] [0xba70a0 0xba70a0] 0xc002848420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:44.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:44.621: INFO: rc: 1
Mar  5 19:27:44.621: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cdef0 exit status 1 <nil> <nil> true [0xc0010aaf60 0xc0010ab008 0xc0010ab1f8] [0xc0010aaf60 0xc0010ab008 0xc0010ab1f8] [0xc0010aafe8 0xc0010ab190] [0xba70a0 0xba70a0] 0xc002848780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:27:54.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:27:54.700: INFO: rc: 1
Mar  5 19:27:54.700: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a6e2a0 exit status 1 <nil> <nil> true [0xc0010ab240 0xc0010ab290 0xc0010ab328] [0xc0010ab240 0xc0010ab290 0xc0010ab328] [0xc0010ab278 0xc0010ab310] [0xba70a0 0xba70a0] 0xc002848ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:28:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:28:04.781: INFO: rc: 1
Mar  5 19:28:04.782: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a6e630 exit status 1 <nil> <nil> true [0xc0010ab338 0xc0010ab488 0xc0010ab610] [0xc0010ab338 0xc0010ab488 0xc0010ab610] [0xc0010ab398 0xc0010ab5b0] [0xba70a0 0xba70a0] 0xc002848ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:28:14.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:28:14.877: INFO: rc: 1
Mar  5 19:28:14.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a6e960 exit status 1 <nil> <nil> true [0xc0010ab6f8 0xc0010ab898 0xc0010ab9d8] [0xc0010ab6f8 0xc0010ab898 0xc0010ab9d8] [0xc0010ab7e8 0xc0010ab958] [0xba70a0 0xba70a0] 0xc002849380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:28:24.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:28:24.958: INFO: rc: 1
Mar  5 19:28:24.958: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc3c0 exit status 1 <nil> <nil> true [0xc0000c4320 0xc0000c5150 0xc0000c5550] [0xc0000c4320 0xc0000c5150 0xc0000c5550] [0xc0000c4948 0xc0000c53b0] [0xba70a0 0xba70a0] 0xc001e962a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:28:34.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:28:35.040: INFO: rc: 1
Mar  5 19:28:35.040: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0011cc810 exit status 1 <nil> <nil> true [0xc0010aa098 0xc0010aa1d8 0xc0010aa420] [0xc0010aa098 0xc0010aa1d8 0xc0010aa420] [0xc0010aa168 0xc0010aa350] [0xba70a0 0xba70a0] 0xc001e96600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Mar  5 19:28:45.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-9090 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 19:28:45.120: INFO: rc: 1
Mar  5 19:28:45.120: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Mar  5 19:28:45.120: INFO: Scaling statefulset ss to 0
Mar  5 19:28:45.135: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  5 19:28:45.140: INFO: Deleting all statefulset in ns statefulset-9090
Mar  5 19:28:45.145: INFO: Scaling statefulset ss to 0
Mar  5 19:28:45.160: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:28:45.164: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:28:45.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9090" for this suite.
Mar  5 19:28:51.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:28:51.370: INFO: namespace statefulset-9090 deletion completed in 6.176986482s

• [SLOW TEST:360.792 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:28:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e51f9c3a-8fd5-4a6f-8e3f-f34aa5472f1f
STEP: Creating a pod to test consume configMaps
Mar  5 19:28:51.532: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d" in namespace "projected-6398" to be "success or failure"
Mar  5 19:28:51.537: INFO: Pod "pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.779281ms
Mar  5 19:28:53.543: INFO: Pod "pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010482172s
STEP: Saw pod success
Mar  5 19:28:53.543: INFO: Pod "pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d" satisfied condition "success or failure"
Mar  5 19:28:53.547: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:28:53.578: INFO: Waiting for pod pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d to disappear
Mar  5 19:28:53.582: INFO: Pod pod-projected-configmaps-eb7591f5-e424-4a9a-ba9b-fa7693b6187d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:28:53.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6398" for this suite.
Mar  5 19:28:59.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:28:59.767: INFO: namespace projected-6398 deletion completed in 6.177677619s

• [SLOW TEST:8.397 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:28:59.767: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  5 19:29:00.228: INFO: Pod name wrapped-volume-race-ba2c548d-1951-4d83-9208-6a9450587462: Found 0 pods out of 5
Mar  5 19:29:05.236: INFO: Pod name wrapped-volume-race-ba2c548d-1951-4d83-9208-6a9450587462: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ba2c548d-1951-4d83-9208-6a9450587462 in namespace emptydir-wrapper-2939, will wait for the garbage collector to delete the pods
Mar  5 19:29:15.340: INFO: Deleting ReplicationController wrapped-volume-race-ba2c548d-1951-4d83-9208-6a9450587462 took: 17.005615ms
Mar  5 19:29:15.640: INFO: Terminating ReplicationController wrapped-volume-race-ba2c548d-1951-4d83-9208-6a9450587462 pods took: 300.242928ms
STEP: Creating RC which spawns configmap-volume pods
Mar  5 19:29:52.666: INFO: Pod name wrapped-volume-race-be41785b-48f8-48ef-a827-1e9f41d2bb00: Found 0 pods out of 5
Mar  5 19:29:57.674: INFO: Pod name wrapped-volume-race-be41785b-48f8-48ef-a827-1e9f41d2bb00: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-be41785b-48f8-48ef-a827-1e9f41d2bb00 in namespace emptydir-wrapper-2939, will wait for the garbage collector to delete the pods
Mar  5 19:30:09.784: INFO: Deleting ReplicationController wrapped-volume-race-be41785b-48f8-48ef-a827-1e9f41d2bb00 took: 17.818383ms
Mar  5 19:30:10.084: INFO: Terminating ReplicationController wrapped-volume-race-be41785b-48f8-48ef-a827-1e9f41d2bb00 pods took: 300.234894ms
STEP: Creating RC which spawns configmap-volume pods
Mar  5 19:30:52.911: INFO: Pod name wrapped-volume-race-783cf7da-4fe5-45e9-9efd-418f988c6a37: Found 0 pods out of 5
Mar  5 19:30:57.918: INFO: Pod name wrapped-volume-race-783cf7da-4fe5-45e9-9efd-418f988c6a37: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-783cf7da-4fe5-45e9-9efd-418f988c6a37 in namespace emptydir-wrapper-2939, will wait for the garbage collector to delete the pods
Mar  5 19:31:08.022: INFO: Deleting ReplicationController wrapped-volume-race-783cf7da-4fe5-45e9-9efd-418f988c6a37 took: 17.030238ms
Mar  5 19:31:08.322: INFO: Terminating ReplicationController wrapped-volume-race-783cf7da-4fe5-45e9-9efd-418f988c6a37 pods took: 300.260861ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:31:45.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2939" for this suite.
Mar  5 19:31:53.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:31:53.897: INFO: namespace emptydir-wrapper-2939 deletion completed in 8.189292538s

• [SLOW TEST:174.130 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:31:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2565
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5128
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2575" for this suite.
Mar  5 19:32:06.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:06.564: INFO: namespace namespaces-2575 deletion completed in 6.177980055s
STEP: Destroying namespace "nsdeletetest-2565" for this suite.
Mar  5 19:32:06.568: INFO: Namespace nsdeletetest-2565 was already deleted
STEP: Destroying namespace "nsdeletetest-5128" for this suite.
Mar  5 19:32:12.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:12.748: INFO: namespace nsdeletetest-5128 deletion completed in 6.179758491s

• [SLOW TEST:18.849 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:32:12.748: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:32:12.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9" in namespace "projected-8464" to be "success or failure"
Mar  5 19:32:12.917: INFO: Pod "downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.902775ms
Mar  5 19:32:14.925: INFO: Pod "downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019040098s
STEP: Saw pod success
Mar  5 19:32:14.925: INFO: Pod "downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9" satisfied condition "success or failure"
Mar  5 19:32:14.933: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9 container client-container: <nil>
STEP: delete the pod
Mar  5 19:32:14.963: INFO: Waiting for pod downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9 to disappear
Mar  5 19:32:14.968: INFO: Pod downwardapi-volume-ffc67f9d-8c2f-4cbb-b7de-27b1eda0a6a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:14.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8464" for this suite.
Mar  5 19:32:20.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:21.150: INFO: namespace projected-8464 deletion completed in 6.174014824s

• [SLOW TEST:8.402 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:32:21.150: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  5 19:32:21.296: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 19:32:21.310: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 19:32:21.314: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-106-110.ec2.internal before test
Mar  5 19:32:21.323: INFO: aws-node-2lp4m from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.323: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.323: INFO: kube-proxy-7gg9d from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.323: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.323: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-bh47c from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.323: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.323: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:32:21.323: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-112-247.ec2.internal before test
Mar  5 19:32:21.333: INFO: kube-proxy-r7sml from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.333: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.333: INFO: aws-node-gxwkp from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.333: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.333: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-7m5vz from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.333: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.333: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:32:21.333: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-164.ec2.internal before test
Mar  5 19:32:21.342: INFO: kube-proxy-l5q7m from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.342: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.342: INFO: aws-node-4q99r from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.342: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.342: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-d9krw from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.342: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:32:21.342: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-155-60.ec2.internal before test
Mar  5 19:32:21.352: INFO: kube-proxy-m8h2w from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.352: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.352: INFO: aws-node-fwjd8 from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.352: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.352: INFO: sonobuoy from sonobuoy started at 2020-03-05 19:01:15 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.352: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 19:32:21.352: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-cv5hn from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.352: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:32:21.352: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-169.ec2.internal before test
Mar  5 19:32:21.371: INFO: aws-node-qgbl2 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.371: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.371: INFO: coredns-59dfd6b59f-xzswv from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.371: INFO: 	Container coredns ready: true, restart count 0
Mar  5 19:32:21.371: INFO: kube-proxy-fkm78 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.371: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.371: INFO: coredns-59dfd6b59f-kw58c from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.371: INFO: 	Container coredns ready: true, restart count 0
Mar  5 19:32:21.371: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-75b4d from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.371: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:32:21.371: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-239-209.ec2.internal before test
Mar  5 19:32:21.389: INFO: kube-proxy-ttknx from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.389: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:32:21.389: INFO: sonobuoy-e2e-job-3f29057810bc417f from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.389: INFO: 	Container e2e ready: true, restart count 0
Mar  5 19:32:21.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.389: INFO: aws-node-bxlx7 from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 19:32:21.389: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:32:21.389: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-nlqlx from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:32:21.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:32:21.389: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f97f53b04232b3], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:22.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-634" for this suite.
Mar  5 19:32:28.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:28.600: INFO: namespace sched-pred-634 deletion completed in 6.174300862s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.451 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:32:28.601: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-acea5f1f-565c-4953-ad01-4f5af966751f
STEP: Creating a pod to test consume configMaps
Mar  5 19:32:28.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458" in namespace "projected-4113" to be "success or failure"
Mar  5 19:32:28.769: INFO: Pod "pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893538ms
Mar  5 19:32:30.775: INFO: Pod "pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458": Phase="Running", Reason="", readiness=true. Elapsed: 2.011480072s
Mar  5 19:32:32.781: INFO: Pod "pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016978702s
STEP: Saw pod success
Mar  5 19:32:32.781: INFO: Pod "pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458" satisfied condition "success or failure"
Mar  5 19:32:32.785: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:32:32.816: INFO: Waiting for pod pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458 to disappear
Mar  5 19:32:32.821: INFO: Pod pod-projected-configmaps-c703c5c7-77de-46be-8595-4f58eb13d458 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:32.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4113" for this suite.
Mar  5 19:32:38.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:39.003: INFO: namespace projected-4113 deletion completed in 6.17465761s

• [SLOW TEST:10.402 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:32:39.004: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  5 19:32:39.162: INFO: Waiting up to 5m0s for pod "downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a" in namespace "downward-api-862" to be "success or failure"
Mar  5 19:32:39.168: INFO: Pod "downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.958037ms
Mar  5 19:32:41.174: INFO: Pod "downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011520504s
Mar  5 19:32:43.179: INFO: Pod "downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017142868s
STEP: Saw pod success
Mar  5 19:32:43.179: INFO: Pod "downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a" satisfied condition "success or failure"
Mar  5 19:32:43.184: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:32:43.213: INFO: Waiting for pod downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a to disappear
Mar  5 19:32:43.218: INFO: Pod downward-api-ae263264-9ebb-43c7-b874-26df1413dd5a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:43.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-862" for this suite.
Mar  5 19:32:49.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:32:49.402: INFO: namespace downward-api-862 deletion completed in 6.176839609s

• [SLOW TEST:10.398 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:32:49.403: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 19:32:49.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7950'
Mar  5 19:32:49.660: INFO: stderr: ""
Mar  5 19:32:49.661: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  5 19:32:54.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pod e2e-test-nginx-pod --namespace=kubectl-7950 -o json'
Mar  5 19:32:54.788: INFO: stderr: ""
Mar  5 19:32:54.788: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-03-05T19:32:49Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7950\",\n        \"resourceVersion\": \"649510\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7950/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b7493df7-6cfc-430a-8d80-ab48041e21b4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-567gt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-192-168-148-164.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-567gt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-567gt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-05T19:32:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-05T19:32:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-05T19:32:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-05T19:32:49Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://986402d0dd50923bafba45ea080bb5f2eeddb89c76329933f2f943fe3ca6e792\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-05T19:32:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.148.164\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.160.64\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-05T19:32:49Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  5 19:32:54.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 replace -f - --namespace=kubectl-7950'
Mar  5 19:32:55.001: INFO: stderr: ""
Mar  5 19:32:55.001: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Mar  5 19:32:55.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete pods e2e-test-nginx-pod --namespace=kubectl-7950'
Mar  5 19:32:59.681: INFO: stderr: ""
Mar  5 19:32:59.682: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:32:59.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7950" for this suite.
Mar  5 19:33:05.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:33:05.875: INFO: namespace kubectl-7950 deletion completed in 6.185628883s

• [SLOW TEST:16.472 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:33:05.875: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  5 19:33:10.055: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-b9d5835f-faac-4c36-80bc-8ec8eb361d0e,GenerateName:,Namespace:events-5204,SelfLink:/api/v1/namespaces/events-5204/pods/send-events-b9d5835f-faac-4c36-80bc-8ec8eb361d0e,UID:0ca19b5c-e86a-4e55-b62f-16a6960c138b,ResourceVersion:649580,Generation:0,CreationTimestamp:2020-03-05 19:33:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 24134455,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gk58z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gk58z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-gk58z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b85d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b85da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:33:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:33:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:33:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:33:06 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:192.168.67.107,StartTime:2020-03-05 19:33:06 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-03-05 19:33:08 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://37bd4358af5624d6152e910c598dea53909bc4cf7f405c45e896e883d5a077ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  5 19:33:12.060: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  5 19:33:14.066: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:33:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5204" for this suite.
Mar  5 19:33:54.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:33:54.269: INFO: namespace events-5204 deletion completed in 40.186568762s

• [SLOW TEST:48.394 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:33:54.269: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Mar  5 19:33:54.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-4467'
Mar  5 19:33:54.732: INFO: stderr: ""
Mar  5 19:33:54.732: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:33:54.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4467'
Mar  5 19:33:54.814: INFO: stderr: ""
Mar  5 19:33:54.814: INFO: stdout: "update-demo-nautilus-nq8bh update-demo-nautilus-t6zrs "
Mar  5 19:33:54.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-nq8bh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:33:54.896: INFO: stderr: ""
Mar  5 19:33:54.896: INFO: stdout: ""
Mar  5 19:33:54.896: INFO: update-demo-nautilus-nq8bh is created but not running
Mar  5 19:33:59.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4467'
Mar  5 19:33:59.985: INFO: stderr: ""
Mar  5 19:33:59.985: INFO: stdout: "update-demo-nautilus-nq8bh update-demo-nautilus-t6zrs "
Mar  5 19:33:59.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-nq8bh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:00.062: INFO: stderr: ""
Mar  5 19:34:00.062: INFO: stdout: "true"
Mar  5 19:34:00.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-nq8bh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:00.154: INFO: stderr: ""
Mar  5 19:34:00.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:34:00.154: INFO: validating pod update-demo-nautilus-nq8bh
Mar  5 19:34:00.160: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:34:00.160: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:34:00.161: INFO: update-demo-nautilus-nq8bh is verified up and running
Mar  5 19:34:00.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-t6zrs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:00.236: INFO: stderr: ""
Mar  5 19:34:00.236: INFO: stdout: "true"
Mar  5 19:34:00.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-t6zrs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:00.310: INFO: stderr: ""
Mar  5 19:34:00.310: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:34:00.310: INFO: validating pod update-demo-nautilus-t6zrs
Mar  5 19:34:00.317: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:34:00.317: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:34:00.317: INFO: update-demo-nautilus-t6zrs is verified up and running
STEP: rolling-update to new replication controller
Mar  5 19:34:00.319: INFO: scanned /root for discovery docs: <nil>
Mar  5 19:34:00.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4467'
Mar  5 19:34:22.763: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  5 19:34:22.763: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:34:22.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4467'
Mar  5 19:34:22.844: INFO: stderr: ""
Mar  5 19:34:22.844: INFO: stdout: "update-demo-kitten-gwx8w update-demo-kitten-w4k27 "
Mar  5 19:34:22.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-kitten-gwx8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:22.921: INFO: stderr: ""
Mar  5 19:34:22.921: INFO: stdout: "true"
Mar  5 19:34:22.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-kitten-gwx8w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:22.998: INFO: stderr: ""
Mar  5 19:34:22.998: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  5 19:34:22.998: INFO: validating pod update-demo-kitten-gwx8w
Mar  5 19:34:23.004: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  5 19:34:23.004: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  5 19:34:23.004: INFO: update-demo-kitten-gwx8w is verified up and running
Mar  5 19:34:23.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-kitten-w4k27 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:23.087: INFO: stderr: ""
Mar  5 19:34:23.087: INFO: stdout: "true"
Mar  5 19:34:23.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-kitten-w4k27 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4467'
Mar  5 19:34:23.161: INFO: stderr: ""
Mar  5 19:34:23.161: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  5 19:34:23.161: INFO: validating pod update-demo-kitten-w4k27
Mar  5 19:34:23.168: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  5 19:34:23.168: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  5 19:34:23.168: INFO: update-demo-kitten-w4k27 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:34:23.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4467" for this suite.
Mar  5 19:34:45.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:34:45.351: INFO: namespace kubectl-4467 deletion completed in 22.175416619s

• [SLOW TEST:51.082 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:34:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:34:45.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d" in namespace "downward-api-2558" to be "success or failure"
Mar  5 19:34:45.514: INFO: Pod "downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650196ms
Mar  5 19:34:47.520: INFO: Pod "downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01027086s
STEP: Saw pod success
Mar  5 19:34:47.520: INFO: Pod "downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d" satisfied condition "success or failure"
Mar  5 19:34:47.525: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d container client-container: <nil>
STEP: delete the pod
Mar  5 19:34:47.552: INFO: Waiting for pod downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d to disappear
Mar  5 19:34:47.557: INFO: Pod downwardapi-volume-7b56a48c-17e2-433d-821b-52969303537d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:34:47.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2558" for this suite.
Mar  5 19:34:53.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:34:53.750: INFO: namespace downward-api-2558 deletion completed in 6.186083767s

• [SLOW TEST:8.399 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:34:53.750: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-8e64c844-4b71-4777-84d2-1ba4a2dd71e0
STEP: Creating a pod to test consume secrets
Mar  5 19:34:53.913: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220" in namespace "projected-8648" to be "success or failure"
Mar  5 19:34:53.921: INFO: Pod "pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220": Phase="Pending", Reason="", readiness=false. Elapsed: 7.50146ms
Mar  5 19:34:55.926: INFO: Pod "pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220": Phase="Running", Reason="", readiness=true. Elapsed: 2.012972607s
Mar  5 19:34:57.931: INFO: Pod "pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018122026s
STEP: Saw pod success
Mar  5 19:34:57.931: INFO: Pod "pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220" satisfied condition "success or failure"
Mar  5 19:34:57.936: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:34:57.968: INFO: Waiting for pod pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220 to disappear
Mar  5 19:34:57.973: INFO: Pod pod-projected-secrets-ffcfe179-a9a2-4f9b-9964-7f3d7ffa0220 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:34:57.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8648" for this suite.
Mar  5 19:35:03.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:35:04.155: INFO: namespace projected-8648 deletion completed in 6.175253107s

• [SLOW TEST:10.405 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:35:04.156: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6031
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-549d3b4c-0963-4e61-89fd-2b75b9891230
STEP: Creating secret with name s-test-opt-upd-9291051a-00a0-4b8f-97c3-96f9230acdf1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-549d3b4c-0963-4e61-89fd-2b75b9891230
STEP: Updating secret s-test-opt-upd-9291051a-00a0-4b8f-97c3-96f9230acdf1
STEP: Creating secret with name s-test-opt-create-4f55a0cc-17da-47d9-84aa-f492f70d5ed3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:35:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6031" for this suite.
Mar  5 19:35:30.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:35:30.651: INFO: namespace secrets-6031 deletion completed in 22.17828478s

• [SLOW TEST:26.495 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:35:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  5 19:35:30.828: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650135,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 19:35:30.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650136,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  5 19:35:30.828: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650137,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  5 19:35:40.872: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650159,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 19:35:40.872: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650160,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  5 19:35:40.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7686,SelfLink:/api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-label-changed,UID:6322b657-6e12-4a78-a7b5-cc461b72fc91,ResourceVersion:650161,Generation:0,CreationTimestamp:2020-03-05 19:35:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:35:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7686" for this suite.
Mar  5 19:35:46.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:35:47.056: INFO: namespace watch-7686 deletion completed in 6.17600271s

• [SLOW TEST:16.405 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:35:47.056: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1677, will wait for the garbage collector to delete the pods
Mar  5 19:35:51.285: INFO: Deleting Job.batch foo took: 11.065518ms
Mar  5 19:35:51.585: INFO: Terminating Job.batch foo pods took: 300.218619ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:36:32.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1677" for this suite.
Mar  5 19:36:38.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:36:38.674: INFO: namespace job-1677 deletion completed in 6.176132747s

• [SLOW TEST:51.618 seconds]
[sig-apps] Job
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:36:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:36:40.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4603" for this suite.
Mar  5 19:37:26.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:37:27.046: INFO: namespace kubelet-test-4603 deletion completed in 46.176682658s

• [SLOW TEST:48.372 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:37:27.046: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 19:37:27.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2342'
Mar  5 19:37:27.280: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 19:37:27.280: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  5 19:37:27.289: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar  5 19:37:27.292: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  5 19:37:27.300: INFO: scanned /root for discovery docs: <nil>
Mar  5 19:37:27.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2342'
Mar  5 19:37:43.109: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  5 19:37:43.109: INFO: stdout: "Created e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228\nScaling up e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  5 19:37:43.109: INFO: stdout: "Created e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228\nScaling up e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  5 19:37:43.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-2342'
Mar  5 19:37:43.189: INFO: stderr: ""
Mar  5 19:37:43.189: INFO: stdout: "e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228-25g5w "
Mar  5 19:37:43.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228-25g5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2342'
Mar  5 19:37:43.265: INFO: stderr: ""
Mar  5 19:37:43.265: INFO: stdout: "true"
Mar  5 19:37:43.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228-25g5w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2342'
Mar  5 19:37:43.341: INFO: stderr: ""
Mar  5 19:37:43.341: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  5 19:37:43.341: INFO: e2e-test-nginx-rc-f4a19dbf85570e76d0a558bf59ce7228-25g5w is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Mar  5 19:37:43.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete rc e2e-test-nginx-rc --namespace=kubectl-2342'
Mar  5 19:37:43.428: INFO: stderr: ""
Mar  5 19:37:43.428: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:37:43.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2342" for this suite.
Mar  5 19:37:49.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:37:49.622: INFO: namespace kubectl-2342 deletion completed in 6.185340076s

• [SLOW TEST:22.576 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:37:49.622: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-4763331e-958f-46ef-ba41-e29013f7c57f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:37:49.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9580" for this suite.
Mar  5 19:37:55.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:37:55.957: INFO: namespace secrets-9580 deletion completed in 6.178427227s

• [SLOW TEST:6.335 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:37:55.957: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 19:37:56.180: INFO: Number of nodes with available pods: 0
Mar  5 19:37:56.180: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 19:37:57.194: INFO: Number of nodes with available pods: 0
Mar  5 19:37:57.194: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 19:37:58.193: INFO: Number of nodes with available pods: 4
Mar  5 19:37:58.193: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 19:37:59.194: INFO: Number of nodes with available pods: 6
Mar  5 19:37:59.194: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  5 19:37:59.225: INFO: Number of nodes with available pods: 5
Mar  5 19:37:59.225: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:00.238: INFO: Number of nodes with available pods: 5
Mar  5 19:38:00.238: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:01.238: INFO: Number of nodes with available pods: 5
Mar  5 19:38:01.238: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:02.239: INFO: Number of nodes with available pods: 5
Mar  5 19:38:02.239: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:03.239: INFO: Number of nodes with available pods: 5
Mar  5 19:38:03.239: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:04.239: INFO: Number of nodes with available pods: 5
Mar  5 19:38:04.239: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:38:05.239: INFO: Number of nodes with available pods: 6
Mar  5 19:38:05.239: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6951, will wait for the garbage collector to delete the pods
Mar  5 19:38:05.313: INFO: Deleting DaemonSet.extensions daemon-set took: 12.775581ms
Mar  5 19:38:05.413: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.273439ms
Mar  5 19:38:15.719: INFO: Number of nodes with available pods: 0
Mar  5 19:38:15.719: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 19:38:15.725: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6951/daemonsets","resourceVersion":"650788"},"items":null}

Mar  5 19:38:15.730: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6951/pods","resourceVersion":"650788"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:38:15.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6951" for this suite.
Mar  5 19:38:21.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:38:21.961: INFO: namespace daemonsets-6951 deletion completed in 6.177682819s

• [SLOW TEST:26.003 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:38:21.961: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  5 19:38:22.107: INFO: namespace kubectl-9466
Mar  5 19:38:22.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-9466'
Mar  5 19:38:22.328: INFO: stderr: ""
Mar  5 19:38:22.328: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  5 19:38:23.335: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:38:23.335: INFO: Found 0 / 1
Mar  5 19:38:24.334: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:38:24.334: INFO: Found 0 / 1
Mar  5 19:38:25.334: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:38:25.334: INFO: Found 0 / 1
Mar  5 19:38:26.334: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:38:26.334: INFO: Found 1 / 1
Mar  5 19:38:26.334: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 19:38:26.341: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:38:26.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 19:38:26.341: INFO: wait on redis-master startup in kubectl-9466 
Mar  5 19:38:26.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-kzj22 redis-master --namespace=kubectl-9466'
Mar  5 19:38:26.440: INFO: stderr: ""
Mar  5 19:38:26.440: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Mar 19:38:25.442 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Mar 19:38:25.442 # Server started, Redis version 3.2.12\n1:M 05 Mar 19:38:25.442 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Mar 19:38:25.442 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  5 19:38:26.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9466'
Mar  5 19:38:26.549: INFO: stderr: ""
Mar  5 19:38:26.549: INFO: stdout: "service/rm2 exposed\n"
Mar  5 19:38:26.554: INFO: Service rm2 in namespace kubectl-9466 found.
STEP: exposing service
Mar  5 19:38:28.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9466'
Mar  5 19:38:28.665: INFO: stderr: ""
Mar  5 19:38:28.665: INFO: stdout: "service/rm3 exposed\n"
Mar  5 19:38:28.671: INFO: Service rm3 in namespace kubectl-9466 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:38:30.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9466" for this suite.
Mar  5 19:38:52.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:38:52.861: INFO: namespace kubectl-9466 deletion completed in 22.172383528s

• [SLOW TEST:30.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:38:52.862: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8554
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a709d9b5-962c-40f0-810b-a96c0f9f16cc
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a709d9b5-962c-40f0-810b-a96c0f9f16cc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:38:57.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8554" for this suite.
Mar  5 19:39:19.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:39:19.282: INFO: namespace projected-8554 deletion completed in 22.176480231s

• [SLOW TEST:26.420 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:39:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:39:19.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1" in namespace "projected-4901" to be "success or failure"
Mar  5 19:39:19.446: INFO: Pod "downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900732ms
Mar  5 19:39:21.452: INFO: Pod "downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011387425s
STEP: Saw pod success
Mar  5 19:39:21.452: INFO: Pod "downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1" satisfied condition "success or failure"
Mar  5 19:39:21.457: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1 container client-container: <nil>
STEP: delete the pod
Mar  5 19:39:21.486: INFO: Waiting for pod downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1 to disappear
Mar  5 19:39:21.490: INFO: Pod downwardapi-volume-d6ded458-86e5-4429-8fb5-b1939ca83dd1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:39:21.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4901" for this suite.
Mar  5 19:39:27.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:39:27.674: INFO: namespace projected-4901 deletion completed in 6.176314702s

• [SLOW TEST:8.392 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:39:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-8099397c-0fb1-4ad3-ada1-9c644ee942f4
STEP: Creating a pod to test consume secrets
Mar  5 19:39:27.837: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335" in namespace "projected-3712" to be "success or failure"
Mar  5 19:39:27.842: INFO: Pod "pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703606ms
Mar  5 19:39:29.847: INFO: Pod "pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010066758s
Mar  5 19:39:31.853: INFO: Pod "pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015523283s
STEP: Saw pod success
Mar  5 19:39:31.853: INFO: Pod "pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335" satisfied condition "success or failure"
Mar  5 19:39:31.858: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:39:31.888: INFO: Waiting for pod pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335 to disappear
Mar  5 19:39:31.893: INFO: Pod pod-projected-secrets-d0024fc7-def8-4571-bea1-766e51684335 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:39:31.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3712" for this suite.
Mar  5 19:39:37.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:39:38.077: INFO: namespace projected-3712 deletion completed in 6.177503771s

• [SLOW TEST:10.403 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:39:38.079: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 19:39:40.257: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:39:40.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4527" for this suite.
Mar  5 19:39:46.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:39:46.462: INFO: namespace container-runtime-4527 deletion completed in 6.178164072s

• [SLOW TEST:8.384 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:39:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6429
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b477fa12-fc4e-48ba-b9ad-604aaf8a2869
STEP: Creating configMap with name cm-test-opt-upd-8e2a7eb9-9209-4e27-96ba-1b76d003014d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b477fa12-fc4e-48ba-b9ad-604aaf8a2869
STEP: Updating configmap cm-test-opt-upd-8e2a7eb9-9209-4e27-96ba-1b76d003014d
STEP: Creating configMap with name cm-test-opt-create-8763a136-39fd-43ab-98ff-5c1a97d09e27
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:39:50.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6429" for this suite.
Mar  5 19:40:12.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:40:12.952: INFO: namespace configmap-6429 deletion completed in 22.17613427s

• [SLOW TEST:26.489 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:40:12.953: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:40:13.103: INFO: Creating deployment "test-recreate-deployment"
Mar  5 19:40:13.109: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  5 19:40:13.118: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  5 19:40:15.129: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  5 19:40:15.134: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  5 19:40:15.144: INFO: Updating deployment test-recreate-deployment
Mar  5 19:40:15.144: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  5 19:40:15.223: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1216,SelfLink:/apis/apps/v1/namespaces/deployment-1216/deployments/test-recreate-deployment,UID:f8d9f4fa-071f-415b-a358-dc1e3fbff3b7,ResourceVersion:651333,Generation:2,CreationTimestamp:2020-03-05 19:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-03-05 19:40:15 +0000 UTC 2020-03-05 19:40:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-03-05 19:40:15 +0000 UTC 2020-03-05 19:40:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  5 19:40:15.228: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-1216,SelfLink:/apis/apps/v1/namespaces/deployment-1216/replicasets/test-recreate-deployment-5c8c9cc69d,UID:9001419b-4e91-4fc7-a3ca-ce1b88fb9b67,ResourceVersion:651331,Generation:1,CreationTimestamp:2020-03-05 19:40:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8d9f4fa-071f-415b-a358-dc1e3fbff3b7 0xc002b85db7 0xc002b85db8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:40:15.228: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  5 19:40:15.229: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-1216,SelfLink:/apis/apps/v1/namespaces/deployment-1216/replicasets/test-recreate-deployment-6df85df6b9,UID:5f3a8eb9-3be8-4a7b-ad4b-5d4a144313e4,ResourceVersion:651321,Generation:2,CreationTimestamp:2020-03-05 19:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8d9f4fa-071f-415b-a358-dc1e3fbff3b7 0xc002b85e87 0xc002b85e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:40:15.233: INFO: Pod "test-recreate-deployment-5c8c9cc69d-wm6np" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-wm6np,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-1216,SelfLink:/api/v1/namespaces/deployment-1216/pods/test-recreate-deployment-5c8c9cc69d-wm6np,UID:72c4df9c-47a3-4ff9-9086-c4fedced6996,ResourceVersion:651332,Generation:0,CreationTimestamp:2020-03-05 19:40:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 9001419b-4e91-4fc7-a3ca-ce1b88fb9b67 0xc0038d6777 0xc0038d6778}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-47558 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-47558,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-47558 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-106-110.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038d67e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038d6800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:40:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:40:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:40:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:40:15 +0000 UTC  }],Message:,Reason:,HostIP:192.168.106.110,PodIP:,StartTime:2020-03-05 19:40:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:40:15.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1216" for this suite.
Mar  5 19:40:21.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:40:21.416: INFO: namespace deployment-1216 deletion completed in 6.174835697s

• [SLOW TEST:8.463 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:40:21.416: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar  5 19:40:23.599: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-370450052 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  5 19:40:28.690: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:40:28.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3117" for this suite.
Mar  5 19:40:34.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:40:34.877: INFO: namespace pods-3117 deletion completed in 6.174935144s

• [SLOW TEST:13.462 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:40:34.878: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-35a89ddd-f6af-4981-9d9d-42ca30c34c1c
STEP: Creating a pod to test consume configMaps
Mar  5 19:40:35.042: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193" in namespace "configmap-3881" to be "success or failure"
Mar  5 19:40:35.047: INFO: Pod "pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696019ms
Mar  5 19:40:37.053: INFO: Pod "pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01025744s
STEP: Saw pod success
Mar  5 19:40:37.053: INFO: Pod "pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193" satisfied condition "success or failure"
Mar  5 19:40:37.058: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:40:37.086: INFO: Waiting for pod pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193 to disappear
Mar  5 19:40:37.090: INFO: Pod pod-configmaps-8b4a892c-1371-4416-8d27-03a104702193 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:40:37.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3881" for this suite.
Mar  5 19:40:43.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:40:43.275: INFO: namespace configmap-3881 deletion completed in 6.177433515s

• [SLOW TEST:8.397 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:40:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8771
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  5 19:40:43.433: INFO: Waiting up to 5m0s for pod "pod-e6756c01-363f-4951-af46-b598344b4573" in namespace "emptydir-8771" to be "success or failure"
Mar  5 19:40:43.438: INFO: Pod "pod-e6756c01-363f-4951-af46-b598344b4573": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926806ms
Mar  5 19:40:45.444: INFO: Pod "pod-e6756c01-363f-4951-af46-b598344b4573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010778592s
STEP: Saw pod success
Mar  5 19:40:45.444: INFO: Pod "pod-e6756c01-363f-4951-af46-b598344b4573" satisfied condition "success or failure"
Mar  5 19:40:45.449: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-e6756c01-363f-4951-af46-b598344b4573 container test-container: <nil>
STEP: delete the pod
Mar  5 19:40:45.477: INFO: Waiting for pod pod-e6756c01-363f-4951-af46-b598344b4573 to disappear
Mar  5 19:40:45.482: INFO: Pod pod-e6756c01-363f-4951-af46-b598344b4573 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:40:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8771" for this suite.
Mar  5 19:40:51.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:40:51.667: INFO: namespace emptydir-8771 deletion completed in 6.177799354s

• [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:40:51.668: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:40:57.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7142" for this suite.
Mar  5 19:41:03.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:41:03.543: INFO: namespace watch-7142 deletion completed in 6.263477371s

• [SLOW TEST:11.876 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:41:03.544: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Mar  5 19:41:03.704: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4357" to be "success or failure"
Mar  5 19:41:03.710: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.80744ms
Mar  5 19:41:05.715: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011112762s
STEP: Saw pod success
Mar  5 19:41:05.715: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  5 19:41:05.720: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  5 19:41:05.752: INFO: Waiting for pod pod-host-path-test to disappear
Mar  5 19:41:05.756: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:41:05.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4357" for this suite.
Mar  5 19:41:11.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:41:11.945: INFO: namespace hostpath-4357 deletion completed in 6.180772053s

• [SLOW TEST:8.401 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:41:11.945: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-1703
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1703 to expose endpoints map[]
Mar  5 19:41:12.113: INFO: Get endpoints failed (5.200311ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  5 19:41:13.119: INFO: successfully validated that service multi-endpoint-test in namespace services-1703 exposes endpoints map[] (1.01108793s elapsed)
STEP: Creating pod pod1 in namespace services-1703
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1703 to expose endpoints map[pod1:[100]]
Mar  5 19:41:15.162: INFO: successfully validated that service multi-endpoint-test in namespace services-1703 exposes endpoints map[pod1:[100]] (2.032323684s elapsed)
STEP: Creating pod pod2 in namespace services-1703
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1703 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  5 19:41:18.229: INFO: successfully validated that service multi-endpoint-test in namespace services-1703 exposes endpoints map[pod1:[100] pod2:[101]] (3.059557629s elapsed)
STEP: Deleting pod pod1 in namespace services-1703
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1703 to expose endpoints map[pod2:[101]]
Mar  5 19:41:19.257: INFO: successfully validated that service multi-endpoint-test in namespace services-1703 exposes endpoints map[pod2:[101]] (1.020082559s elapsed)
STEP: Deleting pod pod2 in namespace services-1703
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1703 to expose endpoints map[]
Mar  5 19:41:20.277: INFO: successfully validated that service multi-endpoint-test in namespace services-1703 exposes endpoints map[] (1.010749307s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:41:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1703" for this suite.
Mar  5 19:41:42.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:41:42.492: INFO: namespace services-1703 deletion completed in 22.175942643s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.547 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:41:42.493: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:41:44.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1369" for this suite.
Mar  5 19:41:50.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:41:50.911: INFO: namespace emptydir-wrapper-1369 deletion completed in 6.18811087s

• [SLOW TEST:8.419 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:41:50.912: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:41:51.071: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f" in namespace "projected-1870" to be "success or failure"
Mar  5 19:41:51.077: INFO: Pod "downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.563071ms
Mar  5 19:41:53.082: INFO: Pod "downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011009907s
STEP: Saw pod success
Mar  5 19:41:53.082: INFO: Pod "downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f" satisfied condition "success or failure"
Mar  5 19:41:53.087: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f container client-container: <nil>
STEP: delete the pod
Mar  5 19:41:53.117: INFO: Waiting for pod downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f to disappear
Mar  5 19:41:53.121: INFO: Pod downwardapi-volume-0ac0db1f-cdf3-4932-888c-b1142e83900f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:41:53.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1870" for this suite.
Mar  5 19:41:59.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:41:59.303: INFO: namespace projected-1870 deletion completed in 6.174538775s

• [SLOW TEST:8.392 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:41:59.304: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4912
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  5 19:41:59.468: INFO: Found 0 stateful pods, waiting for 3
Mar  5 19:42:09.474: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:42:09.475: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:42:09.475: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  5 19:42:09.509: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  5 19:42:19.551: INFO: Updating stateful set ss2
Mar  5 19:42:19.561: INFO: Waiting for Pod statefulset-4912/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Mar  5 19:42:29.609: INFO: Found 2 stateful pods, waiting for 3
Mar  5 19:42:39.615: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:42:39.615: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 19:42:39.615: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  5 19:42:39.646: INFO: Updating stateful set ss2
Mar  5 19:42:39.656: INFO: Waiting for Pod statefulset-4912/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar  5 19:42:49.667: INFO: Waiting for Pod statefulset-4912/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar  5 19:42:59.687: INFO: Updating stateful set ss2
Mar  5 19:42:59.697: INFO: Waiting for StatefulSet statefulset-4912/ss2 to complete update
Mar  5 19:42:59.697: INFO: Waiting for Pod statefulset-4912/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar  5 19:43:09.707: INFO: Waiting for StatefulSet statefulset-4912/ss2 to complete update
Mar  5 19:43:09.707: INFO: Waiting for Pod statefulset-4912/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  5 19:43:19.707: INFO: Deleting all statefulset in ns statefulset-4912
Mar  5 19:43:19.712: INFO: Scaling statefulset ss2 to 0
Mar  5 19:43:39.733: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 19:43:39.738: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:43:39.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4912" for this suite.
Mar  5 19:43:45.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:43:45.942: INFO: namespace statefulset-4912 deletion completed in 6.177012941s

• [SLOW TEST:106.638 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:43:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  5 19:43:46.111: INFO: Waiting up to 5m0s for pod "pod-fe74acd7-0a7f-4dea-b4af-c410de792cea" in namespace "emptydir-1165" to be "success or failure"
Mar  5 19:43:46.116: INFO: Pod "pod-fe74acd7-0a7f-4dea-b4af-c410de792cea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.844704ms
Mar  5 19:43:48.122: INFO: Pod "pod-fe74acd7-0a7f-4dea-b4af-c410de792cea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010451127s
STEP: Saw pod success
Mar  5 19:43:48.122: INFO: Pod "pod-fe74acd7-0a7f-4dea-b4af-c410de792cea" satisfied condition "success or failure"
Mar  5 19:43:48.127: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-fe74acd7-0a7f-4dea-b4af-c410de792cea container test-container: <nil>
STEP: delete the pod
Mar  5 19:43:48.156: INFO: Waiting for pod pod-fe74acd7-0a7f-4dea-b4af-c410de792cea to disappear
Mar  5 19:43:48.161: INFO: Pod pod-fe74acd7-0a7f-4dea-b4af-c410de792cea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:43:48.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1165" for this suite.
Mar  5 19:43:54.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:43:54.346: INFO: namespace emptydir-1165 deletion completed in 6.177200396s

• [SLOW TEST:8.404 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:43:54.346: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Mar  5 19:43:54.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 --namespace=kubectl-2180 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  5 19:43:56.748: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  5 19:43:56.748: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:43:58.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2180" for this suite.
Mar  5 19:44:04.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:44:04.946: INFO: namespace kubectl-2180 deletion completed in 6.180723443s

• [SLOW TEST:10.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:44:04.947: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Mar  5 19:44:05.093: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-370450052 proxy --unix-socket=/tmp/kubectl-proxy-unix895991315/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:44:05.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4543" for this suite.
Mar  5 19:44:11.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:44:11.336: INFO: namespace kubectl-4543 deletion completed in 6.177703475s

• [SLOW TEST:6.390 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:44:11.336: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  5 19:44:11.483: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 19:44:11.498: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 19:44:11.502: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-106-110.ec2.internal before test
Mar  5 19:44:11.511: INFO: aws-node-2lp4m from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.511: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.511: INFO: kube-proxy-7gg9d from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.511: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.511: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-bh47c from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.511: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.511: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-112-247.ec2.internal before test
Mar  5 19:44:11.521: INFO: kube-proxy-r7sml from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.521: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.521: INFO: aws-node-gxwkp from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.521: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.521: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-7m5vz from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.521: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.521: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.521: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-164.ec2.internal before test
Mar  5 19:44:11.530: INFO: kube-proxy-l5q7m from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.530: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.530: INFO: aws-node-4q99r from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.530: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.530: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-d9krw from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.530: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.530: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-155-60.ec2.internal before test
Mar  5 19:44:11.538: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-cv5hn from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.538: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.538: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.538: INFO: kube-proxy-m8h2w from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.538: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.538: INFO: aws-node-fwjd8 from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.538: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.538: INFO: sonobuoy from sonobuoy started at 2020-03-05 19:01:15 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.538: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 19:44:11.538: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-169.ec2.internal before test
Mar  5 19:44:11.551: INFO: aws-node-qgbl2 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.551: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.551: INFO: coredns-59dfd6b59f-xzswv from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.551: INFO: 	Container coredns ready: true, restart count 0
Mar  5 19:44:11.551: INFO: kube-proxy-fkm78 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.551: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.551: INFO: coredns-59dfd6b59f-kw58c from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.551: INFO: 	Container coredns ready: true, restart count 0
Mar  5 19:44:11.551: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-75b4d from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.551: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.551: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-239-209.ec2.internal before test
Mar  5 19:44:11.562: INFO: aws-node-bxlx7 from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.562: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 19:44:11.562: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-nlqlx from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 19:44:11.562: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 19:44:11.562: INFO: kube-proxy-ttknx from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 19:44:11.562: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 19:44:11.562: INFO: sonobuoy-e2e-job-3f29057810bc417f from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 19:44:11.562: INFO: 	Container e2e ready: true, restart count 0
Mar  5 19:44:11.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-192-168-106-110.ec2.internal
STEP: verifying the node has the label node ip-192-168-112-247.ec2.internal
STEP: verifying the node has the label node ip-192-168-148-164.ec2.internal
STEP: verifying the node has the label node ip-192-168-155-60.ec2.internal
STEP: verifying the node has the label node ip-192-168-218-169.ec2.internal
STEP: verifying the node has the label node ip-192-168-239-209.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-2lp4m requesting resource cpu=10m on Node ip-192-168-106-110.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-4q99r requesting resource cpu=10m on Node ip-192-168-148-164.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-bxlx7 requesting resource cpu=10m on Node ip-192-168-239-209.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-fwjd8 requesting resource cpu=10m on Node ip-192-168-155-60.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-gxwkp requesting resource cpu=10m on Node ip-192-168-112-247.ec2.internal
Mar  5 19:44:11.673: INFO: Pod aws-node-qgbl2 requesting resource cpu=10m on Node ip-192-168-218-169.ec2.internal
Mar  5 19:44:11.673: INFO: Pod coredns-59dfd6b59f-kw58c requesting resource cpu=100m on Node ip-192-168-218-169.ec2.internal
Mar  5 19:44:11.673: INFO: Pod coredns-59dfd6b59f-xzswv requesting resource cpu=100m on Node ip-192-168-218-169.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-7gg9d requesting resource cpu=100m on Node ip-192-168-106-110.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-fkm78 requesting resource cpu=100m on Node ip-192-168-218-169.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-l5q7m requesting resource cpu=100m on Node ip-192-168-148-164.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-m8h2w requesting resource cpu=100m on Node ip-192-168-155-60.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-r7sml requesting resource cpu=100m on Node ip-192-168-112-247.ec2.internal
Mar  5 19:44:11.673: INFO: Pod kube-proxy-ttknx requesting resource cpu=100m on Node ip-192-168-239-209.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-192-168-155-60.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-e2e-job-3f29057810bc417f requesting resource cpu=0m on Node ip-192-168-239-209.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-75b4d requesting resource cpu=0m on Node ip-192-168-218-169.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-7m5vz requesting resource cpu=0m on Node ip-192-168-112-247.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-bh47c requesting resource cpu=0m on Node ip-192-168-106-110.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-cv5hn requesting resource cpu=0m on Node ip-192-168-155-60.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-d9krw requesting resource cpu=0m on Node ip-192-168-148-164.ec2.internal
Mar  5 19:44:11.674: INFO: Pod sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-nlqlx requesting resource cpu=0m on Node ip-192-168-239-209.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a040302-913d-464b-857a-c4f32342253a.15f97ff911af3dca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-0a040302-913d-464b-857a-c4f32342253a to ip-192-168-239-209.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a040302-913d-464b-857a-c4f32342253a.15f97ff94666d45d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a040302-913d-464b-857a-c4f32342253a.15f97ff96ac22832], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a040302-913d-464b-857a-c4f32342253a.15f97ff96e44b7af], Reason = [Created], Message = [Created container filler-pod-0a040302-913d-464b-857a-c4f32342253a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a040302-913d-464b-857a-c4f32342253a.15f97ff9788739e5], Reason = [Started], Message = [Started container filler-pod-0a040302-913d-464b-857a-c4f32342253a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f.15f97ff9114a8e89], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f to ip-192-168-218-169.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f.15f97ff9470b4c1d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f.15f97ff9ae153f17], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f.15f97ff9b12202b6], Reason = [Created], Message = [Created container filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f.15f97ff9b9b43a57], Reason = [Started], Message = [Started container filler-pod-7b8f8ab2-8dc8-4cb5-8472-7a2e821b574f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351.15f97ff9121f05f1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351 to ip-192-168-106-110.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351.15f97ff949907bff], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351.15f97ff94bf8c872], Reason = [Created], Message = [Created container filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351.15f97ff955fa3fb4], Reason = [Started], Message = [Started container filler-pod-bb900b70-e498-4023-a35a-2bdc5f643351]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa.15f97ff91002aa49], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa to ip-192-168-112-247.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa.15f97ff945b5764a], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa.15f97ff9a52d0e77], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa.15f97ff9a8452f55], Reason = [Created], Message = [Created container filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa.15f97ff9b1785766], Reason = [Started], Message = [Started container filler-pod-d738a21e-249a-43e9-878d-61588b5e1daa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a.15f97ff910f329c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a to ip-192-168-155-60.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a.15f97ff9463070c7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a.15f97ff9494e811e], Reason = [Created], Message = [Created container filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a.15f97ff9533279db], Reason = [Started], Message = [Started container filler-pod-dd665d8a-d8d4-48dd-bd96-c0daa8d9c35a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859.15f97ff91089a5e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-987/filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859 to ip-192-168-148-164.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859.15f97ff944865c9d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859.15f97ff9474930c9], Reason = [Created], Message = [Created container filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859.15f97ff9515aa772], Reason = [Started], Message = [Started container filler-pod-f290217a-92b2-4ac4-b6ac-d8c85430f859]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f97ffa030d835b], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node ip-192-168-148-164.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-155-60.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-218-169.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-239-209.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-106-110.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-112-247.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:44:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-987" for this suite.
Mar  5 19:44:22.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:44:23.302: INFO: namespace sched-pred-987 deletion completed in 6.387606315s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.966 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:44:23.302: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:44:23.507: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 19:44:23.526: INFO: Number of nodes with available pods: 0
Mar  5 19:44:23.526: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 19:44:24.541: INFO: Number of nodes with available pods: 0
Mar  5 19:44:24.541: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 19:44:25.540: INFO: Number of nodes with available pods: 4
Mar  5 19:44:25.540: INFO: Node ip-192-168-148-164.ec2.internal is running more than one daemon pod
Mar  5 19:44:26.540: INFO: Number of nodes with available pods: 6
Mar  5 19:44:26.540: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-4kjnr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:26.582: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-4kjnr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:27.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-4kjnr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Pod daemon-set-4kjnr is not available
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:28.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:29.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:29.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:29.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:29.598: INFO: Pod daemon-set-kxkq2 is not available
Mar  5 19:44:29.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:29.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:30.598: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:30.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:30.598: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:30.598: INFO: Pod daemon-set-kxkq2 is not available
Mar  5 19:44:30.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:30.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:31.598: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:31.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:31.598: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:31.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:31.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:32.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:32.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:32.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:32.597: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:32.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:32.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:33.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:33.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:33.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:33.597: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:33.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:33.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:34.598: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:34.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:34.598: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:34.598: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:34.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:34.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:35.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:35.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:35.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:35.597: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:35.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:35.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:36.598: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:36.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:36.598: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:36.598: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:36.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:36.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:37.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:37.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:37.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:37.597: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:37.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:37.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:38.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:38.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:38.598: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:38.598: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:38.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:38.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:39.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:39.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:39.597: INFO: Wrong image for pod: daemon-set-kcvf6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:39.597: INFO: Pod daemon-set-kcvf6 is not available
Mar  5 19:44:39.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:39.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:40.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:40.598: INFO: Pod daemon-set-fp4cc is not available
Mar  5 19:44:40.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:40.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:40.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:41.598: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:41.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:41.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:41.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:42.597: INFO: Wrong image for pod: daemon-set-6ch2r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:42.597: INFO: Pod daemon-set-6ch2r is not available
Mar  5 19:44:42.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:42.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:42.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:43.597: INFO: Pod daemon-set-66lmt is not available
Mar  5 19:44:43.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:43.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:43.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:44.597: INFO: Pod daemon-set-66lmt is not available
Mar  5 19:44:44.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:44.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:44.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:45.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:45.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:45.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:46.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:46.598: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:46.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:46.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:47.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:47.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:47.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:47.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:48.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:48.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:48.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:48.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:49.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:49.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:49.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:49.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:50.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:50.598: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:50.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:50.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:51.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:51.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:51.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:51.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:52.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:52.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:52.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:52.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:53.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:53.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:53.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:53.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:54.598: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:54.598: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:54.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:54.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:55.597: INFO: Wrong image for pod: daemon-set-hp8gg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:55.597: INFO: Pod daemon-set-hp8gg is not available
Mar  5 19:44:55.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:55.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:56.597: INFO: Pod daemon-set-2lt4z is not available
Mar  5 19:44:56.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:56.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:57.597: INFO: Pod daemon-set-2lt4z is not available
Mar  5 19:44:57.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:57.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:58.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:58.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:59.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:44:59.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:44:59.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:00.597: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:00.597: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:00.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:01.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:01.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:01.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:02.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:02.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:02.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:03.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:03.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:03.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:04.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:04.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:04.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:05.598: INFO: Wrong image for pod: daemon-set-sw2fk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:05.598: INFO: Pod daemon-set-sw2fk is not available
Mar  5 19:45:05.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:06.597: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:06.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:07.599: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:07.599: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:08.598: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:08.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:09.598: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:09.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:10.597: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:10.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:11.597: INFO: Pod daemon-set-mmqxq is not available
Mar  5 19:45:11.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:12.597: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:13.598: INFO: Wrong image for pod: daemon-set-zlp86. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar  5 19:45:13.598: INFO: Pod daemon-set-zlp86 is not available
Mar  5 19:45:14.597: INFO: Pod daemon-set-gxm66 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  5 19:45:14.617: INFO: Number of nodes with available pods: 5
Mar  5 19:45:14.617: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:45:15.634: INFO: Number of nodes with available pods: 5
Mar  5 19:45:15.634: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:45:16.632: INFO: Number of nodes with available pods: 5
Mar  5 19:45:16.632: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:45:17.633: INFO: Number of nodes with available pods: 5
Mar  5 19:45:17.633: INFO: Node ip-192-168-218-169.ec2.internal is running more than one daemon pod
Mar  5 19:45:18.632: INFO: Number of nodes with available pods: 6
Mar  5 19:45:18.632: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3505, will wait for the garbage collector to delete the pods
Mar  5 19:45:18.727: INFO: Deleting DaemonSet.extensions daemon-set took: 13.156287ms
Mar  5 19:45:19.028: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.270168ms
Mar  5 19:45:29.733: INFO: Number of nodes with available pods: 0
Mar  5 19:45:29.733: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 19:45:29.738: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3505/daemonsets","resourceVersion":"653027"},"items":null}

Mar  5 19:45:29.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3505/pods","resourceVersion":"653027"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:45:29.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3505" for this suite.
Mar  5 19:45:35.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:45:35.982: INFO: namespace daemonsets-3505 deletion completed in 6.179433283s

• [SLOW TEST:72.679 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:45:35.982: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  5 19:45:36.139: INFO: Waiting up to 5m0s for pod "pod-8989157e-848e-4376-929b-3d47b96a828a" in namespace "emptydir-6415" to be "success or failure"
Mar  5 19:45:36.145: INFO: Pod "pod-8989157e-848e-4376-929b-3d47b96a828a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.878728ms
Mar  5 19:45:38.150: INFO: Pod "pod-8989157e-848e-4376-929b-3d47b96a828a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011517483s
STEP: Saw pod success
Mar  5 19:45:38.150: INFO: Pod "pod-8989157e-848e-4376-929b-3d47b96a828a" satisfied condition "success or failure"
Mar  5 19:45:38.155: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-8989157e-848e-4376-929b-3d47b96a828a container test-container: <nil>
STEP: delete the pod
Mar  5 19:45:38.185: INFO: Waiting for pod pod-8989157e-848e-4376-929b-3d47b96a828a to disappear
Mar  5 19:45:38.190: INFO: Pod pod-8989157e-848e-4376-929b-3d47b96a828a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:45:38.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6415" for this suite.
Mar  5 19:45:44.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:45:44.376: INFO: namespace emptydir-6415 deletion completed in 6.178590232s

• [SLOW TEST:8.394 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:45:44.376: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-7e17daf2-09e3-4183-8501-50dc6d93a531 in namespace container-probe-1334
Mar  5 19:45:48.541: INFO: Started pod test-webserver-7e17daf2-09e3-4183-8501-50dc6d93a531 in namespace container-probe-1334
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 19:45:48.546: INFO: Initial restart count of pod test-webserver-7e17daf2-09e3-4183-8501-50dc6d93a531 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:49:49.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1334" for this suite.
Mar  5 19:49:55.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:49:55.433: INFO: namespace container-probe-1334 deletion completed in 6.183777936s

• [SLOW TEST:251.057 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:49:55.434: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  5 19:49:55.613: INFO: Waiting up to 5m0s for pod "pod-8df090c4-0bbc-47cd-970a-d0564e3a422a" in namespace "emptydir-6485" to be "success or failure"
Mar  5 19:49:55.618: INFO: Pod "pod-8df090c4-0bbc-47cd-970a-d0564e3a422a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.594708ms
Mar  5 19:49:57.623: INFO: Pod "pod-8df090c4-0bbc-47cd-970a-d0564e3a422a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010291562s
STEP: Saw pod success
Mar  5 19:49:57.623: INFO: Pod "pod-8df090c4-0bbc-47cd-970a-d0564e3a422a" satisfied condition "success or failure"
Mar  5 19:49:57.628: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-8df090c4-0bbc-47cd-970a-d0564e3a422a container test-container: <nil>
STEP: delete the pod
Mar  5 19:49:57.659: INFO: Waiting for pod pod-8df090c4-0bbc-47cd-970a-d0564e3a422a to disappear
Mar  5 19:49:57.663: INFO: Pod pod-8df090c4-0bbc-47cd-970a-d0564e3a422a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:49:57.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6485" for this suite.
Mar  5 19:50:03.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:50:03.856: INFO: namespace emptydir-6485 deletion completed in 6.185564527s

• [SLOW TEST:8.422 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:50:03.856: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:50:06.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2460" for this suite.
Mar  5 19:50:48.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:50:48.237: INFO: namespace kubelet-test-2460 deletion completed in 42.181300342s

• [SLOW TEST:44.380 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:50:48.237: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 19:50:50.420: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:50:50.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-995" for this suite.
Mar  5 19:50:56.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:50:56.630: INFO: namespace container-runtime-995 deletion completed in 6.18188909s

• [SLOW TEST:8.392 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:50:56.630: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 19:50:56.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7" in namespace "projected-2622" to be "success or failure"
Mar  5 19:50:56.796: INFO: Pod "downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015164ms
Mar  5 19:50:58.802: INFO: Pod "downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010623718s
STEP: Saw pod success
Mar  5 19:50:58.802: INFO: Pod "downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7" satisfied condition "success or failure"
Mar  5 19:50:58.809: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7 container client-container: <nil>
STEP: delete the pod
Mar  5 19:50:58.840: INFO: Waiting for pod downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7 to disappear
Mar  5 19:50:58.844: INFO: Pod downwardapi-volume-6dc97159-f56e-4986-a0d4-5ce11551daf7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:50:58.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2622" for this suite.
Mar  5 19:51:04.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:51:05.036: INFO: namespace projected-2622 deletion completed in 6.184017619s

• [SLOW TEST:8.406 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:51:05.036: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Mar  5 19:51:05.186: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  5 19:51:05.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:05.390: INFO: stderr: ""
Mar  5 19:51:05.390: INFO: stdout: "service/redis-slave created\n"
Mar  5 19:51:05.390: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  5 19:51:05.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:05.659: INFO: stderr: ""
Mar  5 19:51:05.659: INFO: stdout: "service/redis-master created\n"
Mar  5 19:51:05.659: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  5 19:51:05.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:05.913: INFO: stderr: ""
Mar  5 19:51:05.913: INFO: stdout: "service/frontend created\n"
Mar  5 19:51:05.913: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  5 19:51:05.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:06.132: INFO: stderr: ""
Mar  5 19:51:06.132: INFO: stdout: "deployment.apps/frontend created\n"
Mar  5 19:51:06.133: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  5 19:51:06.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:06.456: INFO: stderr: ""
Mar  5 19:51:06.456: INFO: stdout: "deployment.apps/redis-master created\n"
Mar  5 19:51:06.457: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  5 19:51:06.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-5804'
Mar  5 19:51:06.687: INFO: stderr: ""
Mar  5 19:51:06.687: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar  5 19:51:06.687: INFO: Waiting for all frontend pods to be Running.
Mar  5 19:51:26.738: INFO: Waiting for frontend to serve content.
Mar  5 19:51:26.755: INFO: Trying to add a new entry to the guestbook.
Mar  5 19:51:26.770: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  5 19:51:26.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:26.902: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:26.902: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 19:51:26.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:27.033: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:27.033: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 19:51:27.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:27.158: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:27.158: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 19:51:27.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:27.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:27.259: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 19:51:27.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:27.346: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:27.346: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 19:51:27.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-5804'
Mar  5 19:51:27.434: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:51:27.434: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:51:27.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5804" for this suite.
Mar  5 19:52:07.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:52:07.626: INFO: namespace kubectl-5804 deletion completed in 40.184019503s

• [SLOW TEST:62.590 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:52:07.626: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-7jgf
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 19:52:07.804: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7jgf" in namespace "subpath-1288" to be "success or failure"
Mar  5 19:52:07.809: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.601542ms
Mar  5 19:52:09.815: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319483s
Mar  5 19:52:11.820: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 4.015602824s
Mar  5 19:52:13.825: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 6.02116042s
Mar  5 19:52:15.831: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 8.026754973s
Mar  5 19:52:17.837: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 10.032467538s
Mar  5 19:52:19.842: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 12.03802617s
Mar  5 19:52:21.847: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 14.043260852s
Mar  5 19:52:23.853: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 16.048406856s
Mar  5 19:52:25.858: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 18.054030032s
Mar  5 19:52:27.863: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Running", Reason="", readiness=true. Elapsed: 20.059101137s
Mar  5 19:52:29.869: INFO: Pod "pod-subpath-test-secret-7jgf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064486886s
STEP: Saw pod success
Mar  5 19:52:29.869: INFO: Pod "pod-subpath-test-secret-7jgf" satisfied condition "success or failure"
Mar  5 19:52:29.873: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-subpath-test-secret-7jgf container test-container-subpath-secret-7jgf: <nil>
STEP: delete the pod
Mar  5 19:52:29.902: INFO: Waiting for pod pod-subpath-test-secret-7jgf to disappear
Mar  5 19:52:29.907: INFO: Pod pod-subpath-test-secret-7jgf no longer exists
STEP: Deleting pod pod-subpath-test-secret-7jgf
Mar  5 19:52:29.907: INFO: Deleting pod "pod-subpath-test-secret-7jgf" in namespace "subpath-1288"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:52:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1288" for this suite.
Mar  5 19:52:35.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:52:36.100: INFO: namespace subpath-1288 deletion completed in 6.181532369s

• [SLOW TEST:28.474 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:52:36.101: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-971/secret-test-cc495ca6-9f6f-4314-b971-ac18ce322e50
STEP: Creating a pod to test consume secrets
Mar  5 19:52:36.269: INFO: Waiting up to 5m0s for pod "pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96" in namespace "secrets-971" to be "success or failure"
Mar  5 19:52:36.275: INFO: Pod "pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96": Phase="Pending", Reason="", readiness=false. Elapsed: 5.671683ms
Mar  5 19:52:38.281: INFO: Pod "pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011639439s
Mar  5 19:52:40.286: INFO: Pod "pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01709099s
STEP: Saw pod success
Mar  5 19:52:40.286: INFO: Pod "pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96" satisfied condition "success or failure"
Mar  5 19:52:40.291: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96 container env-test: <nil>
STEP: delete the pod
Mar  5 19:52:40.330: INFO: Waiting for pod pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96 to disappear
Mar  5 19:52:40.335: INFO: Pod pod-configmaps-57434411-0952-48dc-8eb6-986a8ca3fa96 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:52:40.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-971" for this suite.
Mar  5 19:52:46.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:52:46.524: INFO: namespace secrets-971 deletion completed in 6.181442091s

• [SLOW TEST:10.423 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:52:46.524: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  5 19:52:46.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654447,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 19:52:46.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654447,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  5 19:52:56.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654469,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  5 19:52:56.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654469,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  5 19:53:06.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654490,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 19:53:06.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654490,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  5 19:53:16.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654512,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 19:53:16.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-a,UID:6551904b-8cb5-44bf-b83a-cb052b491505,ResourceVersion:654512,Generation:0,CreationTimestamp:2020-03-05 19:52:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  5 19:53:26.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-b,UID:073fd89f-efa4-4361-9266-aa754bbf347b,ResourceVersion:654534,Generation:0,CreationTimestamp:2020-03-05 19:53:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 19:53:26.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-b,UID:073fd89f-efa4-4361-9266-aa754bbf347b,ResourceVersion:654534,Generation:0,CreationTimestamp:2020-03-05 19:53:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  5 19:53:36.751: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-b,UID:073fd89f-efa4-4361-9266-aa754bbf347b,ResourceVersion:654556,Generation:0,CreationTimestamp:2020-03-05 19:53:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 19:53:36.751: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7253,SelfLink:/api/v1/namespaces/watch-7253/configmaps/e2e-watch-test-configmap-b,UID:073fd89f-efa4-4361-9266-aa754bbf347b,ResourceVersion:654556,Generation:0,CreationTimestamp:2020-03-05 19:53:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:53:46.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7253" for this suite.
Mar  5 19:53:52.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:53:52.945: INFO: namespace watch-7253 deletion completed in 6.185375325s

• [SLOW TEST:66.420 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:53:52.945: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3842.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3842.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 19:54:07.168: INFO: DNS probes using dns-3842/dns-test-625ab1bc-799d-4120-a617-9684dcc5758f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:54:07.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3842" for this suite.
Mar  5 19:54:13.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:54:13.369: INFO: namespace dns-3842 deletion completed in 6.178431931s

• [SLOW TEST:20.424 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:54:13.369: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  5 19:54:14.586: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0305 19:54:14.586108      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:54:14.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8723" for this suite.
Mar  5 19:54:20.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:54:20.771: INFO: namespace gc-8723 deletion completed in 6.177983515s

• [SLOW TEST:7.402 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:54:20.771: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Mar  5 19:54:20.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 cluster-info'
Mar  5 19:54:21.112: INFO: stderr: ""
Mar  5 19:54:21.112: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:54:21.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3870" for this suite.
Mar  5 19:54:27.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:54:27.306: INFO: namespace kubectl-3870 deletion completed in 6.185557316s

• [SLOW TEST:6.534 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:54:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-5238
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5238 to expose endpoints map[]
Mar  5 19:54:27.476: INFO: Get endpoints failed (4.78886ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  5 19:54:28.481: INFO: successfully validated that service endpoint-test2 in namespace services-5238 exposes endpoints map[] (1.010341433s elapsed)
STEP: Creating pod pod1 in namespace services-5238
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5238 to expose endpoints map[pod1:[80]]
Mar  5 19:54:30.526: INFO: successfully validated that service endpoint-test2 in namespace services-5238 exposes endpoints map[pod1:[80]] (2.031139836s elapsed)
STEP: Creating pod pod2 in namespace services-5238
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5238 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  5 19:54:33.592: INFO: successfully validated that service endpoint-test2 in namespace services-5238 exposes endpoints map[pod1:[80] pod2:[80]] (3.059352464s elapsed)
STEP: Deleting pod pod1 in namespace services-5238
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5238 to expose endpoints map[pod2:[80]]
Mar  5 19:54:34.625: INFO: successfully validated that service endpoint-test2 in namespace services-5238 exposes endpoints map[pod2:[80]] (1.023921213s elapsed)
STEP: Deleting pod pod2 in namespace services-5238
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5238 to expose endpoints map[]
Mar  5 19:54:35.645: INFO: successfully validated that service endpoint-test2 in namespace services-5238 exposes endpoints map[] (1.012353608s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:54:35.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5238" for this suite.
Mar  5 19:54:57.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:54:57.869: INFO: namespace services-5238 deletion completed in 22.185740187s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.563 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:54:57.870: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-29
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e2fca57c-ec2f-4e73-9ea2-608dfdf79afb
STEP: Creating a pod to test consume secrets
Mar  5 19:54:58.044: INFO: Waiting up to 5m0s for pod "pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7" in namespace "secrets-29" to be "success or failure"
Mar  5 19:54:58.050: INFO: Pod "pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.545264ms
Mar  5 19:55:00.055: INFO: Pod "pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010866566s
STEP: Saw pod success
Mar  5 19:55:00.055: INFO: Pod "pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7" satisfied condition "success or failure"
Mar  5 19:55:00.060: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:55:00.087: INFO: Waiting for pod pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7 to disappear
Mar  5 19:55:00.092: INFO: Pod pod-secrets-b77bc733-5296-4f0a-b5bd-149b5f4331c7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:55:00.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-29" for this suite.
Mar  5 19:55:06.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:55:06.285: INFO: namespace secrets-29 deletion completed in 6.185966494s

• [SLOW TEST:8.416 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:55:06.286: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  5 19:55:06.451: INFO: Waiting up to 5m0s for pod "downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94" in namespace "downward-api-8924" to be "success or failure"
Mar  5 19:55:06.457: INFO: Pod "downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195167ms
Mar  5 19:55:08.462: INFO: Pod "downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010799207s
STEP: Saw pod success
Mar  5 19:55:08.462: INFO: Pod "downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94" satisfied condition "success or failure"
Mar  5 19:55:08.467: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94 container dapi-container: <nil>
STEP: delete the pod
Mar  5 19:55:08.497: INFO: Waiting for pod downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94 to disappear
Mar  5 19:55:08.501: INFO: Pod downward-api-d5930cf4-1a0b-48c9-b93c-7b499b150f94 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:55:08.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8924" for this suite.
Mar  5 19:55:14.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:55:14.696: INFO: namespace downward-api-8924 deletion completed in 6.185685276s

• [SLOW TEST:8.410 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:55:14.696: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:55:14.846: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  5 19:55:14.857: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  5 19:55:19.862: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 19:55:19.862: INFO: Creating deployment "test-rolling-update-deployment"
Mar  5 19:55:19.869: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  5 19:55:19.878: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  5 19:55:21.888: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  5 19:55:21.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719034919, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719034919, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719034919, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719034919, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 19:55:23.898: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  5 19:55:23.913: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-3172,SelfLink:/apis/apps/v1/namespaces/deployment-3172/deployments/test-rolling-update-deployment,UID:d377ba10-a5e6-4622-a5bc-f2a73d7f0a2a,ResourceVersion:655044,Generation:1,CreationTimestamp:2020-03-05 19:55:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-05 19:55:19 +0000 UTC 2020-03-05 19:55:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-05 19:55:22 +0000 UTC 2020-03-05 19:55:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 19:55:23.919: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-3172,SelfLink:/apis/apps/v1/namespaces/deployment-3172/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:2a954c44-a2db-4968-bed7-30b4bba2b1db,ResourceVersion:655033,Generation:1,CreationTimestamp:2020-03-05 19:55:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d377ba10-a5e6-4622-a5bc-f2a73d7f0a2a 0xc0022b3557 0xc0022b3558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 19:55:23.919: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  5 19:55:23.919: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-3172,SelfLink:/apis/apps/v1/namespaces/deployment-3172/replicasets/test-rolling-update-controller,UID:54e02ed1-63f3-4d8b-8fb7-5c024e793c67,ResourceVersion:655043,Generation:2,CreationTimestamp:2020-03-05 19:55:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d377ba10-a5e6-4622-a5bc-f2a73d7f0a2a 0xc0022b3487 0xc0022b3488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 19:55:23.924: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-rphbf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-rphbf,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-3172,SelfLink:/api/v1/namespaces/deployment-3172/pods/test-rolling-update-deployment-79f6b9d75c-rphbf,UID:8cff94ed-86d2-4a16-bad5-bc7d8878d86d,ResourceVersion:655032,Generation:0,CreationTimestamp:2020-03-05 19:55:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 2a954c44-a2db-4968-bed7-30b4bba2b1db 0xc0022b3e57 0xc0022b3e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z5gx5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z5gx5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-z5gx5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-155-60.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022b3ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022b3ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:55:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:55:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:55:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 19:55:19 +0000 UTC  }],Message:,Reason:,HostIP:192.168.155.60,PodIP:192.168.158.25,StartTime:2020-03-05 19:55:19 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-05 19:55:20 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e4b825ad48561b82d3acfb10026dfe789da55efd0a5e3b390d7af2e52c077d63}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:55:23.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3172" for this suite.
Mar  5 19:55:29.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:55:30.112: INFO: namespace deployment-3172 deletion completed in 6.18006395s

• [SLOW TEST:15.416 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:55:30.113: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2886
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-34e009ad-e093-4e3a-b0e1-e89fc8053f5e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-34e009ad-e093-4e3a-b0e1-e89fc8053f5e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:55:34.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2886" for this suite.
Mar  5 19:55:56.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:55:56.538: INFO: namespace configmap-2886 deletion completed in 22.180814762s

• [SLOW TEST:26.425 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:55:56.538: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5b2b5625-79eb-4f60-891e-6e4a51494655
STEP: Creating a pod to test consume configMaps
Mar  5 19:55:56.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6" in namespace "projected-6938" to be "success or failure"
Mar  5 19:55:56.710: INFO: Pod "pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550128ms
Mar  5 19:55:58.716: INFO: Pod "pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010038723s
STEP: Saw pod success
Mar  5 19:55:58.716: INFO: Pod "pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6" satisfied condition "success or failure"
Mar  5 19:55:58.720: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:55:58.749: INFO: Waiting for pod pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6 to disappear
Mar  5 19:55:58.753: INFO: Pod pod-projected-configmaps-5a139c8c-3c94-414f-b4bd-d346141ce8b6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:55:58.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6938" for this suite.
Mar  5 19:56:04.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:56:04.947: INFO: namespace projected-6938 deletion completed in 6.183796776s

• [SLOW TEST:8.408 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:56:04.947: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  5 19:56:05.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-8837'
Mar  5 19:56:05.311: INFO: stderr: ""
Mar  5 19:56:05.311: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 19:56:05.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8837'
Mar  5 19:56:05.398: INFO: stderr: ""
Mar  5 19:56:05.399: INFO: stdout: "update-demo-nautilus-48rww update-demo-nautilus-wgzrz "
Mar  5 19:56:05.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-48rww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8837'
Mar  5 19:56:05.477: INFO: stderr: ""
Mar  5 19:56:05.477: INFO: stdout: ""
Mar  5 19:56:05.477: INFO: update-demo-nautilus-48rww is created but not running
Mar  5 19:56:10.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8837'
Mar  5 19:56:10.556: INFO: stderr: ""
Mar  5 19:56:10.556: INFO: stdout: "update-demo-nautilus-48rww update-demo-nautilus-wgzrz "
Mar  5 19:56:10.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-48rww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8837'
Mar  5 19:56:10.630: INFO: stderr: ""
Mar  5 19:56:10.630: INFO: stdout: "true"
Mar  5 19:56:10.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-48rww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8837'
Mar  5 19:56:10.708: INFO: stderr: ""
Mar  5 19:56:10.708: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:56:10.708: INFO: validating pod update-demo-nautilus-48rww
Mar  5 19:56:10.714: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:56:10.714: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:56:10.714: INFO: update-demo-nautilus-48rww is verified up and running
Mar  5 19:56:10.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-wgzrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8837'
Mar  5 19:56:10.814: INFO: stderr: ""
Mar  5 19:56:10.814: INFO: stdout: "true"
Mar  5 19:56:10.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods update-demo-nautilus-wgzrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8837'
Mar  5 19:56:10.900: INFO: stderr: ""
Mar  5 19:56:10.900: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 19:56:10.900: INFO: validating pod update-demo-nautilus-wgzrz
Mar  5 19:56:10.907: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 19:56:10.907: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 19:56:10.907: INFO: update-demo-nautilus-wgzrz is verified up and running
STEP: using delete to clean up resources
Mar  5 19:56:10.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-8837'
Mar  5 19:56:10.992: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:56:10.992: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  5 19:56:10.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8837'
Mar  5 19:56:11.076: INFO: stderr: "No resources found.\n"
Mar  5 19:56:11.076: INFO: stdout: ""
Mar  5 19:56:11.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -l name=update-demo --namespace=kubectl-8837 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 19:56:11.157: INFO: stderr: ""
Mar  5 19:56:11.157: INFO: stdout: "update-demo-nautilus-48rww\nupdate-demo-nautilus-wgzrz\n"
Mar  5 19:56:11.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8837'
Mar  5 19:56:11.749: INFO: stderr: "No resources found.\n"
Mar  5 19:56:11.749: INFO: stdout: ""
Mar  5 19:56:11.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -l name=update-demo --namespace=kubectl-8837 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 19:56:11.832: INFO: stderr: ""
Mar  5 19:56:11.832: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:56:11.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8837" for this suite.
Mar  5 19:56:33.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:56:34.021: INFO: namespace kubectl-8837 deletion completed in 22.181435027s

• [SLOW TEST:29.074 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:56:34.021: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8443
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:56:38.221: INFO: Waiting up to 5m0s for pod "client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62" in namespace "pods-8443" to be "success or failure"
Mar  5 19:56:38.227: INFO: Pod "client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.268227ms
Mar  5 19:56:40.232: INFO: Pod "client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010946869s
STEP: Saw pod success
Mar  5 19:56:40.232: INFO: Pod "client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62" satisfied condition "success or failure"
Mar  5 19:56:40.237: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62 container env3cont: <nil>
STEP: delete the pod
Mar  5 19:56:40.272: INFO: Waiting for pod client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62 to disappear
Mar  5 19:56:40.278: INFO: Pod client-envvars-b6107693-0950-425f-890c-8d9bb28a2b62 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:56:40.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8443" for this suite.
Mar  5 19:57:32.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:57:32.468: INFO: namespace pods-8443 deletion completed in 52.181926404s

• [SLOW TEST:58.446 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:57:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 19:57:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-634'
Mar  5 19:57:32.706: INFO: stderr: ""
Mar  5 19:57:32.706: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Mar  5 19:57:32.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete pods e2e-test-nginx-pod --namespace=kubectl-634'
Mar  5 19:57:42.445: INFO: stderr: ""
Mar  5 19:57:42.445: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:57:42.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-634" for this suite.
Mar  5 19:57:48.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:57:48.629: INFO: namespace kubectl-634 deletion completed in 6.176847982s

• [SLOW TEST:16.162 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:57:48.630: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f3f9ccd8-6c65-4778-8bfe-dc5cfa71cbc7
STEP: Creating a pod to test consume configMaps
Mar  5 19:57:48.797: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa" in namespace "projected-6255" to be "success or failure"
Mar  5 19:57:48.805: INFO: Pod "pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.902256ms
Mar  5 19:57:50.811: INFO: Pod "pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014431042s
Mar  5 19:57:52.816: INFO: Pod "pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019755437s
STEP: Saw pod success
Mar  5 19:57:52.816: INFO: Pod "pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa" satisfied condition "success or failure"
Mar  5 19:57:52.821: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 19:57:52.851: INFO: Waiting for pod pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa to disappear
Mar  5 19:57:52.855: INFO: Pod pod-projected-configmaps-804efb79-b71d-47a6-a571-61020fe9b2aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:57:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6255" for this suite.
Mar  5 19:57:58.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:57:59.043: INFO: namespace projected-6255 deletion completed in 6.180219977s

• [SLOW TEST:10.414 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:57:59.044: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Mar  5 19:57:59.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-9132'
Mar  5 19:57:59.416: INFO: stderr: ""
Mar  5 19:57:59.416: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Mar  5 19:58:00.422: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:58:00.422: INFO: Found 0 / 1
Mar  5 19:58:01.422: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:58:01.422: INFO: Found 1 / 1
Mar  5 19:58:01.422: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 19:58:01.427: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 19:58:01.427: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  5 19:58:01.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132'
Mar  5 19:58:01.530: INFO: stderr: ""
Mar  5 19:58:01.530: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Mar 19:58:00.494 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Mar 19:58:00.494 # Server started, Redis version 3.2.12\n1:M 05 Mar 19:58:00.494 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Mar 19:58:00.494 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  5 19:58:01.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132 --tail=1'
Mar  5 19:58:01.647: INFO: stderr: ""
Mar  5 19:58:01.647: INFO: stdout: "1:M 05 Mar 19:58:00.494 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  5 19:58:01.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132 --limit-bytes=1'
Mar  5 19:58:01.746: INFO: stderr: ""
Mar  5 19:58:01.746: INFO: stdout: " "
STEP: exposing timestamps
Mar  5 19:58:01.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132 --tail=1 --timestamps'
Mar  5 19:58:01.848: INFO: stderr: ""
Mar  5 19:58:01.848: INFO: stdout: "2020-03-05T19:58:00.494858519Z 1:M 05 Mar 19:58:00.494 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  5 19:58:04.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132 --since=1s'
Mar  5 19:58:04.447: INFO: stderr: ""
Mar  5 19:58:04.447: INFO: stdout: ""
Mar  5 19:58:04.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs redis-master-fdjjt redis-master --namespace=kubectl-9132 --since=24h'
Mar  5 19:58:04.545: INFO: stderr: ""
Mar  5 19:58:04.545: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Mar 19:58:00.494 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Mar 19:58:00.494 # Server started, Redis version 3.2.12\n1:M 05 Mar 19:58:00.494 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Mar 19:58:00.494 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Mar  5 19:58:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete --grace-period=0 --force -f - --namespace=kubectl-9132'
Mar  5 19:58:04.632: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 19:58:04.632: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  5 19:58:04.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get rc,svc -l name=nginx --no-headers --namespace=kubectl-9132'
Mar  5 19:58:04.718: INFO: stderr: "No resources found.\n"
Mar  5 19:58:04.718: INFO: stdout: ""
Mar  5 19:58:04.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 get pods -l name=nginx --namespace=kubectl-9132 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 19:58:04.797: INFO: stderr: ""
Mar  5 19:58:04.797: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:58:04.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9132" for this suite.
Mar  5 19:58:18.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:58:18.983: INFO: namespace kubectl-9132 deletion completed in 14.177606035s

• [SLOW TEST:19.939 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:58:18.984: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-b8c63ddc-1d0a-41aa-a495-819920de1fac
STEP: Creating a pod to test consume secrets
Mar  5 19:58:19.152: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b" in namespace "projected-8384" to be "success or failure"
Mar  5 19:58:19.156: INFO: Pod "pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651848ms
Mar  5 19:58:21.162: INFO: Pod "pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010248485s
STEP: Saw pod success
Mar  5 19:58:21.162: INFO: Pod "pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b" satisfied condition "success or failure"
Mar  5 19:58:21.167: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 19:58:21.196: INFO: Waiting for pod pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b to disappear
Mar  5 19:58:21.201: INFO: Pod pod-projected-secrets-9f151dc2-678d-418f-aff4-b62caf6e535b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:58:21.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8384" for this suite.
Mar  5 19:58:27.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:58:27.388: INFO: namespace projected-8384 deletion completed in 6.179517316s

• [SLOW TEST:8.404 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:58:27.388: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9940/configmap-test-189f391a-75f0-4ae9-a346-3f16023c02f3
STEP: Creating a pod to test consume configMaps
Mar  5 19:58:27.556: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4" in namespace "configmap-9940" to be "success or failure"
Mar  5 19:58:27.561: INFO: Pod "pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.548351ms
Mar  5 19:58:29.566: INFO: Pod "pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009527633s
STEP: Saw pod success
Mar  5 19:58:29.566: INFO: Pod "pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4" satisfied condition "success or failure"
Mar  5 19:58:29.570: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4 container env-test: <nil>
STEP: delete the pod
Mar  5 19:58:29.601: INFO: Waiting for pod pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4 to disappear
Mar  5 19:58:29.605: INFO: Pod pod-configmaps-b7a9251a-5165-40e1-bade-ecee41fa82f4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:58:29.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9940" for this suite.
Mar  5 19:58:35.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:58:35.790: INFO: namespace configmap-9940 deletion completed in 6.176951497s

• [SLOW TEST:8.402 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:58:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:58:35.939: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  5 19:58:36.980: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:58:37.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-759" for this suite.
Mar  5 19:58:44.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:58:44.177: INFO: namespace replication-controller-759 deletion completed in 6.178846646s

• [SLOW TEST:8.388 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:58:44.178: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 19:58:44.358: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5d2c5bfa-5dd1-465a-b7fc-f488025f84f1", Controller:(*bool)(0xc002074b72), BlockOwnerDeletion:(*bool)(0xc002074b73)}}
Mar  5 19:58:44.364: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f4b8c62c-8dab-4499-930f-273370b88c50", Controller:(*bool)(0xc002074d32), BlockOwnerDeletion:(*bool)(0xc002074d33)}}
Mar  5 19:58:44.370: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4491ea4c-efc5-4866-86f0-8147893fbb73", Controller:(*bool)(0xc002754e2a), BlockOwnerDeletion:(*bool)(0xc002754e2b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:58:49.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3146" for this suite.
Mar  5 19:58:55.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:58:55.573: INFO: namespace gc-3146 deletion completed in 6.181224679s

• [SLOW TEST:11.395 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:58:55.573: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vp5d4 in namespace proxy-9085
I0305 19:58:55.746583      20 runners.go:180] Created replication controller with name: proxy-service-vp5d4, namespace: proxy-9085, replica count: 1
I0305 19:58:56.797086      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 19:58:57.797316      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 19:58:58.797560      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 19:58:59.797840      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:00.798085      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:01.798314      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:02.798653      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:03.798907      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:04.799578      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:05.805092      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:06.805356      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:07.805597      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:08.805850      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 19:59:09.806076      20 runners.go:180] proxy-service-vp5d4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 19:59:09.811: INFO: setup took 14.088938701s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  5 19:59:09.830: INFO: (0) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 17.984045ms)
Mar  5 19:59:09.830: INFO: (0) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 18.003277ms)
Mar  5 19:59:09.831: INFO: (0) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 18.346023ms)
Mar  5 19:59:09.831: INFO: (0) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 18.088445ms)
Mar  5 19:59:09.831: INFO: (0) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 19.121864ms)
Mar  5 19:59:09.832: INFO: (0) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 19.827901ms)
Mar  5 19:59:09.832: INFO: (0) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 19.768434ms)
Mar  5 19:59:09.832: INFO: (0) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 19.703613ms)
Mar  5 19:59:09.832: INFO: (0) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 20.524265ms)
Mar  5 19:59:09.833: INFO: (0) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 21.366652ms)
Mar  5 19:59:09.834: INFO: (0) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 21.268535ms)
Mar  5 19:59:09.834: INFO: (0) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 21.649104ms)
Mar  5 19:59:09.834: INFO: (0) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 21.438764ms)
Mar  5 19:59:09.834: INFO: (0) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 21.52585ms)
Mar  5 19:59:09.834: INFO: (0) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 22.462751ms)
Mar  5 19:59:09.838: INFO: (0) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 25.628336ms)
Mar  5 19:59:09.844: INFO: (1) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.969948ms)
Mar  5 19:59:09.844: INFO: (1) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.09292ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.825801ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 7.180883ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.033066ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.046535ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 7.304081ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 7.397818ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 7.544347ms)
Mar  5 19:59:09.846: INFO: (1) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 7.618638ms)
Mar  5 19:59:09.848: INFO: (1) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.205641ms)
Mar  5 19:59:09.850: INFO: (1) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.814203ms)
Mar  5 19:59:09.850: INFO: (1) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.102254ms)
Mar  5 19:59:09.850: INFO: (1) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 11.662922ms)
Mar  5 19:59:09.851: INFO: (1) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 12.066163ms)
Mar  5 19:59:09.851: INFO: (1) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 12.389515ms)
Mar  5 19:59:09.857: INFO: (2) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 5.798177ms)
Mar  5 19:59:09.857: INFO: (2) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.92367ms)
Mar  5 19:59:09.858: INFO: (2) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.694706ms)
Mar  5 19:59:09.858: INFO: (2) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.358754ms)
Mar  5 19:59:09.858: INFO: (2) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.876558ms)
Mar  5 19:59:09.858: INFO: (2) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.777888ms)
Mar  5 19:59:09.858: INFO: (2) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 7.258741ms)
Mar  5 19:59:09.859: INFO: (2) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 7.577285ms)
Mar  5 19:59:09.859: INFO: (2) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.517216ms)
Mar  5 19:59:09.859: INFO: (2) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.503797ms)
Mar  5 19:59:09.860: INFO: (2) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 8.9157ms)
Mar  5 19:59:09.862: INFO: (2) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.488928ms)
Mar  5 19:59:09.862: INFO: (2) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.548945ms)
Mar  5 19:59:09.863: INFO: (2) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 11.086661ms)
Mar  5 19:59:09.863: INFO: (2) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 11.359817ms)
Mar  5 19:59:09.863: INFO: (2) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.627545ms)
Mar  5 19:59:09.868: INFO: (3) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.085465ms)
Mar  5 19:59:09.869: INFO: (3) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 5.681252ms)
Mar  5 19:59:09.869: INFO: (3) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.566789ms)
Mar  5 19:59:09.870: INFO: (3) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 5.911175ms)
Mar  5 19:59:09.870: INFO: (3) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.261464ms)
Mar  5 19:59:09.870: INFO: (3) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.162756ms)
Mar  5 19:59:09.870: INFO: (3) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 7.357791ms)
Mar  5 19:59:09.871: INFO: (3) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.480426ms)
Mar  5 19:59:09.871: INFO: (3) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.90021ms)
Mar  5 19:59:09.871: INFO: (3) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.852576ms)
Mar  5 19:59:09.872: INFO: (3) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 9.045077ms)
Mar  5 19:59:09.875: INFO: (3) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 11.08764ms)
Mar  5 19:59:09.876: INFO: (3) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 11.903467ms)
Mar  5 19:59:09.876: INFO: (3) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.905074ms)
Mar  5 19:59:09.876: INFO: (3) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 12.54433ms)
Mar  5 19:59:09.876: INFO: (3) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 12.004836ms)
Mar  5 19:59:09.882: INFO: (4) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 5.453036ms)
Mar  5 19:59:09.882: INFO: (4) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 5.605166ms)
Mar  5 19:59:09.882: INFO: (4) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.811745ms)
Mar  5 19:59:09.884: INFO: (4) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 7.221132ms)
Mar  5 19:59:09.884: INFO: (4) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.949396ms)
Mar  5 19:59:09.884: INFO: (4) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 7.459387ms)
Mar  5 19:59:09.885: INFO: (4) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 7.465039ms)
Mar  5 19:59:09.885: INFO: (4) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.845927ms)
Mar  5 19:59:09.885: INFO: (4) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 7.487023ms)
Mar  5 19:59:09.885: INFO: (4) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 7.988963ms)
Mar  5 19:59:09.885: INFO: (4) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 8.78136ms)
Mar  5 19:59:09.886: INFO: (4) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 9.926753ms)
Mar  5 19:59:09.888: INFO: (4) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.42511ms)
Mar  5 19:59:09.889: INFO: (4) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 12.271926ms)
Mar  5 19:59:09.889: INFO: (4) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 12.028485ms)
Mar  5 19:59:09.889: INFO: (4) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 12.012191ms)
Mar  5 19:59:09.895: INFO: (5) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 5.192851ms)
Mar  5 19:59:09.896: INFO: (5) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.681579ms)
Mar  5 19:59:09.896: INFO: (5) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.157942ms)
Mar  5 19:59:09.896: INFO: (5) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.344373ms)
Mar  5 19:59:09.896: INFO: (5) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.282369ms)
Mar  5 19:59:09.896: INFO: (5) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.027295ms)
Mar  5 19:59:09.897: INFO: (5) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.39414ms)
Mar  5 19:59:09.897: INFO: (5) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.757886ms)
Mar  5 19:59:09.897: INFO: (5) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.659364ms)
Mar  5 19:59:09.897: INFO: (5) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 7.388652ms)
Mar  5 19:59:09.898: INFO: (5) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 8.106526ms)
Mar  5 19:59:09.899: INFO: (5) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.007603ms)
Mar  5 19:59:09.900: INFO: (5) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.168842ms)
Mar  5 19:59:09.900: INFO: (5) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.071602ms)
Mar  5 19:59:09.900: INFO: (5) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.105529ms)
Mar  5 19:59:09.900: INFO: (5) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.247219ms)
Mar  5 19:59:09.905: INFO: (6) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.035555ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.479689ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.917928ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.907413ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.052137ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.284817ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.26953ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.141483ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.084977ms)
Mar  5 19:59:09.907: INFO: (6) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.724555ms)
Mar  5 19:59:09.909: INFO: (6) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 7.997622ms)
Mar  5 19:59:09.911: INFO: (6) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.974122ms)
Mar  5 19:59:09.912: INFO: (6) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.944466ms)
Mar  5 19:59:09.912: INFO: (6) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 11.371427ms)
Mar  5 19:59:09.912: INFO: (6) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.981588ms)
Mar  5 19:59:09.912: INFO: (6) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 11.547115ms)
Mar  5 19:59:09.917: INFO: (7) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 4.933053ms)
Mar  5 19:59:09.919: INFO: (7) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.157089ms)
Mar  5 19:59:09.919: INFO: (7) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.891661ms)
Mar  5 19:59:09.919: INFO: (7) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.96202ms)
Mar  5 19:59:09.919: INFO: (7) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.152046ms)
Mar  5 19:59:09.919: INFO: (7) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 7.31805ms)
Mar  5 19:59:09.920: INFO: (7) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.135259ms)
Mar  5 19:59:09.920: INFO: (7) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.644711ms)
Mar  5 19:59:09.920: INFO: (7) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.443302ms)
Mar  5 19:59:09.920: INFO: (7) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.315734ms)
Mar  5 19:59:09.922: INFO: (7) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 9.719549ms)
Mar  5 19:59:09.923: INFO: (7) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 9.686807ms)
Mar  5 19:59:09.923: INFO: (7) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.563553ms)
Mar  5 19:59:09.923: INFO: (7) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.664096ms)
Mar  5 19:59:09.923: INFO: (7) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.751034ms)
Mar  5 19:59:09.923: INFO: (7) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.235013ms)
Mar  5 19:59:09.929: INFO: (8) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 4.863347ms)
Mar  5 19:59:09.930: INFO: (8) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.285908ms)
Mar  5 19:59:09.930: INFO: (8) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.24373ms)
Mar  5 19:59:09.930: INFO: (8) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.364866ms)
Mar  5 19:59:09.930: INFO: (8) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 5.877406ms)
Mar  5 19:59:09.931: INFO: (8) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.552654ms)
Mar  5 19:59:09.931: INFO: (8) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.302933ms)
Mar  5 19:59:09.931: INFO: (8) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.132818ms)
Mar  5 19:59:09.931: INFO: (8) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.674616ms)
Mar  5 19:59:09.931: INFO: (8) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.728419ms)
Mar  5 19:59:09.932: INFO: (8) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 8.34878ms)
Mar  5 19:59:09.933: INFO: (8) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 9.232464ms)
Mar  5 19:59:09.936: INFO: (8) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 11.219413ms)
Mar  5 19:59:09.936: INFO: (8) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 11.107219ms)
Mar  5 19:59:09.936: INFO: (8) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 11.035595ms)
Mar  5 19:59:09.936: INFO: (8) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.687819ms)
Mar  5 19:59:09.941: INFO: (9) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 4.891109ms)
Mar  5 19:59:09.942: INFO: (9) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 5.825593ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 5.653321ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.312248ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.609471ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.628889ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.39557ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 7.117187ms)
Mar  5 19:59:09.943: INFO: (9) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.607968ms)
Mar  5 19:59:09.944: INFO: (9) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.807847ms)
Mar  5 19:59:09.944: INFO: (9) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 8.056439ms)
Mar  5 19:59:09.946: INFO: (9) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.664419ms)
Mar  5 19:59:09.947: INFO: (9) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 9.492514ms)
Mar  5 19:59:09.947: INFO: (9) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.057236ms)
Mar  5 19:59:09.948: INFO: (9) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.387263ms)
Mar  5 19:59:09.948: INFO: (9) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 11.538722ms)
Mar  5 19:59:09.953: INFO: (10) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 5.000835ms)
Mar  5 19:59:09.954: INFO: (10) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.708087ms)
Mar  5 19:59:09.954: INFO: (10) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.040487ms)
Mar  5 19:59:09.954: INFO: (10) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.377596ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.00536ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.420139ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.707088ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.936673ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.937113ms)
Mar  5 19:59:09.955: INFO: (10) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.498294ms)
Mar  5 19:59:09.956: INFO: (10) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 8.249016ms)
Mar  5 19:59:09.957: INFO: (10) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 9.217447ms)
Mar  5 19:59:09.957: INFO: (10) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 9.437293ms)
Mar  5 19:59:09.958: INFO: (10) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.068303ms)
Mar  5 19:59:09.958: INFO: (10) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 9.971953ms)
Mar  5 19:59:09.958: INFO: (10) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 9.891214ms)
Mar  5 19:59:09.964: INFO: (11) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.085336ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.393825ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.226851ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.142754ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.246677ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.217705ms)
Mar  5 19:59:09.965: INFO: (11) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.532175ms)
Mar  5 19:59:09.966: INFO: (11) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.548885ms)
Mar  5 19:59:09.966: INFO: (11) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.540383ms)
Mar  5 19:59:09.966: INFO: (11) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.651776ms)
Mar  5 19:59:09.967: INFO: (11) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 8.079078ms)
Mar  5 19:59:09.967: INFO: (11) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 8.356731ms)
Mar  5 19:59:09.969: INFO: (11) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 9.460765ms)
Mar  5 19:59:09.969: INFO: (11) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.796289ms)
Mar  5 19:59:09.969: INFO: (11) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.288322ms)
Mar  5 19:59:09.970: INFO: (11) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.554081ms)
Mar  5 19:59:09.975: INFO: (12) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 4.95704ms)
Mar  5 19:59:09.976: INFO: (12) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.428322ms)
Mar  5 19:59:09.976: INFO: (12) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.039694ms)
Mar  5 19:59:09.976: INFO: (12) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.536995ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.230857ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.370248ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.830913ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.664825ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.968276ms)
Mar  5 19:59:09.977: INFO: (12) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.548614ms)
Mar  5 19:59:09.978: INFO: (12) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 8.131234ms)
Mar  5 19:59:09.979: INFO: (12) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 8.900852ms)
Mar  5 19:59:09.980: INFO: (12) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.0376ms)
Mar  5 19:59:09.980: INFO: (12) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 9.911126ms)
Mar  5 19:59:09.981: INFO: (12) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.880323ms)
Mar  5 19:59:09.981: INFO: (12) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 11.052875ms)
Mar  5 19:59:09.986: INFO: (13) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.023489ms)
Mar  5 19:59:09.987: INFO: (13) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.241655ms)
Mar  5 19:59:09.987: INFO: (13) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.199895ms)
Mar  5 19:59:09.987: INFO: (13) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 5.561326ms)
Mar  5 19:59:09.988: INFO: (13) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.024474ms)
Mar  5 19:59:09.988: INFO: (13) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.748265ms)
Mar  5 19:59:09.988: INFO: (13) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.889327ms)
Mar  5 19:59:09.988: INFO: (13) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.011534ms)
Mar  5 19:59:09.988: INFO: (13) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.873535ms)
Mar  5 19:59:09.989: INFO: (13) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.924659ms)
Mar  5 19:59:09.989: INFO: (13) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 7.978167ms)
Mar  5 19:59:09.990: INFO: (13) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 8.999513ms)
Mar  5 19:59:09.991: INFO: (13) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 9.616799ms)
Mar  5 19:59:09.992: INFO: (13) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.352482ms)
Mar  5 19:59:09.992: INFO: (13) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.777251ms)
Mar  5 19:59:09.993: INFO: (13) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.273933ms)
Mar  5 19:59:09.998: INFO: (14) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.173133ms)
Mar  5 19:59:09.998: INFO: (14) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 5.336598ms)
Mar  5 19:59:09.999: INFO: (14) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.687932ms)
Mar  5 19:59:09.999: INFO: (14) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.283265ms)
Mar  5 19:59:09.999: INFO: (14) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.177632ms)
Mar  5 19:59:09.999: INFO: (14) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.030907ms)
Mar  5 19:59:09.999: INFO: (14) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.935732ms)
Mar  5 19:59:10.000: INFO: (14) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.533752ms)
Mar  5 19:59:10.000: INFO: (14) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 5.957558ms)
Mar  5 19:59:10.000: INFO: (14) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 5.759483ms)
Mar  5 19:59:10.000: INFO: (14) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 7.823591ms)
Mar  5 19:59:10.003: INFO: (14) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 9.361858ms)
Mar  5 19:59:10.003: INFO: (14) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.713098ms)
Mar  5 19:59:10.004: INFO: (14) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.130257ms)
Mar  5 19:59:10.004: INFO: (14) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.743008ms)
Mar  5 19:59:10.004: INFO: (14) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.046013ms)
Mar  5 19:59:10.009: INFO: (15) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 4.969169ms)
Mar  5 19:59:10.010: INFO: (15) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.748522ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.090165ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.445107ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.607218ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.122275ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.52153ms)
Mar  5 19:59:10.011: INFO: (15) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.134248ms)
Mar  5 19:59:10.012: INFO: (15) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.886436ms)
Mar  5 19:59:10.012: INFO: (15) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.934515ms)
Mar  5 19:59:10.012: INFO: (15) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 8.502764ms)
Mar  5 19:59:10.012: INFO: (15) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 8.53016ms)
Mar  5 19:59:10.014: INFO: (15) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 10.075448ms)
Mar  5 19:59:10.015: INFO: (15) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.287436ms)
Mar  5 19:59:10.014: INFO: (15) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.173032ms)
Mar  5 19:59:10.015: INFO: (15) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.794046ms)
Mar  5 19:59:10.021: INFO: (16) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 4.952739ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.707291ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.257468ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.245295ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.052997ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.371484ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.671187ms)
Mar  5 19:59:10.022: INFO: (16) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.700669ms)
Mar  5 19:59:10.024: INFO: (16) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 7.446128ms)
Mar  5 19:59:10.024: INFO: (16) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 7.299516ms)
Mar  5 19:59:10.024: INFO: (16) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 8.443371ms)
Mar  5 19:59:10.026: INFO: (16) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.201844ms)
Mar  5 19:59:10.026: INFO: (16) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.419108ms)
Mar  5 19:59:10.027: INFO: (16) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.602109ms)
Mar  5 19:59:10.027: INFO: (16) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.362942ms)
Mar  5 19:59:10.027: INFO: (16) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.216062ms)
Mar  5 19:59:10.032: INFO: (17) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.11536ms)
Mar  5 19:59:10.033: INFO: (17) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.924778ms)
Mar  5 19:59:10.033: INFO: (17) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.167301ms)
Mar  5 19:59:10.033: INFO: (17) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.744511ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.986273ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 6.318175ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.317764ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.407041ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.297469ms)
Mar  5 19:59:10.034: INFO: (17) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.26438ms)
Mar  5 19:59:10.035: INFO: (17) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 8.069387ms)
Mar  5 19:59:10.035: INFO: (17) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 8.283035ms)
Mar  5 19:59:10.037: INFO: (17) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.061975ms)
Mar  5 19:59:10.037: INFO: (17) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 9.851947ms)
Mar  5 19:59:10.038: INFO: (17) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 11.154365ms)
Mar  5 19:59:10.038: INFO: (17) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.70913ms)
Mar  5 19:59:10.044: INFO: (18) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 4.925322ms)
Mar  5 19:59:10.044: INFO: (18) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.109551ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.435091ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 5.698081ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.306744ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 5.557633ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 5.859985ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.444055ms)
Mar  5 19:59:10.045: INFO: (18) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.088325ms)
Mar  5 19:59:10.046: INFO: (18) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.174736ms)
Mar  5 19:59:10.047: INFO: (18) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 8.238304ms)
Mar  5 19:59:10.049: INFO: (18) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 9.889798ms)
Mar  5 19:59:10.049: INFO: (18) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 9.756442ms)
Mar  5 19:59:10.050: INFO: (18) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 11.135256ms)
Mar  5 19:59:10.050: INFO: (18) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.937083ms)
Mar  5 19:59:10.050: INFO: (18) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.879708ms)
Mar  5 19:59:10.055: INFO: (19) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 5.255959ms)
Mar  5 19:59:10.056: INFO: (19) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25/proxy/rewriteme">test</a> (200; 5.202349ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.047516ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">test<... (200; 6.355528ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/http:proxy-service-vp5d4-xbg25:1080/proxy/rewriteme">... (200; 6.364142ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:162/proxy/: bar (200; 6.399733ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:462/proxy/: tls qux (200; 6.404299ms)
Mar  5 19:59:10.057: INFO: (19) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/: <a href="/api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:443/proxy/tlsrewritem... (200; 6.310708ms)
Mar  5 19:59:10.058: INFO: (19) /api/v1/namespaces/proxy-9085/pods/proxy-service-vp5d4-xbg25:160/proxy/: foo (200; 6.746048ms)
Mar  5 19:59:10.058: INFO: (19) /api/v1/namespaces/proxy-9085/pods/https:proxy-service-vp5d4-xbg25:460/proxy/: tls baz (200; 6.859453ms)
Mar  5 19:59:10.058: INFO: (19) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname2/proxy/: bar (200; 8.11448ms)
Mar  5 19:59:10.060: INFO: (19) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname2/proxy/: tls qux (200; 10.263442ms)
Mar  5 19:59:10.062: INFO: (19) /api/v1/namespaces/proxy-9085/services/http:proxy-service-vp5d4:portname1/proxy/: foo (200; 10.945623ms)
Mar  5 19:59:10.062: INFO: (19) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname2/proxy/: bar (200; 10.927064ms)
Mar  5 19:59:10.062: INFO: (19) /api/v1/namespaces/proxy-9085/services/proxy-service-vp5d4:portname1/proxy/: foo (200; 10.695056ms)
Mar  5 19:59:10.062: INFO: (19) /api/v1/namespaces/proxy-9085/services/https:proxy-service-vp5d4:tlsportname1/proxy/: tls baz (200; 10.880216ms)
STEP: deleting ReplicationController proxy-service-vp5d4 in namespace proxy-9085, will wait for the garbage collector to delete the pods
Mar  5 19:59:10.128: INFO: Deleting ReplicationController proxy-service-vp5d4 took: 11.029064ms
Mar  5 19:59:10.228: INFO: Terminating ReplicationController proxy-service-vp5d4 pods took: 100.277216ms
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:59:16.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9085" for this suite.
Mar  5 19:59:22.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:59:22.618: INFO: namespace proxy-9085 deletion completed in 6.181256939s

• [SLOW TEST:27.045 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:59:22.618: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1767
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 19:59:24.816: INFO: DNS probes using dns-test-88b8db08-2985-442e-a011-d6e485e875dc succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 19:59:28.879: INFO: DNS probes using dns-test-b9eeee51-e0ba-48ee-af8e-de364cf95536 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1767.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1767.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 19:59:42.957: INFO: DNS probes using dns-test-bb0ea18a-fdc7-44c8-9bec-7034869062ba succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:59:43.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1767" for this suite.
Mar  5 19:59:49.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:59:49.191: INFO: namespace dns-1767 deletion completed in 6.181542424s

• [SLOW TEST:26.573 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:59:49.192: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  5 19:59:49.360: INFO: Waiting up to 5m0s for pod "pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd" in namespace "emptydir-2333" to be "success or failure"
Mar  5 19:59:49.366: INFO: Pod "pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.520011ms
Mar  5 19:59:51.372: INFO: Pod "pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01123324s
STEP: Saw pod success
Mar  5 19:59:51.372: INFO: Pod "pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd" satisfied condition "success or failure"
Mar  5 19:59:51.377: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd container test-container: <nil>
STEP: delete the pod
Mar  5 19:59:51.405: INFO: Waiting for pod pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd to disappear
Mar  5 19:59:51.411: INFO: Pod pod-0e7a5aae-13e2-4c5e-9c19-9d73c51681dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 19:59:51.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2333" for this suite.
Mar  5 19:59:57.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 19:59:57.600: INFO: namespace emptydir-2333 deletion completed in 6.182360837s

• [SLOW TEST:8.409 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 19:59:57.602: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  5 19:59:57.762: INFO: Waiting up to 5m0s for pod "pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316" in namespace "emptydir-7157" to be "success or failure"
Mar  5 19:59:57.768: INFO: Pod "pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963166ms
Mar  5 19:59:59.774: INFO: Pod "pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011535077s
Mar  5 20:00:01.810: INFO: Pod "pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047589736s
STEP: Saw pod success
Mar  5 20:00:01.810: INFO: Pod "pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316" satisfied condition "success or failure"
Mar  5 20:00:01.815: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316 container test-container: <nil>
STEP: delete the pod
Mar  5 20:00:02.106: INFO: Waiting for pod pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316 to disappear
Mar  5 20:00:02.129: INFO: Pod pod-8a41669a-e9f7-4e3d-92c6-d473b0ecb316 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:00:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7157" for this suite.
Mar  5 20:00:08.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:00:08.390: INFO: namespace emptydir-7157 deletion completed in 6.191811861s

• [SLOW TEST:10.788 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:00:08.391: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:00:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:00:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5660" for this suite.
Mar  5 20:00:54.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:00:54.803: INFO: namespace pods-5660 deletion completed in 44.188050056s

• [SLOW TEST:46.412 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:00:54.803: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Mar  5 20:00:54.966: INFO: Waiting up to 5m0s for pod "client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1" in namespace "containers-830" to be "success or failure"
Mar  5 20:00:54.971: INFO: Pod "client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960619ms
Mar  5 20:00:56.976: INFO: Pod "client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010288403s
STEP: Saw pod success
Mar  5 20:00:56.976: INFO: Pod "client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1" satisfied condition "success or failure"
Mar  5 20:00:56.980: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1 container test-container: <nil>
STEP: delete the pod
Mar  5 20:00:57.010: INFO: Waiting for pod client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1 to disappear
Mar  5 20:00:57.014: INFO: Pod client-containers-35b87c9c-4a8c-40a0-9218-995bbaf811d1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:00:57.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-830" for this suite.
Mar  5 20:01:03.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:01:03.205: INFO: namespace containers-830 deletion completed in 6.182677021s

• [SLOW TEST:8.402 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:01:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:01:03.419: INFO: Create a RollingUpdate DaemonSet
Mar  5 20:01:03.426: INFO: Check that daemon pods launch on every node of the cluster
Mar  5 20:01:03.438: INFO: Number of nodes with available pods: 0
Mar  5 20:01:03.438: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:01:04.451: INFO: Number of nodes with available pods: 0
Mar  5 20:01:04.451: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:01:05.456: INFO: Number of nodes with available pods: 6
Mar  5 20:01:05.456: INFO: Number of running nodes: 6, number of available pods: 6
Mar  5 20:01:05.456: INFO: Update the DaemonSet to trigger a rollout
Mar  5 20:01:05.468: INFO: Updating DaemonSet daemon-set
Mar  5 20:01:09.492: INFO: Roll back the DaemonSet before rollout is complete
Mar  5 20:01:09.503: INFO: Updating DaemonSet daemon-set
Mar  5 20:01:09.503: INFO: Make sure DaemonSet rollback is complete
Mar  5 20:01:09.509: INFO: Wrong image for pod: daemon-set-qrzbg. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  5 20:01:09.509: INFO: Pod daemon-set-qrzbg is not available
Mar  5 20:01:10.523: INFO: Wrong image for pod: daemon-set-qrzbg. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  5 20:01:10.523: INFO: Pod daemon-set-qrzbg is not available
Mar  5 20:01:11.523: INFO: Pod daemon-set-8l8nk is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3013, will wait for the garbage collector to delete the pods
Mar  5 20:01:11.613: INFO: Deleting DaemonSet.extensions daemon-set took: 17.010692ms
Mar  5 20:01:11.913: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.24628ms
Mar  5 20:01:22.818: INFO: Number of nodes with available pods: 0
Mar  5 20:01:22.818: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 20:01:22.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3013/daemonsets","resourceVersion":"656671"},"items":null}

Mar  5 20:01:22.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3013/pods","resourceVersion":"656671"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:01:22.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3013" for this suite.
Mar  5 20:01:28.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:01:29.064: INFO: namespace daemonsets-3013 deletion completed in 6.179311036s

• [SLOW TEST:25.859 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:01:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  5 20:01:29.213: INFO: PodSpec: initContainers in spec.initContainers
Mar  5 20:02:11.816: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-38d08a31-aed2-4d82-a7aa-55ab5b6596b1", GenerateName:"", Namespace:"init-container-5973", SelfLink:"/api/v1/namespaces/init-container-5973/pods/pod-init-38d08a31-aed2-4d82-a7aa-55ab5b6596b1", UID:"9ab6454b-bb71-42ac-9e0b-6fa31c5d60b4", ResourceVersion:"656942", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63719035289, loc:(*time.Location)(0x7ed4a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"213356353"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8kjsz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00207a5c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kjsz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kjsz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kjsz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00202b298), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-192-168-112-247.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0038c4c60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00202b320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00202b340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00202b348), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00202b34c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719035289, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719035289, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719035289, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719035289, loc:(*time.Location)(0x7ed4a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.112.247", PodIP:"192.168.75.119", StartTime:(*v1.Time)(0xc002586920), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001437810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001437880)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2c45f85aa02feb85c7f4d57f15fa1498d554b15d8f80fcf04f69a407a5cb37f3"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002586960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002586940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:02:11.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5973" for this suite.
Mar  5 20:02:33.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:02:34.012: INFO: namespace init-container-5973 deletion completed in 22.187303952s

• [SLOW TEST:64.947 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:02:34.013: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  5 20:02:37.213: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:02:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7953" for this suite.
Mar  5 20:03:00.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:03:00.425: INFO: namespace replicaset-7953 deletion completed in 22.183043037s

• [SLOW TEST:26.413 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:03:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 20:03:00.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-896'
Mar  5 20:03:00.661: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 20:03:00.661: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Mar  5 20:03:04.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete deployment e2e-test-nginx-deployment --namespace=kubectl-896'
Mar  5 20:03:04.759: INFO: stderr: ""
Mar  5 20:03:04.759: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:03:04.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-896" for this suite.
Mar  5 20:03:18.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:03:18.946: INFO: namespace kubectl-896 deletion completed in 14.179579474s

• [SLOW TEST:18.521 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:03:18.947: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 20:03:19.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9229'
Mar  5 20:03:19.180: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 20:03:19.180: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Mar  5 20:03:19.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete jobs e2e-test-nginx-job --namespace=kubectl-9229'
Mar  5 20:03:19.282: INFO: stderr: ""
Mar  5 20:03:19.282: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:03:19.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9229" for this suite.
Mar  5 20:03:25.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:03:25.471: INFO: namespace kubectl-9229 deletion completed in 6.180519928s

• [SLOW TEST:6.525 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:03:25.471: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 20:03:25.699: INFO: Number of nodes with available pods: 0
Mar  5 20:03:25.699: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:03:26.713: INFO: Number of nodes with available pods: 0
Mar  5 20:03:26.713: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:03:27.712: INFO: Number of nodes with available pods: 5
Mar  5 20:03:27.712: INFO: Node ip-192-168-155-60.ec2.internal is running more than one daemon pod
Mar  5 20:03:28.712: INFO: Number of nodes with available pods: 6
Mar  5 20:03:28.712: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  5 20:03:28.741: INFO: Number of nodes with available pods: 5
Mar  5 20:03:28.741: INFO: Node ip-192-168-239-209.ec2.internal is running more than one daemon pod
Mar  5 20:03:29.755: INFO: Number of nodes with available pods: 5
Mar  5 20:03:29.755: INFO: Node ip-192-168-239-209.ec2.internal is running more than one daemon pod
Mar  5 20:03:30.755: INFO: Number of nodes with available pods: 5
Mar  5 20:03:30.755: INFO: Node ip-192-168-239-209.ec2.internal is running more than one daemon pod
Mar  5 20:03:31.755: INFO: Number of nodes with available pods: 6
Mar  5 20:03:31.755: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-232, will wait for the garbage collector to delete the pods
Mar  5 20:03:31.833: INFO: Deleting DaemonSet.extensions daemon-set took: 13.107773ms
Mar  5 20:03:31.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.254385ms
Mar  5 20:03:45.738: INFO: Number of nodes with available pods: 0
Mar  5 20:03:45.738: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 20:03:45.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-232/daemonsets","resourceVersion":"657400"},"items":null}

Mar  5 20:03:45.748: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-232/pods","resourceVersion":"657400"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:03:45.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-232" for this suite.
Mar  5 20:03:51.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:03:51.977: INFO: namespace daemonsets-232 deletion completed in 6.176862898s

• [SLOW TEST:26.506 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:03:51.978: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9594
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  5 20:03:54.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec pod-sharedvolume-553dc4ae-ca6d-4258-ae5b-4b29089cf17b -c busybox-main-container --namespace=emptydir-9594 -- cat /usr/share/volumeshare/shareddata.txt'
Mar  5 20:03:54.375: INFO: stderr: ""
Mar  5 20:03:54.375: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:03:54.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9594" for this suite.
Mar  5 20:04:00.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:04:00.562: INFO: namespace emptydir-9594 deletion completed in 6.178519971s

• [SLOW TEST:8.584 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:04:00.563: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-cd6e1a44-2f59-4556-9d09-b84cdd2e986d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:04:00.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3020" for this suite.
Mar  5 20:04:06.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:04:06.907: INFO: namespace configmap-3020 deletion completed in 6.186495161s

• [SLOW TEST:6.344 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:04:06.907: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7626
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 20:04:07.058: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 20:04:33.237: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.115.120 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:33.237: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:34.363: INFO: Found all expected endpoints: [netserver-0]
Mar  5 20:04:34.368: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.160.64 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:34.368: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:35.479: INFO: Found all expected endpoints: [netserver-1]
Mar  5 20:04:35.484: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.204.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:36.586: INFO: Found all expected endpoints: [netserver-2]
Mar  5 20:04:36.591: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.239.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:36.591: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:37.723: INFO: Found all expected endpoints: [netserver-3]
Mar  5 20:04:37.729: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.67.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:37.729: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:38.847: INFO: Found all expected endpoints: [netserver-4]
Mar  5 20:04:38.852: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.181.242 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7626 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:04:38.852: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:04:39.967: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:04:39.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7626" for this suite.
Mar  5 20:05:03.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:05:04.166: INFO: namespace pod-network-test-7626 deletion completed in 24.190591036s

• [SLOW TEST:57.258 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:05:04.167: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:05:04.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-745'
Mar  5 20:05:04.628: INFO: stderr: ""
Mar  5 20:05:04.628: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  5 20:05:04.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 create -f - --namespace=kubectl-745'
Mar  5 20:05:04.831: INFO: stderr: ""
Mar  5 20:05:04.831: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  5 20:05:05.840: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 20:05:05.840: INFO: Found 0 / 1
Mar  5 20:05:06.837: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 20:05:06.837: INFO: Found 1 / 1
Mar  5 20:05:06.837: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 20:05:06.842: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 20:05:06.842: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 20:05:06.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 describe pod redis-master-jmd8n --namespace=kubectl-745'
Mar  5 20:05:06.936: INFO: stderr: ""
Mar  5 20:05:06.936: INFO: stdout: "Name:           redis-master-jmd8n\nNamespace:      kubectl-745\nPriority:       0\nNode:           ip-192-168-112-247.ec2.internal/192.168.112.247\nStart Time:     Thu, 05 Mar 2020 20:05:04 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             192.168.106.1\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f3d99d81e0661afd8db007652d700092ceaa93f93f7a473b7ac26dabfb1cbb14\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Mar 2020 20:05:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kz696 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kz696:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kz696\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                      Message\n  ----    ------     ----  ----                                      -------\n  Normal  Scheduled  2s    default-scheduler                         Successfully assigned kubectl-745/redis-master-jmd8n to ip-192-168-112-247.ec2.internal\n  Normal  Pulled     1s    kubelet, ip-192-168-112-247.ec2.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, ip-192-168-112-247.ec2.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-192-168-112-247.ec2.internal  Started container redis-master\n"
Mar  5 20:05:06.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 describe rc redis-master --namespace=kubectl-745'
Mar  5 20:05:07.046: INFO: stderr: ""
Mar  5 20:05:07.046: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-745\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-jmd8n\n"
Mar  5 20:05:07.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 describe service redis-master --namespace=kubectl-745'
Mar  5 20:05:07.141: INFO: stderr: ""
Mar  5 20:05:07.141: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-745\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.100.36.36\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.106.1:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  5 20:05:07.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 describe node ip-192-168-106-110.ec2.internal'
Mar  5 20:05:07.266: INFO: stderr: ""
Mar  5 20:05:07.266: INFO: stdout: "Name:               ip-192-168-106-110.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-192-168-106-110.ec2.internal\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Mar 2020 18:31:43 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 05 Mar 2020 20:04:17 +0000   Thu, 05 Mar 2020 18:31:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 05 Mar 2020 20:04:17 +0000   Thu, 05 Mar 2020 18:31:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 05 Mar 2020 20:04:17 +0000   Thu, 05 Mar 2020 18:31:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 05 Mar 2020 20:04:17 +0000   Thu, 05 Mar 2020 18:32:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   192.168.106.110\n  ExternalIP:   52.55.1.119\n  Hostname:     ip-192-168-106-110.ec2.internal\n  InternalDNS:  ip-192-168-106-110.ec2.internal\n  ExternalDNS:  ec2-52-55-1-119.compute-1.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           20959212Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8166376Ki\n pods:                        20\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         1930m\n ephemeral-storage:           18242267924\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6221800Ki\n pods:                        20\nSystem Info:\n Machine ID:                 a43eed139e534f1eb9cd394f6c27ed79\n System UUID:                EC2CDE2D-4209-F053-373D-7E4E2CEDBE98\n Boot ID:                    226977c3-29ac-41f0-8864-26f227e8419d\n Kernel Version:             4.14.165-133.209.amzn2.x86_64\n OS Image:                   Amazon Linux 2\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.15.10-eks-bac369\n Kube-Proxy Version:         v1.15.10-eks-bac369\nProviderID:                  aws:///us-east-1a/i-00958aa5a38b370af\nNon-terminated Pods:         (3 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                aws-node-2lp4m                                             10m (0%)      0 (0%)      0 (0%)           0 (0%)         93m\n  kube-system                kube-proxy-7gg9d                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         93m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-bh47c    0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests   Limits\n  --------                    --------   ------\n  cpu                         110m (5%)  0 (0%)\n  memory                      0 (0%)     0 (0%)\n  ephemeral-storage           0 (0%)     0 (0%)\n  attachable-volumes-aws-ebs  0          0\nEvents:                       <none>\n"
Mar  5 20:05:07.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 describe namespace kubectl-745'
Mar  5 20:05:07.363: INFO: stderr: ""
Mar  5 20:05:07.363: INFO: stdout: "Name:         kubectl-745\nLabels:       e2e-framework=kubectl\n              e2e-run=66f9321d-0b96-4935-8701-8ec853197070\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:05:07.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-745" for this suite.
Mar  5 20:05:29.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:05:29.551: INFO: namespace kubectl-745 deletion completed in 22.179302994s

• [SLOW TEST:25.385 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:05:29.552: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:05:29.758: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  5 20:05:29.770: INFO: Number of nodes with available pods: 0
Mar  5 20:05:29.770: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  5 20:05:29.800: INFO: Number of nodes with available pods: 0
Mar  5 20:05:29.800: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:30.805: INFO: Number of nodes with available pods: 0
Mar  5 20:05:30.805: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:31.805: INFO: Number of nodes with available pods: 1
Mar  5 20:05:31.805: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  5 20:05:31.832: INFO: Number of nodes with available pods: 1
Mar  5 20:05:31.832: INFO: Number of running nodes: 0, number of available pods: 1
Mar  5 20:05:32.837: INFO: Number of nodes with available pods: 0
Mar  5 20:05:32.837: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  5 20:05:32.850: INFO: Number of nodes with available pods: 0
Mar  5 20:05:32.850: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:33.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:33.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:34.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:34.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:35.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:35.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:36.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:36.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:37.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:37.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:38.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:38.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:39.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:39.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:40.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:40.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:41.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:41.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:42.856: INFO: Number of nodes with available pods: 0
Mar  5 20:05:42.856: INFO: Node ip-192-168-106-110.ec2.internal is running more than one daemon pod
Mar  5 20:05:43.856: INFO: Number of nodes with available pods: 1
Mar  5 20:05:43.856: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-640, will wait for the garbage collector to delete the pods
Mar  5 20:05:43.933: INFO: Deleting DaemonSet.extensions daemon-set took: 12.260097ms
Mar  5 20:05:44.233: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.246488ms
Mar  5 20:05:52.539: INFO: Number of nodes with available pods: 0
Mar  5 20:05:52.539: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 20:05:52.544: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-640/daemonsets","resourceVersion":"657989"},"items":null}

Mar  5 20:05:52.549: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-640/pods","resourceVersion":"657989"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:05:52.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-640" for this suite.
Mar  5 20:05:58.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:05:58.797: INFO: namespace daemonsets-640 deletion completed in 6.180593063s

• [SLOW TEST:29.246 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:05:58.798: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-92040904-c9b2-4121-8ee6-3cad0083fb8f
STEP: Creating a pod to test consume configMaps
Mar  5 20:05:58.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118" in namespace "configmap-5697" to be "success or failure"
Mar  5 20:05:58.971: INFO: Pod "pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930311ms
Mar  5 20:06:00.977: INFO: Pod "pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010343993s
STEP: Saw pod success
Mar  5 20:06:00.977: INFO: Pod "pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118" satisfied condition "success or failure"
Mar  5 20:06:00.981: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 20:06:01.013: INFO: Waiting for pod pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118 to disappear
Mar  5 20:06:01.017: INFO: Pod pod-configmaps-04e06b01-08fa-443f-a9fe-dbd57420b118 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:06:01.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5697" for this suite.
Mar  5 20:06:07.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:06:07.206: INFO: namespace configmap-5697 deletion completed in 6.181197042s

• [SLOW TEST:8.408 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:06:07.207: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  5 20:06:47.403: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0305 20:06:47.403851      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:06:47.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9693" for this suite.
Mar  5 20:06:53.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:06:53.601: INFO: namespace gc-9693 deletion completed in 6.190054147s

• [SLOW TEST:46.394 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:06:53.602: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-988c3ede-baeb-45d9-b4db-f718df55f3e3
STEP: Creating a pod to test consume configMaps
Mar  5 20:06:53.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923" in namespace "configmap-5581" to be "success or failure"
Mar  5 20:06:53.779: INFO: Pod "pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923": Phase="Pending", Reason="", readiness=false. Elapsed: 5.877879ms
Mar  5 20:06:55.784: INFO: Pod "pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011366497s
Mar  5 20:06:57.790: INFO: Pod "pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016904061s
STEP: Saw pod success
Mar  5 20:06:57.790: INFO: Pod "pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923" satisfied condition "success or failure"
Mar  5 20:06:57.794: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 20:06:57.824: INFO: Waiting for pod pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923 to disappear
Mar  5 20:06:57.829: INFO: Pod pod-configmaps-a1fedd62-b316-45bc-8e20-023837747923 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:06:57.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5581" for this suite.
Mar  5 20:07:03.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:07:04.023: INFO: namespace configmap-5581 deletion completed in 6.186231398s

• [SLOW TEST:10.422 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:07:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c
Mar  5 20:07:04.186: INFO: Pod name my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c: Found 0 pods out of 1
Mar  5 20:07:09.192: INFO: Pod name my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c: Found 1 pods out of 1
Mar  5 20:07:09.192: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c" are running
Mar  5 20:07:09.197: INFO: Pod "my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c-lpgkm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 20:07:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 20:07:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 20:07:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-05 20:07:04 +0000 UTC Reason: Message:}])
Mar  5 20:07:09.197: INFO: Trying to dial the pod
Mar  5 20:07:14.214: INFO: Controller my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c: Got expected result from replica 1 [my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c-lpgkm]: "my-hostname-basic-91f45d7e-eee6-4b36-9a06-8221ae52951c-lpgkm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:07:14.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1347" for this suite.
Mar  5 20:07:20.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:07:20.406: INFO: namespace replication-controller-1347 deletion completed in 6.184538382s

• [SLOW TEST:16.381 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:07:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Mar  5 20:07:20.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 api-versions'
Mar  5 20:07:20.630: INFO: stderr: ""
Mar  5 20:07:20.630: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.k8s.amazonaws.com/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:07:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9560" for this suite.
Mar  5 20:07:26.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:07:26.818: INFO: namespace kubectl-9560 deletion completed in 6.179627592s

• [SLOW TEST:6.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:07:26.819: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 20:07:26.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1620'
Mar  5 20:07:27.051: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 20:07:27.051: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  5 20:07:27.061: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-cbwpx]
Mar  5 20:07:27.061: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-cbwpx" in namespace "kubectl-1620" to be "running and ready"
Mar  5 20:07:27.066: INFO: Pod "e2e-test-nginx-rc-cbwpx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476179ms
Mar  5 20:07:29.071: INFO: Pod "e2e-test-nginx-rc-cbwpx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009678627s
Mar  5 20:07:31.077: INFO: Pod "e2e-test-nginx-rc-cbwpx": Phase="Running", Reason="", readiness=true. Elapsed: 4.015194907s
Mar  5 20:07:31.077: INFO: Pod "e2e-test-nginx-rc-cbwpx" satisfied condition "running and ready"
Mar  5 20:07:31.077: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-cbwpx]
Mar  5 20:07:31.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 logs rc/e2e-test-nginx-rc --namespace=kubectl-1620'
Mar  5 20:07:31.192: INFO: stderr: ""
Mar  5 20:07:31.192: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Mar  5 20:07:31.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete rc e2e-test-nginx-rc --namespace=kubectl-1620'
Mar  5 20:07:31.282: INFO: stderr: ""
Mar  5 20:07:31.282: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:07:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1620" for this suite.
Mar  5 20:07:37.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:07:37.476: INFO: namespace kubectl-1620 deletion completed in 6.185724319s

• [SLOW TEST:10.657 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:07:37.476: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-c09dd6e9-a3d4-4835-949c-59966a91f7e4
STEP: Creating a pod to test consume configMaps
Mar  5 20:07:37.650: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd" in namespace "projected-7395" to be "success or failure"
Mar  5 20:07:37.656: INFO: Pod "pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.416153ms
Mar  5 20:07:39.661: INFO: Pod "pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011781133s
Mar  5 20:07:41.667: INFO: Pod "pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017605178s
STEP: Saw pod success
Mar  5 20:07:41.667: INFO: Pod "pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd" satisfied condition "success or failure"
Mar  5 20:07:41.673: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 20:07:41.704: INFO: Waiting for pod pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd to disappear
Mar  5 20:07:41.709: INFO: Pod pod-projected-configmaps-9fe2f2a3-e790-44c9-bf1a-265773b63efd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:07:41.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7395" for this suite.
Mar  5 20:07:47.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:07:47.894: INFO: namespace projected-7395 deletion completed in 6.177529603s

• [SLOW TEST:10.418 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:07:47.894: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  5 20:07:52.103: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 20:07:52.108: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 20:07:54.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 20:07:54.113: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 20:07:56.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 20:07:56.114: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 20:07:58.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 20:07:58.113: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 20:08:00.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 20:08:00.113: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:08:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1334" for this suite.
Mar  5 20:08:18.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:08:18.320: INFO: namespace container-lifecycle-hook-1334 deletion completed in 18.184831321s

• [SLOW TEST:30.425 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:08:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  5 20:08:18.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3470,SelfLink:/api/v1/namespaces/watch-3470/configmaps/e2e-watch-test-resource-version,UID:ab8c96f6-735b-4299-add7-69b35af11379,ResourceVersion:658722,Generation:0,CreationTimestamp:2020-03-05 20:08:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 20:08:18.510: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3470,SelfLink:/api/v1/namespaces/watch-3470/configmaps/e2e-watch-test-resource-version,UID:ab8c96f6-735b-4299-add7-69b35af11379,ResourceVersion:658723,Generation:0,CreationTimestamp:2020-03-05 20:08:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:08:18.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3470" for this suite.
Mar  5 20:08:24.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:08:24.699: INFO: namespace watch-3470 deletion completed in 6.181039923s

• [SLOW TEST:6.379 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:08:24.699: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:08:24.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e" in namespace "downward-api-8116" to be "success or failure"
Mar  5 20:08:24.867: INFO: Pod "downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.767106ms
Mar  5 20:08:26.872: INFO: Pod "downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010308266s
STEP: Saw pod success
Mar  5 20:08:26.872: INFO: Pod "downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e" satisfied condition "success or failure"
Mar  5 20:08:26.877: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e container client-container: <nil>
STEP: delete the pod
Mar  5 20:08:26.904: INFO: Waiting for pod downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e to disappear
Mar  5 20:08:26.908: INFO: Pod downwardapi-volume-1d72ec9f-679c-4d93-92cd-9bd8084c6c5e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:08:26.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8116" for this suite.
Mar  5 20:08:32.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:08:33.092: INFO: namespace downward-api-8116 deletion completed in 6.176205149s

• [SLOW TEST:8.393 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:08:33.093: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-e6220b1b-72c1-4b33-917b-8a0cfa011762
STEP: Creating a pod to test consume secrets
Mar  5 20:08:33.259: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a" in namespace "projected-8550" to be "success or failure"
Mar  5 20:08:33.264: INFO: Pod "pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.730619ms
Mar  5 20:08:35.270: INFO: Pod "pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010234423s
STEP: Saw pod success
Mar  5 20:08:35.270: INFO: Pod "pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a" satisfied condition "success or failure"
Mar  5 20:08:35.274: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:08:35.303: INFO: Waiting for pod pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a to disappear
Mar  5 20:08:35.307: INFO: Pod pod-projected-secrets-e246c77b-0668-4823-bbbd-ebe26a61ab2a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:08:35.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8550" for this suite.
Mar  5 20:08:41.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:08:41.494: INFO: namespace projected-8550 deletion completed in 6.179064885s

• [SLOW TEST:8.401 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:08:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  5 20:08:44.200: INFO: Successfully updated pod "annotationupdateb639b252-dd55-4331-88ad-88468265dece"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:08:48.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2953" for this suite.
Mar  5 20:09:10.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:09:10.428: INFO: namespace projected-2953 deletion completed in 22.178783269s

• [SLOW TEST:28.934 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:09:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:09:10.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0" in namespace "downward-api-5853" to be "success or failure"
Mar  5 20:09:10.598: INFO: Pod "downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859265ms
Mar  5 20:09:12.603: INFO: Pod "downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010299081s
STEP: Saw pod success
Mar  5 20:09:12.603: INFO: Pod "downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0" satisfied condition "success or failure"
Mar  5 20:09:12.608: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0 container client-container: <nil>
STEP: delete the pod
Mar  5 20:09:12.635: INFO: Waiting for pod downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0 to disappear
Mar  5 20:09:12.640: INFO: Pod downwardapi-volume-32e8caae-20be-4139-9c4e-42d9df6fbca0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:09:12.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5853" for this suite.
Mar  5 20:09:18.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:09:18.828: INFO: namespace downward-api-5853 deletion completed in 6.180500991s

• [SLOW TEST:8.399 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:09:18.828: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-cc9ecad2-65fd-4f13-834f-a8a101fadf0b
STEP: Creating a pod to test consume secrets
Mar  5 20:09:18.997: INFO: Waiting up to 5m0s for pod "pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5" in namespace "secrets-9961" to be "success or failure"
Mar  5 20:09:19.002: INFO: Pod "pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.861561ms
Mar  5 20:09:21.007: INFO: Pod "pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010365836s
Mar  5 20:09:23.013: INFO: Pod "pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015801106s
STEP: Saw pod success
Mar  5 20:09:23.013: INFO: Pod "pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5" satisfied condition "success or failure"
Mar  5 20:09:23.017: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:09:23.045: INFO: Waiting for pod pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5 to disappear
Mar  5 20:09:23.050: INFO: Pod pod-secrets-fbbd479f-28d1-4c36-8141-4b58be235be5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:09:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9961" for this suite.
Mar  5 20:09:29.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:09:29.237: INFO: namespace secrets-9961 deletion completed in 6.180102949s

• [SLOW TEST:10.409 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:09:29.238: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 20:09:29.399: INFO: Waiting up to 5m0s for pod "pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688" in namespace "emptydir-825" to be "success or failure"
Mar  5 20:09:29.404: INFO: Pod "pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86756ms
Mar  5 20:09:31.410: INFO: Pod "pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010354848s
Mar  5 20:09:33.415: INFO: Pod "pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015968072s
STEP: Saw pod success
Mar  5 20:09:33.415: INFO: Pod "pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688" satisfied condition "success or failure"
Mar  5 20:09:33.420: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688 container test-container: <nil>
STEP: delete the pod
Mar  5 20:09:33.448: INFO: Waiting for pod pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688 to disappear
Mar  5 20:09:33.453: INFO: Pod pod-0be9e01a-f142-4fbf-b9b3-fd340daf8688 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:09:33.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-825" for this suite.
Mar  5 20:09:39.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:09:39.638: INFO: namespace emptydir-825 deletion completed in 6.177927026s

• [SLOW TEST:10.400 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:09:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-db1fcea0-8426-462b-96d3-412670c87186
STEP: Creating secret with name secret-projected-all-test-volume-773e09b5-d4aa-4fb1-a9a1-f677d18e61a4
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  5 20:09:39.810: INFO: Waiting up to 5m0s for pod "projected-volume-82881454-435f-4021-8af4-24f02ed932f7" in namespace "projected-4511" to be "success or failure"
Mar  5 20:09:39.816: INFO: Pod "projected-volume-82881454-435f-4021-8af4-24f02ed932f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.657387ms
Mar  5 20:09:41.826: INFO: Pod "projected-volume-82881454-435f-4021-8af4-24f02ed932f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015435515s
STEP: Saw pod success
Mar  5 20:09:41.826: INFO: Pod "projected-volume-82881454-435f-4021-8af4-24f02ed932f7" satisfied condition "success or failure"
Mar  5 20:09:41.831: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod projected-volume-82881454-435f-4021-8af4-24f02ed932f7 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  5 20:09:41.859: INFO: Waiting for pod projected-volume-82881454-435f-4021-8af4-24f02ed932f7 to disappear
Mar  5 20:09:41.865: INFO: Pod projected-volume-82881454-435f-4021-8af4-24f02ed932f7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:09:41.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4511" for this suite.
Mar  5 20:09:47.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:09:48.054: INFO: namespace projected-4511 deletion completed in 6.181822533s

• [SLOW TEST:8.416 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:09:48.055: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  5 20:09:52.765: INFO: Successfully updated pod "pod-update-08ba0e9a-62a7-40f5-b797-970987e5c18c"
STEP: verifying the updated pod is in kubernetes
Mar  5 20:09:52.774: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:09:52.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-820" for this suite.
Mar  5 20:10:14.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:10:14.960: INFO: namespace pods-820 deletion completed in 22.178440277s

• [SLOW TEST:26.905 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:10:14.960: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  5 20:10:17.662: INFO: Successfully updated pod "labelsupdate762ade0e-1a1d-47a2-8e97-b857adaac5e9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:10:21.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8356" for this suite.
Mar  5 20:10:43.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:10:43.892: INFO: namespace downward-api-8356 deletion completed in 22.180822076s

• [SLOW TEST:28.932 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:10:43.892: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5265.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5265.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 20:10:48.114: INFO: DNS probes using dns-5265/dns-test-df4c835c-d71e-4530-944d-8a7be9479366 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:10:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5265" for this suite.
Mar  5 20:10:54.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:10:54.319: INFO: namespace dns-5265 deletion completed in 6.180904196s

• [SLOW TEST:10.427 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:10:54.320: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  5 20:11:00.523: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:00.647: INFO: Exec stderr: ""
Mar  5 20:11:00.647: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:00.647: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:00.763: INFO: Exec stderr: ""
Mar  5 20:11:00.763: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:00.763: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:00.880: INFO: Exec stderr: ""
Mar  5 20:11:00.880: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:00.880: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:00.999: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  5 20:11:00.999: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:00.999: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.123: INFO: Exec stderr: ""
Mar  5 20:11:01.123: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:01.123: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.242: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  5 20:11:01.242: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.366: INFO: Exec stderr: ""
Mar  5 20:11:01.366: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.509: INFO: Exec stderr: ""
Mar  5 20:11:01.509: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:01.509: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.638: INFO: Exec stderr: ""
Mar  5 20:11:01.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:11:01.638: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:11:01.769: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:11:01.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6766" for this suite.
Mar  5 20:11:41.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:11:41.958: INFO: namespace e2e-kubelet-etc-hosts-6766 deletion completed in 40.178820773s

• [SLOW TEST:47.638 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:11:41.958: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  5 20:11:46.666: INFO: Successfully updated pod "labelsupdatee3f7ca19-6854-4de2-9dd9-c3c782ef8266"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:11:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1234" for this suite.
Mar  5 20:12:10.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:12:10.879: INFO: namespace projected-1234 deletion completed in 22.178309989s

• [SLOW TEST:28.921 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:12:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Mar  5 20:12:11.029: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-370450052 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:12:11.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-356" for this suite.
Mar  5 20:12:17.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:12:17.279: INFO: namespace kubectl-356 deletion completed in 6.179489796s

• [SLOW TEST:6.400 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:12:17.280: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:12:17.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 version'
Mar  5 20:12:17.494: INFO: stderr: ""
Mar  5 20:12:17.494: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.10\", GitCommit:\"1bea6c00a7055edef03f1d4bb58b773fa8917f11\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T20:13:57Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.10-eks-bac369\", GitCommit:\"bac3690554985327ae4d13e42169e8b1c2f37226\", GitTreeState:\"clean\", BuildDate:\"2020-02-26T01:12:54Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:12:17.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9651" for this suite.
Mar  5 20:12:23.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:12:23.691: INFO: namespace kubectl-9651 deletion completed in 6.189095554s

• [SLOW TEST:6.411 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:12:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 20:12:23.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3320'
Mar  5 20:12:23.933: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 20:12:23.933: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Mar  5 20:12:25.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3320'
Mar  5 20:12:26.047: INFO: stderr: ""
Mar  5 20:12:26.047: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:12:26.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3320" for this suite.
Mar  5 20:12:48.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:12:48.247: INFO: namespace kubectl-3320 deletion completed in 22.190742993s

• [SLOW TEST:24.555 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:12:48.247: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 20:12:50.432: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:12:50.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3890" for this suite.
Mar  5 20:12:56.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:12:56.638: INFO: namespace container-runtime-3890 deletion completed in 6.177825358s

• [SLOW TEST:8.391 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:12:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c7611049-3658-4b43-8b19-0181fab9d854
STEP: Creating a pod to test consume secrets
Mar  5 20:12:56.804: INFO: Waiting up to 5m0s for pod "pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe" in namespace "secrets-5481" to be "success or failure"
Mar  5 20:12:56.810: INFO: Pod "pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087336ms
Mar  5 20:12:58.816: INFO: Pod "pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011639183s
STEP: Saw pod success
Mar  5 20:12:58.816: INFO: Pod "pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe" satisfied condition "success or failure"
Mar  5 20:12:58.820: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:12:58.853: INFO: Waiting for pod pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe to disappear
Mar  5 20:12:58.857: INFO: Pod pod-secrets-d30cc233-6b89-484c-a584-5c4ff9d245fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:12:58.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5481" for this suite.
Mar  5 20:13:04.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:13:05.049: INFO: namespace secrets-5481 deletion completed in 6.184646051s

• [SLOW TEST:8.411 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:13:05.050: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar  5 20:13:11.241: INFO: 10 pods remaining
Mar  5 20:13:11.241: INFO: 10 pods has nil DeletionTimestamp
Mar  5 20:13:11.241: INFO: 
Mar  5 20:13:12.241: INFO: 10 pods remaining
Mar  5 20:13:12.241: INFO: 10 pods has nil DeletionTimestamp
Mar  5 20:13:12.241: INFO: 
STEP: Gathering metrics
W0305 20:13:13.248836      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 20:13:13.248: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:13:13.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7068" for this suite.
Mar  5 20:13:19.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:13:19.437: INFO: namespace gc-7068 deletion completed in 6.181233054s

• [SLOW TEST:14.387 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:13:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:13:24.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1074" for this suite.
Mar  5 20:13:46.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:13:46.826: INFO: namespace replication-controller-1074 deletion completed in 22.182408773s

• [SLOW TEST:27.388 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:13:46.826: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:13:46.989: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791" in namespace "downward-api-890" to be "success or failure"
Mar  5 20:13:46.994: INFO: Pod "downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566856ms
Mar  5 20:13:49.000: INFO: Pod "downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011060617s
Mar  5 20:13:51.006: INFO: Pod "downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016643403s
STEP: Saw pod success
Mar  5 20:13:51.006: INFO: Pod "downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791" satisfied condition "success or failure"
Mar  5 20:13:51.010: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791 container client-container: <nil>
STEP: delete the pod
Mar  5 20:13:51.040: INFO: Waiting for pod downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791 to disappear
Mar  5 20:13:51.046: INFO: Pod downwardapi-volume-e5501653-d87b-4941-8bc5-e8e6e5d70791 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:13:51.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-890" for this suite.
Mar  5 20:13:57.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:13:57.231: INFO: namespace downward-api-890 deletion completed in 6.177219572s

• [SLOW TEST:10.405 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:13:57.232: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  5 20:14:03.462: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:03.466: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:05.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:05.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:07.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:07.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:09.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:09.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:11.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:11.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:13.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:13.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:15.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:15.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:17.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:17.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:19.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:19.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:21.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:21.472: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 20:14:23.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 20:14:23.472: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:14:23.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2654" for this suite.
Mar  5 20:14:45.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:14:45.663: INFO: namespace container-lifecycle-hook-2654 deletion completed in 22.183932619s

• [SLOW TEST:48.431 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:14:45.663: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 20:14:47.845: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:14:47.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3304" for this suite.
Mar  5 20:14:53.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:14:54.057: INFO: namespace container-runtime-3304 deletion completed in 6.184189212s

• [SLOW TEST:8.394 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:14:54.058: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Mar  5 20:14:54.219: INFO: Waiting up to 5m0s for pod "var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed" in namespace "var-expansion-6449" to be "success or failure"
Mar  5 20:14:54.226: INFO: Pod "var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378597ms
Mar  5 20:14:56.231: INFO: Pod "var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012064551s
Mar  5 20:14:58.237: INFO: Pod "var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017374967s
STEP: Saw pod success
Mar  5 20:14:58.237: INFO: Pod "var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed" satisfied condition "success or failure"
Mar  5 20:14:58.241: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed container dapi-container: <nil>
STEP: delete the pod
Mar  5 20:14:58.269: INFO: Waiting for pod var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed to disappear
Mar  5 20:14:58.274: INFO: Pod var-expansion-8c656d46-957d-4656-aeb7-aeb3c656b5ed no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:14:58.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6449" for this suite.
Mar  5 20:15:04.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:15:04.467: INFO: namespace var-expansion-6449 deletion completed in 6.186104338s

• [SLOW TEST:10.410 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:15:04.468: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:15:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4402" for this suite.
Mar  5 20:15:10.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:15:10.850: INFO: namespace services-4402 deletion completed in 6.184183346s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.381 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:15:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:15:11.000: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:15:13.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5711" for this suite.
Mar  5 20:15:53.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:15:53.347: INFO: namespace pods-5711 deletion completed in 40.185865113s

• [SLOW TEST:42.497 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:15:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:15:53.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e" in namespace "projected-8566" to be "success or failure"
Mar  5 20:15:53.517: INFO: Pod "downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.803023ms
Mar  5 20:15:55.522: INFO: Pod "downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010429023s
STEP: Saw pod success
Mar  5 20:15:55.522: INFO: Pod "downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e" satisfied condition "success or failure"
Mar  5 20:15:55.527: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e container client-container: <nil>
STEP: delete the pod
Mar  5 20:15:55.557: INFO: Waiting for pod downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e to disappear
Mar  5 20:15:55.562: INFO: Pod downwardapi-volume-5d22453f-404b-44a9-9aeb-f9f06317086e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:15:55.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8566" for this suite.
Mar  5 20:16:01.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:16:01.754: INFO: namespace projected-8566 deletion completed in 6.184900945s

• [SLOW TEST:8.407 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:16:01.755: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-dd2693a5-5f32-4e71-b8d8-1703b92759f0 in namespace container-probe-2813
Mar  5 20:16:05.934: INFO: Started pod liveness-dd2693a5-5f32-4e71-b8d8-1703b92759f0 in namespace container-probe-2813
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 20:16:05.938: INFO: Initial restart count of pod liveness-dd2693a5-5f32-4e71-b8d8-1703b92759f0 is 0
Mar  5 20:16:25.998: INFO: Restart count of pod container-probe-2813/liveness-dd2693a5-5f32-4e71-b8d8-1703b92759f0 is now 1 (20.059701072s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:16:26.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2813" for this suite.
Mar  5 20:16:32.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:16:32.203: INFO: namespace container-probe-2813 deletion completed in 6.181826671s

• [SLOW TEST:30.448 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:16:32.203: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1792
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-71cc8f77-88e7-4b12-9781-f24d58aebc31
STEP: Creating secret with name s-test-opt-upd-9e2a1f43-3b78-452e-bdcd-72cd9528b777
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-71cc8f77-88e7-4b12-9781-f24d58aebc31
STEP: Updating secret s-test-opt-upd-9e2a1f43-3b78-452e-bdcd-72cd9528b777
STEP: Creating secret with name s-test-opt-create-e2bda390-bfc0-4c3e-aaa1-6f1c012f5bb0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:16:36.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1792" for this suite.
Mar  5 20:16:58.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:16:58.703: INFO: namespace projected-1792 deletion completed in 22.183393305s

• [SLOW TEST:26.500 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:16:58.704: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-91
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4c68d90e-efff-4d1c-bc3c-1f829dee6522
STEP: Creating a pod to test consume secrets
Mar  5 20:16:58.872: INFO: Waiting up to 5m0s for pod "pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889" in namespace "secrets-91" to be "success or failure"
Mar  5 20:16:58.877: INFO: Pod "pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889": Phase="Pending", Reason="", readiness=false. Elapsed: 4.602193ms
Mar  5 20:17:00.883: INFO: Pod "pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010112912s
STEP: Saw pod success
Mar  5 20:17:00.883: INFO: Pod "pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889" satisfied condition "success or failure"
Mar  5 20:17:00.887: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:17:00.919: INFO: Waiting for pod pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889 to disappear
Mar  5 20:17:00.924: INFO: Pod pod-secrets-a3c9f9e0-a409-44bb-813a-d79d75367889 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:17:00.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-91" for this suite.
Mar  5 20:17:06.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:17:07.115: INFO: namespace secrets-91 deletion completed in 6.183396613s

• [SLOW TEST:8.411 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:17:07.115: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-e0d08f21-ae89-4d8a-916a-d2a4cd96744a in namespace container-probe-8289
Mar  5 20:17:09.289: INFO: Started pod busybox-e0d08f21-ae89-4d8a-916a-d2a4cd96744a in namespace container-probe-8289
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 20:17:09.294: INFO: Initial restart count of pod busybox-e0d08f21-ae89-4d8a-916a-d2a4cd96744a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:21:09.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8289" for this suite.
Mar  5 20:21:16.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:21:16.168: INFO: namespace container-probe-8289 deletion completed in 6.178084905s

• [SLOW TEST:249.053 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:21:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  5 20:21:16.329: INFO: Waiting up to 5m0s for pod "pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440" in namespace "emptydir-2905" to be "success or failure"
Mar  5 20:21:16.334: INFO: Pod "pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863256ms
Mar  5 20:21:18.339: INFO: Pod "pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010374367s
STEP: Saw pod success
Mar  5 20:21:18.339: INFO: Pod "pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440" satisfied condition "success or failure"
Mar  5 20:21:18.344: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440 container test-container: <nil>
STEP: delete the pod
Mar  5 20:21:18.373: INFO: Waiting for pod pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440 to disappear
Mar  5 20:21:18.377: INFO: Pod pod-75fd6067-b6b1-49cf-aec4-ca44a1eac440 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:21:18.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2905" for this suite.
Mar  5 20:21:24.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:21:24.565: INFO: namespace emptydir-2905 deletion completed in 6.180276622s

• [SLOW TEST:8.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:21:24.566: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:21:24.728: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  5 20:21:29.733: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 20:21:29.733: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  5 20:21:31.778: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1223,SelfLink:/apis/apps/v1/namespaces/deployment-1223/deployments/test-cleanup-deployment,UID:71a905e6-a174-41aa-bbd5-4947bc67e3ff,ResourceVersion:661517,Generation:1,CreationTimestamp:2020-03-05 20:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-05 20:21:29 +0000 UTC 2020-03-05 20:21:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-05 20:21:31 +0000 UTC 2020-03-05 20:21:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 20:21:31.783: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-1223,SelfLink:/apis/apps/v1/namespaces/deployment-1223/replicasets/test-cleanup-deployment-55bbcbc84c,UID:f2f7ae17-e860-45ef-b432-9252e5d108e6,ResourceVersion:661505,Generation:1,CreationTimestamp:2020-03-05 20:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 71a905e6-a174-41aa-bbd5-4947bc67e3ff 0xc002913f27 0xc002913f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 20:21:31.788: INFO: Pod "test-cleanup-deployment-55bbcbc84c-kk8wb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-kk8wb,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-1223,SelfLink:/api/v1/namespaces/deployment-1223/pods/test-cleanup-deployment-55bbcbc84c-kk8wb,UID:a747dc0f-170a-4f5f-a014-e7a92c21ddb8,ResourceVersion:661504,Generation:0,CreationTimestamp:2020-03-05 20:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c f2f7ae17-e860-45ef-b432-9252e5d108e6 0xc001b52537 0xc001b52538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qhdxk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qhdxk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qhdxk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b525a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b525c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 20:21:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 20:21:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 20:21:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-05 20:21:29 +0000 UTC  }],Message:,Reason:,HostIP:192.168.148.164,PodIP:192.168.155.82,StartTime:2020-03-05 20:21:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-05 20:21:30 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e7882440dfb3ef9898f63f124291b22b02534db44974a9e7d31c1e14b67b65bc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:21:31.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1223" for this suite.
Mar  5 20:21:37.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:21:37.976: INFO: namespace deployment-1223 deletion completed in 6.17961788s

• [SLOW TEST:13.410 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:21:37.976: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:21:38.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790" in namespace "projected-5493" to be "success or failure"
Mar  5 20:21:38.140: INFO: Pod "downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790": Phase="Pending", Reason="", readiness=false. Elapsed: 4.597724ms
Mar  5 20:21:40.146: INFO: Pod "downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01008971s
STEP: Saw pod success
Mar  5 20:21:40.146: INFO: Pod "downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790" satisfied condition "success or failure"
Mar  5 20:21:40.151: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790 container client-container: <nil>
STEP: delete the pod
Mar  5 20:21:40.181: INFO: Waiting for pod downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790 to disappear
Mar  5 20:21:40.186: INFO: Pod downwardapi-volume-ad4b76fb-a211-47e5-97cc-7da6b8c57790 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:21:40.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5493" for this suite.
Mar  5 20:21:46.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:21:46.370: INFO: namespace projected-5493 deletion completed in 6.176623292s

• [SLOW TEST:8.393 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:21:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  5 20:21:46.517: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Mar  5 20:21:47.135: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  5 20:21:49.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 20:21:51.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 20:21:53.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 20:21:55.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 20:21:57.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63719036507, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 20:21:59.833: INFO: Waited 623.595792ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:22:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1175" for this suite.
Mar  5 20:22:06.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:22:06.595: INFO: namespace aggregator-1175 deletion completed in 6.269357291s

• [SLOW TEST:20.225 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:22:06.595: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  5 20:22:06.743: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:22:10.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9818" for this suite.
Mar  5 20:22:16.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:22:16.850: INFO: namespace init-container-9818 deletion completed in 6.180169447s

• [SLOW TEST:10.255 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:22:16.851: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4520
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-5d49835d-6e83-4766-a3d8-74fbb3d3800e
STEP: Creating configMap with name cm-test-opt-upd-1b3eafa6-208c-4b17-b4d5-240a285665e7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5d49835d-6e83-4766-a3d8-74fbb3d3800e
STEP: Updating configmap cm-test-opt-upd-1b3eafa6-208c-4b17-b4d5-240a285665e7
STEP: Creating configMap with name cm-test-opt-create-151a5a4d-e995-4cee-b908-36471b3dd547
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:23:37.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4520" for this suite.
Mar  5 20:23:59.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:23:59.890: INFO: namespace projected-4520 deletion completed in 22.171740299s

• [SLOW TEST:103.039 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:23:59.890: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:24:00.056: INFO: (0) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 11.773093ms)
Mar  5 20:24:00.064: INFO: (1) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.717669ms)
Mar  5 20:24:00.072: INFO: (2) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.472018ms)
Mar  5 20:24:00.079: INFO: (3) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.287244ms)
Mar  5 20:24:00.087: INFO: (4) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.344976ms)
Mar  5 20:24:00.094: INFO: (5) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.49725ms)
Mar  5 20:24:00.101: INFO: (6) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.293036ms)
Mar  5 20:24:00.109: INFO: (7) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.398781ms)
Mar  5 20:24:00.116: INFO: (8) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.243623ms)
Mar  5 20:24:00.124: INFO: (9) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.42911ms)
Mar  5 20:24:00.131: INFO: (10) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.394539ms)
Mar  5 20:24:00.138: INFO: (11) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.285179ms)
Mar  5 20:24:00.146: INFO: (12) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.396843ms)
Mar  5 20:24:00.153: INFO: (13) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.257792ms)
Mar  5 20:24:00.160: INFO: (14) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.362628ms)
Mar  5 20:24:00.168: INFO: (15) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.193878ms)
Mar  5 20:24:00.175: INFO: (16) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.223031ms)
Mar  5 20:24:00.182: INFO: (17) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.364972ms)
Mar  5 20:24:00.190: INFO: (18) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.332953ms)
Mar  5 20:24:00.197: INFO: (19) /api/v1/nodes/ip-192-168-106-110.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.307017ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:24:00.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3332" for this suite.
Mar  5 20:24:06.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:24:06.381: INFO: namespace proxy-3332 deletion completed in 6.176926897s

• [SLOW TEST:6.491 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:24:06.382: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-jnqw
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 20:24:06.565: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jnqw" in namespace "subpath-9057" to be "success or failure"
Mar  5 20:24:06.569: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576185ms
Mar  5 20:24:08.575: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 2.010258922s
Mar  5 20:24:10.581: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 4.015681885s
Mar  5 20:24:12.586: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 6.021109978s
Mar  5 20:24:14.592: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 8.026738881s
Mar  5 20:24:16.597: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 10.032251153s
Mar  5 20:24:18.603: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 12.037753992s
Mar  5 20:24:20.608: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 14.043069286s
Mar  5 20:24:22.613: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 16.048592764s
Mar  5 20:24:24.620: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 18.054839163s
Mar  5 20:24:26.625: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Running", Reason="", readiness=true. Elapsed: 20.060562951s
Mar  5 20:24:28.631: INFO: Pod "pod-subpath-test-downwardapi-jnqw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.066061769s
STEP: Saw pod success
Mar  5 20:24:28.631: INFO: Pod "pod-subpath-test-downwardapi-jnqw" satisfied condition "success or failure"
Mar  5 20:24:28.636: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-subpath-test-downwardapi-jnqw container test-container-subpath-downwardapi-jnqw: <nil>
STEP: delete the pod
Mar  5 20:24:28.664: INFO: Waiting for pod pod-subpath-test-downwardapi-jnqw to disappear
Mar  5 20:24:28.668: INFO: Pod pod-subpath-test-downwardapi-jnqw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jnqw
Mar  5 20:24:28.668: INFO: Deleting pod "pod-subpath-test-downwardapi-jnqw" in namespace "subpath-9057"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:24:28.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9057" for this suite.
Mar  5 20:24:34.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:24:34.856: INFO: namespace subpath-9057 deletion completed in 6.175815945s

• [SLOW TEST:28.474 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:24:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:24:37.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6327" for this suite.
Mar  5 20:25:17.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:25:17.228: INFO: namespace kubelet-test-6327 deletion completed in 40.176431997s

• [SLOW TEST:42.372 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:25:17.229: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  5 20:25:19.915: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1275 pod-service-account-bce504ba-18aa-41c3-ae70-c71d213ec6ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  5 20:25:20.230: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1275 pod-service-account-bce504ba-18aa-41c3-ae70-c71d213ec6ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  5 20:25:20.439: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1275 pod-service-account-bce504ba-18aa-41c3-ae70-c71d213ec6ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:25:20.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1275" for this suite.
Mar  5 20:25:26.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:25:26.847: INFO: namespace svcaccounts-1275 deletion completed in 6.177534748s

• [SLOW TEST:9.618 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:25:26.847: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Mar  5 20:25:27.537: INFO: created pod pod-service-account-defaultsa
Mar  5 20:25:27.537: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  5 20:25:27.544: INFO: created pod pod-service-account-mountsa
Mar  5 20:25:27.544: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  5 20:25:27.553: INFO: created pod pod-service-account-nomountsa
Mar  5 20:25:27.553: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  5 20:25:27.560: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  5 20:25:27.560: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  5 20:25:27.568: INFO: created pod pod-service-account-mountsa-mountspec
Mar  5 20:25:27.568: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  5 20:25:27.575: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  5 20:25:27.575: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  5 20:25:27.582: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  5 20:25:27.582: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  5 20:25:27.590: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  5 20:25:27.590: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  5 20:25:27.596: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  5 20:25:27.597: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:25:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9781" for this suite.
Mar  5 20:25:33.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:25:33.788: INFO: namespace svcaccounts-9781 deletion completed in 6.181457861s

• [SLOW TEST:6.941 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:25:33.788: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  5 20:25:33.937: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:25:37.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7838" for this suite.
Mar  5 20:25:59.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:26:00.082: INFO: namespace init-container-7838 deletion completed in 22.191330031s

• [SLOW TEST:26.295 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:26:00.083: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3788/configmap-test-c2ad39cb-d428-45fb-82c7-dd5d8e1efb59
STEP: Creating a pod to test consume configMaps
Mar  5 20:26:00.249: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15" in namespace "configmap-3788" to be "success or failure"
Mar  5 20:26:00.254: INFO: Pod "pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735592ms
Mar  5 20:26:02.260: INFO: Pod "pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010884513s
STEP: Saw pod success
Mar  5 20:26:02.260: INFO: Pod "pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15" satisfied condition "success or failure"
Mar  5 20:26:02.265: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15 container env-test: <nil>
STEP: delete the pod
Mar  5 20:26:02.297: INFO: Waiting for pod pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15 to disappear
Mar  5 20:26:02.302: INFO: Pod pod-configmaps-cbed1835-bd2e-4098-9bac-28c3804f6a15 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:26:02.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3788" for this suite.
Mar  5 20:26:08.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:26:08.485: INFO: namespace configmap-3788 deletion completed in 6.175664111s

• [SLOW TEST:8.403 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:26:08.486: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  5 20:26:08.632: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 20:26:08.647: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 20:26:08.652: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-106-110.ec2.internal before test
Mar  5 20:26:08.663: INFO: aws-node-2lp4m from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.663: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.663: INFO: kube-proxy-7gg9d from kube-system started at 2020-03-05 18:31:43 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.663: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.663: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-bh47c from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.663: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.663: INFO: 	Container systemd-logs ready: true, restart count 1
Mar  5 20:26:08.663: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-112-247.ec2.internal before test
Mar  5 20:26:08.671: INFO: kube-proxy-r7sml from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.671: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.671: INFO: aws-node-gxwkp from kube-system started at 2020-03-05 18:31:37 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.671: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.671: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-7m5vz from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.671: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.671: INFO: 	Container systemd-logs ready: true, restart count 1
Mar  5 20:26:08.671: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-164.ec2.internal before test
Mar  5 20:26:08.682: INFO: kube-proxy-l5q7m from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.682: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.682: INFO: aws-node-4q99r from kube-system started at 2020-03-05 18:31:40 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.682: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.682: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-d9krw from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.682: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.682: INFO: 	Container systemd-logs ready: true, restart count 1
Mar  5 20:26:08.682: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-155-60.ec2.internal before test
Mar  5 20:26:08.691: INFO: kube-proxy-m8h2w from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.691: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.691: INFO: aws-node-fwjd8 from kube-system started at 2020-03-05 18:31:33 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.691: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.691: INFO: sonobuoy from sonobuoy started at 2020-03-05 19:01:15 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.691: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 20:26:08.691: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-cv5hn from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.691: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.691: INFO: 	Container systemd-logs ready: true, restart count 1
Mar  5 20:26:08.691: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-169.ec2.internal before test
Mar  5 20:26:08.702: INFO: aws-node-qgbl2 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.702: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.702: INFO: coredns-59dfd6b59f-xzswv from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.702: INFO: 	Container coredns ready: true, restart count 0
Mar  5 20:26:08.702: INFO: kube-proxy-fkm78 from kube-system started at 2020-03-05 18:24:42 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.702: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.702: INFO: coredns-59dfd6b59f-kw58c from kube-system started at 2020-03-05 18:30:33 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.702: INFO: 	Container coredns ready: true, restart count 0
Mar  5 20:26:08.702: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-75b4d from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.702: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.703: INFO: 	Container systemd-logs ready: true, restart count 1
Mar  5 20:26:08.703: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-239-209.ec2.internal before test
Mar  5 20:26:08.714: INFO: kube-proxy-ttknx from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.714: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  5 20:26:08.714: INFO: sonobuoy-e2e-job-3f29057810bc417f from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.714: INFO: 	Container e2e ready: true, restart count 0
Mar  5 20:26:08.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 20:26:08.714: INFO: aws-node-bxlx7 from kube-system started at 2020-03-05 18:31:47 +0000 UTC (1 container statuses recorded)
Mar  5 20:26:08.714: INFO: 	Container aws-node ready: true, restart count 0
Mar  5 20:26:08.714: INFO: sonobuoy-systemd-logs-daemon-set-742eae50fb4e42a6-nlqlx from sonobuoy started at 2020-03-05 19:01:20 +0000 UTC (2 container statuses recorded)
Mar  5 20:26:08.714: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  5 20:26:08.714: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8f3d9002-1c3d-4576-af45-f2b1d7933376 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8f3d9002-1c3d-4576-af45-f2b1d7933376 off the node ip-192-168-106-110.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8f3d9002-1c3d-4576-af45-f2b1d7933376
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:26:12.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1096" for this suite.
Mar  5 20:26:24.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:26:25.004: INFO: namespace sched-pred-1096 deletion completed in 12.180809858s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.518 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:26:25.005: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-6eae94b8-2cb3-4ff7-a8f1-f55f77e6af5f in namespace container-probe-4978
Mar  5 20:26:27.175: INFO: Started pod busybox-6eae94b8-2cb3-4ff7-a8f1-f55f77e6af5f in namespace container-probe-4978
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 20:26:27.180: INFO: Initial restart count of pod busybox-6eae94b8-2cb3-4ff7-a8f1-f55f77e6af5f is 0
Mar  5 20:27:15.324: INFO: Restart count of pod container-probe-4978/busybox-6eae94b8-2cb3-4ff7-a8f1-f55f77e6af5f is now 1 (48.143867486s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:27:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4978" for this suite.
Mar  5 20:27:21.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:27:21.524: INFO: namespace container-probe-4978 deletion completed in 6.17743027s

• [SLOW TEST:56.519 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:27:21.525: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-8d6f1a9b-1412-4852-87e0-efc6a7ef8b7c
STEP: Creating a pod to test consume configMaps
Mar  5 20:27:21.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9" in namespace "configmap-2541" to be "success or failure"
Mar  5 20:27:21.699: INFO: Pod "pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.537303ms
Mar  5 20:27:23.705: INFO: Pod "pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013208379s
STEP: Saw pod success
Mar  5 20:27:23.705: INFO: Pod "pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9" satisfied condition "success or failure"
Mar  5 20:27:23.710: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 20:27:23.744: INFO: Waiting for pod pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9 to disappear
Mar  5 20:27:23.748: INFO: Pod pod-configmaps-4ebdd6ac-3642-46c0-84ba-376f80454ac9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:27:23.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2541" for this suite.
Mar  5 20:27:29.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:27:29.930: INFO: namespace configmap-2541 deletion completed in 6.174149079s

• [SLOW TEST:8.405 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:27:29.930: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 20:27:30.089: INFO: Waiting up to 5m0s for pod "pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a" in namespace "emptydir-3207" to be "success or failure"
Mar  5 20:27:30.094: INFO: Pod "pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763872ms
Mar  5 20:27:32.099: INFO: Pod "pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00996367s
STEP: Saw pod success
Mar  5 20:27:32.099: INFO: Pod "pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a" satisfied condition "success or failure"
Mar  5 20:27:32.104: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a container test-container: <nil>
STEP: delete the pod
Mar  5 20:27:32.134: INFO: Waiting for pod pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a to disappear
Mar  5 20:27:32.140: INFO: Pod pod-8e669d3a-b1f1-47e1-a382-77fa9718ae1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:27:32.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3207" for this suite.
Mar  5 20:27:38.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:27:38.321: INFO: namespace emptydir-3207 deletion completed in 6.174298618s

• [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:27:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2830
STEP: Creating secret with name secret-test-1ff27ffa-306a-4269-bc9e-4e259a5899bf
STEP: Creating a pod to test consume secrets
Mar  5 20:27:38.636: INFO: Waiting up to 5m0s for pod "pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a" in namespace "secrets-1454" to be "success or failure"
Mar  5 20:27:38.641: INFO: Pod "pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.132136ms
Mar  5 20:27:40.647: INFO: Pod "pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688843s
Mar  5 20:27:42.652: INFO: Pod "pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016240673s
STEP: Saw pod success
Mar  5 20:27:42.652: INFO: Pod "pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a" satisfied condition "success or failure"
Mar  5 20:27:42.657: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:27:42.685: INFO: Waiting for pod pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a to disappear
Mar  5 20:27:42.690: INFO: Pod pod-secrets-7d8ca618-69dd-45db-8373-d3a498c4b09a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:27:42.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1454" for this suite.
Mar  5 20:27:48.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:27:48.872: INFO: namespace secrets-1454 deletion completed in 6.174773764s
STEP: Destroying namespace "secret-namespace-2830" for this suite.
Mar  5 20:27:54.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:27:55.048: INFO: namespace secret-namespace-2830 deletion completed in 6.176402339s

• [SLOW TEST:16.726 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:27:55.048: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  5 20:27:55.206: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  5 20:28:00.212: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:28:01.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7662" for this suite.
Mar  5 20:28:07.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:28:07.423: INFO: namespace replication-controller-7662 deletion completed in 6.179842285s

• [SLOW TEST:12.374 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:28:07.423: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Mar  5 20:28:07.588: INFO: Waiting up to 5m0s for pod "var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62" in namespace "var-expansion-9589" to be "success or failure"
Mar  5 20:28:07.592: INFO: Pod "var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652575ms
Mar  5 20:28:09.598: INFO: Pod "var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010224167s
STEP: Saw pod success
Mar  5 20:28:09.598: INFO: Pod "var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62" satisfied condition "success or failure"
Mar  5 20:28:09.602: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62 container dapi-container: <nil>
STEP: delete the pod
Mar  5 20:28:09.631: INFO: Waiting for pod var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62 to disappear
Mar  5 20:28:09.636: INFO: Pod var-expansion-63e60855-0210-4df6-8b55-ed6cb7ce0e62 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:28:09.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9589" for this suite.
Mar  5 20:28:15.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:28:15.829: INFO: namespace var-expansion-9589 deletion completed in 6.184888865s

• [SLOW TEST:8.406 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:28:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9707
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  5 20:28:15.989: INFO: Waiting up to 5m0s for pod "pod-56ff43d5-669f-4d11-add2-79d797199e93" in namespace "emptydir-9707" to be "success or failure"
Mar  5 20:28:15.995: INFO: Pod "pod-56ff43d5-669f-4d11-add2-79d797199e93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720645ms
Mar  5 20:28:18.000: INFO: Pod "pod-56ff43d5-669f-4d11-add2-79d797199e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011269468s
STEP: Saw pod success
Mar  5 20:28:18.000: INFO: Pod "pod-56ff43d5-669f-4d11-add2-79d797199e93" satisfied condition "success or failure"
Mar  5 20:28:18.005: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-56ff43d5-669f-4d11-add2-79d797199e93 container test-container: <nil>
STEP: delete the pod
Mar  5 20:28:18.033: INFO: Waiting for pod pod-56ff43d5-669f-4d11-add2-79d797199e93 to disappear
Mar  5 20:28:18.038: INFO: Pod pod-56ff43d5-669f-4d11-add2-79d797199e93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:28:18.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9707" for this suite.
Mar  5 20:28:24.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:28:24.226: INFO: namespace emptydir-9707 deletion completed in 6.180954241s

• [SLOW TEST:8.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:28:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  5 20:28:24.386: INFO: Waiting up to 5m0s for pod "pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34" in namespace "emptydir-8460" to be "success or failure"
Mar  5 20:28:24.392: INFO: Pod "pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.703862ms
Mar  5 20:28:26.397: INFO: Pod "pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011167505s
Mar  5 20:28:28.403: INFO: Pod "pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01653452s
STEP: Saw pod success
Mar  5 20:28:28.403: INFO: Pod "pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34" satisfied condition "success or failure"
Mar  5 20:28:28.407: INFO: Trying to get logs from node ip-192-168-106-110.ec2.internal pod pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34 container test-container: <nil>
STEP: delete the pod
Mar  5 20:28:28.436: INFO: Waiting for pod pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34 to disappear
Mar  5 20:28:28.441: INFO: Pod pod-2dc62a61-65f0-4be4-a3f0-e369a32c9b34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:28:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8460" for this suite.
Mar  5 20:28:34.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:28:34.622: INFO: namespace emptydir-8460 deletion completed in 6.173634872s

• [SLOW TEST:10.396 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:28:34.623: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5897
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 20:28:34.769: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 20:28:56.946: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.67.109&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.067: INFO: Waiting for endpoints: map[]
Mar  5 20:28:57.073: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.250.72&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.188: INFO: Waiting for endpoints: map[]
Mar  5 20:28:57.193: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.160.64&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.321: INFO: Waiting for endpoints: map[]
Mar  5 20:28:57.327: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.101.152&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:57.327: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.447: INFO: Waiting for endpoints: map[]
Mar  5 20:28:57.452: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.237.10&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:57.452: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.593: INFO: Waiting for endpoints: map[]
Mar  5 20:28:57.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.145.128:8080/dial?request=hostName&protocol=udp&host=192.168.157.13&port=8081&tries=1'] Namespace:pod-network-test-5897 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:28:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:28:57.722: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:28:57.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5897" for this suite.
Mar  5 20:29:19.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:29:19.905: INFO: namespace pod-network-test-5897 deletion completed in 22.174599706s

• [SLOW TEST:45.282 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:29:19.906: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-5vlm
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 20:29:20.076: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5vlm" in namespace "subpath-3470" to be "success or failure"
Mar  5 20:29:20.081: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681093ms
Mar  5 20:29:22.086: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 2.010119155s
Mar  5 20:29:24.091: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 4.015446995s
Mar  5 20:29:26.097: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 6.020955313s
Mar  5 20:29:28.102: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 8.026427052s
Mar  5 20:29:30.108: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 10.032440321s
Mar  5 20:29:32.114: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 12.038079412s
Mar  5 20:29:34.119: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 14.04363712s
Mar  5 20:29:36.125: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 16.049271482s
Mar  5 20:29:38.131: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 18.054869476s
Mar  5 20:29:40.136: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Running", Reason="", readiness=true. Elapsed: 20.060457013s
Mar  5 20:29:42.142: INFO: Pod "pod-subpath-test-configmap-5vlm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.066068909s
STEP: Saw pod success
Mar  5 20:29:42.142: INFO: Pod "pod-subpath-test-configmap-5vlm" satisfied condition "success or failure"
Mar  5 20:29:42.147: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod pod-subpath-test-configmap-5vlm container test-container-subpath-configmap-5vlm: <nil>
STEP: delete the pod
Mar  5 20:29:42.175: INFO: Waiting for pod pod-subpath-test-configmap-5vlm to disappear
Mar  5 20:29:42.180: INFO: Pod pod-subpath-test-configmap-5vlm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5vlm
Mar  5 20:29:42.180: INFO: Deleting pod "pod-subpath-test-configmap-5vlm" in namespace "subpath-3470"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:29:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3470" for this suite.
Mar  5 20:29:48.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:29:48.368: INFO: namespace subpath-3470 deletion completed in 6.176102002s

• [SLOW TEST:28.462 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:29:48.369: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:30:48.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-380" for this suite.
Mar  5 20:31:10.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:31:10.731: INFO: namespace container-probe-380 deletion completed in 22.190052559s

• [SLOW TEST:82.362 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:31:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3425
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3425
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3425
Mar  5 20:31:10.897: INFO: Found 0 stateful pods, waiting for 1
Mar  5 20:31:20.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  5 20:31:20.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 20:31:21.100: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 20:31:21.100: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 20:31:21.100: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 20:31:21.106: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  5 20:31:31.111: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 20:31:31.112: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 20:31:31.133: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999851s
Mar  5 20:31:32.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994179975s
Mar  5 20:31:33.145: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98844954s
Mar  5 20:31:34.151: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982502809s
Mar  5 20:31:35.156: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976702393s
Mar  5 20:31:36.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970923128s
Mar  5 20:31:37.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965145446s
Mar  5 20:31:38.174: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959330871s
Mar  5 20:31:39.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953165475s
Mar  5 20:31:40.185: INFO: Verifying statefulset ss doesn't scale past 1 for another 947.388345ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3425
Mar  5 20:31:41.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 20:31:41.390: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 20:31:41.390: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 20:31:41.390: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 20:31:41.396: INFO: Found 1 stateful pods, waiting for 3
Mar  5 20:31:51.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 20:31:51.402: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 20:31:51.402: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  5 20:31:51.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 20:31:51.620: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 20:31:51.620: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 20:31:51.620: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 20:31:51.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 20:31:51.853: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 20:31:51.853: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 20:31:51.853: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 20:31:51.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 20:31:52.066: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  5 20:31:52.066: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 20:31:52.066: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 20:31:52.066: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 20:31:52.072: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  5 20:32:02.083: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 20:32:02.083: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 20:32:02.083: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 20:32:02.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998352s
Mar  5 20:32:03.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993394199s
Mar  5 20:32:04.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987345876s
Mar  5 20:32:05.120: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980637421s
Mar  5 20:32:06.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974429854s
Mar  5 20:32:07.132: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968306027s
Mar  5 20:32:08.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962324922s
Mar  5 20:32:09.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956443166s
Mar  5 20:32:10.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950344188s
Mar  5 20:32:11.156: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.279796ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3425
Mar  5 20:32:12.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 20:32:12.367: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 20:32:12.367: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 20:32:12.367: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 20:32:12.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 20:32:12.567: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 20:32:12.567: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 20:32:12.567: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 20:32:12.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-370450052 exec --namespace=statefulset-3425 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 20:32:12.767: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  5 20:32:12.767: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 20:32:12.767: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 20:32:12.767: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  5 20:32:32.791: INFO: Deleting all statefulset in ns statefulset-3425
Mar  5 20:32:32.796: INFO: Scaling statefulset ss to 0
Mar  5 20:32:32.811: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 20:32:32.816: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:32:32.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3425" for this suite.
Mar  5 20:32:38.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:32:39.025: INFO: namespace statefulset-3425 deletion completed in 6.178250774s

• [SLOW TEST:88.293 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:32:39.025: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:33:05.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1283" for this suite.
Mar  5 20:33:11.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:33:11.667: INFO: namespace container-runtime-1283 deletion completed in 6.177125113s

• [SLOW TEST:32.643 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:33:11.668: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:33:11.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb" in namespace "projected-8902" to be "success or failure"
Mar  5 20:33:11.833: INFO: Pod "downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012966ms
Mar  5 20:33:13.838: INFO: Pod "downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547901s
Mar  5 20:33:15.844: INFO: Pod "downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017254718s
STEP: Saw pod success
Mar  5 20:33:15.844: INFO: Pod "downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb" satisfied condition "success or failure"
Mar  5 20:33:15.849: INFO: Trying to get logs from node ip-192-168-148-164.ec2.internal pod downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb container client-container: <nil>
STEP: delete the pod
Mar  5 20:33:15.878: INFO: Waiting for pod downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb to disappear
Mar  5 20:33:15.883: INFO: Pod downwardapi-volume-5aeb51d5-7e31-4365-8caf-27dc44bc1ebb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:33:15.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8902" for this suite.
Mar  5 20:33:21.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:33:22.062: INFO: namespace projected-8902 deletion completed in 6.171706889s

• [SLOW TEST:10.394 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:33:22.063: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  5 20:33:22.216: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:33:32.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9333" for this suite.
Mar  5 20:33:38.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:33:38.631: INFO: namespace pods-9333 deletion completed in 6.173368181s

• [SLOW TEST:16.568 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:33:38.632: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  5 20:33:38.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472" in namespace "downward-api-5740" to be "success or failure"
Mar  5 20:33:38.797: INFO: Pod "downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472": Phase="Pending", Reason="", readiness=false. Elapsed: 5.805964ms
Mar  5 20:33:40.802: INFO: Pod "downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011365744s
Mar  5 20:33:42.808: INFO: Pod "downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016946726s
STEP: Saw pod success
Mar  5 20:33:42.808: INFO: Pod "downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472" satisfied condition "success or failure"
Mar  5 20:33:42.813: INFO: Trying to get logs from node ip-192-168-155-60.ec2.internal pod downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472 container client-container: <nil>
STEP: delete the pod
Mar  5 20:33:42.841: INFO: Waiting for pod downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472 to disappear
Mar  5 20:33:42.846: INFO: Pod downwardapi-volume-8209f092-73f8-4203-af17-1d46795de472 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:33:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5740" for this suite.
Mar  5 20:33:48.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:33:49.029: INFO: namespace downward-api-5740 deletion completed in 6.175388499s

• [SLOW TEST:10.397 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:33:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1887
I0305 20:33:49.181698      20 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1887, replica count: 1
I0305 20:33:50.232123      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 20:33:51.232370      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 20:33:51.347: INFO: Created: latency-svc-k8trv
Mar  5 20:33:51.353: INFO: Got endpoints: latency-svc-k8trv [20.248638ms]
Mar  5 20:33:51.368: INFO: Created: latency-svc-rwfc7
Mar  5 20:33:51.373: INFO: Got endpoints: latency-svc-rwfc7 [19.313135ms]
Mar  5 20:33:51.375: INFO: Created: latency-svc-hfnk5
Mar  5 20:33:51.384: INFO: Got endpoints: latency-svc-hfnk5 [30.752962ms]
Mar  5 20:33:51.388: INFO: Created: latency-svc-lfsg8
Mar  5 20:33:51.393: INFO: Got endpoints: latency-svc-lfsg8 [39.099167ms]
Mar  5 20:33:51.395: INFO: Created: latency-svc-hwnfl
Mar  5 20:33:51.401: INFO: Got endpoints: latency-svc-hwnfl [46.835925ms]
Mar  5 20:33:51.403: INFO: Created: latency-svc-jdr9c
Mar  5 20:33:51.409: INFO: Got endpoints: latency-svc-jdr9c [54.634368ms]
Mar  5 20:33:51.412: INFO: Created: latency-svc-sc27z
Mar  5 20:33:51.417: INFO: Got endpoints: latency-svc-sc27z [62.64496ms]
Mar  5 20:33:51.420: INFO: Created: latency-svc-lkhzh
Mar  5 20:33:51.425: INFO: Got endpoints: latency-svc-lkhzh [70.412546ms]
Mar  5 20:33:51.428: INFO: Created: latency-svc-mszd8
Mar  5 20:33:51.434: INFO: Got endpoints: latency-svc-mszd8 [78.909083ms]
Mar  5 20:33:51.437: INFO: Created: latency-svc-g2thp
Mar  5 20:33:51.442: INFO: Got endpoints: latency-svc-g2thp [87.661096ms]
Mar  5 20:33:51.445: INFO: Created: latency-svc-vrsgx
Mar  5 20:33:51.451: INFO: Got endpoints: latency-svc-vrsgx [97.078115ms]
Mar  5 20:33:51.455: INFO: Created: latency-svc-p5zgc
Mar  5 20:33:51.460: INFO: Got endpoints: latency-svc-p5zgc [105.856347ms]
Mar  5 20:33:51.472: INFO: Created: latency-svc-g8fs5
Mar  5 20:33:51.481: INFO: Created: latency-svc-tc4fp
Mar  5 20:33:51.482: INFO: Got endpoints: latency-svc-g8fs5 [126.924419ms]
Mar  5 20:33:51.487: INFO: Got endpoints: latency-svc-tc4fp [132.004211ms]
Mar  5 20:33:51.489: INFO: Created: latency-svc-dxtvs
Mar  5 20:33:51.497: INFO: Got endpoints: latency-svc-dxtvs [142.058908ms]
Mar  5 20:33:51.497: INFO: Created: latency-svc-g5x8z
Mar  5 20:33:51.503: INFO: Got endpoints: latency-svc-g5x8z [147.557071ms]
Mar  5 20:33:51.506: INFO: Created: latency-svc-t2mm7
Mar  5 20:33:51.513: INFO: Got endpoints: latency-svc-t2mm7 [140.543749ms]
Mar  5 20:33:51.514: INFO: Created: latency-svc-svk95
Mar  5 20:33:51.520: INFO: Got endpoints: latency-svc-svk95 [135.387751ms]
Mar  5 20:33:51.522: INFO: Created: latency-svc-4c6t7
Mar  5 20:33:51.528: INFO: Got endpoints: latency-svc-4c6t7 [134.753449ms]
Mar  5 20:33:51.532: INFO: Created: latency-svc-5xblq
Mar  5 20:33:51.538: INFO: Got endpoints: latency-svc-5xblq [136.942008ms]
Mar  5 20:33:51.540: INFO: Created: latency-svc-nwtln
Mar  5 20:33:51.545: INFO: Got endpoints: latency-svc-nwtln [136.826867ms]
Mar  5 20:33:51.548: INFO: Created: latency-svc-4bs94
Mar  5 20:33:51.553: INFO: Got endpoints: latency-svc-4bs94 [136.225339ms]
Mar  5 20:33:51.557: INFO: Created: latency-svc-jzbrf
Mar  5 20:33:51.562: INFO: Got endpoints: latency-svc-jzbrf [137.279429ms]
Mar  5 20:33:51.565: INFO: Created: latency-svc-lq962
Mar  5 20:33:51.570: INFO: Got endpoints: latency-svc-lq962 [136.024123ms]
Mar  5 20:33:51.573: INFO: Created: latency-svc-gqxxm
Mar  5 20:33:51.578: INFO: Got endpoints: latency-svc-gqxxm [135.198023ms]
Mar  5 20:33:51.580: INFO: Created: latency-svc-nn46v
Mar  5 20:33:51.585: INFO: Got endpoints: latency-svc-nn46v [133.745417ms]
Mar  5 20:33:51.588: INFO: Created: latency-svc-l8swn
Mar  5 20:33:51.594: INFO: Got endpoints: latency-svc-l8swn [133.739762ms]
Mar  5 20:33:51.597: INFO: Created: latency-svc-qc6ql
Mar  5 20:33:51.602: INFO: Got endpoints: latency-svc-qc6ql [120.124994ms]
Mar  5 20:33:51.605: INFO: Created: latency-svc-2jg5d
Mar  5 20:33:51.611: INFO: Got endpoints: latency-svc-2jg5d [124.102002ms]
Mar  5 20:33:51.615: INFO: Created: latency-svc-t9twd
Mar  5 20:33:51.619: INFO: Got endpoints: latency-svc-t9twd [122.386882ms]
Mar  5 20:33:51.622: INFO: Created: latency-svc-qtxdj
Mar  5 20:33:51.627: INFO: Got endpoints: latency-svc-qtxdj [123.674255ms]
Mar  5 20:33:51.629: INFO: Created: latency-svc-6f4gz
Mar  5 20:33:51.635: INFO: Got endpoints: latency-svc-6f4gz [121.711078ms]
Mar  5 20:33:51.640: INFO: Created: latency-svc-sh89x
Mar  5 20:33:51.646: INFO: Got endpoints: latency-svc-sh89x [125.721595ms]
Mar  5 20:33:51.648: INFO: Created: latency-svc-5lkkq
Mar  5 20:33:51.653: INFO: Got endpoints: latency-svc-5lkkq [124.879568ms]
Mar  5 20:33:51.655: INFO: Created: latency-svc-48lbf
Mar  5 20:33:51.664: INFO: Got endpoints: latency-svc-48lbf [126.68332ms]
Mar  5 20:33:51.665: INFO: Created: latency-svc-58vbh
Mar  5 20:33:51.670: INFO: Got endpoints: latency-svc-58vbh [123.659028ms]
Mar  5 20:33:51.672: INFO: Created: latency-svc-j54gg
Mar  5 20:33:51.680: INFO: Created: latency-svc-d7wfn
Mar  5 20:33:51.687: INFO: Created: latency-svc-2bklk
Mar  5 20:33:51.695: INFO: Created: latency-svc-79vwn
Mar  5 20:33:51.703: INFO: Got endpoints: latency-svc-j54gg [149.62345ms]
Mar  5 20:33:51.703: INFO: Created: latency-svc-4wpjz
Mar  5 20:33:51.712: INFO: Created: latency-svc-cd8gg
Mar  5 20:33:51.719: INFO: Created: latency-svc-2p5jb
Mar  5 20:33:51.727: INFO: Created: latency-svc-stg59
Mar  5 20:33:51.734: INFO: Created: latency-svc-xk2lq
Mar  5 20:33:51.744: INFO: Created: latency-svc-kjwjd
Mar  5 20:33:51.751: INFO: Created: latency-svc-gbhwr
Mar  5 20:33:51.754: INFO: Got endpoints: latency-svc-d7wfn [192.398785ms]
Mar  5 20:33:51.757: INFO: Created: latency-svc-pzb65
Mar  5 20:33:51.764: INFO: Created: latency-svc-2brrg
Mar  5 20:33:51.775: INFO: Created: latency-svc-wmpz5
Mar  5 20:33:51.782: INFO: Created: latency-svc-2b7jq
Mar  5 20:33:51.790: INFO: Created: latency-svc-qlrdm
Mar  5 20:33:51.799: INFO: Created: latency-svc-zzrwk
Mar  5 20:33:51.802: INFO: Got endpoints: latency-svc-2bklk [231.606433ms]
Mar  5 20:33:51.817: INFO: Created: latency-svc-glt5w
Mar  5 20:33:51.852: INFO: Got endpoints: latency-svc-79vwn [274.357595ms]
Mar  5 20:33:51.866: INFO: Created: latency-svc-2g692
Mar  5 20:33:51.902: INFO: Got endpoints: latency-svc-4wpjz [316.470039ms]
Mar  5 20:33:51.916: INFO: Created: latency-svc-sjqnh
Mar  5 20:33:51.952: INFO: Got endpoints: latency-svc-cd8gg [357.583612ms]
Mar  5 20:33:51.966: INFO: Created: latency-svc-9ztc7
Mar  5 20:33:52.002: INFO: Got endpoints: latency-svc-2p5jb [399.615954ms]
Mar  5 20:33:52.015: INFO: Created: latency-svc-8zv6l
Mar  5 20:33:52.053: INFO: Got endpoints: latency-svc-stg59 [441.430571ms]
Mar  5 20:33:52.067: INFO: Created: latency-svc-2s5zv
Mar  5 20:33:52.102: INFO: Got endpoints: latency-svc-xk2lq [482.368764ms]
Mar  5 20:33:52.117: INFO: Created: latency-svc-j6bjv
Mar  5 20:33:52.152: INFO: Got endpoints: latency-svc-kjwjd [525.18031ms]
Mar  5 20:33:52.166: INFO: Created: latency-svc-jwmrk
Mar  5 20:33:52.202: INFO: Got endpoints: latency-svc-gbhwr [567.13848ms]
Mar  5 20:33:52.215: INFO: Created: latency-svc-dts5z
Mar  5 20:33:52.252: INFO: Got endpoints: latency-svc-pzb65 [606.221993ms]
Mar  5 20:33:52.268: INFO: Created: latency-svc-c8zvv
Mar  5 20:33:52.302: INFO: Got endpoints: latency-svc-2brrg [648.97369ms]
Mar  5 20:33:52.316: INFO: Created: latency-svc-vn4ph
Mar  5 20:33:52.352: INFO: Got endpoints: latency-svc-wmpz5 [687.384672ms]
Mar  5 20:33:52.365: INFO: Created: latency-svc-8d755
Mar  5 20:33:52.402: INFO: Got endpoints: latency-svc-2b7jq [732.368475ms]
Mar  5 20:33:52.417: INFO: Created: latency-svc-7tb6h
Mar  5 20:33:52.452: INFO: Got endpoints: latency-svc-qlrdm [748.998989ms]
Mar  5 20:33:52.466: INFO: Created: latency-svc-mknzm
Mar  5 20:33:52.502: INFO: Got endpoints: latency-svc-zzrwk [747.476027ms]
Mar  5 20:33:52.516: INFO: Created: latency-svc-9cv27
Mar  5 20:33:52.552: INFO: Got endpoints: latency-svc-glt5w [750.375937ms]
Mar  5 20:33:52.566: INFO: Created: latency-svc-6p64b
Mar  5 20:33:52.602: INFO: Got endpoints: latency-svc-2g692 [749.306795ms]
Mar  5 20:33:52.615: INFO: Created: latency-svc-4kx87
Mar  5 20:33:52.652: INFO: Got endpoints: latency-svc-sjqnh [749.968011ms]
Mar  5 20:33:52.666: INFO: Created: latency-svc-c8dxj
Mar  5 20:33:52.702: INFO: Got endpoints: latency-svc-9ztc7 [749.820862ms]
Mar  5 20:33:52.716: INFO: Created: latency-svc-sdh5f
Mar  5 20:33:52.752: INFO: Got endpoints: latency-svc-8zv6l [749.358491ms]
Mar  5 20:33:52.765: INFO: Created: latency-svc-l96fg
Mar  5 20:33:52.802: INFO: Got endpoints: latency-svc-2s5zv [748.932285ms]
Mar  5 20:33:52.816: INFO: Created: latency-svc-cg2zv
Mar  5 20:33:52.852: INFO: Got endpoints: latency-svc-j6bjv [750.559852ms]
Mar  5 20:33:52.865: INFO: Created: latency-svc-rkxm2
Mar  5 20:33:52.902: INFO: Got endpoints: latency-svc-jwmrk [750.052794ms]
Mar  5 20:33:52.916: INFO: Created: latency-svc-7jghq
Mar  5 20:33:52.953: INFO: Got endpoints: latency-svc-dts5z [750.220888ms]
Mar  5 20:33:52.967: INFO: Created: latency-svc-km99f
Mar  5 20:33:53.003: INFO: Got endpoints: latency-svc-c8zvv [750.776038ms]
Mar  5 20:33:53.019: INFO: Created: latency-svc-qj6lf
Mar  5 20:33:53.054: INFO: Got endpoints: latency-svc-vn4ph [751.647795ms]
Mar  5 20:33:53.068: INFO: Created: latency-svc-hbv9w
Mar  5 20:33:53.102: INFO: Got endpoints: latency-svc-8d755 [750.242549ms]
Mar  5 20:33:53.116: INFO: Created: latency-svc-xhwdv
Mar  5 20:33:53.152: INFO: Got endpoints: latency-svc-7tb6h [749.770064ms]
Mar  5 20:33:53.165: INFO: Created: latency-svc-kkgsg
Mar  5 20:33:53.202: INFO: Got endpoints: latency-svc-mknzm [750.075683ms]
Mar  5 20:33:53.216: INFO: Created: latency-svc-fp7vf
Mar  5 20:33:53.253: INFO: Got endpoints: latency-svc-9cv27 [750.474924ms]
Mar  5 20:33:53.267: INFO: Created: latency-svc-94kvh
Mar  5 20:33:53.302: INFO: Got endpoints: latency-svc-6p64b [749.851592ms]
Mar  5 20:33:53.317: INFO: Created: latency-svc-lk787
Mar  5 20:33:53.353: INFO: Got endpoints: latency-svc-4kx87 [751.348719ms]
Mar  5 20:33:53.373: INFO: Created: latency-svc-mtslk
Mar  5 20:33:53.403: INFO: Got endpoints: latency-svc-c8dxj [750.334935ms]
Mar  5 20:33:53.417: INFO: Created: latency-svc-zrd7m
Mar  5 20:33:53.453: INFO: Got endpoints: latency-svc-sdh5f [750.712916ms]
Mar  5 20:33:53.467: INFO: Created: latency-svc-hrt9s
Mar  5 20:33:53.503: INFO: Got endpoints: latency-svc-l96fg [751.047303ms]
Mar  5 20:33:53.517: INFO: Created: latency-svc-55n7z
Mar  5 20:33:53.552: INFO: Got endpoints: latency-svc-cg2zv [750.308605ms]
Mar  5 20:33:53.567: INFO: Created: latency-svc-65hb9
Mar  5 20:33:53.602: INFO: Got endpoints: latency-svc-rkxm2 [749.647166ms]
Mar  5 20:33:53.624: INFO: Created: latency-svc-6tfb4
Mar  5 20:33:53.652: INFO: Got endpoints: latency-svc-7jghq [750.144046ms]
Mar  5 20:33:53.666: INFO: Created: latency-svc-nlzrk
Mar  5 20:33:53.703: INFO: Got endpoints: latency-svc-km99f [750.036297ms]
Mar  5 20:33:53.718: INFO: Created: latency-svc-q56hl
Mar  5 20:33:53.752: INFO: Got endpoints: latency-svc-qj6lf [749.130403ms]
Mar  5 20:33:53.766: INFO: Created: latency-svc-6zbfh
Mar  5 20:33:53.804: INFO: Got endpoints: latency-svc-hbv9w [749.121711ms]
Mar  5 20:33:53.817: INFO: Created: latency-svc-knfzq
Mar  5 20:33:53.853: INFO: Got endpoints: latency-svc-xhwdv [750.302325ms]
Mar  5 20:33:53.866: INFO: Created: latency-svc-g74jh
Mar  5 20:33:53.902: INFO: Got endpoints: latency-svc-kkgsg [750.11173ms]
Mar  5 20:33:53.916: INFO: Created: latency-svc-gvb9t
Mar  5 20:33:53.952: INFO: Got endpoints: latency-svc-fp7vf [749.664494ms]
Mar  5 20:33:53.966: INFO: Created: latency-svc-v27nc
Mar  5 20:33:54.002: INFO: Got endpoints: latency-svc-94kvh [749.293365ms]
Mar  5 20:33:54.017: INFO: Created: latency-svc-kcmsr
Mar  5 20:33:54.052: INFO: Got endpoints: latency-svc-lk787 [748.741112ms]
Mar  5 20:33:54.065: INFO: Created: latency-svc-5lkps
Mar  5 20:33:54.102: INFO: Got endpoints: latency-svc-mtslk [748.104827ms]
Mar  5 20:33:54.115: INFO: Created: latency-svc-x4sjf
Mar  5 20:33:54.153: INFO: Got endpoints: latency-svc-zrd7m [749.967678ms]
Mar  5 20:33:54.167: INFO: Created: latency-svc-xjc97
Mar  5 20:33:54.202: INFO: Got endpoints: latency-svc-hrt9s [748.594797ms]
Mar  5 20:33:54.216: INFO: Created: latency-svc-sdcqj
Mar  5 20:33:54.252: INFO: Got endpoints: latency-svc-55n7z [748.736708ms]
Mar  5 20:33:54.265: INFO: Created: latency-svc-fwwfk
Mar  5 20:33:54.302: INFO: Got endpoints: latency-svc-65hb9 [749.359685ms]
Mar  5 20:33:54.316: INFO: Created: latency-svc-7ztkz
Mar  5 20:33:54.353: INFO: Got endpoints: latency-svc-6tfb4 [750.591174ms]
Mar  5 20:33:54.367: INFO: Created: latency-svc-gllhp
Mar  5 20:33:54.403: INFO: Got endpoints: latency-svc-nlzrk [750.315687ms]
Mar  5 20:33:54.417: INFO: Created: latency-svc-m6msb
Mar  5 20:33:54.452: INFO: Got endpoints: latency-svc-q56hl [749.30818ms]
Mar  5 20:33:54.466: INFO: Created: latency-svc-sxljn
Mar  5 20:33:54.502: INFO: Got endpoints: latency-svc-6zbfh [750.217855ms]
Mar  5 20:33:54.517: INFO: Created: latency-svc-52lp8
Mar  5 20:33:54.552: INFO: Got endpoints: latency-svc-knfzq [748.706161ms]
Mar  5 20:33:54.567: INFO: Created: latency-svc-hb78c
Mar  5 20:33:54.605: INFO: Got endpoints: latency-svc-g74jh [752.737733ms]
Mar  5 20:33:54.622: INFO: Created: latency-svc-67m9x
Mar  5 20:33:54.653: INFO: Got endpoints: latency-svc-gvb9t [750.628788ms]
Mar  5 20:33:54.666: INFO: Created: latency-svc-ltq84
Mar  5 20:33:54.702: INFO: Got endpoints: latency-svc-v27nc [749.748734ms]
Mar  5 20:33:54.717: INFO: Created: latency-svc-5nlwv
Mar  5 20:33:54.752: INFO: Got endpoints: latency-svc-kcmsr [749.998608ms]
Mar  5 20:33:54.766: INFO: Created: latency-svc-tm5q4
Mar  5 20:33:54.802: INFO: Got endpoints: latency-svc-5lkps [750.072968ms]
Mar  5 20:33:54.817: INFO: Created: latency-svc-2z48f
Mar  5 20:33:54.852: INFO: Got endpoints: latency-svc-x4sjf [750.430522ms]
Mar  5 20:33:54.867: INFO: Created: latency-svc-ncqfh
Mar  5 20:33:54.902: INFO: Got endpoints: latency-svc-xjc97 [749.458635ms]
Mar  5 20:33:54.916: INFO: Created: latency-svc-9fk9h
Mar  5 20:33:54.952: INFO: Got endpoints: latency-svc-sdcqj [750.395345ms]
Mar  5 20:33:54.966: INFO: Created: latency-svc-44rlc
Mar  5 20:33:55.002: INFO: Got endpoints: latency-svc-fwwfk [749.951367ms]
Mar  5 20:33:55.016: INFO: Created: latency-svc-plmrw
Mar  5 20:33:55.052: INFO: Got endpoints: latency-svc-7ztkz [750.023948ms]
Mar  5 20:33:55.067: INFO: Created: latency-svc-z65lz
Mar  5 20:33:55.102: INFO: Got endpoints: latency-svc-gllhp [749.478005ms]
Mar  5 20:33:55.116: INFO: Created: latency-svc-w96h4
Mar  5 20:33:55.152: INFO: Got endpoints: latency-svc-m6msb [749.02853ms]
Mar  5 20:33:55.166: INFO: Created: latency-svc-fx28g
Mar  5 20:33:55.204: INFO: Got endpoints: latency-svc-sxljn [751.000421ms]
Mar  5 20:33:55.218: INFO: Created: latency-svc-qkhld
Mar  5 20:33:55.252: INFO: Got endpoints: latency-svc-52lp8 [750.089249ms]
Mar  5 20:33:55.270: INFO: Created: latency-svc-fhpmq
Mar  5 20:33:55.303: INFO: Got endpoints: latency-svc-hb78c [749.85902ms]
Mar  5 20:33:55.316: INFO: Created: latency-svc-hpjxf
Mar  5 20:33:55.352: INFO: Got endpoints: latency-svc-67m9x [746.712982ms]
Mar  5 20:33:55.367: INFO: Created: latency-svc-lrvbn
Mar  5 20:33:55.402: INFO: Got endpoints: latency-svc-ltq84 [749.497287ms]
Mar  5 20:33:55.416: INFO: Created: latency-svc-dsz72
Mar  5 20:33:55.452: INFO: Got endpoints: latency-svc-5nlwv [750.049694ms]
Mar  5 20:33:55.466: INFO: Created: latency-svc-hhdh6
Mar  5 20:33:55.502: INFO: Got endpoints: latency-svc-tm5q4 [749.619631ms]
Mar  5 20:33:55.516: INFO: Created: latency-svc-lfj4p
Mar  5 20:33:55.552: INFO: Got endpoints: latency-svc-2z48f [750.073021ms]
Mar  5 20:33:55.566: INFO: Created: latency-svc-sd8qj
Mar  5 20:33:55.602: INFO: Got endpoints: latency-svc-ncqfh [750.175071ms]
Mar  5 20:33:55.626: INFO: Created: latency-svc-c648m
Mar  5 20:33:55.652: INFO: Got endpoints: latency-svc-9fk9h [749.564172ms]
Mar  5 20:33:55.667: INFO: Created: latency-svc-ddvxs
Mar  5 20:33:55.703: INFO: Got endpoints: latency-svc-44rlc [750.893312ms]
Mar  5 20:33:55.719: INFO: Created: latency-svc-zjqd7
Mar  5 20:33:55.752: INFO: Got endpoints: latency-svc-plmrw [750.236939ms]
Mar  5 20:33:55.766: INFO: Created: latency-svc-z2pvr
Mar  5 20:33:55.802: INFO: Got endpoints: latency-svc-z65lz [749.318185ms]
Mar  5 20:33:55.817: INFO: Created: latency-svc-vcqkc
Mar  5 20:33:55.853: INFO: Got endpoints: latency-svc-w96h4 [750.394285ms]
Mar  5 20:33:55.867: INFO: Created: latency-svc-rdhkr
Mar  5 20:33:55.902: INFO: Got endpoints: latency-svc-fx28g [750.107168ms]
Mar  5 20:33:55.916: INFO: Created: latency-svc-4476t
Mar  5 20:33:55.952: INFO: Got endpoints: latency-svc-qkhld [747.512054ms]
Mar  5 20:33:55.966: INFO: Created: latency-svc-fmjf8
Mar  5 20:33:56.002: INFO: Got endpoints: latency-svc-fhpmq [749.605109ms]
Mar  5 20:33:56.017: INFO: Created: latency-svc-79rrn
Mar  5 20:33:56.052: INFO: Got endpoints: latency-svc-hpjxf [749.555235ms]
Mar  5 20:33:56.067: INFO: Created: latency-svc-8hptg
Mar  5 20:33:56.105: INFO: Got endpoints: latency-svc-lrvbn [752.063469ms]
Mar  5 20:33:56.119: INFO: Created: latency-svc-tm8pm
Mar  5 20:33:56.155: INFO: Got endpoints: latency-svc-dsz72 [752.72271ms]
Mar  5 20:33:56.170: INFO: Created: latency-svc-7j75n
Mar  5 20:33:56.203: INFO: Got endpoints: latency-svc-hhdh6 [750.17255ms]
Mar  5 20:33:56.217: INFO: Created: latency-svc-hhpg4
Mar  5 20:33:56.253: INFO: Got endpoints: latency-svc-lfj4p [750.553127ms]
Mar  5 20:33:56.268: INFO: Created: latency-svc-8wnfd
Mar  5 20:33:56.304: INFO: Got endpoints: latency-svc-sd8qj [751.434368ms]
Mar  5 20:33:56.318: INFO: Created: latency-svc-qqp7p
Mar  5 20:33:56.352: INFO: Got endpoints: latency-svc-c648m [749.223347ms]
Mar  5 20:33:56.366: INFO: Created: latency-svc-p2ddk
Mar  5 20:33:56.403: INFO: Got endpoints: latency-svc-ddvxs [751.052085ms]
Mar  5 20:33:56.420: INFO: Created: latency-svc-kxsfl
Mar  5 20:33:56.452: INFO: Got endpoints: latency-svc-zjqd7 [748.739716ms]
Mar  5 20:33:56.466: INFO: Created: latency-svc-sk94r
Mar  5 20:33:56.502: INFO: Got endpoints: latency-svc-z2pvr [749.262106ms]
Mar  5 20:33:56.517: INFO: Created: latency-svc-rwsf2
Mar  5 20:33:56.552: INFO: Got endpoints: latency-svc-vcqkc [750.136889ms]
Mar  5 20:33:56.568: INFO: Created: latency-svc-567z4
Mar  5 20:33:56.603: INFO: Got endpoints: latency-svc-rdhkr [750.408218ms]
Mar  5 20:33:56.618: INFO: Created: latency-svc-kwjvv
Mar  5 20:33:56.652: INFO: Got endpoints: latency-svc-4476t [749.584945ms]
Mar  5 20:33:56.667: INFO: Created: latency-svc-2w5kw
Mar  5 20:33:56.702: INFO: Got endpoints: latency-svc-fmjf8 [750.083417ms]
Mar  5 20:33:56.716: INFO: Created: latency-svc-lzzzk
Mar  5 20:33:56.752: INFO: Got endpoints: latency-svc-79rrn [750.109341ms]
Mar  5 20:33:56.766: INFO: Created: latency-svc-hl9vf
Mar  5 20:33:56.802: INFO: Got endpoints: latency-svc-8hptg [749.919292ms]
Mar  5 20:33:56.816: INFO: Created: latency-svc-sfkr6
Mar  5 20:33:56.853: INFO: Got endpoints: latency-svc-tm8pm [747.54686ms]
Mar  5 20:33:56.866: INFO: Created: latency-svc-hklx9
Mar  5 20:33:56.903: INFO: Got endpoints: latency-svc-7j75n [747.397651ms]
Mar  5 20:33:56.916: INFO: Created: latency-svc-n5mdj
Mar  5 20:33:56.952: INFO: Got endpoints: latency-svc-hhpg4 [749.74963ms]
Mar  5 20:33:56.968: INFO: Created: latency-svc-qm4m2
Mar  5 20:33:57.002: INFO: Got endpoints: latency-svc-8wnfd [748.52433ms]
Mar  5 20:33:57.020: INFO: Created: latency-svc-2r97k
Mar  5 20:33:57.052: INFO: Got endpoints: latency-svc-qqp7p [748.09838ms]
Mar  5 20:33:57.066: INFO: Created: latency-svc-l8hng
Mar  5 20:33:57.102: INFO: Got endpoints: latency-svc-p2ddk [749.9584ms]
Mar  5 20:33:57.116: INFO: Created: latency-svc-cs99r
Mar  5 20:33:57.154: INFO: Got endpoints: latency-svc-kxsfl [750.380988ms]
Mar  5 20:33:57.168: INFO: Created: latency-svc-6jqwd
Mar  5 20:33:57.203: INFO: Got endpoints: latency-svc-sk94r [750.100978ms]
Mar  5 20:33:57.217: INFO: Created: latency-svc-g66vt
Mar  5 20:33:57.252: INFO: Got endpoints: latency-svc-rwsf2 [750.377641ms]
Mar  5 20:33:57.266: INFO: Created: latency-svc-7rdhh
Mar  5 20:33:57.302: INFO: Got endpoints: latency-svc-567z4 [749.715157ms]
Mar  5 20:33:57.316: INFO: Created: latency-svc-tkt7h
Mar  5 20:33:57.353: INFO: Got endpoints: latency-svc-kwjvv [749.378822ms]
Mar  5 20:33:57.369: INFO: Created: latency-svc-88mkr
Mar  5 20:33:57.402: INFO: Got endpoints: latency-svc-2w5kw [749.868716ms]
Mar  5 20:33:57.416: INFO: Created: latency-svc-w2t6d
Mar  5 20:33:57.453: INFO: Got endpoints: latency-svc-lzzzk [750.699512ms]
Mar  5 20:33:57.467: INFO: Created: latency-svc-2fpn2
Mar  5 20:33:57.502: INFO: Got endpoints: latency-svc-hl9vf [750.010878ms]
Mar  5 20:33:57.516: INFO: Created: latency-svc-f9q8p
Mar  5 20:33:57.552: INFO: Got endpoints: latency-svc-sfkr6 [750.1725ms]
Mar  5 20:33:57.567: INFO: Created: latency-svc-l65wt
Mar  5 20:33:57.602: INFO: Got endpoints: latency-svc-hklx9 [749.58496ms]
Mar  5 20:33:57.618: INFO: Created: latency-svc-h2mbs
Mar  5 20:33:57.653: INFO: Got endpoints: latency-svc-n5mdj [750.446282ms]
Mar  5 20:33:57.677: INFO: Created: latency-svc-5qntp
Mar  5 20:33:57.703: INFO: Got endpoints: latency-svc-qm4m2 [750.139941ms]
Mar  5 20:33:57.718: INFO: Created: latency-svc-jzzst
Mar  5 20:33:57.753: INFO: Got endpoints: latency-svc-2r97k [750.330028ms]
Mar  5 20:33:57.766: INFO: Created: latency-svc-qzcgp
Mar  5 20:33:57.802: INFO: Got endpoints: latency-svc-l8hng [749.738909ms]
Mar  5 20:33:57.816: INFO: Created: latency-svc-9swgz
Mar  5 20:33:57.852: INFO: Got endpoints: latency-svc-cs99r [750.109274ms]
Mar  5 20:33:57.866: INFO: Created: latency-svc-4sfbk
Mar  5 20:33:57.903: INFO: Got endpoints: latency-svc-6jqwd [748.171046ms]
Mar  5 20:33:57.919: INFO: Created: latency-svc-s79p4
Mar  5 20:33:57.952: INFO: Got endpoints: latency-svc-g66vt [749.377804ms]
Mar  5 20:33:57.966: INFO: Created: latency-svc-5nk9c
Mar  5 20:33:58.002: INFO: Got endpoints: latency-svc-7rdhh [749.571647ms]
Mar  5 20:33:58.016: INFO: Created: latency-svc-xpxbm
Mar  5 20:33:58.052: INFO: Got endpoints: latency-svc-tkt7h [749.583141ms]
Mar  5 20:33:58.066: INFO: Created: latency-svc-d8mbq
Mar  5 20:33:58.102: INFO: Got endpoints: latency-svc-88mkr [749.062986ms]
Mar  5 20:33:58.119: INFO: Created: latency-svc-2lmx4
Mar  5 20:33:58.153: INFO: Got endpoints: latency-svc-w2t6d [750.042419ms]
Mar  5 20:33:58.167: INFO: Created: latency-svc-mt42v
Mar  5 20:33:58.202: INFO: Got endpoints: latency-svc-2fpn2 [748.718652ms]
Mar  5 20:33:58.216: INFO: Created: latency-svc-258lg
Mar  5 20:33:58.252: INFO: Got endpoints: latency-svc-f9q8p [749.904339ms]
Mar  5 20:33:58.266: INFO: Created: latency-svc-nzbj7
Mar  5 20:33:58.303: INFO: Got endpoints: latency-svc-l65wt [750.080006ms]
Mar  5 20:33:58.317: INFO: Created: latency-svc-pkr96
Mar  5 20:33:58.357: INFO: Got endpoints: latency-svc-h2mbs [754.47771ms]
Mar  5 20:33:58.371: INFO: Created: latency-svc-r96w2
Mar  5 20:33:58.402: INFO: Got endpoints: latency-svc-5qntp [748.808095ms]
Mar  5 20:33:58.416: INFO: Created: latency-svc-j2n9s
Mar  5 20:33:58.453: INFO: Got endpoints: latency-svc-jzzst [749.803564ms]
Mar  5 20:33:58.467: INFO: Created: latency-svc-7rxmx
Mar  5 20:33:58.502: INFO: Got endpoints: latency-svc-qzcgp [749.256596ms]
Mar  5 20:33:58.516: INFO: Created: latency-svc-2x5xx
Mar  5 20:33:58.553: INFO: Got endpoints: latency-svc-9swgz [750.422495ms]
Mar  5 20:33:58.566: INFO: Created: latency-svc-xsnhl
Mar  5 20:33:58.602: INFO: Got endpoints: latency-svc-4sfbk [749.52211ms]
Mar  5 20:33:58.616: INFO: Created: latency-svc-7r4k7
Mar  5 20:33:58.653: INFO: Got endpoints: latency-svc-s79p4 [750.259903ms]
Mar  5 20:33:58.667: INFO: Created: latency-svc-lp2qj
Mar  5 20:33:58.702: INFO: Got endpoints: latency-svc-5nk9c [749.69724ms]
Mar  5 20:33:58.716: INFO: Created: latency-svc-zxpmz
Mar  5 20:33:58.752: INFO: Got endpoints: latency-svc-xpxbm [749.761876ms]
Mar  5 20:33:58.766: INFO: Created: latency-svc-ckqpv
Mar  5 20:33:58.804: INFO: Got endpoints: latency-svc-d8mbq [751.482417ms]
Mar  5 20:33:58.817: INFO: Created: latency-svc-7fmbq
Mar  5 20:33:58.852: INFO: Got endpoints: latency-svc-2lmx4 [750.584025ms]
Mar  5 20:33:58.866: INFO: Created: latency-svc-q69h4
Mar  5 20:33:58.902: INFO: Got endpoints: latency-svc-mt42v [749.044011ms]
Mar  5 20:33:58.916: INFO: Created: latency-svc-z9prx
Mar  5 20:33:58.954: INFO: Got endpoints: latency-svc-258lg [752.295563ms]
Mar  5 20:33:58.968: INFO: Created: latency-svc-6mlnp
Mar  5 20:33:59.002: INFO: Got endpoints: latency-svc-nzbj7 [750.006587ms]
Mar  5 20:33:59.016: INFO: Created: latency-svc-j5vl6
Mar  5 20:33:59.053: INFO: Got endpoints: latency-svc-pkr96 [749.793672ms]
Mar  5 20:33:59.066: INFO: Created: latency-svc-hg8d5
Mar  5 20:33:59.102: INFO: Got endpoints: latency-svc-r96w2 [745.417961ms]
Mar  5 20:33:59.116: INFO: Created: latency-svc-2k5bq
Mar  5 20:33:59.152: INFO: Got endpoints: latency-svc-j2n9s [750.13768ms]
Mar  5 20:33:59.166: INFO: Created: latency-svc-kngnz
Mar  5 20:33:59.202: INFO: Got endpoints: latency-svc-7rxmx [749.501023ms]
Mar  5 20:33:59.252: INFO: Got endpoints: latency-svc-2x5xx [750.259991ms]
Mar  5 20:33:59.305: INFO: Got endpoints: latency-svc-xsnhl [751.97467ms]
Mar  5 20:33:59.353: INFO: Got endpoints: latency-svc-7r4k7 [750.701816ms]
Mar  5 20:33:59.402: INFO: Got endpoints: latency-svc-lp2qj [749.302089ms]
Mar  5 20:33:59.452: INFO: Got endpoints: latency-svc-zxpmz [749.839271ms]
Mar  5 20:33:59.502: INFO: Got endpoints: latency-svc-ckqpv [750.323688ms]
Mar  5 20:33:59.552: INFO: Got endpoints: latency-svc-7fmbq [748.285737ms]
Mar  5 20:33:59.602: INFO: Got endpoints: latency-svc-q69h4 [750.066067ms]
Mar  5 20:33:59.652: INFO: Got endpoints: latency-svc-z9prx [750.316588ms]
Mar  5 20:33:59.702: INFO: Got endpoints: latency-svc-6mlnp [747.613671ms]
Mar  5 20:33:59.752: INFO: Got endpoints: latency-svc-j5vl6 [749.91486ms]
Mar  5 20:33:59.803: INFO: Got endpoints: latency-svc-hg8d5 [750.314793ms]
Mar  5 20:33:59.852: INFO: Got endpoints: latency-svc-2k5bq [750.160313ms]
Mar  5 20:33:59.902: INFO: Got endpoints: latency-svc-kngnz [749.497087ms]
Mar  5 20:33:59.902: INFO: Latencies: [19.313135ms 30.752962ms 39.099167ms 46.835925ms 54.634368ms 62.64496ms 70.412546ms 78.909083ms 87.661096ms 97.078115ms 105.856347ms 120.124994ms 121.711078ms 122.386882ms 123.659028ms 123.674255ms 124.102002ms 124.879568ms 125.721595ms 126.68332ms 126.924419ms 132.004211ms 133.739762ms 133.745417ms 134.753449ms 135.198023ms 135.387751ms 136.024123ms 136.225339ms 136.826867ms 136.942008ms 137.279429ms 140.543749ms 142.058908ms 147.557071ms 149.62345ms 192.398785ms 231.606433ms 274.357595ms 316.470039ms 357.583612ms 399.615954ms 441.430571ms 482.368764ms 525.18031ms 567.13848ms 606.221993ms 648.97369ms 687.384672ms 732.368475ms 745.417961ms 746.712982ms 747.397651ms 747.476027ms 747.512054ms 747.54686ms 747.613671ms 748.09838ms 748.104827ms 748.171046ms 748.285737ms 748.52433ms 748.594797ms 748.706161ms 748.718652ms 748.736708ms 748.739716ms 748.741112ms 748.808095ms 748.932285ms 748.998989ms 749.02853ms 749.044011ms 749.062986ms 749.121711ms 749.130403ms 749.223347ms 749.256596ms 749.262106ms 749.293365ms 749.302089ms 749.306795ms 749.30818ms 749.318185ms 749.358491ms 749.359685ms 749.377804ms 749.378822ms 749.458635ms 749.478005ms 749.497087ms 749.497287ms 749.501023ms 749.52211ms 749.555235ms 749.564172ms 749.571647ms 749.583141ms 749.584945ms 749.58496ms 749.605109ms 749.619631ms 749.647166ms 749.664494ms 749.69724ms 749.715157ms 749.738909ms 749.748734ms 749.74963ms 749.761876ms 749.770064ms 749.793672ms 749.803564ms 749.820862ms 749.839271ms 749.851592ms 749.85902ms 749.868716ms 749.904339ms 749.91486ms 749.919292ms 749.951367ms 749.9584ms 749.967678ms 749.968011ms 749.998608ms 750.006587ms 750.010878ms 750.023948ms 750.036297ms 750.042419ms 750.049694ms 750.052794ms 750.066067ms 750.072968ms 750.073021ms 750.075683ms 750.080006ms 750.083417ms 750.089249ms 750.100978ms 750.107168ms 750.109274ms 750.109341ms 750.11173ms 750.136889ms 750.13768ms 750.139941ms 750.144046ms 750.160313ms 750.1725ms 750.17255ms 750.175071ms 750.217855ms 750.220888ms 750.236939ms 750.242549ms 750.259903ms 750.259991ms 750.302325ms 750.308605ms 750.314793ms 750.315687ms 750.316588ms 750.323688ms 750.330028ms 750.334935ms 750.375937ms 750.377641ms 750.380988ms 750.394285ms 750.395345ms 750.408218ms 750.422495ms 750.430522ms 750.446282ms 750.474924ms 750.553127ms 750.559852ms 750.584025ms 750.591174ms 750.628788ms 750.699512ms 750.701816ms 750.712916ms 750.776038ms 750.893312ms 751.000421ms 751.047303ms 751.052085ms 751.348719ms 751.434368ms 751.482417ms 751.647795ms 751.97467ms 752.063469ms 752.295563ms 752.72271ms 752.737733ms 754.47771ms]
Mar  5 20:33:59.903: INFO: 50 %ile: 749.605109ms
Mar  5 20:33:59.903: INFO: 90 %ile: 750.591174ms
Mar  5 20:33:59.903: INFO: 99 %ile: 752.737733ms
Mar  5 20:33:59.903: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:33:59.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1887" for this suite.
Mar  5 20:34:11.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:34:12.086: INFO: namespace svc-latency-1887 deletion completed in 12.175183674s

• [SLOW TEST:23.057 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:34:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  5 20:34:12.250: INFO: (0) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 10.084885ms)
Mar  5 20:34:12.258: INFO: (1) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.43625ms)
Mar  5 20:34:12.266: INFO: (2) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 8.297226ms)
Mar  5 20:34:12.276: INFO: (3) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 9.365235ms)
Mar  5 20:34:12.283: INFO: (4) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.313999ms)
Mar  5 20:34:12.290: INFO: (5) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.296828ms)
Mar  5 20:34:12.298: INFO: (6) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.223412ms)
Mar  5 20:34:12.305: INFO: (7) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.40403ms)
Mar  5 20:34:12.312: INFO: (8) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.379339ms)
Mar  5 20:34:12.320: INFO: (9) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.480019ms)
Mar  5 20:34:12.329: INFO: (10) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 8.771748ms)
Mar  5 20:34:12.336: INFO: (11) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.51017ms)
Mar  5 20:34:12.344: INFO: (12) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.466024ms)
Mar  5 20:34:12.351: INFO: (13) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.582097ms)
Mar  5 20:34:12.359: INFO: (14) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.454397ms)
Mar  5 20:34:12.367: INFO: (15) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.710042ms)
Mar  5 20:34:12.374: INFO: (16) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.507454ms)
Mar  5 20:34:12.382: INFO: (17) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.456724ms)
Mar  5 20:34:12.389: INFO: (18) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.518041ms)
Mar  5 20:34:12.396: INFO: (19) /api/v1/nodes/ip-192-168-106-110.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="boot.log... (200; 7.37298ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:34:12.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9930" for this suite.
Mar  5 20:34:18.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:34:18.583: INFO: namespace proxy-9930 deletion completed in 6.179371071s

• [SLOW TEST:6.497 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:34:18.584: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:34:18.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8699" for this suite.
Mar  5 20:34:24.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:34:24.948: INFO: namespace kubelet-test-8699 deletion completed in 6.174310298s

• [SLOW TEST:6.364 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:34:24.948: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-15897b82-c32b-497f-ab78-a57aed63ced8
STEP: Creating a pod to test consume secrets
Mar  5 20:34:25.113: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7" in namespace "projected-4229" to be "success or failure"
Mar  5 20:34:25.118: INFO: Pod "pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.956781ms
Mar  5 20:34:27.124: INFO: Pod "pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010585376s
STEP: Saw pod success
Mar  5 20:34:27.124: INFO: Pod "pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7" satisfied condition "success or failure"
Mar  5 20:34:27.129: INFO: Trying to get logs from node ip-192-168-112-247.ec2.internal pod pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 20:34:27.159: INFO: Waiting for pod pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7 to disappear
Mar  5 20:34:27.164: INFO: Pod pod-projected-secrets-8e77c5fc-7b10-4cf6-b405-255537d3ebe7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:34:27.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4229" for this suite.
Mar  5 20:34:33.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:34:33.350: INFO: namespace projected-4229 deletion completed in 6.178997369s

• [SLOW TEST:8.402 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:34:33.350: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  5 20:34:33.517: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9247,SelfLink:/api/v1/namespaces/watch-9247/configmaps/e2e-watch-test-watch-closed,UID:a0be3939-b91c-4006-a2c0-9c3e63be3720,ResourceVersion:665863,Generation:0,CreationTimestamp:2020-03-05 20:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 20:34:33.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9247,SelfLink:/api/v1/namespaces/watch-9247/configmaps/e2e-watch-test-watch-closed,UID:a0be3939-b91c-4006-a2c0-9c3e63be3720,ResourceVersion:665864,Generation:0,CreationTimestamp:2020-03-05 20:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  5 20:34:33.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9247,SelfLink:/api/v1/namespaces/watch-9247/configmaps/e2e-watch-test-watch-closed,UID:a0be3939-b91c-4006-a2c0-9c3e63be3720,ResourceVersion:665865,Generation:0,CreationTimestamp:2020-03-05 20:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 20:34:33.541: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9247,SelfLink:/api/v1/namespaces/watch-9247/configmaps/e2e-watch-test-watch-closed,UID:a0be3939-b91c-4006-a2c0-9c3e63be3720,ResourceVersion:665866,Generation:0,CreationTimestamp:2020-03-05 20:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:34:33.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9247" for this suite.
Mar  5 20:34:39.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:34:39.722: INFO: namespace watch-9247 deletion completed in 6.173669996s

• [SLOW TEST:6.372 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  5 20:34:39.722: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1153
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 20:34:39.870: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 20:35:06.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.67.107&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.066: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.189: INFO: Waiting for endpoints: map[]
Mar  5 20:35:06.195: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.115.0&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.195: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.327: INFO: Waiting for endpoints: map[]
Mar  5 20:35:06.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.204.112&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.333: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.451: INFO: Waiting for endpoints: map[]
Mar  5 20:35:06.456: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.160.64&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.456: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.565: INFO: Waiting for endpoints: map[]
Mar  5 20:35:06.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.157.13&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.571: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.701: INFO: Waiting for endpoints: map[]
Mar  5 20:35:06.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.109:8080/dial?request=hostName&protocol=http&host=192.168.231.186&port=8080&tries=1'] Namespace:pod-network-test-1153 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 20:35:06.706: INFO: >>> kubeConfig: /tmp/kubeconfig-370450052
Mar  5 20:35:06.825: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  5 20:35:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1153" for this suite.
Mar  5 20:35:28.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 20:35:29.015: INFO: namespace pod-network-test-1153 deletion completed in 22.176607864s

• [SLOW TEST:49.292 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
Mar  5 20:35:29.015: INFO: Running AfterSuite actions on all nodes
Mar  5 20:35:29.015: INFO: Running AfterSuite actions on node 1
Mar  5 20:35:29.015: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5613.415 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h33m35.887435252s
Test Suite Passed
